{"tab": "twitter", "completed": "1", "twitter": {"1111952335603974144": {"author": "@SirFizX_ELHS", "followers": "164", "datetime": "2019-03-30 11:24:30", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111136367755579392": {"author": "@arxivml", "followers": "788", "datetime": "2019-03-28 05:22:08", "content_summary": "\"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\", Subutai Ahmad, Luiz Scheinkman https://t.co/vDSoqJhTPX"}, "1111363427610607616": {"author": "@swarmingdeep", "followers": "447", "datetime": "2019-03-28 20:24:23", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111095303803817984": {"author": "@iandanforth", "followers": "1,951", "datetime": "2019-03-28 02:38:58", "content_summary": "RT @BrundageBot: How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. Subutai Ahmad and Luiz Scheinkman https://t.c\u2026"}, "1263189002406039553": {"author": "@AhmadAyazAmin1", "followers": "0", "datetime": "2020-05-20 19:25:01", "content_summary": "My science fair project extends https://t.co/SbQfnvUir2 by @Numenta on adversarial attacks and occlusion reasoning. Sparse networks are very promising, don't you think? Link to project: https://t.co/REbGmGHJY5 #MachineLearning #DeepLearning"}, "1192738559532453888": {"author": "@incyd__", "followers": "47", "datetime": "2019-11-08 09:40:07", "content_summary": "Why #DL is so dense? @Numenta became a starting point of my #AGI journey. I was quite disappointed to find out that they turned their focus to the mainstream DL. Yet, they proved me wrong as changes they suggest in this paper are fundamental https://t.co/"}, "1192984874434478080": {"author": "@rhyolight", "followers": "2,824", "datetime": "2019-11-09 01:58:53", "content_summary": "RT @incyd__: Why #DL is so dense? @Numenta became a starting point of my #AGI journey. I was quite disappointed to find out that they turne\u2026"}, "1111369012951187456": {"author": "@rhyolight", "followers": "2,824", "datetime": "2019-03-28 20:46:35", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111380735967952896": {"author": "@omnidelic", "followers": "171", "datetime": "2019-03-28 21:33:10", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111994995068813312": {"author": "@nicolarohrseitz", "followers": "546", "datetime": "2019-03-30 14:14:01", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111893742175309824": {"author": "@westis96", "followers": "307", "datetime": "2019-03-30 07:31:40", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1147447676100915200": {"author": "@a_biersack", "followers": "213", "datetime": "2019-07-06 10:10:19", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111670648005451776": {"author": "@Vratislav_", "followers": "207", "datetime": "2019-03-29 16:45:10", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111282507541110784": {"author": "@bgoncalves", "followers": "4,096", "datetime": "2019-03-28 15:02:51", "content_summary": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.11257v1 [cs.LG]) https://t.co/ylJLOLzfxx"}, "1201627945032392705": {"author": "@l_zvi", "followers": "53", "datetime": "2019-12-02 22:23:21", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111837859416137728": {"author": "@cghosh_", "followers": "281", "datetime": "2019-03-30 03:49:37", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1192745848544940034": {"author": "@EldarSilver", "followers": "1,940", "datetime": "2019-11-08 10:09:05", "content_summary": "RT @incyd__: Why #DL is so dense? @Numenta became a starting point of my #AGI journey. I was quite disappointed to find out that they turne\u2026"}, "1177768173594140674": {"author": "@max_pechyonkin", "followers": "668", "datetime": "2019-09-28 02:13:09", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111700140203671552": {"author": "@agvaughan", "followers": "285", "datetime": "2019-03-29 18:42:22", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1111100064678768640": {"author": "@arxiv_cs_LG", "followers": "322", "datetime": "2019-03-28 02:57:53", "content_summary": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. Subutai Ahmad and Luiz Scheinkman https://t.co/U4leUF39Yj"}, "1111760678417686528": {"author": "@frankolken", "followers": "2,193", "datetime": "2019-03-29 22:42:55", "content_summary": "RT @bgoncalves: How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.11257v1 [cs.LG]) https://t.co/ylJL\u2026"}, "1193087530490585088": {"author": "@heghbalz", "followers": "1,314", "datetime": "2019-11-09 08:46:48", "content_summary": "RT @incyd__: Why #DL is so dense? @Numenta became a starting point of my #AGI journey. I was quite disappointed to find out that they turne\u2026"}, "1189959087703744512": {"author": "@timcash", "followers": "96", "datetime": "2019-10-31 17:35:29", "content_summary": "Great work from Numenta https://t.co/OsZdyULhcs"}, "1193312840641372161": {"author": "@NeuroSyntheSys", "followers": "546", "datetime": "2019-11-09 23:42:06", "content_summary": "RT @SubutaiAhmad: Nice review of our recent paper, \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\". Thanks!\u2026"}, "1202520349105147904": {"author": "@l_zvi", "followers": "53", "datetime": "2019-12-05 09:29:27", "content_summary": "@Numenta @fastdotai Based on paper https://t.co/YhHKMxGWrU"}, "1111367879386189825": {"author": "@SpaceTime_A", "followers": "86", "datetime": "2019-03-28 20:42:05", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111907863037968384": {"author": "@v_lomonaco", "followers": "4,728", "datetime": "2019-03-30 08:27:47", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1111363226082729984": {"author": "@IntelligenceEx1", "followers": "1,559", "datetime": "2019-03-28 20:23:35", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111348619284791296": {"author": "@udmrzn", "followers": "1,347", "datetime": "2019-03-28 19:25:33", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.112\u2026"}, "1192996066544644096": {"author": "@rhyolight", "followers": "2,824", "datetime": "2019-11-09 02:43:21", "content_summary": "RT @SubutaiAhmad: Nice review of our recent paper, \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\". Thanks!\u2026"}, "1111349830352277505": {"author": "@Thei_Geurts", "followers": "82", "datetime": "2019-03-28 19:30:22", "content_summary": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. https://t.co/ogwIwfjYrP"}, "1111319732869828609": {"author": "@mlmemoirs", "followers": "1,287", "datetime": "2019-03-28 17:30:46", "content_summary": "#arXiv #machinelearning [cs.LG] How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.11257v1 [cs.LG]) https://t.co/grhL7LAdHv Most artificial networks today rely on dense representations, whereas biological networks rel"}, "1111320890002808839": {"author": "@arXiv__ml", "followers": "1,804", "datetime": "2019-03-28 17:35:22", "content_summary": "#arXiv #machinelearning [cs.LG] How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.11257v1 [cs.LG]) https://t.co/012OYojKot Most artificial networks today rely on dense representations, whereas biological networks rel"}, "1111698726014386177": {"author": "@AdamMarblestone", "followers": "2,732", "datetime": "2019-03-29 18:36:45", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1111776219685515265": {"author": "@poolio", "followers": "7,640", "datetime": "2019-03-29 23:44:41", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111132216766033920": {"author": "@StatsPapers", "followers": "5,454", "datetime": "2019-03-28 05:05:38", "content_summary": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. https://t.co/KoHwZSRLRT"}, "1111431188504178688": {"author": "@osamu_uk", "followers": "763", "datetime": "2019-03-29 00:53:39", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1193029701629087745": {"author": "@Astro_Erik", "followers": "742", "datetime": "2019-11-09 04:57:00", "content_summary": "RT @SubutaiAhmad: Nice review of our recent paper, \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\". Thanks!\u2026"}, "1111356954163593216": {"author": "@Numenta", "followers": "8,210", "datetime": "2019-03-28 19:58:40", "content_summary": "Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @SubutaiAhmad and Luiz Scheinkman walk through our implementation of brain-like SDRs in practical systems as a proof of concept. http"}, "1113060070911164416": {"author": "@cto_movidius", "followers": "4,812", "datetime": "2019-04-02 12:46:15", "content_summary": "RT @ElectronNest: \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\" https://t.co/YJbLD2BEBo"}, "1147429786693832704": {"author": "@bendiken", "followers": "1,986", "datetime": "2019-07-06 08:59:14", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1263189024199438336": {"author": "@clairebotai", "followers": "5,233", "datetime": "2020-05-20 19:25:06", "content_summary": "RT @AhmadAyazAmin1: My science fair project extends https://t.co/SbQfnvUir2 by @Numenta on adversarial attacks and occlusion reasoning. Sp\u2026"}, "1111099220990279680": {"author": "@DanielAdamec5", "followers": "131", "datetime": "2019-03-28 02:54:32", "content_summary": "RT @BrundageBot: How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. Subutai Ahmad and Luiz Scheinkman https://t.c\u2026"}, "1111507955667603456": {"author": "@dann_benjamin", "followers": "462", "datetime": "2019-03-29 05:58:42", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111701902327193600": {"author": "@NeuroSyntheSys", "followers": "546", "datetime": "2019-03-29 18:49:22", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1111618763168862210": {"author": "@ArjanRegeer", "followers": "216", "datetime": "2019-03-29 13:19:00", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111395056924143617": {"author": "@mthiboust", "followers": "346", "datetime": "2019-03-28 22:30:04", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1111453551899766784": {"author": "@Astro_Erik", "followers": "742", "datetime": "2019-03-29 02:22:31", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1114692129912025088": {"author": "@jsdelfino", "followers": "54", "datetime": "2019-04-07 00:51:28", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1113241563973410816": {"author": "@arxiv_pop", "followers": "714", "datetime": "2019-04-03 00:47:26", "content_summary": "2019/03/27 \u6295\u7a3f 5\u4f4d LG(Machine Learning) How Can We Be So Dense? The Benefits of Using Highly Sparse Representations https://t.co/kDLI81WScQ 12 Tweets 34 Retweets 117 Favorites"}, "1229834237366267904": {"author": "@SubutaiAhmad", "followers": "2,022", "datetime": "2020-02-18 18:25:06", "content_summary": "@blake_camp_1 @iandanforth @SaraASolla @neuro_data @tyrell_turing @JonAMichaels @Numenta Please see this paper for how we scale to real valued sparse representations and larger networks. Currently training fine on Imagenet (though that is not yet mentioned"}, "1192994269662830592": {"author": "@javierluraschi", "followers": "1,355", "datetime": "2019-11-09 02:36:13", "content_summary": "RT @SubutaiAhmad: Nice review of our recent paper, \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\". Thanks!\u2026"}, "1111446567976747009": {"author": "@ProjectAGI", "followers": "576", "datetime": "2019-03-29 01:54:46", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111692502686289921": {"author": "@Numenta", "followers": "8,210", "datetime": "2019-03-29 18:12:01", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1111432072113999873": {"author": "@fxpi", "followers": "80", "datetime": "2019-03-29 00:57:10", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111357359098597380": {"author": "@LionelBisschoff", "followers": "497", "datetime": "2019-03-28 20:00:17", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1216373415692492800": {"author": "@rhyolight", "followers": "2,824", "datetime": "2020-01-12 14:56:36", "content_summary": "@connectedregio1 @ylecun @smjain @yudapearl Nope! We don\u2019t care about that at the moment. The only thing (aside from neuroscience research) we are doing now is applying what we have learned to DL in some interesting ways. https://t.co/eH27uvQ1Q5"}, "1111240195330973698": {"author": "@arxiv_in_review", "followers": "1,345", "datetime": "2019-03-28 12:14:43", "content_summary": "#ICML2019 How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.11257v1 [cs\\.LG]) https://t.co/ZTZcmlaoA2"}, "1111733135509766144": {"author": "@Hansatyam", "followers": "81", "datetime": "2019-03-29 20:53:29", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111629831135993863": {"author": "@Terry_McDonough", "followers": "698", "datetime": "2019-03-29 14:02:59", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111448118933872641": {"author": "@ballymazoo", "followers": "914", "datetime": "2019-03-29 02:00:55", "content_summary": "i often wonder that myself https://t.co/9vjFLkQGSs"}, "1113049169726328833": {"author": "@CheFavs", "followers": "17", "datetime": "2019-04-02 12:02:56", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1112466883494793216": {"author": "@akamburov", "followers": "532", "datetime": "2019-03-31 21:29:08", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111074594981593088": {"author": "@StatMLPapers", "followers": "9,764", "datetime": "2019-03-28 01:16:40", "content_summary": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. (arXiv:1903.11257v1 [cs.LG]) https://t.co/yJ6sG4FWSP"}, "1183228698608226304": {"author": "@alexyida", "followers": "40", "datetime": "2019-10-13 03:51:19", "content_summary": "https://t.co/xb34j7wvbc"}, "1111536656396550145": {"author": "@cortical_io", "followers": "1,919", "datetime": "2019-03-29 07:52:44", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111444301878169603": {"author": "@Astro_Erik", "followers": "742", "datetime": "2019-03-29 01:45:45", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1193086183087194113": {"author": "@anirbanakash", "followers": "327", "datetime": "2019-11-09 08:41:27", "content_summary": "RT @incyd__: Why #DL is so dense? @Numenta became a starting point of my #AGI journey. I was quite disappointed to find out that they turne\u2026"}, "1111388870900211712": {"author": "@SubutaiAhmad", "followers": "2,022", "datetime": "2019-03-28 22:05:30", "content_summary": "Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentiable sparse layers that can be dropped into standard deep learning systems. These networks were a lot more noise resistant on the initia"}, "1111510935699492864": {"author": "@ParallelFibers", "followers": "378", "datetime": "2019-03-29 06:10:32", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1112304092515352577": {"author": "@NeslihanCaniken", "followers": "999", "datetime": "2019-03-31 10:42:15", "content_summary": "RT @SubutaiAhmad: Our new paper shows how sparse representations can be more robust to noise and interference. We create simple differentia\u2026"}, "1193072776359370753": {"author": "@e7mul", "followers": "16", "datetime": "2019-11-09 07:48:10", "content_summary": "RT @incyd__: Why #DL is so dense? @Numenta became a starting point of my #AGI journey. I was quite disappointed to find out that they turne\u2026"}, "1111816985543933953": {"author": "@Roswitamind", "followers": "980", "datetime": "2019-03-30 02:26:40", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1111082714365612033": {"author": "@BrundageBot", "followers": "3,913", "datetime": "2019-03-28 01:48:56", "content_summary": "How Can We Be So Dense? The Benefits of Using Highly Sparse Representations. Subutai Ahmad and Luiz Scheinkman https://t.co/b6Fm0V8Zkk"}, "1112099898546143233": {"author": "@johnnyprothero", "followers": "198", "datetime": "2019-03-30 21:10:52", "content_summary": "RT @Numenta: Our new paper is up on @arXiv, titled \u201cHow Can We Be So Dense? The Benefits of Using Highly Sparse Representations,\u201d where @Su\u2026"}, "1113058330694496259": {"author": "@ElectronNest", "followers": "231", "datetime": "2019-04-02 12:39:20", "content_summary": "\"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\" https://t.co/YJbLD2BEBo"}, "1192991361366032386": {"author": "@SubutaiAhmad", "followers": "2,022", "datetime": "2019-11-09 02:24:39", "content_summary": "Nice review of our recent paper, \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\". Thanks! The paper is a simple example of injecting neuroscience principles into deep learning. This is just the start, more to come..."}}, "citation_id": "57891004", "queriedAt": "2020-06-04 01:22:10"}