{"twitter": {"1128837751074361344": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-05-16 01:41:07", "followers": "3,527", "author": "@arxiv_cscl"}, "1120359748040306690": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-22 16:12:33", "followers": "87", "author": "@geeogi"}, "1094275006320787456": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-09 16:41:06", "followers": "3,527", "author": "@arxiv_cscl"}, "1137001201847939073": {"content_summary": "I'll be presenting this next Wednesday at 3:10, room 201, and the poster is Pacific Ballroom #258!", "datetime": "2019-06-07 14:19:45", "followers": "273", "author": "@AsaCoopStick"}, "1093728974742736896": {"content_summary": "\"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning\", Asa Cooper Stickland, \u2026 https://t.co/zB7fC6IAUR", "datetime": "2019-02-08 04:31:22", "followers": "781", "author": "@arxivml"}, "1120310935204376576": {"content_summary": "Congrats Asa! @EdiDataScience", "datetime": "2019-04-22 12:58:35", "followers": "99", "author": "@tomsherborne"}, "1122984686379249665": {"content_summary": "RT @tomsherborne: Congrats Asa! @EdiDataScience https://t.co/dlDdObN34c", "datetime": "2019-04-29 22:03:07", "followers": "983", "author": "@EdiDataScience"}, "1129018908726349825": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4w1gF", "datetime": "2019-05-16 13:40:58", "followers": "3,527", "author": "@arxiv_cscl"}, "1094185584111874049": {"content_summary": "@Deep__AI Giving credit where it is due: https://t.co/sdU10JWAzu is actually the latest work by Asa Cooper Stickland, @asacoopstick (with some advice from me). Examining ways to inflate the parameters in BERT by only a small fraction when training for mult", "datetime": "2019-02-09 10:45:46", "followers": "9,243", "author": "@driainmurray"}, "1093689531172298752": {"content_summary": "\"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,\" Stickland and Murray: https://t.co/809F8W3hlF", "datetime": "2019-02-08 01:54:38", "followers": "25,873", "author": "@Miles_Brundage"}, "1094471234719543296": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-10 05:40:51", "followers": "3,527", "author": "@arxiv_cscl"}, "1093693991457964032": {"content_summary": "RT @Miles_Brundage: \"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,\" Stickland and Murray: http\u2026", "datetime": "2019-02-08 02:12:22", "followers": "2", "author": "@sivia89024"}, "1094652480980504576": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-10 17:41:03", "followers": "3,527", "author": "@arxiv_cscl"}, "1094093818431983621": {"content_summary": "RT @arxiv_cscl: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-09 04:41:08", "followers": "1,348", "author": "@udmrzn"}, "1096781060002594816": {"content_summary": "RT @arxiv_cscl: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-16 14:39:16", "followers": "16", "author": "@supercoderhawk"}, "1094585045732347904": {"content_summary": "RT @arxiv_cscl: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-10 13:13:05", "followers": "619", "author": "@KouroshMeshgi"}, "1093701094100869121": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-08 02:40:35", "followers": "3,527", "author": "@arxiv_cscl"}, "1128824919461117955": {"content_summary": "https://t.co/hkbPp9MPWi BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. (arXiv:1902.02671v2 [cs.LG] UPDATED) #NLProc", "datetime": "2019-05-16 00:50:08", "followers": "4,212", "author": "@arxiv_cs_cl"}, "1093690500400312321": {"content_summary": "RT @Miles_Brundage: \"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,\" Stickland and Murray: http\u2026", "datetime": "2019-02-08 01:58:29", "followers": "219", "author": "@AssistedEvolve"}, "1120315955958120448": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-22 13:18:33", "followers": "318", "author": "@a_d_robertson"}, "1123313195379089409": {"content_summary": "@zacharylipton @furlanel Perhaps https://t.co/TSGeEqp9Rg (or https://t.co/gPLlDSlBhu for vision)? Also, https://t.co/LjiOIZEUiO and https://t.co/4uNx1X9CCk?", "datetime": "2019-04-30 19:48:30", "followers": "1,976", "author": "@StrongDuality"}, "1096503087190163456": {"content_summary": "#ICML2019 BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. (arXiv:1902.02671v1 [cs\\.LG]) https://t.co/OrmwGdUTTs", "datetime": "2019-02-15 20:14:42", "followers": "1,313", "author": "@arxiv_in_review"}, "1094298361820311557": {"content_summary": "BERT and Pals: Examining ways to inflate the parameters in BERT by only a small fraction when training for multiple tasks. https://t.co/3I5YJckXBK", "datetime": "2019-02-09 18:13:55", "followers": "31", "author": "@mrjreid"}, "1123014759878545408": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-30 00:02:38", "followers": "4", "author": "@golifang123"}, "1094277788180201472": {"content_summary": "RT @driainmurray: @Deep__AI Giving credit where it is due: https://t.co/sdU10JWAzu is actually the latest work by Asa Cooper Stickland, @as\u2026", "datetime": "2019-02-09 16:52:10", "followers": "23,382", "author": "@Deep__AI"}, "1120323808383119362": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-22 13:49:45", "followers": "9,243", "author": "@driainmurray"}, "1129635269387476992": {"content_summary": "How to make multi-task learning on BERT with additional projection layer https://t.co/TRgA9NnCFK", "datetime": "2019-05-18 06:30:10", "followers": "4", "author": "@irapiont"}, "1129033881229889539": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-05-16 14:40:28", "followers": "3,527", "author": "@arxiv_cscl"}, "1093748895161569280": {"content_summary": "RT @arxiv_cscl: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-08 05:50:32", "followers": "619", "author": "@KouroshMeshgi"}, "1093961634765139968": {"content_summary": "RT @arxiv_cscl: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-08 19:55:53", "followers": "1,408", "author": "@RexDouglass"}, "1120549518519472130": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-23 04:46:38", "followers": "135", "author": "@deebuls"}, "1120353658825248768": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/HAdyPV8FTt", "datetime": "2019-04-22 15:48:22", "followers": "931", "author": "@AISC_TO"}, "1123490908362760195": {"content_summary": "RT @sarahwiegreffe: @zacharylipton These two do it for BERT: https://t.co/4teWsISYu3 and https://t.co/th5vuDAasv", "datetime": "2019-05-01 07:34:40", "followers": "996", "author": "@ialuronico"}, "1093897331567718400": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-08 15:40:22", "followers": "3,527", "author": "@arxiv_cscl"}, "1123334734912704512": {"content_summary": "@zacharylipton These two do it for BERT: https://t.co/4teWsISYu3 and https://t.co/th5vuDAasv", "datetime": "2019-04-30 21:14:06", "followers": "528", "author": "@sarahwiegreffe"}, "1094078695625641986": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning https://t.co/ymgZc4eqp7", "datetime": "2019-02-09 03:41:02", "followers": "3,527", "author": "@arxiv_cscl"}, "1093702895894183936": {"content_summary": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. Asa Cooper Stickland and Iain Murray https://t.co/aFuDFB20md", "datetime": "2019-02-08 02:47:45", "followers": "3,897", "author": "@BrundageBot"}, "1120314369957560321": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-22 13:12:14", "followers": "446", "author": "@benrozemberczki"}, "1093730089953579008": {"content_summary": "RT @arxiv_cs_cl: https://t.co/hkbPp9MPWi BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. (arXiv:\u2026", "datetime": "2019-02-08 04:35:48", "followers": "65", "author": "@orsonady"}, "1093783624715927552": {"content_summary": "\u5148\u306eHoulsby\u7b49 \u3068\u540c\u3058 adapter \u3068\u3044\u3046\u7528\u8a9e\u304c\u3042\u308b\u3082\u3001\" keep the BERT model fixed while training adapter modules \" \u3068\u3044\u3046\u308f\u3051\u3067\u306f\u306a\u304f\u3001vs fine-tuning \u306e\u4f4d\u7f6e\u3065\u3051\u306f\u306a\u3044\u3002 RTE: 76.6% accuracy https://t.co/afQfcfIeEC (BERT_015)", "datetime": "2019-02-08 08:08:32", "followers": "110", "author": "@su_9qu"}, "1120309909533409280": {"content_summary": "BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate BERT with few parameters when training for multiple tasks (from the GLUE benchmark). https://t.co/b9rhya0X8x", "datetime": "2019-04-22 12:54:31", "followers": "273", "author": "@AsaCoopStick"}, "1094023423691345920": {"content_summary": "'Projected attention layers' to include layers with task-specific inductive biases from a pre-trained LM (e.g. BERT) --- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning Asa Cooper Stickland, Iain Murray https://t.", "datetime": "2019-02-09 00:01:24", "followers": "536", "author": "@dtsbourg"}, "1093689228779753473": {"content_summary": "https://t.co/hkbPp9MPWi BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. (arXiv:1902.02671v1 [cs.LG]) #NLProc", "datetime": "2019-02-08 01:53:26", "followers": "4,212", "author": "@arxiv_cs_cl"}, "1120332485727068160": {"content_summary": "RT @AsaCoopStick: BERT and PALs (https://t.co/gS4M08JBVf; by me and @driainmurray) got accepted into #ICML2019! We examine how to inflate B\u2026", "datetime": "2019-04-22 14:24:14", "followers": "4,866", "author": "@EdinburghNLP"}}, "queriedAt": "2020-06-03 22:52:34", "tab": "twitter", "completed": "1", "citation_id": "55124466"}