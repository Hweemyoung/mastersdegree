{"citation_id": "53305040", "queriedAt": "2020-05-09 13:19:51", "completed": "0", "twitter": {"1258370586562252801": {"followers": "7,423", "content_summary": "@KordingLab I haven't read this yet, but is it related to this? Either way \u2014 this is a very cool paper below: https://t.co/E3jsIdJH34", "author": "@o_guest", "datetime": "2020-05-07 12:18:21"}, "1242995396605087744": {"followers": "3,446", "content_summary": "\u300aReconciling modern machine learning practice and the bias-variance trade-off\u300b by Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal https://t.co/KjVAix017p (machine learning) H/T @MikeAIrvine", "author": "@reiver", "datetime": "2020-03-26 02:02:50"}, "1257721039007764483": {"followers": "982", "content_summary": "@MaartenvSmeden If it's relating to a deep neural network, there's some evidence that they are difficult to over-parametrize: https://t.co/CTFfw27ii1", "author": "@mitkoveta", "datetime": "2020-05-05 17:17:17"}, "1258501808416317447": {"followers": "1,936", "content_summary": "ML Twitter: is the the \"double descent\" phenomenon widely confirmed when training deep neural nets? Did anyone prove that extremely large models with perfect training fit increase generalisation in practice? https://t.co/U1fgYRrndh https://t.co/D4b7nDASMo", "author": "@GiorgioPatrini", "datetime": "2020-05-07 20:59:47"}}, "tab": "twitter"}