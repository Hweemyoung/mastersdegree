{"queriedAt": "2020-06-03 14:36:19", "completed": "1", "citation_id": "54908065", "tab": "twitter", "twitter": {"1092446070783778816": {"author": "@udmrzn", "datetime": "2019-02-04 15:33:34", "content_summary": "RT @StatMLPapers: Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/8mjTkyZzHi", "followers": "1,348"}, "1242604088790941696": {"author": "@advadnoun", "datetime": "2020-03-25 00:07:55", "content_summary": "@gwern I'm especially interested in this wrt GANs - about a year ago there was a paper on using a student-teacher paradigm (https://t.co/OR8zy7YwNy) but not much on the topic has passed my radar since.", "followers": "155"}, "1092253372659519489": {"author": "@BrundageBot", "datetime": "2019-02-04 02:47:51", "content_summary": "Compressing GANs using Knowledge Distillation. Angeline Aguinaldo, Ping-Yeh Chiang, Alex Gain, Ameya Patil, Kolten Pearson, and Soheil Feizi https://t.co/dsbHGa8D4t", "followers": "3,903"}, "1092314852771725312": {"author": "@arxivml", "datetime": "2019-02-04 06:52:09", "content_summary": "\"Compressing GANs using Knowledge Distillation\", Angeline Aguinaldo, Ping-Yeh Chiang, Alex Gain, Ameya Patil, Kolte\u2026 https://t.co/UQrBbCNkLo", "followers": "783"}, "1092238616850558981": {"author": "@StatMLPapers", "datetime": "2019-02-04 01:49:13", "content_summary": "Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/8mjTkyZzHi", "followers": "9,746"}, "1092506574843772932": {"author": "@bhskrdtt", "datetime": "2019-02-04 19:33:59", "content_summary": "RT @arxiv_org: Compressing GANs using Knowledge Distillation. https://t.co/1LPAETo83X https://t.co/dx1vLsesmw", "followers": "19"}, "1092467864769126401": {"author": "@arXiv__ml", "datetime": "2019-02-04 17:00:10", "content_summary": "#arXiv #machinelearning [cs.LG] Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/pnMoGzmEOr Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super reso", "followers": "1,786"}, "1092645941406969856": {"author": "@syoyo", "datetime": "2019-02-05 04:47:47", "content_summary": "RT @arxiv_org: Compressing GANs using Knowledge Distillation. https://t.co/1LPAETo83X https://t.co/dx1vLsesmw", "followers": "3,009"}, "1092515935498526721": {"author": "@mlmemoirs", "datetime": "2019-02-04 20:11:11", "content_summary": "#arXiv #machinelearning [cs.LG] Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/mLEuBehWpu Generative Adversarial Networks (GANs) have been used in several machine learning tasks such as domain transfer, super reso", "followers": "1,281"}, "1092365095840161792": {"author": "@HengjianJia", "datetime": "2019-02-04 10:11:48", "content_summary": "RT @StatMLPapers: Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/8mjTkyZzHi", "followers": "55"}, "1092262898213314560": {"author": "@arxiv_cs_LG", "datetime": "2019-02-04 03:25:42", "content_summary": "Compressing GANs using Knowledge Distillation. Angeline Aguinaldo, Ping-Yeh Chiang, Alex Gain, Ameya Patil, Kolten Pearson, and Soheil Feizi https://t.co/uBSNezzRUs", "followers": "320"}, "1092478610793992192": {"author": "@mmmgaber", "datetime": "2019-02-04 17:42:52", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/pnM\u2026", "followers": "686"}, "1092525936694706181": {"author": "@Pardoe_AI", "datetime": "2019-02-04 20:50:56", "content_summary": "RT @mlmemoirs: #arXiv #machinelearning [cs.LG] Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs.LG]) https://t.co/mLE\u2026", "followers": "13,720"}, "1092906112968323072": {"author": "@maggie_albrecht", "datetime": "2019-02-05 22:01:37", "content_summary": "RT @arxiv_org: Compressing GANs using Knowledge Distillation. https://t.co/1LPAETo83X https://t.co/dx1vLsesmw", "followers": "1,907"}, "1093015106659733504": {"author": "@arxiv_in_review", "datetime": "2019-02-06 05:14:43", "content_summary": "#ICML2019 Compressing GANs using Knowledge Distillation. (arXiv:1902.00159v1 [cs\\.LG]) https://t.co/8OeIjFd5J8", "followers": "1,318"}, "1092501663175176192": {"author": "@arxiv_org", "datetime": "2019-02-04 19:14:28", "content_summary": "Compressing GANs using Knowledge Distillation. https://t.co/1LPAETo83X https://t.co/dx1vLsesmw", "followers": "12,792"}, "1092716416371814401": {"author": "@s_aiueo32", "datetime": "2019-02-05 09:27:50", "content_summary": "@wisteria0gps \u30d4\u30c3\u30bf\u30ea\u306a\u30bf\u30a4\u30c8\u30eb\u304c(\u8aad\u3093\u3067\u306f\u306a\u3044) https://t.co/QYCOUeQfn1", "followers": "442"}}}