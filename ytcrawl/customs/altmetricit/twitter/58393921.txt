{"citation_id": "58393921", "queriedAt": "2020-05-09 12:35:59", "completed": "0", "twitter": {"1231344177356705792": {"followers": "19", "content_summary": "@Smerity Not too far off the classical architectures, but Xie et Al https://t.co/3s8HzThGlo explores randomly wired neural layers. Bonus for me because network science was one of the classes I enjoyed most.", "author": "@d_aumiller", "datetime": "2020-02-22 22:25:03"}, "1231543604767682560": {"followers": "2", "content_summary": "RT @d_aumiller: @Smerity Not too far off the classical architectures, but Xie et Al https://t.co/3s8HzThGlo explores randomly wired neura\u2026", "author": "@CheungZikai", "datetime": "2020-02-23 11:37:31"}, "1189338575676030978": {"followers": "699", "content_summary": "https://t.co/MABTxA8a0j -- Yet again, random search comes, or even just random initialization, comes back as a competitive baseline, *when* done through a generative process, that I'm sure is inducing an implicit simplicity inductive bias.", "author": "@guillefix", "datetime": "2019-10-30 00:29:47"}, "1221510059978641409": {"followers": "625", "content_summary": "@bayesianbrain There were quite a few papers on this topic, about doing architecture search instead of learning the weights. Eg https://t.co/mvqTgJ6Nqg And neuroscience has an even longer history of exploring this idea: see the famous \"Silent synapses\" co", "author": "@ampanmdagaba", "datetime": "2020-01-26 19:07:47"}}, "tab": "twitter"}