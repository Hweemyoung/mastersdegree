{"citation_id": "42948930", "completed": "1", "queriedAt": "2020-05-14 13:49:18", "tab": "twitter", "twitter": {"1083597989212577792": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "164,097", "datetime": "2019-01-11 05:34:27", "author": "@ceobillionaire"}, "1191376686895353856": {"content_summary": "RT @sei_shinagawa: BatchNormalization\u3068\u3044\u3046\u3068\u3001\u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306f\u3042\u3093\u307e\u6027\u80fd\u306b\u95a2\u4fc2\u306a\u304f\u3066\u3001\u5b9f\u969b\u306e\u52b9\u679c\u306f\u6b63\u898f\u5316\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3068\u3053\u308d\u3060\u3063\u3066\u8a71\u304c\u3042\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u3069\u3046\u306a\u3063\u305f\u3093\u3084\u308d\uff1f\uff08\u3061\u3083\u3093\u3068\u8aad\u3093\u3067\u306a\u3044\u3057\u691c\u8a3c\u3082\u3057\u3066\u306a\u3044\u306e\u3067\u5224\u65ad\u3092\u5148\u9001\u2026", "followers": "296", "datetime": "2019-11-04 15:28:31", "author": "@rose_miura"}, "1124304529082679298": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,456", "datetime": "2019-05-03 13:27:42", "author": "@Gabriel_Oguna"}, "1015036358027509761": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "628", "datetime": "2018-07-06 00:55:00", "author": "@oshtim"}, "1003839786342277121": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry https://t.co/fUTI9XHCCT", "followers": "308", "datetime": "2018-06-05 03:23:50", "author": "@arxiv_cs_LG"}, "1124088463995932673": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1", "datetime": "2019-05-02 23:09:09", "author": "@ZhiWeiLee3"}, "1001829771569844224": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "637", "datetime": "2018-05-30 14:16:45", "author": "@allmeasures"}, "1100107689290616832": {"content_summary": "@RogerGrosse, please read the paper before making *factually* incorrect statements like this (see thread for a [partial] list). Also, tweets like this reinforce the already unwelcoming and toxic ML atmosphere, esp. for younger researchers (including those", "followers": "4,679", "datetime": "2019-02-25 18:58:06", "author": "@aleks_madry"}, "1124125303159177216": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "38", "datetime": "2019-05-03 01:35:32", "author": "@experiencor"}, "1002227176731742209": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "61", "datetime": "2018-05-31 16:35:54", "author": "@AbishekPrasann2"}, "1002314342593847297": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "14", "datetime": "2018-05-31 22:22:16", "author": "@samrussellnz"}, "1171947239729586176": {"content_summary": "RT @gupta__abhay: @dcpage3 Wondering if you have looked at this work - https://t.co/hPKWuxLHwr? It takes a very different approach to expla\u2026", "followers": "2,676", "datetime": "2019-09-12 00:42:50", "author": "@ayirpelle"}, "1170932305587769344": {"content_summary": "RT @TheGregYang: 11/ The blog post is subtitled \u201cor How I learned to stop worrying and love reduced internal covariate shift\u201d but https://t\u2026", "followers": "1,939", "datetime": "2019-09-09 05:29:51", "author": "@EldarSilver"}, "1124124340528664576": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1", "datetime": "2019-05-03 01:31:42", "author": "@twnming"}, "1003259030767165440": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "26", "datetime": "2018-06-03 12:56:07", "author": "@tokoton01"}, "1002249949252411393": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "268", "datetime": "2018-05-31 18:06:23", "author": "@corbyjerez"}, "1014834477627990016": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "615", "datetime": "2018-07-05 11:32:48", "author": "@KouroshMeshgi"}, "1171946861738917890": {"content_summary": "RT @aleks_madry: @dcpage3 Since people seem to be wondering: if you interpret ICS as \"improving optimization landscape\" then our conclusion\u2026", "followers": "2,676", "datetime": "2019-09-12 00:41:20", "author": "@ayirpelle"}, "1011150143725268992": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "4,006", "datetime": "2018-06-25 07:32:35", "author": "@ballforest"}, "1001701531987709955": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "103", "datetime": "2018-05-30 05:47:10", "author": "@jongen87"}, "1003979616317575173": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "36", "datetime": "2018-06-05 12:39:28", "author": "@AlesiaChernikov"}, "1070409931365453826": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "1,022", "datetime": "2018-12-05 20:09:49", "author": "@desertnaut"}, "1250569166030901249": {"content_summary": "@AntonySagayara6 https://t.co/VwcShElZjx", "followers": "1,396", "datetime": "2020-04-15 23:38:18", "author": "@theshawwn"}, "1001647515165450241": {"content_summary": "https://t.co/dkeJCOx2Ez S Santurkar et. al. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/uH6AP6cFx6", "followers": "193", "datetime": "2018-05-30 02:12:31", "author": "@arXiv_stat_ML"}, "1001637006550724610": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) - Shibani Santurkar https://t.co/Ldj4Q7b0Q5", "followers": "862", "datetime": "2018-05-30 01:30:46", "author": "@deep_rl"}, "1004380773783306240": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "19", "datetime": "2018-06-06 15:13:31", "author": "@ML_Coni"}, "1077624175471267840": {"content_summary": "On batch normalization... https://t.co/HtN9Dzxmou https://t.co/DUM4MtdMpq", "followers": "339", "datetime": "2018-12-25 17:56:39", "author": "@IUILab"}, "1124243304357482496": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "8", "datetime": "2019-05-03 09:24:25", "author": "@budmitr"}, "1002444144084598784": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "84", "datetime": "2018-06-01 06:58:03", "author": "@ashi__no"}, "1124313867486625792": {"content_summary": "RT @sschoenholz: Great recap of our recent batch norm work (to appear at ICLR). https://t.co/rxh1bccszk", "followers": "19", "datetime": "2019-05-03 14:04:49", "author": "@MassBassLol"}, "1001638055286116352": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "918", "datetime": "2018-05-30 01:34:56", "author": "@karthikabinav"}, "1003309499627790336": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "945", "datetime": "2018-06-03 16:16:39", "author": "@andrew_ilyas"}, "1001652178535223296": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "172", "datetime": "2018-05-30 02:31:03", "author": "@CalebRollins4"}, "1002269698199875585": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "56", "datetime": "2018-05-31 19:24:51", "author": "@Tim_Scholtes"}, "1001889225988562944": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "0", "datetime": "2018-05-30 18:13:00", "author": "@MavracicTin"}, "1014900858021138432": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "401", "datetime": "2018-07-05 15:56:35", "author": "@offbyjuan"}, "1124179048123383808": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "70", "datetime": "2019-05-03 05:09:05", "author": "@sumitsethy"}, "1003321522423939075": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "114", "datetime": "2018-06-03 17:04:26", "author": "@rajpratim"}, "1124196726443692032": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "432", "datetime": "2019-05-03 06:19:20", "author": "@bhargavbardipur"}, "1124165166151553024": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "270", "datetime": "2019-05-03 04:13:56", "author": "@kotti_sasikanth"}, "1014834281523261442": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "26", "datetime": "2018-07-05 11:32:02", "author": "@iampratikbhatia"}, "1001699193533198336": {"content_summary": "RT @bremen79: I really hope nobody seriously thought BatchNorm reduces the \"covariate shift\"... https://t.co/NtfC90OFCC", "followers": "418", "datetime": "2018-05-30 05:37:53", "author": "@SileyeBaba"}, "1001762015143251968": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "23", "datetime": "2018-05-30 09:47:30", "author": "@fayzur20"}, "1003248356041388032": {"content_summary": "Another (important) step forward to understand how DNN are trained https://t.co/GRFTSmpNxP", "followers": "306", "datetime": "2018-06-03 12:13:42", "author": "@juarelerrr"}, "1083599948552245249": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "252", "datetime": "2019-01-11 05:42:14", "author": "@_DLPBGJ80C04Z_"}, "1001808236813701121": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "160,804", "datetime": "2018-05-30 12:51:11", "author": "@Quebec_AI"}, "1002541494425698304": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "822", "datetime": "2018-06-01 13:24:53", "author": "@morioka"}, "1002218881790959617": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "11", "datetime": "2018-05-31 16:02:56", "author": "@JawadLakis"}, "1171104604794294272": {"content_summary": "RT @TheGregYang: 11/ The blog post is subtitled \u201cor How I learned to stop worrying and love reduced internal covariate shift\u201d but https://t\u2026", "followers": "44", "datetime": "2019-09-09 16:54:30", "author": "@jeandut14000"}, "1059283372818612224": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "303", "datetime": "2018-11-05 03:16:51", "author": "@utsushiiro"}, "1014822622624927744": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effectiveness of batch norm comes from smoother loss landscape. https://t.co/rAWOvciTwt https://t.co/7MJ2IVX0PW", "followers": "2,385", "datetime": "2018-07-05 10:45:42", "author": "@trustswz"}, "1126005051196157954": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "16", "datetime": "2019-05-08 06:04:59", "author": "@Tzeny25"}, "1003870077123547136": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "58", "datetime": "2018-06-05 05:24:12", "author": "@_WizDom13_"}, "1070286747504918528": {"content_summary": "RT @68kirk: @ErmiaBivatan @aerinykim https://t.co/1Nml5qPT0B", "followers": "822", "datetime": "2018-12-05 12:00:20", "author": "@ErmiaBivatan"}, "1070325686450208768": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "4,578", "datetime": "2018-12-05 14:35:03", "author": "@ACuriousMindset"}, "1171946810597761024": {"content_summary": "RT @Towards_Entropy: @dcpage3 What do you think of the results of https://t.co/H44pNMsfFK The authors inject random noise after batchnorm\u2026", "followers": "2,676", "datetime": "2019-09-12 00:41:08", "author": "@ayirpelle"}, "1124017163210231809": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "111", "datetime": "2019-05-02 18:25:49", "author": "@alsombra7"}, "1059089579876134912": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "595", "datetime": "2018-11-04 14:26:47", "author": "@cddadr"}, "1015341820728012800": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "34", "datetime": "2018-07-06 21:08:48", "author": "@dnl0x00"}, "1014886853416124417": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "2,676", "datetime": "2018-07-05 15:00:56", "author": "@ayirpelle"}, "1124931707986169856": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,020", "datetime": "2019-05-05 06:59:54", "author": "@serrjoa"}, "1001622767681220608": {"content_summary": "Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do though, both empirically and in theory, is to smoothen out the optimization landscape.) (with @ShibaniSan @tsiprasd @andrew_ilyas) https", "followers": "4,679", "datetime": "2018-05-30 00:34:11", "author": "@aleks_madry"}, "1003273172425347072": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "458", "datetime": "2018-06-03 13:52:18", "author": "@sajidshahbs"}, "1001895304554340353": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "328", "datetime": "2018-05-30 18:37:09", "author": "@martisamuser"}, "1235601076608368640": {"content_summary": "https://t.co/4v1PLkg2FE internal covariate shift is not solved at all by BN, instead it can also be worsened. On the other side, BN improves the loss smoothness dramatically. It's a 2-years old result but the paper deserves to be read by the broadest possi", "followers": "367", "datetime": "2020-03-05 16:20:27", "author": "@mdigangiPA"}, "1015451782724513792": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "94", "datetime": "2018-07-07 04:25:45", "author": "@ozlhubby"}, "1015065484553805824": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "822", "datetime": "2018-07-06 02:50:45", "author": "@ErmiaBivatan"}, "1015044466321903617": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "148", "datetime": "2018-07-06 01:27:14", "author": "@_erincon"}, "1059631603545063424": {"content_summary": "\u89e3\u6790\u7684\u3059\u304e\u3066\u50d5\u306b\u306f\u307e\u3060\u8aad\u3081\u306a\u3044 https://t.co/s1DwhbaexJ", "followers": "595", "datetime": "2018-11-06 02:20:35", "author": "@cddadr"}, "1058711813930409984": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "353", "datetime": "2018-11-03 13:25:41", "author": "@hrnbskgc"}, "1100435919818100746": {"content_summary": "Mea culpa: the differences in definition I referred to here are inessential to the paper's argument. And it was sloppy of me to reference this paper as part of a longer thread without making clear who was saying what. Consider this tweet retracted.", "followers": "5,469", "datetime": "2019-02-26 16:42:23", "author": "@RogerGrosse"}, "1001804345392926725": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "3,965", "datetime": "2018-05-30 12:35:43", "author": "@tangming2005"}, "1002253021504557057": {"content_summary": "#DeepLearning #AI | How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/CBWCgcWCi0", "followers": "3,640", "datetime": "2018-05-31 18:18:35", "author": "@hasdid"}, "1099762907695497217": {"content_summary": "RT @RogerGrosse: Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between dif\u2026", "followers": "48", "datetime": "2019-02-24 20:08:04", "author": "@mzadrogaPL"}, "1001785881135632384": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,083", "datetime": "2018-05-30 11:22:21", "author": "@rosstaylor90"}, "1136180921227333632": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is N https://t.co/oir98xna1z (Popularity:16.4) #Machine_Learning #Neural_and_Evolutionary_Computing", "followers": "93", "datetime": "2019-06-05 08:00:15", "author": "@poqaa_ai"}, "1001734692914122752": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "371", "datetime": "2018-05-30 07:58:56", "author": "@hrmoaddeli"}, "1002129843796959234": {"content_summary": "BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081adaptive step\u306a\u6700\u9069\u5316\u624b\u6cd5\u3067\u7dda\u578b\u53ce\u675f\u3059\u308b\u3053\u3068\u3092\u8a3c\u660e(https://t.co/EFEy88RgCr )", "followers": "2,653", "datetime": "2018-05-31 10:09:08", "author": "@mosko_mule"}, "1002887389977694209": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "31", "datetime": "2018-06-02 12:19:21", "author": "@CherguiSafouane"}, "1015193783837831168": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "159,742", "datetime": "2018-07-06 11:20:34", "author": "@Montreal_IA"}, "1124073144296321026": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "735", "datetime": "2019-05-02 22:08:16", "author": "@unsorsodicorda"}, "1124791597969752064": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2,653", "datetime": "2019-05-04 21:43:09", "author": "@mosko_mule"}, "1124305524332175360": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "21", "datetime": "2019-05-03 13:31:40", "author": "@Sunghyo_Chung"}, "1136141329212166144": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "41", "datetime": "2019-06-05 05:22:56", "author": "@aiton5"}, "1003615624348819456": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "9", "datetime": "2018-06-04 12:33:05", "author": "@lostujjwal"}, "1001683032905043969": {"content_summary": "RT @_brohrer_: Conclusion: Batch Normalization makes deep neural networks easier for gradient descent to navigate. (It smooths the paramete\u2026", "followers": "270", "datetime": "2018-05-30 04:33:40", "author": "@kotti_sasikanth"}, "1123995775422402561": {"content_summary": "1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper https://t.co/FM7jrTMWU2 shows BN causes grad explosion in randomly initialized deep BN net. Contradiction? We clarify below https://t.c", "followers": "3,547", "datetime": "2019-05-02 17:00:50", "author": "@TheGregYang"}, "1070674509684989952": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "126", "datetime": "2018-12-06 13:41:09", "author": "@alexginsca"}, "1002764623978823680": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "458", "datetime": "2018-06-02 04:11:31", "author": "@sajidshahbs"}, "1003298775446650880": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "11,920", "datetime": "2018-06-03 15:34:03", "author": "@Rosenchild"}, "1001680938521317376": {"content_summary": "RT @crcrpar: https://t.co/I9RLMXhmOF", "followers": "4,006", "datetime": "2018-05-30 04:25:20", "author": "@ballforest"}, "1001655353304670208": {"content_summary": "Batch norm is the best example of the empiricism of DL. The fact that it is universally adopted, while we are still figuring out just what it does exactly (other than getting us better models), is telling about the state of the field. https://t.co/ExTlazH1", "followers": "3,729", "datetime": "2018-05-30 02:43:40", "author": "@EmmanuelAmeisen"}, "1015044125056380928": {"content_summary": "Very interesting work Wenzhe :) https://t.co/PD12ZOK9yQ", "followers": "96", "datetime": "2018-07-06 01:25:52", "author": "@uku_peter"}, "1001966640530157568": {"content_summary": "BN\u306e\u7406\u8ad6\u306b\u8e0f\u307f\u8fbc\u3093\u3060\u3058\u3083\u3093 How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/Pxob2uV9zI", "followers": "653", "datetime": "2018-05-30 23:20:37", "author": "@Johnny9904"}, "1002393971400704000": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "380", "datetime": "2018-06-01 03:38:41", "author": "@harujoh"}, "1124368431468486656": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "45", "datetime": "2019-05-03 17:41:38", "author": "@artuskg"}, "1058707569634553856": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "1,188", "datetime": "2018-11-03 13:08:49", "author": "@hrs1985"}, "1015596999012438017": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "180", "datetime": "2018-07-07 14:02:48", "author": "@johnnyprothero"}, "1124258248972914688": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "615", "datetime": "2019-05-03 10:23:48", "author": "@KouroshMeshgi"}, "1001860685754765313": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "303", "datetime": "2018-05-30 16:19:35", "author": "@Qihong_Lu"}, "1124272250557829121": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "327", "datetime": "2019-05-03 11:19:27", "author": "@aditya_soni2k17"}, "1191117394770456576": {"content_summary": "RT @sei_shinagawa: BatchNormalization\u3068\u3044\u3046\u3068\u3001\u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306f\u3042\u3093\u307e\u6027\u80fd\u306b\u95a2\u4fc2\u306a\u304f\u3066\u3001\u5b9f\u969b\u306e\u52b9\u679c\u306f\u6b63\u898f\u5316\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3068\u3053\u308d\u3060\u3063\u3066\u8a71\u304c\u3042\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u3069\u3046\u306a\u3063\u305f\u3093\u3084\u308d\uff1f\uff08\u3061\u3083\u3093\u3068\u8aad\u3093\u3067\u306a\u3044\u3057\u691c\u8a3c\u3082\u3057\u3066\u306a\u3044\u306e\u3067\u5224\u65ad\u3092\u5148\u9001\u2026", "followers": "1,379", "datetime": "2019-11-03 22:18:11", "author": "@momiji_fullmoon"}, "1003941315288055808": {"content_summary": "https://t.co/mzmrJJAoBU BN\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u4ee5\u5916\u306e\u7406\u7531\u3067\u3001\u6709\u52b9\u3067\u3042\u308b\u3068\u3044\u3046\u8a71(\u9a5a\u304d)", "followers": "592", "datetime": "2018-06-05 10:07:16", "author": "@shohomiura"}, "1124391139400806400": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "99", "datetime": "2019-05-03 19:11:52", "author": "@DSaience"}, "1100098325603409920": {"content_summary": "Batch normalization doesn't reduce covariate shift at all. And even if it did, that has nothing to do with why it works. https://t.co/mooo4uerAZ", "followers": "320", "datetime": "2019-02-25 18:20:54", "author": "@mohammad_d1993"}, "1039471487952941056": {"content_summary": "RT @ranoiaru: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/kGbxOZoQdZ \u3068\u3044\u3046\u306a\u304b\u2026", "followers": "1,188", "datetime": "2018-09-11 11:11:29", "author": "@hrs1985"}, "1001679417092001802": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,286", "datetime": "2018-05-30 04:19:17", "author": "@heghbalz"}, "1125238758478110720": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "12,547", "datetime": "2019-05-06 03:20:00", "author": "@ml_review"}, "1003235078888148993": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "78,987", "datetime": "2018-06-03 11:20:56", "author": "@machinelearnbot"}, "1070348182293540864": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "179,069", "datetime": "2018-12-05 16:04:27", "author": "@Montreal_AI"}, "1002198081041981440": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "65", "datetime": "2018-05-31 14:40:17", "author": "@ginnaNN01"}, "1124184601478479872": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "43", "datetime": "2019-05-03 05:31:09", "author": "@frabarbuto"}, "1002137148252872704": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "2,432", "datetime": "2018-05-31 10:38:09", "author": "@jinbeizame007"}, "1125082658776555520": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2,141", "datetime": "2019-05-05 16:59:43", "author": "@jaschasd"}, "1002666191498776576": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "223", "datetime": "2018-06-01 21:40:23", "author": "@haizad_94"}, "1001688512964460544": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,052", "datetime": "2018-05-30 04:55:26", "author": "@jmschreiber91"}, "1124485212916740097": {"content_summary": "RT @hackerfriendly: TIL that some ML researchers had an idea (batch normalization) that worked, then tacked on a post-hoc mathematical anal\u2026", "followers": "6,193", "datetime": "2019-05-04 01:25:41", "author": "@kscottz"}, "1124003996379820035": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,286", "datetime": "2019-05-02 17:33:30", "author": "@heghbalz"}, "1001641025557811200": {"content_summary": "RT @crcrpar: https://t.co/I9RLMXhmOF", "followers": "2,050", "datetime": "2018-05-30 01:46:44", "author": "@shunk031"}, "1110843768490455042": {"content_summary": "\"How Does Batch Normalization Help Optimization?\" https://t.co/UFMR1lZF3h", "followers": "226", "datetime": "2019-03-27 09:59:27", "author": "@ElectronNest"}, "1015169964054106112": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "1,103", "datetime": "2018-07-06 09:45:55", "author": "@permutans"}, "1124198887189704704": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2", "datetime": "2019-05-03 06:27:55", "author": "@WenzhongW"}, "1002421042005118976": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1,286", "datetime": "2018-06-01 05:26:15", "author": "@heghbalz"}, "1002091686439407616": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "106", "datetime": "2018-05-31 07:37:30", "author": "@FerreiraFabioDE"}, "1002347397467418625": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "730", "datetime": "2018-06-01 00:33:36", "author": "@activecontour"}, "1124455499695452161": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "98", "datetime": "2019-05-03 23:27:37", "author": "@andrewliao11"}, "1002155331072311296": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "689", "datetime": "2018-05-31 11:50:24", "author": "@pasoconhoshii"}, "1015028038176792576": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "30", "datetime": "2018-07-06 00:21:57", "author": "@koenboeckx"}, "1010090428711034880": {"content_summary": "Batchnorm myth, debunked! I really enjoyed reading it, very well written! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/wchtpovj52", "followers": "1,286", "datetime": "2018-06-22 09:21:39", "author": "@heghbalz"}, "1001651485464150021": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "601", "datetime": "2018-05-30 02:28:18", "author": "@ShibaniSan"}, "1001926112665063425": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,061", "datetime": "2018-05-30 20:39:34", "author": "@mvaldenegro"}, "1058718906414653445": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "254", "datetime": "2018-11-03 13:53:51", "author": "@mshero_y"}, "1003230573098397696": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @andrew_ilyas @aleks_madry \"it makes the landscape of optimization problem be significantly more smooth\" https://t.co/vHCCddkNjT https", "followers": "12,547", "datetime": "2018-06-03 11:03:02", "author": "@ml_review"}, "1015036198165700608": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "13", "datetime": "2018-07-06 00:54:22", "author": "@poasc_f"}, "1125788595028152320": {"content_summary": "RT @TheGregYang: I'll be explaining how batchnorm causes gradient explosion 4:30-6:30pm tomorrow (Wednesday 05/08) at Great Hall BC #12. Co\u2026", "followers": "1,939", "datetime": "2019-05-07 15:44:51", "author": "@EldarSilver"}, "1124149974193856512": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "455", "datetime": "2019-05-03 03:13:34", "author": "@carranzadanielh"}, "1001637672094494722": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "114", "datetime": "2018-05-30 01:33:25", "author": "@zizzivon"}, "1002144930049835009": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "716", "datetime": "2018-05-31 11:09:04", "author": "@minux302"}, "1003444813436366853": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "43", "datetime": "2018-06-04 01:14:21", "author": "@JKRasmusVorrath"}, "1002206511781720064": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "217", "datetime": "2018-05-31 15:13:47", "author": "@AssistedEvolve"}, "1058920346147934209": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "8,954", "datetime": "2018-11-04 03:14:18", "author": "@nardtree"}, "1001624940976586752": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "948", "datetime": "2018-05-30 00:42:49", "author": "@ihabilyas"}, "1001705178020700161": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,939", "datetime": "2018-05-30 06:01:39", "author": "@EldarSilver"}, "1001855115865837568": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "4,802", "datetime": "2018-05-30 15:57:27", "author": "@IntuitMachine"}, "1125955488066105345": {"content_summary": "RT @TheGregYang: I'll be explaining how batchnorm causes gradient explosion 4:30-6:30pm tomorrow (Wednesday 05/08) at Great Hall BC #12. Co\u2026", "followers": "771", "datetime": "2019-05-08 02:48:02", "author": "@real_asli"}, "1015571282807656450": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "19", "datetime": "2018-07-07 12:20:36", "author": "@expertosays"}, "1015385330445213697": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "141", "datetime": "2018-07-07 00:01:42", "author": "@mahesh_goud"}, "1001622894068359168": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "945", "datetime": "2018-05-30 00:34:41", "author": "@andrew_ilyas"}, "1002115128094023680": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "453", "datetime": "2018-05-31 09:10:39", "author": "@m_rajchl"}, "1175113028083290113": {"content_summary": "@dcpage3 @fastml_extra https://t.co/gM7ssYMTxv.", "followers": "5", "datetime": "2019-09-20 18:22:33", "author": "@Jacoed"}, "1002159084458541056": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "5,845", "datetime": "2018-05-31 12:05:19", "author": "@bttyeo"}, "1211529416708214789": {"content_summary": "How Does Batch Normalization Help Optimization? https://t.co/4krqow9GV7 Effectiveness of BatchNorm is more related to predictiveness and behavior of the gradients affected by gradient and Hessian of the loss with respect to the layer outputs than internal", "followers": "18", "datetime": "2019-12-30 06:08:16", "author": "@ShinobuKinjo1"}, "1019387974197415937": {"content_summary": "RT @onepaperperday: \"How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)\": https://t.co/IOGHZNKo\u2026", "followers": "649", "datetime": "2018-07-18 01:06:47", "author": "@JoshARhoads"}, "1001860549456678912": {"content_summary": "It looks like this is Batch Normalization week! Another paper on why BN works so well. This argues against the usual explanation in terms of reduction of internal covariate shift #DeepLearning https://t.co/r6OwPl2WQF https://t.co/u581yHT0m7", "followers": "735", "datetime": "2018-05-30 16:19:03", "author": "@unsorsodicorda"}, "1011149953849102337": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "740", "datetime": "2018-06-25 07:31:49", "author": "@vmpmember"}, "1024226441804693504": {"content_summary": "batch normalization\u3068covariate shift\u3068\u306e\u95a2\u4fc2\u6027\u304c\u30a4\u30de\u30a4\u30c1\u7406\u89e3\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3053\u308c\u306f\u3044\u3044\u52c9\u5f37\u306e\u30c1\u30e3\u30f3\u30b9\uff01\u3068\u601d\u3063\u3066\u60c5\u5831\u91cf\u7d71\u8a08\u5b66\u306e\u672c\u3082\u5165\u624b\u3057\u3066\u308f\u304f\u308f\u304f\u3057\u3066\u305f\u3051\u3069\u3001\u672c\u5f53\u306b\u3042\u307e\u308a\u95a2\u4fc2\u304c\u306a\u304b\u3063\u305f\u3089\u3057\u3044 https://t.co/7Oqclk5ejc", "followers": "339", "datetime": "2018-07-31 09:33:07", "author": "@_naru_jpn"}, "1125855666055327745": {"content_summary": "RT @TheGregYang: I'll be explaining how batchnorm causes gradient explosion 4:30-6:30pm tomorrow (Wednesday 05/08) at Great Hall BC #12. Co\u2026", "followers": "2,875", "datetime": "2019-05-07 20:11:22", "author": "@sschoenholz"}, "1058920442625314816": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "704", "datetime": "2018-11-04 03:14:41", "author": "@Nyamuz"}, "1058879318716166144": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "26", "datetime": "2018-11-04 00:31:17", "author": "@fumiaki_sato_"}, "1058617559749021696": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "296", "datetime": "2018-11-03 07:11:09", "author": "@rose_miura"}, "1001884317499277313": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1,113", "datetime": "2018-05-30 17:53:30", "author": "@achristensen56"}, "1001772510084395009": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "98", "datetime": "2018-05-30 10:29:13", "author": "@treasured_write"}, "1001859497193713664": {"content_summary": "Great summary from @arimorcos ! https://t.co/abFLseg7Rt", "followers": "2,014", "datetime": "2018-05-30 16:14:52", "author": "@niru_m"}, "1176503318245326849": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "414", "datetime": "2019-09-24 14:27:04", "author": "@NonLocalityGuy"}, "1125697499870695424": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "245", "datetime": "2019-05-07 09:42:53", "author": "@Rovio_Red"}, "1126424118092451840": {"content_summary": "Insightful talk about BatchNorm by @andrew_ilyas In addition, there are some really nice tricks to inspect the learning landscape Talk: https://t.co/8ayRVDpf2e Paper: https://t.co/LDhq5P80di #DeepLearning #computervision #NeurIPS2018", "followers": "1,336", "datetime": "2019-05-09 09:50:12", "author": "@jigarkdoshi"}, "1002332486091747328": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "2,530", "datetime": "2018-05-31 23:34:21", "author": "@_329_"}, "1002426680210743297": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "16", "datetime": "2018-06-01 05:48:39", "author": "@ItoHatabo"}, "1070325800476516358": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "159,742", "datetime": "2018-12-05 14:35:31", "author": "@Montreal_IA"}, "1124292280469291009": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "261", "datetime": "2019-05-03 12:39:02", "author": "@GuptaRajat033"}, "1002423494926200832": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "4,615", "datetime": "2018-06-01 05:35:59", "author": "@HITStales"}, "1131471285186248704": {"content_summary": "\"[BatchNorm] reparametrizes the underlying optimization problem to make its landscape significantly smooth[er].\" Less bumpy, so we can use a higher learning rate, hence faster convergence. Paper: How Does Batch Normalization Help Optimization? Link: https", "followers": "375", "datetime": "2019-05-23 08:05:50", "author": "@remykarem"}, "1124089390484152321": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,061", "datetime": "2019-05-02 23:12:49", "author": "@mvaldenegro"}, "1058941893508100096": {"content_summary": "\u6b7b\u306c\u3068\u3053\u304c\u6e1b\u3089\u305b\u308b\u3063\u3066\u3053\u3068\u304b", "followers": "658", "datetime": "2018-11-04 04:39:56", "author": "@virtualion"}, "1124198642380939264": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "44", "datetime": "2019-05-03 06:26:57", "author": "@jeandut14000"}, "1002246802815537154": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "100", "datetime": "2018-05-31 17:53:53", "author": "@aaqib_saeed"}, "1001670996225601536": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "816", "datetime": "2018-05-30 03:45:50", "author": "@alex_gude"}, "1002265042753130496": {"content_summary": "Good summary of the paper https://t.co/HBFg7OXEEH", "followers": "381", "datetime": "2018-05-31 19:06:22", "author": "@thomasjelonek"}, "1083598121584742400": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "2,136", "datetime": "2019-01-11 05:34:59", "author": "@theChrisChua"}, "1112963468175556608": {"content_summary": "This research uncovers a more fundamental impact of #BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training", "followers": "95", "datetime": "2019-04-02 06:22:23", "author": "@huisier"}, "1001679343205191681": {"content_summary": "RT @roydanroy: I'd like to understand the continuous time limit of batch norm. Happy to know answer when adding Gaussian noise to gradient\u2026", "followers": "1,286", "datetime": "2018-05-30 04:19:00", "author": "@heghbalz"}, "1124784523449913347": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2,822", "datetime": "2019-05-04 21:15:02", "author": "@WonderMicky"}, "1002535242253598721": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/iqxaQpsPe8 #deeplearning #machinelearning #datascience", "followers": "30,496", "datetime": "2018-06-01 13:00:02", "author": "@v_vashishta"}, "1111180820004241409": {"content_summary": "https://t.co/DSnux8j5fS", "followers": "52", "datetime": "2019-03-28 08:18:46", "author": "@hoangcuong0605"}, "1058898881172377600": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "1,497", "datetime": "2018-11-04 01:49:01", "author": "@mhangyo"}, "1117279807090221056": {"content_summary": "RT @decodyng: A somewhat niche question for #machinelearning Twitter: have there been any notable critiques published of Santurkar et al's\u2026", "followers": "604", "datetime": "2019-04-14 04:13:58", "author": "@rharang"}, "1002455031503732736": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "587", "datetime": "2018-06-01 07:41:18", "author": "@MAEA_2"}, "1001751573494616064": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "207", "datetime": "2018-05-30 09:06:01", "author": "@eram1205"}, "1171139949602754561": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "42", "datetime": "2019-09-09 19:14:57", "author": "@MishakinSergey"}, "1015119066430869504": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "1,633", "datetime": "2018-07-06 06:23:40", "author": "@ThiboNeveu"}, "1070426509037326337": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "4,531", "datetime": "2018-12-05 21:15:41", "author": "@ChrSzegedy"}, "1124163963665600514": {"content_summary": "As I said earlier too, BN is too much of a mess. Anyways next paper for our paper reading group!", "followers": "3,195", "datetime": "2019-05-03 04:09:09", "author": "@A_K_Nain"}, "1015037056014139392": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "216", "datetime": "2018-07-06 00:57:47", "author": "@KSKSKSKS2"}, "1002212876029865984": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "217", "datetime": "2018-05-31 15:39:04", "author": "@AssistedEvolve"}, "1124073439478861825": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,939", "datetime": "2019-05-02 22:09:26", "author": "@EldarSilver"}, "1099746412437356544": {"content_summary": "RT @RogerGrosse: Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between dif\u2026", "followers": "849", "datetime": "2019-02-24 19:02:31", "author": "@geoffroeder"}, "1002451644385189888": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "407", "datetime": "2018-06-01 07:27:51", "author": "@nishantiam"}, "1002190033896529920": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "23", "datetime": "2018-05-31 14:08:18", "author": "@ThinklabAI"}, "1001858365864202247": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "4,312", "datetime": "2018-05-30 16:10:22", "author": "@a_hun1972"}, "1058714025486254081": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "4,615", "datetime": "2018-11-03 13:34:28", "author": "@HITStales"}, "1002104871867305984": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1", "datetime": "2018-05-31 08:29:54", "author": "@YuriTrushkov"}, "1001659839209238529": {"content_summary": "I really hope nobody seriously thought BatchNorm reduces the \"covariate shift\"... https://t.co/NtfC90OFCC", "followers": "1,252", "datetime": "2018-05-30 03:01:30", "author": "@bremen79"}, "1003266221775745025": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "1,286", "datetime": "2018-06-03 13:24:41", "author": "@heghbalz"}, "1001725994200322049": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "265", "datetime": "2018-05-30 07:24:22", "author": "@Avsecz"}, "1191108867821686784": {"content_summary": "RT @sei_shinagawa: BatchNormalization\u3068\u3044\u3046\u3068\u3001\u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306f\u3042\u3093\u307e\u6027\u80fd\u306b\u95a2\u4fc2\u306a\u304f\u3066\u3001\u5b9f\u969b\u306e\u52b9\u679c\u306f\u6b63\u898f\u5316\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3068\u3053\u308d\u3060\u3063\u3066\u8a71\u304c\u3042\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u3069\u3046\u306a\u3063\u305f\u3093\u3084\u308d\uff1f\uff08\u3061\u3083\u3093\u3068\u8aad\u3093\u3067\u306a\u3044\u3057\u691c\u8a3c\u3082\u3057\u3066\u306a\u3044\u306e\u3067\u5224\u65ad\u3092\u5148\u9001\u2026", "followers": "822", "datetime": "2019-11-03 21:44:18", "author": "@morioka"}, "1100108363160002561": {"content_summary": "RT @aleks_madry: @RogerGrosse, please read the paper before making *factually* incorrect statements like this (see thread for a [partial] l\u2026", "followers": "740", "datetime": "2019-02-25 19:00:47", "author": "@tsiprasd"}, "1003612073568489472": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "948", "datetime": "2018-06-04 12:18:59", "author": "@ihabilyas"}, "1070640153608183808": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "2", "datetime": "2018-12-06 11:24:38", "author": "@stellazhao9"}, "1058706850646900736": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "474", "datetime": "2018-11-03 13:05:57", "author": "@pacifinapacific"}, "1001903887043919875": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "528", "datetime": "2018-05-30 19:11:15", "author": "@dtsbourg"}, "1002036841158193153": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "160,804", "datetime": "2018-05-31 03:59:34", "author": "@Quebec_AI"}, "1015168934943703041": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "456", "datetime": "2018-07-06 09:41:49", "author": "@PerthMLGroup"}, "1001843926062518272": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "179,069", "datetime": "2018-05-30 15:12:59", "author": "@Montreal_AI"}, "1001638879890309120": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "2,387", "datetime": "2018-05-30 01:38:13", "author": "@kamalikac"}, "1058923030343118848": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "254", "datetime": "2018-11-04 03:24:58", "author": "@utahkaA"}, "1058714035179253761": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "4,615", "datetime": "2018-11-03 13:34:30", "author": "@HITStales"}, "1003143694604570624": {"content_summary": "How Does Batch Normalization Help Optimization? https://t.co/GJNreuLxvw", "followers": "206", "datetime": "2018-06-03 05:17:48", "author": "@ashiskumarpanda"}, "1001739916987060224": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "33", "datetime": "2018-05-30 08:19:42", "author": "@l0r3nz0val3r10"}, "1001629067152535552": {"content_summary": "Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/xJM0PtNfdS", "followers": "2,927", "datetime": "2018-05-30 00:59:13", "author": "@evolvingstuff"}, "1001791989539581953": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "164,097", "datetime": "2018-05-30 11:46:37", "author": "@ceobillionaire"}, "1208062354669416448": {"content_summary": "\"we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm.\" it's exciting working at the cutting edge - using techniques which we're still learning to understand https://t.co/oacat48JcF #machinelea", "followers": "993", "datetime": "2019-12-20 16:31:24", "author": "@rzeta0"}, "1008565651768860672": {"content_summary": "https://t.co/MH09zKIIvb", "followers": "24", "datetime": "2018-06-18 04:22:44", "author": "@megulo25"}, "1002758792357011456": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "220", "datetime": "2018-06-02 03:48:21", "author": "@ceshine_en"}, "1058722662661255168": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "1,764", "datetime": "2018-11-03 14:08:47", "author": "@jaialkdanel"}, "1058581881434779649": {"content_summary": "Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.co/gDX1UhXnPS", "followers": "1,236", "datetime": "2018-11-03 04:49:22", "author": "@Tzawa"}, "1002649503613247488": {"content_summary": "RT @mattmayo13: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/tS\u2026", "followers": "14", "datetime": "2018-06-01 20:34:04", "author": "@e7mul"}, "1002284833496813568": {"content_summary": "Interesting paper from FAIR on batch norm. They assert that batch norm's effectiveness doesn't come from internal covariate shift, but from a drastically smoother optimization landscape. https://t.co/ef4HF4Vya3 #deeplearning #machinelearning https://t.co/c", "followers": "1,188", "datetime": "2018-05-31 20:25:00", "author": "@joeddata"}, "1002666675907579905": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "18,127", "datetime": "2018-06-01 21:42:18", "author": "@Datascience__"}, "1124138496891838464": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "336", "datetime": "2019-05-03 02:27:57", "author": "@DerekChia"}, "1058705833456885760": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "4,006", "datetime": "2018-11-03 13:01:55", "author": "@ballforest"}, "1070365703927037952": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? - https://t.co/9D1COPFWPY https://t.co/oyb7RVuALd", "followers": "195", "datetime": "2018-12-05 17:14:04", "author": "@hereticreader"}, "1001833085393043456": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "164,097", "datetime": "2018-05-30 14:29:55", "author": "@ceobillionaire"}, "1059244900917534721": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "82", "datetime": "2018-11-05 00:43:58", "author": "@RyoHWS"}, "1253222348901306368": {"content_summary": "How Does Batch Normalization Help Optimization? (2018): https://t.co/OA5AdFFo5U - via @pipedream", "followers": "36", "datetime": "2020-04-23 07:21:06", "author": "@HackerNewsNow"}, "1002187591708393473": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "406", "datetime": "2018-05-31 13:58:36", "author": "@upaver20"}, "1058725819357384704": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "1,554", "datetime": "2018-11-03 14:21:20", "author": "@tackman"}, "1003308279596908544": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "601", "datetime": "2018-06-03 16:11:49", "author": "@ShibaniSan"}, "1002176785239478272": {"content_summary": "Sounds very interesting https://t.co/tQtiekO3lk", "followers": "15", "datetime": "2018-05-31 13:15:39", "author": "@IraShavitt"}, "1125239166042689536": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,290", "datetime": "2019-05-06 03:21:37", "author": "@tak_yamm"}, "1117726208051957760": {"content_summary": "RT @decodyng: A somewhat niche question for #machinelearning Twitter: have there been any notable critiques published of Santurkar et al's\u2026", "followers": "456", "datetime": "2019-04-15 09:47:49", "author": "@PerthMLGroup"}, "1059280064984961024": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "279", "datetime": "2018-11-05 03:03:42", "author": "@ADMIS_Walker"}, "1003838917433421824": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "5", "datetime": "2018-06-05 03:20:22", "author": "@HfYama"}, "1002151482517405696": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "845", "datetime": "2018-05-31 11:35:07", "author": "@toshiemon18"}, "1002421838922878978": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "242", "datetime": "2018-06-01 05:29:25", "author": "@koushik_here"}, "1003807338724126720": {"content_summary": "RT @hshimodaira: \u8b1b\u7fa9\u6e96\u5099\u3067covariate shift\u691c\u7d22\u3057\u3066\u305f\u3089\u307f\u3064\u3051\u305f\uff0e\u7406\u8ad6\u7814\u7a76\u306f\u697d\u3057\u3044\u306d\uff0e https://t.co/tfgix0vAX0", "followers": "254", "datetime": "2018-06-05 01:14:54", "author": "@mshero_y"}, "1070283561524023296": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "432", "datetime": "2018-12-05 11:47:40", "author": "@bhargavbardipur"}, "1124312929128538112": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2", "datetime": "2019-05-03 14:01:05", "author": "@kobejean"}, "1002195952294289408": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "482", "datetime": "2018-05-31 14:31:49", "author": "@izariuo440"}, "1002245333819232256": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "37,494", "datetime": "2018-05-31 17:48:03", "author": "@sedielem"}, "1059087892385980416": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "127", "datetime": "2018-11-04 14:20:05", "author": "@6uux_uxxx_ut"}, "1124279625692086272": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "741", "datetime": "2019-05-03 11:48:45", "author": "@JoaoVictor_AC"}, "1124096328496242689": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "126", "datetime": "2019-05-02 23:40:24", "author": "@renato145"}, "1071088947147751424": {"content_summary": "RT @Synced_Global: \u201cHow Does Batch Normalization Help Optimization?\u201d by @ShibaniSan, @tsiprasd, @andrew_ilyas and @aleks_madry from @MIT. A\u2026", "followers": "179,069", "datetime": "2018-12-07 17:07:59", "author": "@Montreal_AI"}, "1003343435846291456": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "11,624", "datetime": "2018-06-03 18:31:30", "author": "@AjitJaokar"}, "1001909646502825984": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "103", "datetime": "2018-05-30 19:34:08", "author": "@avmoldovan"}, "1002554586941419522": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "19", "datetime": "2018-06-01 14:16:54", "author": "@ki1la"}, "1059000540250566656": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "60", "datetime": "2018-11-04 08:32:58", "author": "@zyuer"}, "1001695042564698112": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "28,195", "datetime": "2018-05-30 05:21:23", "author": "@halvarflake"}, "1125944474201235456": {"content_summary": "RT @TheGregYang: I'll be explaining how batchnorm causes gradient explosion 4:30-6:30pm tomorrow (Wednesday 05/08) at Great Hall BC #12. Co\u2026", "followers": "327", "datetime": "2019-05-08 02:04:16", "author": "@aditya_soni2k17"}, "1058895446540673024": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "1,353", "datetime": "2018-11-04 01:35:22", "author": "@MathSorcerer"}, "1058708486945554433": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "1,667", "datetime": "2018-11-03 13:12:27", "author": "@Scaled_Wurm"}, "1001713860456349697": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "3,410", "datetime": "2018-05-30 06:36:09", "author": "@_onionesque"}, "1058923848265912320": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "984", "datetime": "2018-11-04 03:28:13", "author": "@pei2_dir"}, "1071088170387697664": {"content_summary": "RT @Synced_Global: \u201cHow Does Batch Normalization Help Optimization?\u201d by @ShibaniSan, @tsiprasd, @andrew_ilyas and @aleks_madry from @MIT. A\u2026", "followers": "626", "datetime": "2018-12-07 17:04:54", "author": "@choongng"}, "1001664010222231553": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,238", "datetime": "2018-05-30 03:18:04", "author": "@jhasomesh"}, "1002546765780795393": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "79", "datetime": "2018-06-01 13:45:50", "author": "@yksym_t"}, "1003496926921285632": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "3,456", "datetime": "2018-06-04 04:41:26", "author": "@danilobzdok"}, "1003273372678017024": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "83", "datetime": "2018-06-03 13:53:06", "author": "@fqz7c3"}, "1125404374002307073": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "3,007", "datetime": "2019-05-06 14:18:06", "author": "@syoyo"}, "1124753677267419136": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "54", "datetime": "2019-05-04 19:12:28", "author": "@drorhilman"}, "1002473663860432896": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "126", "datetime": "2018-06-01 08:55:21", "author": "@nakaet"}, "1004231788754014218": {"content_summary": "\ud83e\udd16 How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) \u270d\ufe0f Shibani Santurkar et al. \ud83d\udd17 https://t.co/2wfFTXJbfd \ud83d\udd0a Tweeted by @aleks_madry, @ml_review et al. #MachineLearning #statML #csLG #csNE https://t.co/VOmZilOd", "followers": "311", "datetime": "2018-06-06 05:21:30", "author": "@arxivtrends"}, "1002236475482701824": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "193", "datetime": "2018-05-31 17:12:51", "author": "@alexhock"}, "1013438102071750657": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "25", "datetime": "2018-07-01 15:04:06", "author": "@zaheerkhancs"}, "1002532801462358021": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "2", "datetime": "2018-06-01 12:50:20", "author": "@Eddhuan"}, "1002143085537542144": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "1,655", "datetime": "2018-05-31 11:01:45", "author": "@olanleed"}, "1124111617287966721": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "63", "datetime": "2019-05-03 00:41:09", "author": "@kuanchen22"}, "1015046775978090496": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "1", "datetime": "2018-07-06 01:36:24", "author": "@JiaxingWang1"}, "1002336597445136387": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "255", "datetime": "2018-05-31 23:50:42", "author": "@christian_hudon"}, "1001657462431219713": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "2,676", "datetime": "2018-05-30 02:52:03", "author": "@ayirpelle"}, "1002135244940574720": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "353", "datetime": "2018-05-31 10:30:35", "author": "@hrnbskgc"}, "1058904411785687041": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "6,181", "datetime": "2018-11-04 02:10:59", "author": "@adhara_mathphys"}, "1070284820377493505": {"content_summary": "@ErmiaBivatan @aerinykim https://t.co/1Nml5qPT0B", "followers": "70", "datetime": "2018-12-05 11:52:40", "author": "@68kirk"}, "1017051703957442561": {"content_summary": "Neat paper on #BatchNorm. Alternatives hinted at. Interesting. HT: @thomasquintana cc:@bbriniotis https://t.co/QhpeYcr7HA", "followers": "3,864", "datetime": "2018-07-11 14:23:16", "author": "@drsxr"}, "1171874988439351296": {"content_summary": "@dcpage3 Wondering if you have looked at this work - https://t.co/hPKWuxLHwr? It takes a very different approach to explaining why BatchNorm works.", "followers": "73", "datetime": "2019-09-11 19:55:44", "author": "@gupta__abhay"}, "1124298721242562560": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "49", "datetime": "2019-05-03 13:04:38", "author": "@RyanRod96136776"}, "1001858581581516800": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "4,679", "datetime": "2018-05-30 16:11:14", "author": "@aleks_madry"}, "1058924614565261312": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "891", "datetime": "2018-11-04 03:31:16", "author": "@tattaka_sun"}, "1003719983854022658": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "14", "datetime": "2018-06-04 19:27:47", "author": "@Joydeep09286001"}, "1003466197348880386": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "152", "datetime": "2018-06-04 02:39:19", "author": "@HubGenomics"}, "1001647343605821440": {"content_summary": "[R] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/D4OnGIOTTB", "followers": "4,453", "datetime": "2018-05-30 02:11:51", "author": "@GiovanniToschi"}, "1002549251422015488": {"content_summary": "Great summary! https://t.co/DYi35hngw0", "followers": "112", "datetime": "2018-06-01 13:55:42", "author": "@MSripadarao"}, "1002039566696632326": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "945", "datetime": "2018-05-31 04:10:24", "author": "@andrew_ilyas"}, "1070318377225371652": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "1,654", "datetime": "2018-12-05 14:06:01", "author": "@keunwoochoi"}, "1070322787070459909": {"content_summary": "paper https://t.co/TRWkC59U1d", "followers": "4,688", "datetime": "2018-12-05 14:23:32", "author": "@artificialsoph"}, "1124207912631590912": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "145", "datetime": "2019-05-03 07:03:47", "author": "@esvhd"}, "1118933304756060161": {"content_summary": "How does Batch Normalization help optimization? It does not reduce internal covariance shift. But it makes the optimization landscape significantly smoother (Lipschitzness of the loss AND the gradient) https://t.co/gvUV6v7tQJ", "followers": "51", "datetime": "2019-04-18 17:44:23", "author": "@antbrl"}, "1172034288063254533": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,103", "datetime": "2019-09-12 06:28:44", "author": "@permutans"}, "1002147977178238976": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "428", "datetime": "2018-05-31 11:21:11", "author": "@okdshin"}, "1124166420739215360": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "67", "datetime": "2019-05-03 04:18:55", "author": "@dksdc"}, "1003239356331634689": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "225", "datetime": "2018-06-03 11:37:56", "author": "@UweAimeVan"}, "1125787898949853184": {"content_summary": "I'll be explaining how batchnorm causes gradient explosion 4:30-6:30pm tomorrow (Wednesday 05/08) at Great Hall BC #12. Come hang out! #ICLR2019 #batchnorm #deeplearning", "followers": "3,547", "datetime": "2019-05-07 15:42:05", "author": "@TheGregYang"}, "1170905020859990016": {"content_summary": "11/ The blog post is subtitled \u201cor How I learned to stop worrying and love reduced internal covariate shift\u201d but https://t.co/6UbMAqPOdZ shows \u201cinternal covariate shift\u201d is likely not a valid explanation in specific cases, and I tend to believe this in gen", "followers": "3,547", "datetime": "2019-09-09 03:41:25", "author": "@TheGregYang"}, "1009330664351518720": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "2", "datetime": "2018-06-20 07:02:37", "author": "@jacksonjack1993"}, "1001677960770998275": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "3,623", "datetime": "2018-05-30 04:13:30", "author": "@tarantulae"}, "1002417265919197185": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "587", "datetime": "2018-06-01 05:11:14", "author": "@hirokingXX"}, "1003308992205123584": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "244", "datetime": "2018-06-03 16:14:38", "author": "@milena_petkovic"}, "1002418469109764096": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1,926", "datetime": "2018-06-01 05:16:01", "author": "@lschmidt3"}, "1058893163048235008": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "914", "datetime": "2018-11-04 01:26:18", "author": "@ougai_quantum"}, "1001691948636426241": {"content_summary": "RT @roydanroy: I'd like to understand the continuous time limit of batch norm. Happy to know answer when adding Gaussian noise to gradient\u2026", "followers": "4,891", "datetime": "2018-05-30 05:09:05", "author": "@IgorCarron"}, "1011151342453133312": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "255", "datetime": "2018-06-25 07:37:20", "author": "@kamujun18"}, "1003232821845032961": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "8,784", "datetime": "2018-06-03 11:11:58", "author": "@laminahmed11"}, "1001860341624594437": {"content_summary": "RT @gomessdegomess: BN\u306e\u7406\u8ad6\u306b\u8e0f\u307f\u8fbc\u3093\u3060\u8ad6\u6587\u3002 How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t\u2026", "followers": "3,943", "datetime": "2018-05-30 16:18:13", "author": "@chihirow"}, "1001680718634864642": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "7,125", "datetime": "2018-05-30 04:24:28", "author": "@anshulkundaje"}, "1003474728030531585": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "96", "datetime": "2018-06-04 03:13:13", "author": "@MsCheikh"}, "1015238087083941888": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "1,764", "datetime": "2018-07-06 14:16:36", "author": "@jaialkdanel"}, "1170905022336401408": {"content_summary": "12/ In any case, if any of you guys are interested further, I\u2019m down to Skype, as some of the nuances may be better hammered out verbally :) In the mean time, here's a relevant twitter thread again if you haven't seen it https://t.co/dlSUFwLRe7", "followers": "3,547", "datetime": "2019-09-09 03:41:26", "author": "@TheGregYang"}, "1002111805420957696": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "202", "datetime": "2018-05-31 08:57:27", "author": "@stes_io"}, "1002948933553983488": {"content_summary": "RT @mattmayo13: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/tS\u2026", "followers": "289", "datetime": "2018-06-02 16:23:54", "author": "@MarcoZorzi"}, "1124550179347206144": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "37", "datetime": "2019-05-04 05:43:50", "author": "@manuelschmidt90"}, "1058692144704053248": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "12", "datetime": "2018-11-03 12:07:31", "author": "@peketktk"}, "1124664316479909888": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "344", "datetime": "2019-05-04 13:17:22", "author": "@jefkine"}, "1058592772242911232": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "2,060", "datetime": "2018-11-03 05:32:39", "author": "@yo_ehara"}, "1058847985667358721": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "782", "datetime": "2018-11-03 22:26:46", "author": "@K_Ryuichirou"}, "1001684726162362368": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "85", "datetime": "2018-05-30 04:40:23", "author": "@_alwc"}, "1124271348757467136": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "304", "datetime": "2019-05-03 11:15:52", "author": "@sebastien_wood"}, "1117259820363476994": {"content_summary": "A somewhat niche question for #machinelearning Twitter: have there been any notable critiques published of Santurkar et al's paper (linked) demonstrating that Batch Norm's doesn't seem to be aiding optimization through reducing internal covariate shift? ht", "followers": "1,792", "datetime": "2019-04-14 02:54:33", "author": "@decodyng"}, "1002269696916381701": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "25,260", "datetime": "2018-05-31 19:24:51", "author": "@alexjc"}, "1126498205544402944": {"content_summary": "RT @jigarkdoshi: Insightful talk about BatchNorm by @andrew_ilyas In addition, there are some really nice tricks to inspect the learning la\u2026", "followers": "130", "datetime": "2019-05-09 14:44:36", "author": "@jkronand"}, "1124484574019346432": {"content_summary": "TIL that some ML researchers had an idea (batch normalization) that worked, then tacked on a post-hoc mathematical analysis to claim that it reduced \"internal covariate shift\". The technique works, but not for that reason. https://t.co/tBXkbOUrNU https://t", "followers": "777", "datetime": "2019-05-04 01:23:09", "author": "@hackerfriendly"}, "1070325623397195776": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "164,097", "datetime": "2018-12-05 14:34:48", "author": "@ceobillionaire"}, "1002340145033502720": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "24", "datetime": "2018-06-01 00:04:47", "author": "@aAnzai2017"}, "1002424707369484289": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "473", "datetime": "2018-06-01 05:40:49", "author": "@conjugate_box"}, "1002444758906630144": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "456", "datetime": "2018-06-01 07:00:29", "author": "@PerthMLGroup"}, "1001622824522469376": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "740", "datetime": "2018-05-30 00:34:25", "author": "@tsiprasd"}, "1001875294196822016": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "491", "datetime": "2018-05-30 17:17:38", "author": "@jbohnslav"}, "1003466111105445890": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "39", "datetime": "2018-06-04 02:38:59", "author": "@_sharma_mohit"}, "1001637639882076161": {"content_summary": "https://t.co/I9RLMXhmOF", "followers": "1,308", "datetime": "2018-05-30 01:33:17", "author": "@crcrpar"}, "1124008132714299394": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2,509", "datetime": "2019-05-02 17:49:56", "author": "@ilyaraz2"}, "1124293928839204864": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "360", "datetime": "2019-05-03 12:45:35", "author": "@NyatzAnger"}, "1001941490501906433": {"content_summary": "Pretty cool for understanding batch norm https://t.co/ypXe5SpfHI", "followers": "224", "datetime": "2018-05-30 21:40:41", "author": "@MeHanumarna"}, "1001680592273100800": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "165", "datetime": "2018-05-30 04:23:58", "author": "@dbparedes"}, "1003241929684017152": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "410", "datetime": "2018-06-03 11:48:10", "author": "@jcvasquezc1"}, "1002912310783086593": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "28", "datetime": "2018-06-02 13:58:22", "author": "@LSTM_Layer"}, "1003616848401743872": {"content_summary": "\u8b1b\u7fa9\u6e96\u5099\u3067covariate shift\u691c\u7d22\u3057\u3066\u305f\u3089\u307f\u3064\u3051\u305f\uff0e\u7406\u8ad6\u7814\u7a76\u306f\u697d\u3057\u3044\u306d\uff0e https://t.co/tfgix0vAX0", "followers": "4,389", "datetime": "2018-06-04 12:37:57", "author": "@hshimodaira"}, "1002253066375192584": {"content_summary": "RT @hasdid: #DeepLearning #AI | How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co\u2026", "followers": "91", "datetime": "2018-05-31 18:18:46", "author": "@imabit_inc"}, "1002156424569368576": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "822", "datetime": "2018-05-31 11:54:45", "author": "@morioka"}, "1001629446690910211": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). (arXiv:1805.11604v1 https://t.co/t2MVXVNdTW", "followers": "759", "datetime": "2018-05-30 01:00:44", "author": "@M157q_News_RSS"}, "1001709149300969472": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,938", "datetime": "2018-05-30 06:17:26", "author": "@GiorgioPatrini"}, "1070483394260156416": {"content_summary": "[1805.11604v3] How Does Batch Normalization Help Optimization? https://t.co/905m4HGtYr", "followers": "40", "datetime": "2018-12-06 01:01:44", "author": "@lixinlang9527"}, "1191089082304102400": {"content_summary": "BatchNormalization\u3068\u3044\u3046\u3068\u3001\u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306f\u3042\u3093\u307e\u6027\u80fd\u306b\u95a2\u4fc2\u306a\u304f\u3066\u3001\u5b9f\u969b\u306e\u52b9\u679c\u306f\u6b63\u898f\u5316\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3068\u3053\u308d\u3060\u3063\u3066\u8a71\u304c\u3042\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u3069\u3046\u306a\u3063\u305f\u3093\u3084\u308d\uff1f\uff08\u3061\u3083\u3093\u3068\u8aad\u3093\u3067\u306a\u3044\u3057\u691c\u8a3c\u3082\u3057\u3066\u306a\u3044\u306e\u3067\u5224\u65ad\u3092\u5148\u9001\u308a\u306b\u3057\u3066\u308b\uff09 https://t.co/14QsQmn7JI", "followers": "2,058", "datetime": "2019-11-03 20:25:41", "author": "@sei_shinagawa"}, "1001731183812767744": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "127", "datetime": "2018-05-30 07:45:00", "author": "@AmalFeriani"}, "1001860174401941505": {"content_summary": "BN\u306e\u7406\u8ad6\u306b\u8e0f\u307f\u8fbc\u3093\u3060\u8ad6\u6587\u3002 How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/bwqAvv7gGJ", "followers": "3,513", "datetime": "2018-05-30 16:17:33", "author": "@gomessdegomess"}, "1071326358901538819": {"content_summary": "Look at closely to understand batchnorm", "followers": "29", "datetime": "2018-12-08 08:51:22", "author": "@KursatOzdalli"}, "1148509110712881154": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "11,920", "datetime": "2019-07-09 08:28:04", "author": "@Rosenchild"}, "1070315001968320513": {"content_summary": "@sbourke @aerinykim Equal contribution, I suspect. https://t.co/muBHmHHJ3d", "followers": "767", "datetime": "2018-12-05 13:52:36", "author": "@johndburger"}, "1002422505901043713": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "308", "datetime": "2018-06-01 05:32:04", "author": "@lorensipro"}, "1125244316647997441": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "822", "datetime": "2019-05-06 03:42:05", "author": "@morioka"}, "1058668472303939584": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "379", "datetime": "2018-11-03 10:33:27", "author": "@namahoge"}, "1124099633205014528": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,416", "datetime": "2019-05-02 23:53:31", "author": "@RexDouglass"}, "1002151561898803200": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "526", "datetime": "2018-05-31 11:35:26", "author": "@kbyk_01"}, "1015034384921985024": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "81,374", "datetime": "2018-07-06 00:47:10", "author": "@hardmaru"}, "1071347600425668609": {"content_summary": "RT @Synced_Global: \u201cHow Does Batch Normalization Help Optimization?\u201d by @ShibaniSan, @tsiprasd, @andrew_ilyas and @aleks_madry from @MIT. A\u2026", "followers": "44", "datetime": "2018-12-08 10:15:47", "author": "@jeandut14000"}, "1001644989854683139": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "709", "datetime": "2018-05-30 02:02:29", "author": "@QuanquanGu"}, "1123998535492489222": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "4,350", "datetime": "2019-05-02 17:11:48", "author": "@aerinykim"}, "1002229507363762177": {"content_summary": "https://t.co/4DCeuwA2JF How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)", "followers": "428", "datetime": "2018-05-31 16:45:09", "author": "@Jotarun"}, "1005868537284448256": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1,930", "datetime": "2018-06-10 17:45:22", "author": "@bballinger"}, "1002138511875571712": {"content_summary": "BatchNorm\u3067\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6e1b\u5c11\u3057\u306a\u3044\u3063\u3066\u3069\u3046\u3044\u3046\u3053\u3068\u3060\u308d\u3046\uff1f\u6a19\u6e96\u5316\u3067\u5206\u5e03\u304c\u8fd1\u304f\u306a\u3089\u306a\u3044\u306e\u304b\u306a\uff1f\u305d\u308c\u3068\u3082\u305d\u306e\u3042\u3068\u306e\u7dda\u5f62\u5909\u63db\u3067\u751f\u3058\u308b\u5206\u5e03\u306e\u5dee\u304c\u5927\u304d\u3044\u306e\u304b\u306a\uff1f https://t.co/f3OmvAyBqY", "followers": "2,432", "datetime": "2018-05-31 10:43:34", "author": "@jinbeizame007"}, "1004166049854443520": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). (arXiv:1805.11604v2 https://t.co/t2MVXVNdTW", "followers": "759", "datetime": "2018-06-06 01:00:17", "author": "@M157q_News_RSS"}, "1015191577663139840": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "226", "datetime": "2018-07-06 11:11:48", "author": "@ElectronNest"}, "1001720663529340928": {"content_summary": "RT @reddit_ml: [R] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/Gtp0T1QydD", "followers": "1,711", "datetime": "2018-05-30 07:03:11", "author": "@Keiku"}, "1002101166346555392": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "76,883", "datetime": "2018-05-31 08:15:10", "author": "@NandoDF"}, "1002661258070581251": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "1,714", "datetime": "2018-06-01 21:20:47", "author": "@wu_ming_80"}, "1070284220445179909": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "541", "datetime": "2018-12-05 11:50:17", "author": "@o_ursu"}, "1015039202033987585": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "98", "datetime": "2018-07-06 01:06:18", "author": "@treasured_write"}, "1002170865449095170": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "18", "datetime": "2018-05-31 12:52:08", "author": "@CyberStoic"}, "1058725890253774848": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "3,164", "datetime": "2018-11-03 14:21:37", "author": "@tjmlab"}, "1001670164042207239": {"content_summary": "I'd like to understand the continuous time limit of batch norm. Happy to know answer when adding Gaussian noise to gradient with variance equal to step size, if it makes the maths easier (i.e., like SGLD). SGLD converges to exp(-beta*loss). What does SGLD+", "followers": "14,852", "datetime": "2018-05-30 03:42:31", "author": "@roydanroy"}, "1001695141651009537": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "761", "datetime": "2018-05-30 05:21:47", "author": "@yieldthought"}, "1002104790439153666": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "185", "datetime": "2018-05-31 08:29:34", "author": "@mckinley_scan"}, "1002228494909124608": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "110", "datetime": "2018-05-31 16:41:08", "author": "@ankou06"}, "1124297261444403200": {"content_summary": "Turns out Batchnorm are not helpful in really deep (non-skip) nets! A really neat summary of past landscape of Batchnorm papers.", "followers": "508", "datetime": "2019-05-03 12:58:50", "author": "@sksq96"}, "1002476768694030337": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "182", "datetime": "2018-06-01 09:07:41", "author": "@drdeanjones"}, "1002186773940789248": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "602", "datetime": "2018-05-31 13:55:21", "author": "@Swall0wTech"}, "1002663013344739329": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "3,374", "datetime": "2018-06-01 21:27:45", "author": "@renato_umeton"}, "1170905019677204485": {"content_summary": "10/ This is why we have to be careful when we saying something like \u201cBN helps because it makes training better\u201d (https://t.co/ZC7rQzDRUc https://t.co/6UbMAqPOdZ) because this same \u201cbenefit\u201d can cause poor generalization, but we really want to explain both", "followers": "3,547", "datetime": "2019-09-09 03:41:25", "author": "@TheGregYang"}, "1070725999795494913": {"content_summary": "\u201cBatchNorm [\u2026] makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.\u201d (machine learning) https://t.co/ATEtSIrC9X", "followers": "3,446", "datetime": "2018-12-06 17:05:46", "author": "@reiver"}, "1126438692770463744": {"content_summary": "RT @jigarkdoshi: Insightful talk about BatchNorm by @andrew_ilyas In addition, there are some really nice tricks to inspect the learning la\u2026", "followers": "1,286", "datetime": "2019-05-09 10:48:07", "author": "@heghbalz"}, "1136133839271546880": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "337", "datetime": "2019-06-05 04:53:10", "author": "@hello42n0"}, "1099838080708677633": {"content_summary": "RT @RogerGrosse: Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between dif\u2026", "followers": "220", "datetime": "2019-02-25 01:06:47", "author": "@david_macedo"}, "1015163211639762945": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "217", "datetime": "2018-07-06 09:19:05", "author": "@AssistedEvolve"}, "1001863977511550977": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "287", "datetime": "2018-05-30 16:32:40", "author": "@nagaraj_arvind"}, "1001862244467003393": {"content_summary": "Very cool to start seeing some science sneak into our alchemy ;) https://t.co/nQPbEKB0AA", "followers": "756", "datetime": "2018-05-30 16:25:47", "author": "@noodlefrenzy"}, "1124093860047740928": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "289", "datetime": "2019-05-02 23:30:35", "author": "@dannyehb"}, "1071085787297603584": {"content_summary": "\u201cHow Does Batch Normalization Help Optimization?\u201d by @ShibaniSan, @tsiprasd, @andrew_ilyas and @aleks_madry from @MIT. Accepted by #NeurIPS2018 Watch the full video at https://t.co/diwF0e9Jao Read the full paper at https://t.co/pZ0B4qyKF8 https://t.co/lt", "followers": "2,923", "datetime": "2018-12-07 16:55:26", "author": "@Synced_Global"}, "1015073025862103042": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "131", "datetime": "2018-07-06 03:20:43", "author": "@yujiabao2510"}, "1001787856367468544": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "615", "datetime": "2018-05-30 11:30:11", "author": "@KouroshMeshgi"}, "1002443506407784449": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "456", "datetime": "2018-06-01 06:55:31", "author": "@PerthMLGroup"}, "1124782962761961474": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,938", "datetime": "2019-05-04 21:08:50", "author": "@GiorgioPatrini"}, "1134362005530275841": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "35", "datetime": "2019-05-31 07:32:32", "author": "@sumanthmeenank2"}, "1099692138252636160": {"content_summary": "Fair point!", "followers": "1,286", "datetime": "2019-02-24 15:26:51", "author": "@heghbalz"}, "1002249382648074241": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "784", "datetime": "2018-05-31 18:04:08", "author": "@muktabh"}, "1071127547155488768": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "179,069", "datetime": "2018-12-07 19:41:22", "author": "@Montreal_AI"}, "1002416881704136704": {"content_summary": "Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u529b\u306b\u5bfe\u3059\u308b\u52fe\u914d\u306e\u5909\u52d5\u3092\u6291\u5236\u3057\u6ed1\u3089\u304b\u306b\u3059\u308b\u70b9\u3068\u3044\u3046 https://t.co/zbRWMtpUFE", "followers": "11,458", "datetime": "2018-06-01 05:09:43", "author": "@icoxfog417"}, "1058753778273288192": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "4,531", "datetime": "2018-11-03 16:12:26", "author": "@yoshizaki_kkgk"}, "1001672679663915008": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,423", "datetime": "2018-05-30 03:52:31", "author": "@skornblith"}, "1124270287737032704": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "750", "datetime": "2019-05-03 11:11:39", "author": "@Cometml"}, "1015059286316277760": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "249", "datetime": "2018-07-06 02:26:07", "author": "@dangpzanco"}, "1022822509295665152": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "11,920", "datetime": "2018-07-27 12:34:23", "author": "@Rosenchild"}, "1015105711905431553": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "10", "datetime": "2018-07-06 05:30:36", "author": "@yarphs"}, "1124166115192496129": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "81", "datetime": "2019-05-03 04:17:42", "author": "@Hansatyam"}, "1004000040711573504": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "158", "datetime": "2018-06-05 14:00:37", "author": "@QSR_AI_Lab"}, "1002406115999735808": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/W8z8XZXgH2", "followers": "640", "datetime": "2018-06-01 04:26:56", "author": "@strategist922"}, "1003303143629783041": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "225", "datetime": "2018-06-03 15:51:24", "author": "@datadonk23"}, "1002105732899201025": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "561", "datetime": "2018-05-31 08:33:19", "author": "@DrSamirBhatt"}, "1003519684359147520": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "344", "datetime": "2018-06-04 06:11:51", "author": "@jefkine"}, "1058708921605545984": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "211", "datetime": "2018-11-03 13:14:11", "author": "@dontsentouin"}, "1001629196966232064": {"content_summary": "Q: Why batch normalization facilities training? A: Because it smoothes the loss landscape. https://t.co/YfNBh4gcjD", "followers": "1,673", "datetime": "2018-05-30 00:59:44", "author": "@TheGradient"}, "1124108276961038337": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "2,676", "datetime": "2019-05-03 00:27:52", "author": "@ayirpelle"}, "1168393097752891394": {"content_summary": "I was reading why BatchNorm works and it opened up other questions - Covariate shift( reading again) - Lipschitz functions - Taylor series(because the higher-order stuff can change things) https://t.co/EvS1ipe5B7 https://t.co/b5oYbG9jG1 #DeepLearning #", "followers": "199", "datetime": "2019-09-02 05:19:56", "author": "@a_random_obsrvr"}, "1003635925124091904": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "5", "datetime": "2018-06-04 13:53:45", "author": "@xtracko"}, "1001711106937774080": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry https://t.co/GLMFaCjfkK", "followers": "3,878", "datetime": "2018-05-30 06:25:13", "author": "@BrundageBot"}, "1002196034951446529": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "797", "datetime": "2018-05-31 14:32:09", "author": "@drscotthawley"}, "1001856542268952576": {"content_summary": "Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch norm works. They perform a number of clever experiments to work it out, finding that internal covariate shift is a red herring! htt", "followers": "2,995", "datetime": "2018-05-30 16:03:07", "author": "@arimorcos"}, "1046280779293028352": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "305", "datetime": "2018-09-30 06:09:11", "author": "@subhobrata1"}, "1001751805968121856": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,251", "datetime": "2018-05-30 09:06:56", "author": "@biggiobattista"}, "1003621055183720448": {"content_summary": "RT @hshimodaira: \u8b1b\u7fa9\u6e96\u5099\u3067covariate shift\u691c\u7d22\u3057\u3066\u305f\u3089\u307f\u3064\u3051\u305f\uff0e\u7406\u8ad6\u7814\u7a76\u306f\u697d\u3057\u3044\u306d\uff0e https://t.co/tfgix0vAX0", "followers": "379", "datetime": "2018-06-04 12:54:40", "author": "@wetto1210"}, "1124193322724028417": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "163", "datetime": "2019-05-03 06:05:49", "author": "@HouhouinK"}, "1124377658450563073": {"content_summary": "read, and read I need to understand this in case any of my student ask me this next term \ud83d\ude48", "followers": "163", "datetime": "2019-05-03 18:18:18", "author": "@Undeedz"}, "1099725121546117120": {"content_summary": "RT @RogerGrosse: Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between dif\u2026", "followers": "2,676", "datetime": "2019-02-24 17:37:55", "author": "@ayirpelle"}, "1002142705047097344": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "2,041", "datetime": "2018-05-31 11:00:14", "author": "@dosei_sanga"}, "1002801774514716673": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "377", "datetime": "2018-06-02 06:39:08", "author": "@BenCox_AI"}, "1058739782589665280": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "37", "datetime": "2018-11-03 15:16:49", "author": "@bzircon6"}, "1058980212916899841": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "175", "datetime": "2018-11-04 07:12:12", "author": "@suzumikasumi"}, "1002557774570622976": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "8,627", "datetime": "2018-06-01 14:29:34", "author": "@Jack_Burdick"}, "1124381373605777408": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "120", "datetime": "2019-05-03 18:33:04", "author": "@sengkim123"}, "1002462322139394048": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "75", "datetime": "2018-06-01 08:10:17", "author": "@RyutaroYamauchi"}, "1001874616774725632": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "33,955", "datetime": "2018-05-30 17:14:57", "author": "@aiprgirl"}, "1002421890323922944": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "2,041", "datetime": "2018-06-01 05:29:37", "author": "@dosei_sanga"}, "1070325881707618306": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "33,955", "datetime": "2018-12-05 14:35:50", "author": "@montrealdotai"}, "1058740811381170176": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "843", "datetime": "2018-11-03 15:20:54", "author": "@kazanagisora"}, "1038067197145317376": {"content_summary": "Interesting experiments and analysis - How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/KKS8s2U3F8", "followers": "52", "datetime": "2018-09-07 14:11:20", "author": "@thao_nguyen26"}, "1001877545523077120": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "89", "datetime": "2018-05-30 17:26:35", "author": "@namanmabus"}, "1001851623658934272": {"content_summary": "#paperoftheday How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/hTaNRdLDxE https://t.co/fhJtgy7XZM", "followers": "4,183", "datetime": "2018-05-30 15:43:35", "author": "@aykuterdemml"}, "1001888679969816576": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "604", "datetime": "2018-05-30 18:10:50", "author": "@rharang"}, "1015124689348431872": {"content_summary": "https://t.co/0gDpipKXiL #ai #machinelearning #artificialintelligence via @hardmaru", "followers": "2,276", "datetime": "2018-07-06 06:46:00", "author": "@future_of_AI"}, "1124487635441979392": {"content_summary": "RT @hackerfriendly: TIL that some ML researchers had an idea (batch normalization) that worked, then tacked on a post-hoc mathematical anal\u2026", "followers": "1,061", "datetime": "2019-05-04 01:35:18", "author": "@DanielOCL"}, "1001630181046579206": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "793", "datetime": "2018-05-30 01:03:39", "author": "@StylianosIordan"}, "1171104660368896000": {"content_summary": "RT @TheGregYang: 12/ In any case, if any of you guys are interested further, I\u2019m down to Skype, as some of the nuances may be better hammer\u2026", "followers": "44", "datetime": "2019-09-09 16:54:43", "author": "@jeandut14000"}, "1058904142490398726": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "690", "datetime": "2018-11-04 02:09:55", "author": "@SythonUK"}, "1002417232385724416": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "1,667", "datetime": "2018-06-01 05:11:06", "author": "@Scaled_Wurm"}, "1002175748726079488": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "226", "datetime": "2018-05-31 13:11:32", "author": "@ElectronNest"}, "1124106205272662017": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "26", "datetime": "2019-05-03 00:19:38", "author": "@LewisGeer"}, "1001644459942076417": {"content_summary": "RT @crcrpar: https://t.co/I9RLMXhmOF", "followers": "1,607", "datetime": "2018-05-30 02:00:23", "author": "@syinari0123"}, "1070614330566094848": {"content_summary": "RT @68kirk: @ErmiaBivatan @aerinykim https://t.co/1Nml5qPT0B", "followers": "1,416", "datetime": "2018-12-06 09:42:02", "author": "@RexDouglass"}, "1058896087283421184": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "415", "datetime": "2018-11-04 01:37:55", "author": "@Subaru_Saito"}, "1100341207601999872": {"content_summary": "RT @RogerGrosse: Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between dif\u2026", "followers": "1,103", "datetime": "2019-02-26 10:26:01", "author": "@permutans"}, "1002076977505980416": {"content_summary": "Theory behind BatchNorm. Good to see such work to make the black art of deep learning more interpretable. https://t.co/u5nsNIRVbu", "followers": "248", "datetime": "2018-05-31 06:39:03", "author": "@gsksantosh"}, "1058724979246678016": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "16", "datetime": "2018-11-03 14:17:59", "author": "@pro_about_pro"}, "1002227200605671424": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "49", "datetime": "2018-05-31 16:35:59", "author": "@RenaudBougues"}, "1002432715763867648": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "138", "datetime": "2018-06-01 06:12:38", "author": "@kisser_caffe"}, "1002201672007942145": {"content_summary": "Nice summary of some recent work trying to understand batchnorm https://t.co/GlZhEhyTov", "followers": "486", "datetime": "2018-05-31 14:54:33", "author": "@cpburgess_"}, "1160824223994191874": {"content_summary": "How Does Batch Normalization Help Optimization? https://t.co/CP9WSYkyvU #MachineLearning", "followers": "1,712", "datetime": "2019-08-12 08:03:56", "author": "@alexey_r"}, "1124336244580868096": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,212", "datetime": "2019-05-03 15:33:44", "author": "@nfusi"}, "1001843547450949632": {"content_summary": "How Does Batch Normalization Help Optimization? https://t.co/7xJrdwagQ4", "followers": "253", "datetime": "2018-05-30 15:11:29", "author": "@ajmbell"}, "1003293530285137921": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "63", "datetime": "2018-06-03 15:13:12", "author": "@prasanna4ai"}, "1003968576573853696": {"content_summary": "Alchemy becoming chemistry https://t.co/smRNo7Cybl", "followers": "370", "datetime": "2018-06-05 11:55:36", "author": "@desipoika"}, "1001661451025580032": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "587", "datetime": "2018-05-30 03:07:54", "author": "@ethancaballero"}, "1002149857899970562": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "302", "datetime": "2018-05-31 11:28:39", "author": "@cepera_ang"}, "1002661076276740096": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jykXuEC2r3 https://t.co/GjSbs2pI8x", "followers": "169,788", "datetime": "2018-06-01 21:20:03", "author": "@kdnuggets"}, "1050443666907488256": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "784", "datetime": "2018-10-11 17:51:01", "author": "@mitsuse_t"}, "1172030719746379776": {"content_summary": "RT @TheGregYang: @yoavgo @srush_nlp 1/ Thanks for the ping @srush_nlp! I think we still don't have a good idea why it works, but we have be\u2026", "followers": "48", "datetime": "2019-09-12 06:14:33", "author": "@chbalajitilak"}, "1124134792566493185": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "72", "datetime": "2019-05-03 02:13:14", "author": "@gabeibagon"}, "1001725126067531776": {"content_summary": "@dipti_nemade https://t.co/80Px8MyIgi The plotting wizardry isn\u2019t mine but here is the paper pdf.", "followers": "11,390", "datetime": "2018-05-30 07:20:55", "author": "@_brohrer_"}, "1100110920313958400": {"content_summary": "RT @aleks_madry: @RogerGrosse, please read the paper before making *factually* incorrect statements like this (see thread for a [partial] l\u2026", "followers": "601", "datetime": "2019-02-25 19:10:57", "author": "@ShibaniSan"}, "1001675013253197824": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "198", "datetime": "2018-05-30 04:01:48", "author": "@omar_javd"}, "1136131956209414144": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "1,055", "datetime": "2019-06-05 04:45:41", "author": "@stealthinu"}, "1002162135521251329": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "170", "datetime": "2018-05-31 12:17:27", "author": "@iCEO_AJ"}, "1123999190873444353": {"content_summary": "Great recap of our recent batch norm work (to appear at ICLR).", "followers": "2,875", "datetime": "2019-05-02 17:14:24", "author": "@sschoenholz"}, "1124272216735006720": {"content_summary": "RT @A_K_Nain: As I said earlier too, BN is too much of a mess. Anyways next paper for our paper reading group! https://t.co/PAM1LHnLRx", "followers": "327", "datetime": "2019-05-03 11:19:19", "author": "@aditya_soni2k17"}, "1125867721831227392": {"content_summary": "RT @TheGregYang: I'll be explaining how batchnorm causes gradient explosion 4:30-6:30pm tomorrow (Wednesday 05/08) at Great Hall BC #12. Co\u2026", "followers": "468", "datetime": "2019-05-07 20:59:17", "author": "@murefil"}, "1001910400538107905": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "614", "datetime": "2018-05-30 19:37:08", "author": "@logan_engstrom"}, "1058998363935322112": {"content_summary": "\u9762\u767d\u3044\uff01", "followers": "131", "datetime": "2018-11-04 08:24:19", "author": "@s_s_satoc"}, "1002724002622664704": {"content_summary": "RT @Panzoto: Have you ever used #BatchNormalization? Recent theoretical work revealed that it make optimization landscape significantly smo\u2026", "followers": "615", "datetime": "2018-06-02 01:30:06", "author": "@KouroshMeshgi"}, "1070325538462621697": {"content_summary": "How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minute summary: https://t.co/Kiqmf6Zgo1 #AI #AIFirst #ArtificialIntelligence #MontrealAI #NeurIPS #MachineLearning #EvolutionaryComputing", "followers": "179,069", "datetime": "2018-12-05 14:34:28", "author": "@Montreal_AI"}, "1003407873760784384": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "862", "datetime": "2018-06-03 22:47:34", "author": "@PyDataBA"}, "1001898433198768128": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "859", "datetime": "2018-05-30 18:49:35", "author": "@cosminnegruseri"}, "1002089471242915840": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "7,141", "datetime": "2018-05-31 07:28:42", "author": "@santoroAI"}, "1001724597258063872": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,086", "datetime": "2018-05-30 07:18:49", "author": "@ak1010"}, "1171936081455001600": {"content_summary": "@jeremyphoward @Towards_Entropy @dcpage3 FWIW, there isn't really any conflict between what we find and the findings in this thread (see our reply there). Also, not sure I follow why ResNets are needed? Or why 1.25 is not enough given it induces more insta", "followers": "4,679", "datetime": "2019-09-11 23:58:29", "author": "@aleks_madry"}, "1124202563648143363": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "20", "datetime": "2019-05-03 06:42:32", "author": "@Boristream"}, "1028518503744065536": {"content_summary": "Amazing work from MIT for understanding batch normalization carefully. https://t.co/VcTAbejoxF", "followers": "119", "datetime": "2018-08-12 05:48:14", "author": "@Krishna_DN94"}, "1002086210314088448": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "62", "datetime": "2018-05-31 07:15:45", "author": "@BernhardGeiger"}, "1073470551933501440": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "27", "datetime": "2018-12-14 06:51:38", "author": "@OrganumTech"}, "1070278704897646593": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "2,049", "datetime": "2018-12-05 11:28:22", "author": "@vnfrombucharest"}, "1001936201585225730": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "2,894", "datetime": "2018-05-30 21:19:40", "author": "@nsaphra"}, "1124295598218174464": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "167", "datetime": "2019-05-03 12:52:13", "author": "@sarot9158"}, "1001855589801115651": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "3,310", "datetime": "2018-05-30 15:59:20", "author": "@infoecho"}, "1171104586809139202": {"content_summary": "RT @TheGregYang: 10/ This is why we have to be careful when we saying something like \u201cBN helps because it makes training better\u201d (https://t\u2026", "followers": "44", "datetime": "2019-09-09 16:54:26", "author": "@jeandut14000"}, "1001677047306047489": {"content_summary": "RT @bremen79: I really hope nobody seriously thought BatchNorm reduces the \"covariate shift\"... https://t.co/NtfC90OFCC", "followers": "3,355", "datetime": "2018-05-30 04:09:52", "author": "@nomad421"}, "1010199329217261574": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "268", "datetime": "2018-06-22 16:34:23", "author": "@sc_codeUM"}, "1002661837962383360": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/citptAJl9t https://t.co/b4lcvPZQ3x", "followers": "458", "datetime": "2018-06-01 21:23:05", "author": "@bytebiscuit"}, "1124086082071666691": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "99,879", "datetime": "2019-05-02 22:59:41", "author": "@jeremyphoward"}, "1124101279582695425": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "72", "datetime": "2019-05-03 00:00:04", "author": "@adn_twitts"}, "1169652361846870021": {"content_summary": "@yoavgo @srush_nlp 1/ Thanks for the ping @srush_nlp! I think we still don't have a good idea why it works, but we have better ideas of some failure modes of batchnorm and how people implicitly avoid them https://t.co/dlSUFwLRe7 https://t.co/FnYNEresi0", "followers": "3,547", "datetime": "2019-09-05 16:43:48", "author": "@TheGregYang"}, "1124133615774752768": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "217", "datetime": "2019-05-03 02:08:34", "author": "@AssistedEvolve"}, "1001940871045054464": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "2,676", "datetime": "2018-05-30 21:38:13", "author": "@ayirpelle"}, "1001687848028987392": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "1,134", "datetime": "2018-05-30 04:52:48", "author": "@arunprakashml"}, "1117378387054436352": {"content_summary": "RT @decodyng: A somewhat niche question for #machinelearning Twitter: have there been any notable critiques published of Santurkar et al's\u2026", "followers": "217", "datetime": "2019-04-14 10:45:42", "author": "@AssistedEvolve"}, "1001735849216704512": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "38", "datetime": "2018-05-30 08:03:32", "author": "@pess_r"}, "1003406568635936769": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "822", "datetime": "2018-06-03 22:42:23", "author": "@ErmiaBivatan"}, "1124814470855962624": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "1,871", "datetime": "2019-05-04 23:14:02", "author": "@born2data"}, "1001645858805374976": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "1,036", "datetime": "2018-05-30 02:05:57", "author": "@kleinsound"}, "1003354920723021824": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "4,679", "datetime": "2018-06-03 19:17:09", "author": "@aleks_madry"}, "1002130479338872832": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "4,006", "datetime": "2018-05-31 10:11:39", "author": "@ballforest"}, "1062208734846316544": {"content_summary": "How Does Batch Normalization Help Optimization? https://t.co/OQ6HBLnwPp", "followers": "14", "datetime": "2018-11-13 05:01:11", "author": "@LvguanInn"}, "1015046904089075712": {"content_summary": "- 'batchnorm is for whitening/reducing internal covariate shift, *obviously*' (https://t.co/QI8qO7yOGy)", "followers": "20,465", "datetime": "2018-07-06 01:36:55", "author": "@gwern"}, "1170916124218802176": {"content_summary": "RT @TheGregYang: 10/ This is why we have to be careful when we saying something like \u201cBN helps because it makes training better\u201d (https://t\u2026", "followers": "1,286", "datetime": "2019-09-09 04:25:33", "author": "@heghbalz"}, "1099689977175334917": {"content_summary": "Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between different batches for fixed weights, which is clearly different from what the original BN paper was referring to. https://t.co/rVmmUQDIM", "followers": "5,469", "datetime": "2019-02-24 15:18:16", "author": "@RogerGrosse"}, "1070278236582670336": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "7,210", "datetime": "2018-12-05 11:26:31", "author": "@fastml_extra"}, "1002574247015960576": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/tS7fHlq1Md https://t.co/ON5wKO5wtP", "followers": "5,543", "datetime": "2018-06-01 15:35:02", "author": "@mattmayo13"}, "1070725603450511360": {"content_summary": "\"How Does Batch Normalization Help Optimization?\" by Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry https://t.co/yPS2BeqaAj (machine learning)", "followers": "3,446", "datetime": "2018-12-06 17:04:11", "author": "@reiver"}, "1058666126362603520": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "12,763", "datetime": "2018-11-03 10:24:08", "author": "@jaguring1"}, "1124775630896287744": {"content_summary": "@GiorgioPatrini I guess the \"here what we know today\" part on batch norm will need to be changed again after the paper by @TheGregYang https://t.co/e8n8ydf3eY", "followers": "735", "datetime": "2019-05-04 20:39:42", "author": "@unsorsodicorda"}, "1001861278191800321": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "145", "datetime": "2018-05-30 16:21:57", "author": "@esvhd"}, "1124267275668852737": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "299", "datetime": "2019-05-03 10:59:41", "author": "@500zainraza"}, "1001880322047447040": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1,264", "datetime": "2018-05-30 17:37:37", "author": "@sroecker"}, "1001625783939444737": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "7,580", "datetime": "2018-05-30 00:46:10", "author": "@poolio"}, "1015658114123780096": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "48", "datetime": "2018-07-07 18:05:39", "author": "@unknowlake"}, "1003246734355529728": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "371", "datetime": "2018-06-03 12:07:15", "author": "@hrmoaddeli"}, "1002448407615553536": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "254", "datetime": "2018-06-01 07:14:59", "author": "@mshero_y"}, "1001631434069676033": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "781", "datetime": "2018-05-30 01:08:37", "author": "@HrSaghir"}, "1003280909741813760": {"content_summary": "Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonly believed Internal Covariate Shift, but the smoothing of the optimization landscape. https://t.co/2a7nGz0XBP https://t.co/OTjezJo5", "followers": "4,751", "datetime": "2018-06-03 14:23:03", "author": "@Slychief"}, "1002610303979290624": {"content_summary": "RT @v_vashishta: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/iqxaQpsPe8 #de\u2026", "followers": "114", "datetime": "2018-06-01 17:58:18", "author": "@rajpratim"}, "1070509970368561152": {"content_summary": "RT @DatazDude: How Does Batch Normalization Help Optimization? https://t.co/Djl2abodqh https://t.co/h0OEOrs6a3", "followers": "1,058", "datetime": "2018-12-06 02:47:20", "author": "@BigsnarfDude"}, "1002361087864369153": {"content_summary": "We need more of this https://t.co/DaOHvWstFn", "followers": "101", "datetime": "2018-06-01 01:28:00", "author": "@deep_jamesc"}, "1059006740857901057": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "256", "datetime": "2018-11-04 08:57:37", "author": "@dizzy_my_future"}, "1069457883748659200": {"content_summary": "This is pretty thought provoking yet easy to read as far as papers go: \"How Does Batch Normalization Help Optimization?\" https://t.co/8ShdGlPlzP", "followers": "12,040", "datetime": "2018-12-03 05:06:43", "author": "@citnaj"}, "1044153824456396800": {"content_summary": "@tarinziyaee @RogerGrosse Here https://t.co/1Nml5qPT0B. This is illuminating.", "followers": "70", "datetime": "2018-09-24 09:17:25", "author": "@68kirk"}, "1002937670497177601": {"content_summary": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/RcU9eoGbov", "followers": "3,137", "datetime": "2018-06-02 15:39:08", "author": "@fabtar"}, "1002106903718068224": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "99", "datetime": "2018-05-31 08:37:58", "author": "@zhanweiz"}, "1001859289600770049": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "601", "datetime": "2018-05-30 16:14:02", "author": "@ShibaniSan"}, "1058724059142225926": {"content_summary": "\u3053\u306e\u6587\u8108\u3060\u3068\u4f55\u306b\u5bfe\u3057\u3066\u6709\u52b9\u306a\u306e\u304b\u308f\u304b\u3089\u306a\u3044\u306a\u3002\u3068\u308a\u3042\u3048\u305a\u660e\u65e5\u8ad6\u6587\u5185\u5bb9\u78ba\u8a8d\u3002", "followers": "226", "datetime": "2018-11-03 14:14:20", "author": "@ElectronNest"}, "1002134215499001856": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "4,615", "datetime": "2018-05-31 10:26:30", "author": "@HITStales"}, "1001646319755243520": {"content_summary": "RT @evolvingstuff: Great analysis! How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://\u2026", "followers": "25,578", "datetime": "2018-05-30 02:07:46", "author": "@Miles_Brundage"}, "1070275725960335360": {"content_summary": "Paper: https://t.co/mbnhidMpda", "followers": "23,517", "datetime": "2018-12-05 11:16:32", "author": "@johnplattml"}, "1002406916964933634": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/97XAu4NwU6", "followers": "8,911", "datetime": "2018-06-01 04:30:07", "author": "@Deep_In_Depth"}, "1070366709284974592": {"content_summary": "How Does Batch Normalization Help Optimization? https://t.co/Djl2abodqh https://t.co/h0OEOrs6a3", "followers": "137", "datetime": "2018-12-05 17:18:04", "author": "@DatazDude"}, "1136117451991633921": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "128", "datetime": "2019-06-05 03:48:03", "author": "@kodai_nakashima"}, "1125259219509235713": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "65", "datetime": "2019-05-06 04:41:18", "author": "@kli_nlpr"}, "1002749924847509504": {"content_summary": "RT @kdnuggets: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/jyk\u2026", "followers": "241", "datetime": "2018-06-02 03:13:06", "author": "@sabarinathan_7"}, "1124785630993686528": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "826", "datetime": "2019-05-04 21:19:26", "author": "@chiheb_tr"}, "1015129421102178304": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "53", "datetime": "2018-07-06 07:04:48", "author": "@CLagares7"}, "1058705790935035904": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "486", "datetime": "2018-11-03 13:01:45", "author": "@nmygle"}, "1003669129511997441": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "169", "datetime": "2018-06-04 16:05:42", "author": "@nicococo23"}, "1002151281471799296": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "16,954", "datetime": "2018-05-31 11:34:19", "author": "@kazoo04"}, "1001628507787616256": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "84", "datetime": "2018-05-30 00:57:00", "author": "@sjain_stanford"}, "1124259113079193600": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "244", "datetime": "2019-05-03 10:27:14", "author": "@vandit_25"}, "1002585401343774720": {"content_summary": "RT @mattmayo13: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/tS\u2026", "followers": "4,802", "datetime": "2018-06-01 16:19:21", "author": "@IntuitMachine"}, "1002420016866738176": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "95", "datetime": "2018-06-01 05:22:10", "author": "@summer4an"}, "1015034502718951424": {"content_summary": "@trustswz Link to the landing page: https://t.co/9LH5xi6EjY", "followers": "81,374", "datetime": "2018-07-06 00:47:38", "author": "@hardmaru"}, "1002690608035696641": {"content_summary": "RT @mattmayo13: [1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/tS\u2026", "followers": "1,348", "datetime": "2018-06-01 23:17:24", "author": "@udmrzn"}, "1125244254736007168": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "78,987", "datetime": "2019-05-06 03:41:51", "author": "@machinelearnflx"}, "1124335653678747653": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "4,327", "datetime": "2019-05-03 15:31:23", "author": "@jekbradbury"}, "1070325718876389377": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "160,804", "datetime": "2018-12-05 14:35:11", "author": "@Quebec_AI"}, "1070674616014790656": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "2,136", "datetime": "2018-12-06 13:41:35", "author": "@theChrisChua"}, "1058706576360452096": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "1,607", "datetime": "2018-11-03 13:04:52", "author": "@tereka114"}, "1015140166657298432": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "52", "datetime": "2018-07-06 07:47:30", "author": "@hakanardo"}, "1058929108766224386": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "1,197", "datetime": "2018-11-04 03:49:08", "author": "@menphim"}, "1001756138973483008": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,053", "datetime": "2018-05-30 09:24:09", "author": "@parand"}, "1002113019843874816": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "139", "datetime": "2018-05-31 09:02:16", "author": "@JosephELemley"}, "1002254036568018944": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "276", "datetime": "2018-05-31 18:22:37", "author": "@kadarakos"}, "1124076431104008192": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "573", "datetime": "2019-05-02 22:21:20", "author": "@timrudner"}, "1006401071537967104": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "11,920", "datetime": "2018-06-12 05:01:28", "author": "@Rosenchild"}, "1001865457060794368": {"content_summary": "Batch norm works by smoothing the optimization landscape rather than by reducing internal covariate shift. https://t.co/jZyp9sXz0Q", "followers": "77", "datetime": "2018-05-30 16:38:33", "author": "@rajivpoc"}, "1002497429692977152": {"content_summary": "Batch Normalization is a technique that have been used in #deeplearning for over 3 years. In this paper, researchers try to prove that BN does not affect internal covariate shift that has been a way of explaining why it works until today. #machinelearning", "followers": "195", "datetime": "2018-06-01 10:29:47", "author": "@F1sherKK"}, "1003268545550209029": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "152", "datetime": "2018-06-03 13:33:55", "author": "@alirg1"}, "1002214774619320320": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "1,248", "datetime": "2018-05-31 15:46:37", "author": "@daisuzu"}, "1001659130686287872": {"content_summary": "Important paper, especially given some of the weirdness of batch normalization (such as that it seems to be better to compute the stats over a subset of a batch when training large barches). https://t.co/nqEGRvh55e", "followers": "1,380", "datetime": "2018-05-30 02:58:41", "author": "@kyrpov"}, "1058891380569780224": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "4,006", "datetime": "2018-11-04 01:19:13", "author": "@ballforest"}, "1015938390406189057": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "37", "datetime": "2018-07-08 12:39:22", "author": "@manuelschmidt90"}, "1070276980719767552": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "3,515", "datetime": "2018-12-05 11:21:31", "author": "@agibsonccc"}, "1001765017660702720": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "206", "datetime": "2018-05-30 09:59:26", "author": "@erik_nijkamp"}, "1223371348526993408": {"content_summary": "@dcpage3 @jeremyphoward What can you say about this paper? https://t.co/vU3QF6IBcK The authors provide experimental evidence that internal covariate shift and BN effectiveness are not connected and that there exist a different mechanism helping optimizatio", "followers": "84", "datetime": "2020-01-31 22:23:53", "author": "@guitaricet"}, "1001794437909856256": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "10", "datetime": "2018-05-30 11:56:21", "author": "@adversarial_ML"}, "1014899354757365761": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "111", "datetime": "2018-07-05 15:50:36", "author": "@nimblel"}, "1070326360156061696": {"content_summary": "RT @Montreal_AI: How Does Batch Normalization Help Optimization? #NeurIPS2018 Santurkar et al.: https://t.co/C0bMHHs6pt This is a 3-minut\u2026", "followers": "239", "datetime": "2018-12-05 14:37:44", "author": "@chiefaiofficers"}, "1015252016996597762": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "1,216", "datetime": "2018-07-06 15:11:57", "author": "@devnag"}, "1002261000865726464": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "568", "datetime": "2018-05-31 18:50:18", "author": "@cxhrndz"}, "1003656615529238529": {"content_summary": "RT @Slychief: Highly interesting observation! - How Does Batch Normalization in #DeepLearning Help Optimization It Is Not About the commonl\u2026", "followers": "260", "datetime": "2018-06-04 15:15:58", "author": "@GapDataInst"}, "1124126738219319297": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "375", "datetime": "2019-05-03 01:41:14", "author": "@remykarem"}, "1001701310411001856": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "1,020", "datetime": "2018-05-30 05:46:17", "author": "@serrjoa"}, "1100109625398095873": {"content_summary": "RT @aleks_madry: @RogerGrosse, please read the paper before making *factually* incorrect statements like this (see thread for a [partial] l\u2026", "followers": "945", "datetime": "2019-02-25 19:05:48", "author": "@andrew_ilyas"}, "1002270156704374784": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "628", "datetime": "2018-05-31 19:26:41", "author": "@oshtim"}, "1001733126819901441": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "116", "datetime": "2018-05-30 07:52:43", "author": "@gnthibault"}, "1003235749288906752": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "332", "datetime": "2018-06-03 11:23:36", "author": "@NoguerMiquel"}, "1001823246751293443": {"content_summary": "RT @aleks_madry: Think BatchNorm helps training due to reducing internal covariate shift? Think again. (What BatchNorm *does* seem to do th\u2026", "followers": "4", "datetime": "2018-05-30 13:50:49", "author": "@nejla_gh"}, "1070314825698308096": {"content_summary": "RT @johnplattml: Paper: https://t.co/mbnhidMpda https://t.co/sPGuv2EGe5", "followers": "106", "datetime": "2018-12-05 13:51:54", "author": "@tmichalp"}, "1124222768386461696": {"content_summary": "RT @sschoenholz: Great recap of our recent batch norm work (to appear at ICLR). https://t.co/rxh1bccszk", "followers": "178", "datetime": "2019-05-03 08:02:49", "author": "@namhoonlee09"}, "1058707644720934912": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "618", "datetime": "2018-11-03 13:09:06", "author": "@zaltoprofen"}, "1002426067749990400": {"content_summary": "RT @icoxfog417: Batch\u6b63\u898f\u5316\u306f\u6709\u52b9\u3060\u304c\u305d\u308c\u306f\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u8a71\u3002BN\u3092\u5c0e\u5165\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u610f\u56f3\u7684\u306b\u5171\u5909\u91cf\u30b7\u30d5\u30c8(\u30ec\u30a4\u30e4\u51fa\u529b\u306e\u5e73\u5747/\u5206\u6563\u3092\u5909\u52d5)\u3057\u3066\u3082\u6027\u80fd\u306b\u5909\u5316\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d(=\u305d\u3082\u305d\u3082\u30b7\u30d5\u30c8\u306f\u5f71\u97ff\u306a\u3044)\u3002\u771f\u306e\u52b9\u679c\u306f(\u6b63\u898f\u5316\u306b\u3088\u308a)\u51fa\u2026", "followers": "1,711", "datetime": "2018-06-01 05:46:13", "author": "@Keiku"}, "1019333754630885381": {"content_summary": "\"How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)\": https://t.co/IOGHZNKoqk #ml #nn #2018", "followers": "2,132", "datetime": "2018-07-17 21:31:20", "author": "@onepaperperday"}, "1002232636104785920": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "7,580", "datetime": "2018-05-31 16:57:35", "author": "@poolio"}, "1015243123167907841": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "222", "datetime": "2018-07-06 14:36:37", "author": "@AkimiyaF"}, "1001637765073661952": {"content_summary": "RT @crcrpar: https://t.co/I9RLMXhmOF", "followers": "2,041", "datetime": "2018-05-30 01:33:47", "author": "@dosei_sanga"}, "1251940577756581889": {"content_summary": "@1Sarim https://t.co/VwcShElZjx seems interesting.", "followers": "1,396", "datetime": "2020-04-19 18:27:48", "author": "@theshawwn"}, "1001672484356280320": {"content_summary": "Conclusion: Batch Normalization makes deep neural networks easier for gradient descent to navigate. (It smooths the parameter space energy landscape.) https://t.co/vPpqTvYD9v", "followers": "11,390", "datetime": "2018-05-30 03:51:45", "author": "@_brohrer_"}, "1118941416904953856": {"content_summary": "RT @antbrl: How does Batch Normalization help optimization? It does not reduce internal covariance shift. But it makes the optimization lan\u2026", "followers": "79", "datetime": "2019-04-18 18:16:37", "author": "@dimtion"}, "1001690438275928065": {"content_summary": "Batch Normalization works because it smooths the parameter landscape, not because it reduces the variability of input to each layer. https://t.co/n2hZyHJNr9", "followers": "406", "datetime": "2018-05-30 05:03:05", "author": "@mattpetersen_ai"}, "1058921488382754817": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "707", "datetime": "2018-11-04 03:18:51", "author": "@Amboinensis"}, "1002104414537166848": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "1,876", "datetime": "2018-05-31 08:28:05", "author": "@MannyKayy"}, "1124111659814014978": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "164,097", "datetime": "2019-05-03 00:41:19", "author": "@ceobillionaire"}, "1058847973986258944": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "782", "datetime": "2018-11-03 22:26:44", "author": "@K_Ryuichirou"}, "1001859751553224704": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "720", "datetime": "2018-05-30 16:15:53", "author": "@angelamczhou"}, "1009330713341018112": {"content_summary": "RT @ml_review: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) By @ShibaniSan @tsiprasd @and\u2026", "followers": "2", "datetime": "2018-06-20 07:02:49", "author": "@jacksonjack1993"}, "1171935768283103232": {"content_summary": "@dcpage3 Since people seem to be wondering: if you interpret ICS as \"improving optimization landscape\" then our conclusions in https://t.co/Tlo71NADjK (also cf https://t.co/OWz58qZB1E) agree. We only provide evidence against the distributional stability vi", "followers": "4,679", "datetime": "2019-09-11 23:57:15", "author": "@aleks_madry"}, "1058938176046227456": {"content_summary": "RT @Tzawa: Batch Normalization\u304c\u6709\u52b9\u306a\u306e\u306f\u3001\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u3048\u308b\u304b\u3089\u300c\u3067\u306f\u306a\u3044\u300d\u3088\uff01\u3063\u3066\u3044\u3046\u8ad6\u6587\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u3044\u308d\u3093\u306a\u6559\u79d1\u66f8\u306b\u3082\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u304c\u6291\u3048\u3089\u308c\u308b\u304b\u3089\u3060\u3063\u3066\u3044\u3046\u8aac\u660e\u304c\u3042\u308b\u3051\u3069\u3001\u305d\u3046\u3058\u3083\u306a\u304f\u3066\u3001loss\u304c\u5b89\u5b9a\u3059\u308b\u304b\u3089\u3089\u3057\u3044\u3002 https://t.\u2026", "followers": "716", "datetime": "2018-11-04 04:25:09", "author": "@minux302"}, "1002662111816945665": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/swVEHWw1Xf https://t.co/DWxrSFdgHH", "followers": "380", "datetime": "2018-06-01 21:24:10", "author": "@aforanalytic"}, "1099708370301452288": {"content_summary": "RT @RogerGrosse: Note that the ICS effect \"debunked\" by the \"How Batch Norm Helps\" paper is actually the variability in moments between dif\u2026", "followers": "4,802", "datetime": "2019-02-24 16:31:21", "author": "@IntuitMachine"}, "1070389262175125505": {"content_summary": "https://t.co/kdi4sizsvz", "followers": "21", "datetime": "2018-12-05 18:47:41", "author": "@StefanFiott"}, "1009330683464912896": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "2", "datetime": "2018-06-20 07:02:42", "author": "@jacksonjack1993"}, "1002352619044720645": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "3", "datetime": "2018-06-01 00:54:21", "author": "@snowdirt7"}, "1131884032487297025": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "944", "datetime": "2019-05-24 11:25:57", "author": "@sannykimchi"}, "1124094863853723648": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "101", "datetime": "2019-05-02 23:34:34", "author": "@davidgordian"}, "1136119118162157568": {"content_summary": "RT @mosko_mule: BatchNorm\u306b\u3088\u3063\u3066 * \u5b9f\u306f\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6e1b\u5c11\u3057\u306a\u3044\u304c\u3001\u76ee\u7684\u51fd\u6570\u304c\u975e\u60c5\u306b\u6ed1\u3089\u304b\u306b\u306a\u308a\u5b66\u7fd2\u3057\u3084\u3059\u304f\u306a\u308b(https://t.co/N6jAR4x7w3 ) * \uff08MLP+\u5165\u529b\u304c\u30ac\u30a6\u30b7\u30a2\u30f3\u306e\u6642\uff09\u9577\u3055\u30fb\u65b9\u5411\u304c\u5206\u96e2\u3057\u3001\u66f2\u7387\u304c\u5358\u7d14\u5316\u3055\u308c\u308b\u305f\u3081a\u2026", "followers": "690", "datetime": "2019-06-05 03:54:40", "author": "@SythonUK"}, "1015081459806232577": {"content_summary": "RT @trustswz: How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). The paper suggests the effect\u2026", "followers": "325", "datetime": "2018-07-06 03:54:13", "author": "@vodkamomo"}, "1124193003042635776": {"content_summary": "RT @TheGregYang: 1/ Does batchnorm make optimization landscape more smooth? https://t.co/5J92tRz8ag says yes, but our new @iclr2019 paper h\u2026", "followers": "89", "datetime": "2019-05-03 06:04:33", "author": "@singularpattern"}, "1001710276700508161": {"content_summary": "Does BatchNorm works? Yes but not because of what you think, claim these two independent submissions. It's remarkable how the approaches differ: https://t.co/yXn0C88dkJ (ETH Zurich) https://t.co/nOeBC52gTG (MIT)", "followers": "84", "datetime": "2018-05-30 06:21:55", "author": "@emilecontal"}, "1002443929130807296": {"content_summary": "RT @arimorcos: Timely paper from @ShibaniSan, Dimitris Tsipras, @andrew_ilyas , and @aleks_madry providing some new insights into why batch\u2026", "followers": "459", "datetime": "2018-06-01 06:57:11", "author": "@FrankHarwald"}, "1171880280287666176": {"content_summary": "@dcpage3 What do you think of the results of https://t.co/H44pNMsfFK The authors inject random noise after batchnorm layers to simulate increasing covariate shift and find that noisy batchnorm still has the training benefits of batchnorm.", "followers": "116", "datetime": "2019-09-11 20:16:45", "author": "@Towards_Entropy"}, "1001850524478586881": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/N9WrcDsvSh", "followers": "195", "datetime": "2018-05-30 15:39:13", "author": "@hereticreader"}, "1002661145268965376": {"content_summary": "[1805.11604] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) https://t.co/6VIdB31x8b https://t.co/7tcyrdDd9M", "followers": "179", "datetime": "2018-06-01 21:20:20", "author": "@MDFBasha"}, "1002657117600452610": {"content_summary": "Have you ever used #BatchNormalization? Recent theoretical work revealed that it make optimization landscape significantly smoother and accelerates optimization with gradient-based methods. #AI #CNN https://t.co/m9ku4H4lam https://t.co/iUsRumMWB9", "followers": "77", "datetime": "2018-06-01 21:04:19", "author": "@Panzoto"}}}