{"citation_id": "60515731", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 14:49:20", "twitter": {"1129209589960855553": {"followers": "170", "datetime": "2019-05-17 02:18:40", "author": "@hiro6391", "content_summary": "BatchNorm\u3068Dropout\u3092\u884c\u3046\u5c64\u3092Independent-Component (IC) \u5c64\u3068\u547c\u3073\u3001\u3053\u308c\u3092Conv\u5c64\u306e\u524d\u306b\u5165\u308c\u308b\u3068\u5b66\u7fd2\u304c\u5b89\u5b9a\u3057\u3001\u53ce\u675f\u901f\u5ea6\u304c\u65e9\u304f\u306a\u308a\u3001\u6c4e\u5316\u6027\u80fd\u304c\u4e0a\u304c\u308b\u3068\u8a00\u3063\u3066\u3044\u308b\u3088\u3046\u3060\u3002\u659c\u3081\u8aad\u307f\u3060\u3051\u3069 https://t.co/0kJRTnVY50"}, "1128910000531726337": {"followers": "98", "datetime": "2019-05-16 06:28:12", "author": "@treasured_write", "content_summary": "RT @arxiv_cs_LG: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. Guangyong Chen, Pengfei C\u2026"}, "1185132292320256002": {"followers": "154", "datetime": "2019-10-18 09:55:31", "author": "@DCasBol", "content_summary": "Revisiting https://t.co/0HtUmfQ4lF --- I always found very unpractical to place the BN just before the ReLU. Shifting data augmentation for CNN with global average pooling doesn't make sense to me either \ud83d\ude05 https://t.co/wZBxu2DiqS"}, "1128982945350475776": {"followers": "111", "datetime": "2019-05-16 11:18:04", "author": "@alsombra7", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129167275624259584": {"followers": "127", "datetime": "2019-05-16 23:30:32", "author": "@GiseopK", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128880301562843136": {"followers": "344", "datetime": "2019-05-16 04:30:12", "author": "@jefkine", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128859588244099072": {"followers": "308", "datetime": "2019-05-16 03:07:53", "author": "@arxiv_cs_LG", "content_summary": "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. Guangyong Chen, Pengfei Chen, Yujun Shi, Chang-Yu Hsieh, Benben Liao, and Shengyu Zhang https://t.co/WPMcuCkuOA"}, "1129031738947072000": {"followers": "99", "datetime": "2019-05-16 14:31:57", "author": "@DSaience", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129053824646230016": {"followers": "2,676", "datetime": "2019-05-16 15:59:43", "author": "@ayirpelle", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128986818169282561": {"followers": "67", "datetime": "2019-05-16 11:33:27", "author": "@dksdc", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129021080818077697": {"followers": "992", "datetime": "2019-05-16 13:49:36", "author": "@datahack_", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128980298635644928": {"followers": "1,286", "datetime": "2019-05-16 11:07:33", "author": "@heghbalz", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129015370151600128": {"followers": "1,939", "datetime": "2019-05-16 13:26:54", "author": "@EldarSilver", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128978753940594688": {"followers": "154", "datetime": "2019-05-16 11:01:25", "author": "@DCasBol", "content_summary": "\"we should not place Batch Normalization before ReLU\" FINALLY SOMEONE SAID IT!"}, "1128933397341102081": {"followers": "771", "datetime": "2019-05-16 08:01:11", "author": "@arxivml", "content_summary": "\"Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks\", Guangyong Chen, \u2026 https://t.co/h035vlaDUh"}, "1129230317175287809": {"followers": "4", "datetime": "2019-05-17 03:41:02", "author": "@tomaxent", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128985998564691968": {"followers": "340", "datetime": "2019-05-16 11:30:12", "author": "@Cedias_", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1191261820830531584": {"followers": "154", "datetime": "2019-11-04 07:52:05", "author": "@DCasBol", "content_summary": "@amArunava @jeremyphoward @rasbt Sure! This is a very interesting one :) https://t.co/0HtUmfQ4lF"}, "1129009989819936768": {"followers": "1,153", "datetime": "2019-05-16 13:05:32", "author": "@jd_mashiro", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129118459978059776": {"followers": "156", "datetime": "2019-05-16 20:16:33", "author": "@bunkei_ac_math", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129023279165988864": {"followers": "217", "datetime": "2019-05-16 13:58:20", "author": "@AssistedEvolve", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128844174357356550": {"followers": "3,878", "datetime": "2019-05-16 02:06:38", "author": "@BrundageBot", "content_summary": "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. Guangyong Chen, Pengfei Chen, Yujun Shi, Chang-Yu Hsieh, Benben Liao, and Shengyu Zhang https://t.co/ZIKb0oamrx"}, "1128919007854305280": {"followers": "2,162", "datetime": "2019-05-16 07:04:00", "author": "@pm_girl", "content_summary": "#Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.LG]) https://t.co/jShrqGdeWk #machinelearning"}, "1129421778298839043": {"followers": "33,955", "datetime": "2019-05-17 16:21:50", "author": "@montrealdotai", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128994105051308032": {"followers": "79", "datetime": "2019-05-16 12:02:24", "author": "@mfharoon", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128824438617792513": {"followers": "9,685", "datetime": "2019-05-16 00:48:13", "author": "@StatMLPapers", "content_summary": "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.LG]) https://t.co/MOn6LSdDHd"}, "1129219212289880064": {"followers": "456", "datetime": "2019-05-17 02:56:54", "author": "@PerthMLGroup", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128877221505060866": {"followers": "1,348", "datetime": "2019-05-16 04:17:57", "author": "@udmrzn", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Netwo\u2026"}, "1128827047873269763": {"followers": "780", "datetime": "2019-05-16 00:58:35", "author": "@disigandalf", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128852595857080321": {"followers": "1,708", "datetime": "2019-05-16 02:40:06", "author": "@arXiv__ml", "content_summary": "#arXiv #machinelearning [cs.LG] Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.LG]) https://t.co/XDMFBT9Yzq In this work, we propose a novel technique to boost training efficiency o"}, "1129070152790814726": {"followers": "561", "datetime": "2019-05-16 17:04:36", "author": "@DrSamirBhatt", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1187183984964329473": {"followers": "1,724", "datetime": "2019-10-24 01:48:13", "author": "@whisponchan", "content_summary": "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks https://t.co/cyyw4QPdGR"}, "1128861068841766913": {"followers": "90", "datetime": "2019-05-16 03:13:46", "author": "@y0shoku", "content_summary": "RT @StatsPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. https://t.co/EkPns5ZHFD"}, "1128984929063149568": {"followers": "206", "datetime": "2019-05-16 11:25:57", "author": "@erik_nijkamp", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128965505056948226": {"followers": "164,106", "datetime": "2019-05-16 10:08:46", "author": "@ceobillionaire", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1232983940442607620": {"followers": "154", "datetime": "2020-02-27 11:00:53", "author": "@DCasBol", "content_summary": "@DeepMind Batch Normalization should be used like BN-conv-ReLU, but in the paper it's used like BN-ReLU-conv, which makes the centering effect of BN useless. https://t.co/0HtUmfQ4lF"}, "1128860853338484736": {"followers": "5,454", "datetime": "2019-05-16 03:12:55", "author": "@StatsPapers", "content_summary": "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. https://t.co/EkPns5ZHFD"}, "1128979826520526848": {"followers": "56,127", "datetime": "2019-05-16 11:05:40", "author": "@IntelligenceTV", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129706881751158784": {"followers": "1,303", "datetime": "2019-05-18 11:14:44", "author": "@arxiv_in_review", "content_summary": "#ICML2019 Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs\\.LG]) https://t.co/FMB2tR86oW"}, "1130636222190063616": {"followers": "692", "datetime": "2019-05-21 00:47:36", "author": "@arxiv_pop", "content_summary": "2019/05/14 \u6295\u7a3f 2\u4f4d LG(Machine Learning) Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks https://t.co/mVsCndiiHj 8 Tweets 32 Retweets 98 Favorites"}, "1129084109144375297": {"followers": "338", "datetime": "2019-05-16 18:00:03", "author": "@arthpajot", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129022367852847104": {"followers": "4,802", "datetime": "2019-05-16 13:54:43", "author": "@cto_maverick", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1129281411846774784": {"followers": "48", "datetime": "2019-05-17 07:04:04", "author": "@lizoratech", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1233373248219336704": {"followers": "217", "datetime": "2020-02-28 12:47:51", "author": "@AssistedEvolve", "content_summary": "RT @DCasBol: @DeepMind Batch Normalization should be used like BN-conv-ReLU, but in the paper it's used like BN-ReLU-conv, which makes the\u2026"}, "1131553101821882370": {"followers": "8", "datetime": "2019-05-23 13:30:57", "author": "@pieorpi1", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128999772679884806": {"followers": "5", "datetime": "2019-05-16 12:24:56", "author": "@thuy_ng_ch", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}, "1128977584446955520": {"followers": "5,735", "datetime": "2019-05-16 10:56:46", "author": "@jm_alexia", "content_summary": "RT @StatMLPapers: Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. (arXiv:1905.05928v1 [cs.\u2026"}}}