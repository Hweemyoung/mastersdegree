{"tab": "twitter", "completed": "1", "twitter": {"1098117606496559104": {"author": "@astro_pyotr", "followers": "912", "datetime": "2019-02-20 07:10:14", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097896206028689409": {"author": "@teenvan1995", "followers": "354", "datetime": "2019-02-19 16:30:28", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1098009471618445314": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-02-20 00:00:32", "content_summary": "RT @shimon8282: New paper from our lab. https://t.co/1ig1jdXxqd"}, "1097785173775507458": {"author": "@linkoffate", "followers": "394", "datetime": "2019-02-19 09:09:15", "content_summary": "RT @Miles_Brundage: \"Fast Efficient Hyperparameter Tuning for Policy Gradients,\" Paul et al.: https://t.co/a4aOaODUmB"}, "1097878799285403648": {"author": "@casdewitt", "followers": "403", "datetime": "2019-02-19 15:21:18", "content_summary": "awesome work on hyperparameter optimisation by colleagues Supratik Paul, @y0b1byte and @shimon8282 at @whi_rl !"}, "1097921359081099264": {"author": "@EricSchles", "followers": "2,077", "datetime": "2019-02-19 18:10:25", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097983994564694017": {"author": "@EricSchles", "followers": "2,077", "datetime": "2019-02-19 22:19:18", "content_summary": "RT @Miles_Brundage: \"Fast Efficient Hyperparameter Tuning for Policy Gradients,\" Paul et al.: https://t.co/a4aOaODUmB"}, "1097748277464727552": {"author": "@ProfAnirban", "followers": "83", "datetime": "2019-02-19 06:42:39", "content_summary": "This sounds super cool! #deeplearning"}, "1097867883042881549": {"author": "@bcmcmahan", "followers": "563", "datetime": "2019-02-19 14:37:55", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1099833192486326273": {"author": "@arxiv_pop", "followers": "708", "datetime": "2019-02-25 00:47:21", "content_summary": "2019/02/18 \u6295\u7a3f 3\u4f4d LG(Machine Learning) Fast Efficient Hyperparameter Tuning for Policy Gradients https://t.co/fMRdst7GQ8 7 Tweets 25 Retweets 136 Favorites"}, "1099220995255296000": {"author": "@arxiv_in_review", "followers": "1,313", "datetime": "2019-02-23 08:14:42", "content_summary": "#ICML2019 Fast Efficient Hyperparameter Tuning for Policy Gradients. (arXiv:1902.06583v1 [cs\\.LG]) https://t.co/7tXGADhy6x"}, "1098106776304652294": {"author": "@Illarionov_msu", "followers": "141", "datetime": "2019-02-20 06:27:11", "content_summary": "RT @shimon8282: New paper from our lab. https://t.co/1ig1jdXxqd"}, "1192096797017214977": {"author": "@whi_rl", "followers": "3,231", "datetime": "2019-11-06 15:09:59", "content_summary": "Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally expensive hyperparameter search for policy gradient methods! Blog: https://t.co/8vDToXvN6p Paper: https://t.co/Oa3SXyaSdD Code:"}, "1203686213347168256": {"author": "@whi_rl", "followers": "3,231", "datetime": "2019-12-08 14:42:11", "content_summary": "\u201cFast Efficient Hyperparameter Tuning for Policy Gradient Methods\u201d \u2013 Supratik Paul, @y0b1byte, @shimon8282 (Poster #206, Wed morning) Paper: https://t.co/Oa3SXyaSdD Blog: https://t.co/8vDToXvN6p https://t.co/DYbFBDZkAN"}, "1097720258511683584": {"author": "@arxivml", "followers": "780", "datetime": "2019-02-19 04:51:18", "content_summary": "\"Fast Efficient Hyperparameter Tuning for Policy Gradients\", Supratik Paul, Vitaly Kurin, Shimon Whiteson https://t.co/AstEwRQXlC"}, "1097937403325620225": {"author": "@mimoralea", "followers": "661", "datetime": "2019-02-19 19:14:10", "content_summary": "RT @shimon8282: New paper from our lab. https://t.co/1ig1jdXxqd"}, "1097862967482376193": {"author": "@whi_rl", "followers": "3,231", "datetime": "2019-02-19 14:18:23", "content_summary": "New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free meta-learning algorithm that automatically learns an optimal schedule for hyperparameters in policy gradient methods. Preprint:"}, "1097892661879427072": {"author": "@ivenzor", "followers": "443", "datetime": "2019-02-19 16:16:23", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097692827910324225": {"author": "@arxiv_cs_LG", "followers": "318", "datetime": "2019-02-19 03:02:18", "content_summary": "Fast Efficient Hyperparameter Tuning for Policy Gradients. Supratik Paul, Vitaly Kurin, and Shimon Whiteson https://t.co/wADLOAg1Z0"}, "1097863212027076610": {"author": "@d_kangin", "followers": "118", "datetime": "2019-02-19 14:19:21", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1192097306910306304": {"author": "@y0b1byte", "followers": "1,954", "datetime": "2019-11-06 15:12:00", "content_summary": "Check out HOOF, HOOF is amazing! https://t.co/QbjpHkDJto"}, "1098235246103785472": {"author": "@cghosh_", "followers": "279", "datetime": "2019-02-20 14:57:41", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097852614396559360": {"author": "@casdewitt", "followers": "403", "datetime": "2019-02-19 13:37:15", "content_summary": "exciting new work on hyperparameter tuning for RL by colleagues Supratik Paul, @yobibyte and @shimon8282 at @whi_rl"}, "1192435182982164480": {"author": "@subhobrata1", "followers": "306", "datetime": "2019-11-07 13:34:36", "content_summary": "RT @whi_rl: Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally\u2026"}, "1097860047198191617": {"author": "@y0b1byte", "followers": "1,954", "datetime": "2019-02-19 14:06:47", "content_summary": "Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.co/nHMdG4TV1D), our latest work @whi_rl with Supratik Paul, @y0b1byte and @shimon8282."}, "1097890529159856128": {"author": "@pranjaltandon2", "followers": "144", "datetime": "2019-02-19 16:07:54", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097873266923003904": {"author": "@FeryalMP", "followers": "5,860", "datetime": "2019-02-19 14:59:18", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097867365252059137": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-02-19 14:35:51", "content_summary": "RT @Miles_Brundage: \"Fast Efficient Hyperparameter Tuning for Policy Gradients,\" Paul et al.: https://t.co/a4aOaODUmB"}, "1192154261385334785": {"author": "@shimon8282", "followers": "7,472", "datetime": "2019-11-06 18:58:19", "content_summary": "RT @whi_rl: Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally\u2026"}, "1192102303345586176": {"author": "@_rockt", "followers": "14,109", "datetime": "2019-11-06 15:31:51", "content_summary": "RT @whi_rl: Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally\u2026"}, "1098031012867497984": {"author": "@KouroshMeshgi", "followers": "619", "datetime": "2019-02-20 01:26:08", "content_summary": "RT @shimon8282: New paper from our lab. https://t.co/1ig1jdXxqd"}, "1097875533713395715": {"author": "@letranger14", "followers": "325", "datetime": "2019-02-19 15:08:19", "content_summary": "RT @Miles_Brundage: \"Fast Efficient Hyperparameter Tuning for Policy Gradients,\" Paul et al.: https://t.co/a4aOaODUmB"}, "1097915259694780417": {"author": "@JeanMarcJAzzi", "followers": "365", "datetime": "2019-02-19 17:46:10", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097884402854645760": {"author": "@nntsn", "followers": "788", "datetime": "2019-02-19 15:43:34", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1192155665885777922": {"author": "@RokoMijicUK", "followers": "1,490", "datetime": "2019-11-06 19:03:54", "content_summary": "RT @whi_rl: Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally\u2026"}, "1097916475845689344": {"author": "@Miles_Brundage", "followers": "25,873", "datetime": "2019-02-19 17:51:00", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097740719295389697": {"author": "@Miles_Brundage", "followers": "25,873", "datetime": "2019-02-19 06:12:37", "content_summary": "\"Fast Efficient Hyperparameter Tuning for Policy Gradients,\" Paul et al.: https://t.co/a4aOaODUmB"}, "1104413143260815360": {"author": "@HariharasudhanA", "followers": "47", "datetime": "2019-03-09 16:06:27", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1098061104868216832": {"author": "@stl950116", "followers": "11", "datetime": "2019-02-20 03:25:43", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1177400917164847104": {"author": "@y0b1byte", "followers": "1,954", "datetime": "2019-09-27 01:53:48", "content_summary": "@EugeneVinitsky Show them HOOF! https://t.co/1B4DxDS8Ta"}, "1098087678589906944": {"author": "@goloskokovic", "followers": "128", "datetime": "2019-02-20 05:11:18", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097863002009845765": {"author": "@whi_rl", "followers": "3,231", "datetime": "2019-02-19 14:18:31", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097914078503292928": {"author": "@unsorsodicorda", "followers": "740", "datetime": "2019-02-19 17:41:29", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097772564053667840": {"author": "@Ashot_", "followers": "20,838", "datetime": "2019-02-19 08:19:09", "content_summary": "RT @ProfAnirban: This sounds super cool! #deeplearning https://t.co/ST8bBXkIpd"}, "1097740978071339009": {"author": "@BedabrataChoud1", "followers": "150", "datetime": "2019-02-19 06:13:38", "content_summary": "RT @Miles_Brundage: \"Fast Efficient Hyperparameter Tuning for Policy Gradients,\" Paul et al.: https://t.co/a4aOaODUmB"}, "1192396295643770880": {"author": "@PMinervini", "followers": "1,339", "datetime": "2019-11-07 11:00:05", "content_summary": "RT @whi_rl: Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally\u2026"}, "1097864179850072065": {"author": "@y0b1byte", "followers": "1,954", "datetime": "2019-02-19 14:23:12", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097872146825707520": {"author": "@_el_cangrejo", "followers": "12", "datetime": "2019-02-19 14:54:51", "content_summary": "RT @y0b1byte: Always got confused about how to choose hyperparameters for your RL algorithm? Try HOOF (https://t.co/1B4DxE9KhK) https://t.c\u2026"}, "1097982040895561728": {"author": "@deepcontrolai", "followers": "20", "datetime": "2019-02-19 22:11:32", "content_summary": "RT @whi_rl: New work by S Paul, V Kurin @y0b1byte, S Whiteson @shimon8282: Hyperparameter Optimisation on the Fly (HOOF) is a gradient-free\u2026"}, "1097886689798832128": {"author": "@shimon8282", "followers": "7,472", "datetime": "2019-02-19 15:52:39", "content_summary": "New paper from our lab."}, "1192120882937913345": {"author": "@crosspacific1", "followers": "0", "datetime": "2019-11-06 16:45:41", "content_summary": "RT @whi_rl: Check out our latest blog post on HOOF (to appear at #NeurIPS2019) designed to save you from doing finicky and computationally\u2026"}, "1097693566279868416": {"author": "@BrundageBot", "followers": "3,889", "datetime": "2019-02-19 03:05:15", "content_summary": "Fast Efficient Hyperparameter Tuning for Policy Gradients. Supratik Paul, Vitaly Kurin, and Shimon Whiteson https://t.co/sskRjOvUZp"}}, "citation_id": "55703206", "queriedAt": "2020-06-03 23:44:51"}