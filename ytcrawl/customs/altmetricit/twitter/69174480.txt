{"citation_id": "69174480", "queriedAt": "2020-05-09 12:34:48", "completed": "0", "twitter": {"1256584371059990530": {"followers": "324", "content_summary": "Great paper and even better talk. The first 5-10 minutes are the best explanation what happened in NLP in the last 2-3 years, I have seen. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/6Xp6wGXlvH https://t", "author": "@simecek", "datetime": "2020-05-02 14:00:34"}, "1257226496222965761": {"followers": "519", "content_summary": "@StephenLLH @voxmenthe @huggingface Depends which version. The largest version of T5 beats BERT and friends in quite a few tasks \ud83d\ude00 You can find this in the paper. https://t.co/LTVXBTQsLP", "author": "@AND__SO", "datetime": "2020-05-04 08:32:09"}, "1255652743227695104": {"followers": "360", "content_summary": "@hadyelsahar @GoogleAI But then models have gotten even bigger like the 11B parameter model from text to text transformer. https://t.co/E4rTP0G8FU which has shown large improvements over many benchmarks.", "author": "@_arohan_", "datetime": "2020-04-30 00:18:37"}, "1256573573457743873": {"followers": "504", "content_summary": "Brno Reading Group is not taking place due COVID-19. If you want to read ML paper together, you can take advantage of DairAI Zoom paper reading session. Starting right now. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author": "@mlmucz", "datetime": "2020-05-02 13:17:40"}}, "tab": "twitter"}