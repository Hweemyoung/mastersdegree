{"tab": "twitter", "completed": "1", "twitter": {"1095877997230542848": {"author": "@arxiv_cs_LG", "followers": "322", "datetime": "2019-02-14 02:50:49", "content_summary": "Stochastic Gradient Descent Escapes Saddle Points Efficiently. Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan https://t.co/AcwfiSLzcn"}, "1095923564564357120": {"author": "@arxivml", "followers": "787", "datetime": "2019-02-14 05:51:53", "content_summary": "\"Stochastic Gradient Descent Escapes Saddle Points Efficiently\", Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M\uff0e Kak\u2026 https://t.co/5yKe7vAoAe"}, "1096304924554604544": {"author": "@bazile", "followers": "127", "datetime": "2019-02-15 07:07:17", "content_summary": "RT @StatMLPapers: Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/rDCernUJwQ"}, "1095867403328602112": {"author": "@sysc_kt", "followers": "307", "datetime": "2019-02-14 02:08:43", "content_summary": "RT @StatMLPapers: Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/rDCernUJwQ"}, "1096054176734474240": {"author": "@jastner109", "followers": "194", "datetime": "2019-02-14 14:30:54", "content_summary": "RT @StatMLPapers: Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/rDCernUJwQ"}, "1096743484986257408": {"author": "@owltrainlab", "followers": "45", "datetime": "2019-02-16 12:09:57", "content_summary": "Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/j7b049CKEY #papers- ai #ml #feedly"}, "1095875832596320262": {"author": "@arXiv__ml", "followers": "1,803", "datetime": "2019-02-14 02:42:13", "content_summary": "#arXiv #machinelearning [cs.LG] Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/tmLLzoPuJ6 This paper considers the perturbed stochastic gradient descent algorithm and shows that it finds $\\epsilon$"}, "1095867107680542722": {"author": "@TJO_datasci", "followers": "39,633", "datetime": "2019-02-14 02:07:33", "content_summary": "RT @StatMLPapers: Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/rDCernUJwQ"}, "1095876686695067648": {"author": "@mlmemoirs", "followers": "1,287", "datetime": "2019-02-14 02:45:37", "content_summary": "#arXiv #machinelearning [cs.LG] Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/hWnfwUqEC5 This paper considers the perturbed stochastic gradient descent algorithm and shows that it finds $\\epsilon$"}, "1095864368716226560": {"author": "@StatMLPapers", "followers": "9,764", "datetime": "2019-02-14 01:56:40", "content_summary": "Stochastic Gradient Descent Escapes Saddle Points Efficiently. (arXiv:1902.04811v1 [cs.LG]) https://t.co/rDCernUJwQ"}}, "citation_id": "55431280", "queriedAt": "2020-06-03 23:20:53"}