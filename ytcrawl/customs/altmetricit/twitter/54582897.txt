{"twitter": {"1089724761226334208": {"author": "@TheCuriousLuke", "datetime": "2019-01-28 03:20:03", "content_summary": "RT @mlmemoirs: #arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/dIs88\u2026", "followers": "4,494"}, "1089708728713129984": {"author": "@msarozz", "datetime": "2019-01-28 02:16:21", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/wtN3K\u2026", "followers": "1,802"}, "1154673295607967745": {"author": "@deepsemantic", "datetime": "2019-07-26 08:42:21", "content_summary": "I will give a poster presentation of our work: State-Regularized Recurrent Neural Networks(https://t.co/IGIuGfOWPX) at ACL BlackboxNLP 2019. Code: https://t.co/u59pRuIHOf https://t.co/Oj0ZSFOV0H", "followers": "17"}, "1089785987167866880": {"author": "@JMAi1729", "datetime": "2019-01-28 07:23:21", "content_summary": "https://t.co/IVNOdbASpO", "followers": "4"}, "1089708519576780801": {"author": "@arXiv__ml", "datetime": "2019-01-28 02:15:31", "content_summary": "#arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/wtN3KdVqXd Recurrent neural networks are a widely used class of neural architectures. They have, however, two shortcomings. First, it is", "followers": "1,660"}, "1188681571127189505": {"author": "@caro__lawrence", "datetime": "2019-10-28 04:59:05", "content_summary": "RT @Mniepert: Really like this type of work. My hope is more people will get interested. We showed in our paper the equivalence between pro\u2026", "followers": "138"}, "1089762967430807552": {"author": "@arxivml", "datetime": "2019-01-28 05:51:52", "content_summary": "\"State-Regularized Recurrent Neural Networks\", Cheng Wang, Mathias Niepert https://t.co/HSGNPkuEmv", "followers": "767"}, "1089700456698396672": {"author": "@StatMLPapers", "datetime": "2019-01-28 01:43:29", "content_summary": "State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/w3Zz8g7Nar", "followers": "9,641"}, "1089708869411061761": {"author": "@machine_ml", "datetime": "2019-01-28 02:16:54", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/wtN3K\u2026", "followers": "9,218"}, "1089721579192373250": {"author": "@mlmemoirs", "datetime": "2019-01-28 03:07:25", "content_summary": "#arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/dIs88NEF07 Recurrent neural networks are a widely used class of neural architectures. They have, however, two shortcomings. First, it is", "followers": "1,231"}, "1188513817585692672": {"author": "@heghbalz", "datetime": "2019-10-27 17:52:30", "content_summary": "RT @Mniepert: Really like this type of work. My hope is more people will get interested. We showed in our paper the equivalence between pro\u2026", "followers": "1,280"}, "1089768875036573696": {"author": "@treasured_write", "datetime": "2019-01-28 06:15:21", "content_summary": "RT @arxiv_cs_LG: State-Regularized Recurrent Neural Networks. Cheng Wang and Mathias Niepert https://t.co/RdbST8WZhn", "followers": "96"}, "1188513618171715586": {"author": "@Mniepert", "datetime": "2019-10-27 17:51:42", "content_summary": "Really like this type of work. My hope is more people will get interested. We showed in our paper the equivalence between probabilistic FSMs and RNNs with attention based stochastic transition functions. https://t.co/CTBINJnS9Q", "followers": "808"}, "1089965008337940481": {"author": "@arxiv_in_review", "datetime": "2019-01-28 19:14:43", "content_summary": "#ICML2019 State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs\\.LG]) https://t.co/I7SHm03rmf", "followers": "1,295"}, "1189922390622580736": {"author": "@Mniepert", "datetime": "2019-10-31 15:09:40", "content_summary": "@gail_w @yoavgo Great work. We also introduced a method for learning PDFAs that exactly model the transition behavior of RNNs with a max number of states. https://t.co/CTBINJnS9Q Changing the soft max temperature moves the behavior of a PDFA to a DFA. (see", "followers": "808"}, "1089937489085849606": {"author": "@udmrzn", "datetime": "2019-01-28 17:25:22", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/wtN3K\u2026", "followers": "1,348"}, "1089721308978561025": {"author": "@arxiv_cs_LG", "datetime": "2019-01-28 03:06:20", "content_summary": "State-Regularized Recurrent Neural Networks. Cheng Wang and Mathias Niepert https://t.co/RdbST8WZhn", "followers": "304"}, "1089772123747278848": {"author": "@treasured_write", "datetime": "2019-01-28 06:28:15", "content_summary": "RT @BrundageBot: State-Regularized Recurrent Neural Networks. Cheng Wang and Mathias Niepert https://t.co/46UOXECAHQ", "followers": "96"}, "1089713019188363264": {"author": "@BrundageBot", "datetime": "2019-01-28 02:33:24", "content_summary": "State-Regularized Recurrent Neural Networks. Cheng Wang and Mathias Niepert https://t.co/46UOXECAHQ", "followers": "3,838"}, "1089821991442366464": {"author": "@ham_gretsky", "datetime": "2019-01-28 09:46:25", "content_summary": "RT @mlmemoirs: #arXiv #machinelearning [cs.LG] State-Regularized Recurrent Neural Networks. (arXiv:1901.08817v1 [cs.LG]) https://t.co/dIs88\u2026", "followers": "18,427"}}, "queriedAt": "2020-05-21 20:10:18", "completed": "1", "citation_id": "54582897", "tab": "twitter"}