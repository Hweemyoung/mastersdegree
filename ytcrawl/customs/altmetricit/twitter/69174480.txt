{"citation_id": "69174480", "completed": "1", "queriedAt": "2020-05-14 13:51:21", "tab": "twitter", "twitter": {"1189372373704810496": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "9", "datetime": "2019-10-30 02:44:06", "author": "@SlobodanVuceti4"}, "1188211786840821760": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "640", "datetime": "2019-10-26 21:52:20", "author": "@akkikiki"}, "1187519435852042240": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "456", "datetime": "2019-10-25 00:01:11", "author": "@PerthMLGroup"}, "1245505070755536896": {"content_summary": "RT @Thom_Wolf: Notebook with demo: https://t.co/OX1eWMtMvL Release notes for Transformers 2.7.0: https://t.co/JDBmxHFdY8 T5 paper on \"Exp\u2026", "followers": "165", "datetime": "2020-04-02 00:15:23", "author": "@samurairodeo"}, "1187202029623500800": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "415", "datetime": "2019-10-24 02:59:55", "author": "@__nggih"}, "1188396029537746944": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "209", "datetime": "2019-10-27 10:04:27", "author": "@s3works"}, "1187718258935230464": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "723", "datetime": "2019-10-25 13:11:14", "author": "@tsubame959"}, "1187249761738313728": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "151,963", "datetime": "2019-10-24 06:09:35", "author": "@JeffDean"}, "1187922188755787776": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-26 02:41:34", "author": "@arxiv_cscl"}, "1188493152010682368": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "294", "datetime": "2019-10-27 16:30:23", "author": "@rose_miura"}, "1187621269845499905": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "99", "datetime": "2019-10-25 06:45:50", "author": "@adssidhu86"}, "1187186074889707520": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "129", "datetime": "2019-10-24 01:56:31", "author": "@itscientist"}, "1187227027692146694": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "222", "datetime": "2019-10-24 04:39:15", "author": "@deepak_s_rawat"}, "1187558205016170498": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "233", "datetime": "2019-10-25 02:35:14", "author": "@royam0820"}, "1187884490565308416": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5", "datetime": "2019-10-26 00:11:47", "author": "@LiamCroteau"}, "1187395254200045569": {"content_summary": "Great work! Text to text Transformer with a masking loss does better than other Transfer learning techniques.", "followers": "1,967", "datetime": "2019-10-24 15:47:43", "author": "@nikiparmar09"}, "1187646950738481152": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "299", "datetime": "2019-10-25 08:27:53", "author": "@500zainraza"}, "1187166912699977728": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,663", "datetime": "2019-10-24 00:40:23", "author": "@sigitpurnomo"}, "1187846726243123200": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "1,878", "datetime": "2019-10-25 21:41:43", "author": "@sesquipedale"}, "1198046221333651457": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "187", "datetime": "2019-11-23 01:10:52", "author": "@ryanzhumich"}, "1187266084706996224": {"content_summary": "Mother of all NLP research!", "followers": "381", "datetime": "2019-10-24 07:14:27", "author": "@divyeshp7"}, "1187424829533773825": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "593", "datetime": "2019-10-24 17:45:15", "author": "@moss"}, "1187196469641654272": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4,434", "datetime": "2019-10-24 02:37:50", "author": "@jonathanfly"}, "1187370863450116098": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "218", "datetime": "2019-10-24 14:10:48", "author": "@AssistedEvolve"}, "1187491009682444288": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "35", "datetime": "2019-10-24 22:08:13", "author": "@lena_uskova"}, "1187423784200003584": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "86", "datetime": "2019-10-24 17:41:06", "author": "@tiagomluis"}, "1194955993601298435": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "9", "datetime": "2019-11-14 12:31:24", "author": "@do_work_brain"}, "1187889502842425345": {"content_summary": "Most popular computer science paper of the day: \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" https://t.co/EsxQnm3uUR https://t.co/4kDPtE1mBD", "followers": "280", "datetime": "2019-10-26 00:31:42", "author": "@HotCompScience"}, "1187868450707787776": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "16", "datetime": "2019-10-25 23:08:02", "author": "@pro_about_pro"}, "1188403129768914944": {"content_summary": "RT @arxiv_cscl: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "11,930", "datetime": "2019-10-27 10:32:40", "author": "@Rosenchild"}, "1187771003742515200": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "54", "datetime": "2019-10-25 16:40:49", "author": "@ainewsantenna"}, "1187184128929783809": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu https://t.co/vvsFTgubby", "followers": "3,882", "datetime": "2019-10-24 01:48:47", "author": "@BrundageBot"}, "1192665540151504896": {"content_summary": "https://t.co/0p9ZBJddzz #TransferLearning", "followers": "180", "datetime": "2019-11-08 04:49:58", "author": "@iVanPeer"}, "1187353358102941697": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "6,225", "datetime": "2019-10-24 13:01:15", "author": "@alienelf"}, "1187416137153306624": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "2,262", "datetime": "2019-10-24 17:10:42", "author": "@AdilMouja"}, "1188118272899719171": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-26 15:40:45", "author": "@arxiv_cscl"}, "1187989096767115268": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,649", "datetime": "2019-10-26 07:07:27", "author": "@alexk_z"}, "1187937184910737408": {"content_summary": "Another \"extractive\" summary of the same paper. :)", "followers": "128", "datetime": "2019-10-26 03:41:10", "author": "@ekshakhs"}, "1187326432697032704": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "29", "datetime": "2019-10-24 11:14:15", "author": "@summerstay1"}, "1187571508056444929": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "13", "datetime": "2019-10-25 03:28:06", "author": "@singhkrrohit"}, "1187169051920891904": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "25,578", "datetime": "2019-10-24 00:48:53", "author": "@Miles_Brundage"}, "1190050622437036033": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "2,937", "datetime": "2019-10-31 23:39:13", "author": "@ServerlessFan"}, "1194047558294749184": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "209", "datetime": "2019-11-12 00:21:36", "author": "@thebrahminator"}, "1187368990395396096": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "331", "datetime": "2019-10-24 14:03:22", "author": "@pywirrarika"}, "1245514485533569025": {"content_summary": "RT @icoxfog417: Hugging Face\u306eTransformer\u306b\u8907\u6570\u30bf\u30b9\u30af\u3092\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u5b66\u7fd2\u3057\u305fT5\u306e\u30e2\u30c7\u30eb\u304c\u642d\u8f09\u3002\u30b5\u30f3\u30d7\u30eb\u306eNotebook\u3082\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u3002", "followers": "12,765", "datetime": "2020-04-02 00:52:48", "author": "@jaguring1"}, "1187485077342568448": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "122", "datetime": "2019-10-24 21:44:39", "author": "@hustwj"}, "1187391826392735744": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "32", "datetime": "2019-10-24 15:34:06", "author": "@numPerfecto28"}, "1187201334703001600": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "52", "datetime": "2019-10-24 02:57:09", "author": "@NamanGoyal21"}, "1187502730438303746": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "177", "datetime": "2019-10-24 22:54:48", "author": "@meshidenn"}, "1187258441464594432": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "710", "datetime": "2019-10-24 06:44:05", "author": "@snehaark"}, "1188109312129933312": {"content_summary": "\u7d71\u4e00\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u30c6\u30ad\u30b9\u30c8\u3078\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3092\u4f7f\u7528\u3057\u305f\u8ee2\u79fb\u5b66\u7fd2\u306e\u9650\u754c\u306e\u8abf\u67fb\u3000\u8981\u7d04\u3001\u8cea\u554f\u3078\u306e\u56de\u7b54\u3001\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u306a\u3069\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406(NLP)\u3092\u3001\u57fa\u790e\u7684\u306a\u624b\u6cd5\u3067\u3042\u308b\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5de7\u307f\u306a\u7d44\u5408\u305b\u3092\u30d9\u30fc\u30b9\u3068\u3057\u305f\u624b\u6cd5\u3067\u3001\u512a\u308c\u305f\u6c4e\u7528\u6027\u3068\u7cbe\u5ea6\u3092\u5b9f\u73fe\u3002\u8a73\u7d30\u30ec\u30b7\u30d4\u3068\u5b9f\u88c5\u3042\u308a\u3002https://t.co/b2AcLWf2Vx", "followers": "26", "datetime": "2019-10-26 15:05:08", "author": "@KwhRd100"}, "1187638377572646912": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5,315", "datetime": "2019-10-25 07:53:49", "author": "@HubBucket"}, "1187428421124665345": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "283", "datetime": "2019-10-24 17:59:31", "author": "@ludelcorro"}, "1187648131090878464": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "241", "datetime": "2019-10-25 08:32:34", "author": "@sabarinathan_7"}, "1187310089088028673": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2", "datetime": "2019-10-24 10:09:19", "author": "@trees_random"}, "1188314587621408768": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-27 04:40:50", "author": "@arxiv_cscl"}, "1187514212915896320": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "670", "datetime": "2019-10-24 23:40:25", "author": "@GoDominic"}, "1187330702725079040": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,288", "datetime": "2019-10-24 11:31:13", "author": "@heghbalz"}, "1187540861468344321": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "264", "datetime": "2019-10-25 01:26:19", "author": "@swordfive55"}, "1255652743227695104": {"content_summary": "@hadyelsahar @GoogleAI But then models have gotten even bigger like the 11B parameter model from text to text transformer. https://t.co/E4rTP0G8FU which has shown large improvements over many benchmarks.", "followers": "366", "datetime": "2020-04-30 00:18:37", "author": "@_arohan_"}, "1187833354294575105": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,304", "datetime": "2019-10-25 20:48:35", "author": "@pierre_guillou"}, "1187180249953525760": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "874", "datetime": "2019-10-24 01:33:22", "author": "@mrdrozdov"}, "1187207874415607809": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "458", "datetime": "2019-10-24 03:23:09", "author": "@sajidshahbs"}, "1187564356151599104": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "209", "datetime": "2019-10-25 02:59:41", "author": "@s3works"}, "1228794204924583936": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "164,079", "datetime": "2020-02-15 21:32:23", "author": "@ceobillionaire"}, "1187541248141250560": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "69", "datetime": "2019-10-25 01:27:51", "author": "@trammyyy"}, "1190579534392778752": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "509", "datetime": "2019-11-02 10:40:55", "author": "@bhagirathl"}, "1187601771146944512": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "306", "datetime": "2019-10-25 05:28:21", "author": "@juarelerrr"}, "1187705596046393344": {"content_summary": "RT @Quantum_Stat: THE END OF SESAME STREET: Google's T5 Transformer released yesterday. Achieves SOTA results on SuperGlue, 0.9 away from h\u2026", "followers": "456", "datetime": "2019-10-25 12:20:55", "author": "@abhibisht89"}, "1187172601967169536": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "245", "datetime": "2019-10-24 01:02:59", "author": "@MarkTan57229491"}, "1188544527595753472": {"content_summary": "Wondering how much co2 the experiments in the paper produced... really appreciate their great work and share with the community Best work over BERT I ever read this year!", "followers": "5", "datetime": "2019-10-27 19:54:32", "author": "@Hepeng2012"}, "1202020432351641601": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "118", "datetime": "2019-12-04 00:22:58", "author": "@Shahroz07"}, "1200071964997701633": {"content_summary": "T5 by google explores the field of transfer learning in NLP. Very good systematic study on how to pretrain and transfer transformer models for downstream tasks: https://t.co/7DefPHTP0f cc @fbk_mt https://t.co/J0QpUOa64U", "followers": "183", "datetime": "2019-11-28 15:20:27", "author": "@AT_Amir"}, "1187314960033931265": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "127", "datetime": "2019-10-24 10:28:40", "author": "@venomsnake006"}, "1187168017693757441": {"content_summary": "93.8 on Winograd Schema Challenge \ud83d\udc40", "followers": "337", "datetime": "2019-10-24 00:44:46", "author": "@iamasharkskin"}, "1194627331362455553": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "20", "datetime": "2019-11-13 14:45:25", "author": "@JoshDashes"}, "1187723890367315968": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "136", "datetime": "2019-10-25 13:33:36", "author": "@samarthbhargav"}, "1187195507749326848": {"content_summary": "New transformer craziness! Bringing back decoder modules, systematic exploration of training decisions, top of the leaderboard on superGLUE, & more. Also very thrilling to see a fellow Oberlin comp sci alumn out here", "followers": "302", "datetime": "2019-10-24 02:34:00", "author": "@societyoftrees"}, "1187415626026848262": {"content_summary": "\ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-text format.\" https://t.co/znJ5DefCoG https://t.co/pBZrdompF9", "followers": "12,924", "datetime": "2019-10-24 17:08:41", "author": "@deliprao"}, "1187342295269556224": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "443", "datetime": "2019-10-24 12:17:17", "author": "@georgecushen"}, "1187474812362350593": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,328", "datetime": "2019-10-24 21:03:52", "author": "@joehalliwell"}, "1188453714631237638": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "330", "datetime": "2019-10-27 13:53:40", "author": "@mr_ubik"}, "1187441914733158400": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "2,828", "datetime": "2019-10-24 18:53:08", "author": "@WonderMicky"}, "1187544483908538371": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-25 01:40:43", "author": "@arxiv_cscl"}, "1187681103017992192": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "824", "datetime": "2019-10-25 10:43:35", "author": "@morioka"}, "1187514355790696449": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "218", "datetime": "2019-10-24 23:41:00", "author": "@AssistedEvolve"}, "1195388426696024064": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "98", "datetime": "2019-11-15 17:09:44", "author": "@shsriva"}, "1187193355978936320": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "628", "datetime": "2019-10-24 02:25:27", "author": "@anselmlevskaya"}, "1187775867990282240": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/Q0cbbZx2Pe (Credit: Suhas Pai)", "followers": "907", "datetime": "2019-10-25 17:00:09", "author": "@AISC_TO"}, "1194044372603551744": {"content_summary": "Looks really interesting!", "followers": "100", "datetime": "2019-11-12 00:08:57", "author": "@mrdanieldsouza"}, "1187362450598838276": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "216", "datetime": "2019-10-24 13:37:23", "author": "@KSKSKSKS2"}, "1192328401962577920": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,455", "datetime": "2019-11-07 06:30:18", "author": "@XOR0sha2wine"}, "1188238678822395904": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,333", "datetime": "2019-10-26 23:39:12", "author": "@clured"}, "1187343872663400448": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "590", "datetime": "2019-10-24 12:23:33", "author": "@codekee"}, "1193147577681743872": {"content_summary": "#goodread Google's releases new #transferlearning research on it's #NLP #T5 #model is pretty sweet #ai #datascience #explainability https://t.co/z7ZrQsjuzx", "followers": "2,910", "datetime": "2019-11-09 12:45:24", "author": "@carsondahlberg"}, "1187486745715068929": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,767", "datetime": "2019-10-24 21:51:17", "author": "@sravsatuluri"}, "1187669384313147392": {"content_summary": "RT @jmhessel: Table 15 from T5 might be the most computationally expensive table ever constructed in the history of natural language proces\u2026", "followers": "590", "datetime": "2019-10-25 09:57:01", "author": "@codekee"}, "1187653215040335872": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "554", "datetime": "2019-10-25 08:52:46", "author": "@FBWM8888"}, "1187534947839426560": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "162", "datetime": "2019-10-25 01:02:49", "author": "@AjayyXc"}, "1187243064747646976": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "17", "datetime": "2019-10-24 05:42:59", "author": "@ryokkie"}, "1187253195371929600": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "527", "datetime": "2019-10-24 06:23:14", "author": "@MysteryHacker1"}, "1187244018545156098": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "43", "datetime": "2019-10-24 05:46:46", "author": "@xavier_gastaldi"}, "1233693186377293826": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,437", "datetime": "2020-02-29 09:59:11", "author": "@EmilStenstrom"}, "1188333249329688577": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "13", "datetime": "2019-10-27 05:54:59", "author": "@GiangTr24025714"}, "1203313602372890624": {"content_summary": "text-to-text transfer transformer \u3081\u3063\u3061\u3083\u826f\u3044\u3002\u5922\u304c\u5e83\u304c\u308b\u3057\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5e83\u304c\u308b\u3002BART \u3082\u4f3c\u305f\u3088\u3046\u306a\u8a71\u3060\u3051\u3069 T5 \u306f\u8ad6\u6587\u81ea\u4f53\u304c\u3081\u3061\u3083\u304f\u3061\u3083\u9577\u3044\u5206\u3001\u6bd4\u8f03\u3068\u304b\u8003\u5bdf\u306b\u5bcc\u3093\u3067\u3066\u304b\u306a\u308a\u8aad\u307f\u5fdc\u3048\u3042\u308b\u3002 https://t.co/9qCkluBHyG", "followers": "662", "datetime": "2019-12-07 14:01:33", "author": "@agatan_"}, "1188226937862459392": {"content_summary": "RT @arxiv_cscl: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "292", "datetime": "2019-10-26 22:52:32", "author": "@Shujian_Liu"}, "1187590021575892992": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "17", "datetime": "2019-10-25 04:41:40", "author": "@pidueck"}, "1187393916154384387": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "155", "datetime": "2019-10-24 15:42:24", "author": "@forodeeplearn"}, "1187265957825261568": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "10", "datetime": "2019-10-24 07:13:57", "author": "@mateusz_ozga"}, "1187354878575706112": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "46", "datetime": "2019-10-24 13:07:17", "author": "@helboukkouri"}, "1187361524303745026": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "6,225", "datetime": "2019-10-24 13:33:42", "author": "@alienelf"}, "1188326039979618305": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "7,671", "datetime": "2019-10-27 05:26:20", "author": "@ynupc"}, "1187306129753174016": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "182", "datetime": "2019-10-24 09:53:35", "author": "@Dak_ssh"}, "1187583584384667648": {"content_summary": "RT @nikiparmar09: Great work! Text to text Transformer with a masking loss does better than other Transfer learning techniques. https://t.c\u2026", "followers": "302", "datetime": "2019-10-25 04:16:05", "author": "@subhobrata1"}, "1187201191945523200": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,976", "datetime": "2019-10-24 02:56:35", "author": "@StrongDuality"}, "1188404676942807041": {"content_summary": "RT @arxiv_cscl: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "11,930", "datetime": "2019-10-27 10:38:49", "author": "@Rosenchild"}, "1187242322339209217": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "28,418", "datetime": "2019-10-24 05:40:02", "author": "@ogrisel"}, "1187587947702431744": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "500", "datetime": "2019-10-25 04:33:25", "author": "@sakak"}, "1187406708966154241": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "202", "datetime": "2019-10-24 16:33:15", "author": "@AngryEgyptianX"}, "1187486611103027202": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3", "datetime": "2019-10-24 21:50:45", "author": "@bellar3464"}, "1188173819401256960": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "63", "datetime": "2019-10-26 19:21:28", "author": "@dhama108"}, "1187331309384929281": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,217", "datetime": "2019-10-24 11:33:38", "author": "@pragmaticml"}, "1187391085682647040": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "4,460", "datetime": "2019-10-24 15:31:10", "author": "@ada_rob"}, "1191478670226944001": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "407", "datetime": "2019-11-04 22:13:46", "author": "@nishantiam"}, "1189374966749224963": {"content_summary": "RT @fhiyo_: GLUE, SuperGLUE\u3068\u304b\u3067\u65b0\u3057\u3044SoTA\u306e\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u3044\u308b https://t.co/MfeP9G5QVj", "followers": "1,878", "datetime": "2019-10-30 02:54:24", "author": "@sesquipedale"}, "1235573261645352960": {"content_summary": "@zaidalyafeai @seb_ruder I haven't seen a survey paper per se, but I would start with Sebastian's Ph.D. thesis. I also found this overview helpful: https://t.co/VR3y6aBF2t ... although it's probably outdated already. A recent paper by Google also provid", "followers": "5,219", "datetime": "2020-03-05 14:29:56", "author": "@omarsar0"}, "1244646464145547271": {"content_summary": "Notebook with demo: https://t.co/OX1eWMtMvL Release notes for Transformers 2.7.0: https://t.co/JDBmxHFdY8 T5 paper on \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\": https://t.co/M1sgtxwHrn", "followers": "18,442", "datetime": "2020-03-30 15:23:36", "author": "@Thom_Wolf"}, "1187427508532080641": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "3,632", "datetime": "2019-10-24 17:55:54", "author": "@HamelHusain"}, "1187558364609310720": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "159", "datetime": "2019-10-25 02:35:52", "author": "@sageken"}, "1187361813039476737": {"content_summary": "Ok, maybe machine learning isn't complete bullshit.", "followers": "964", "datetime": "2019-10-24 13:34:51", "author": "@kchoudhu"}, "1192622792342007808": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "415", "datetime": "2019-11-08 02:00:06", "author": "@unrahu1"}, "1187390350626709504": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "518", "datetime": "2019-10-24 15:28:14", "author": "@SrikarVaradaraj"}, "1187909134492065792": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5,336", "datetime": "2019-10-26 01:49:42", "author": "@maxbraun"}, "1187465179425996801": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "7", "datetime": "2019-10-24 20:25:35", "author": "@siyunhe2"}, "1190060091329921024": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "3,229", "datetime": "2019-11-01 00:16:50", "author": "@WIOMAX_VA"}, "1187598646394540032": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,080", "datetime": "2019-10-25 05:15:56", "author": "@_josh_meyer_"}, "1211133502172758017": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3", "datetime": "2019-12-29 03:55:03", "author": "@athenayxw"}, "1187211585334284288": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "172", "datetime": "2019-10-24 03:37:53", "author": "@RISHABH50631460"}, "1187209843205120000": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,070", "datetime": "2019-10-24 03:30:58", "author": "@EricSchles"}, "1188173466761121792": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "361", "datetime": "2019-10-26 19:20:04", "author": "@austinvhuang"}, "1187330099944640512": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,584", "datetime": "2019-10-24 11:28:50", "author": "@aneesha"}, "1188318227710922752": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to-Text\u3068\u3057\u3066\u6271\u3044\uff0cEnc-Dec Transformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3066\u8ee2\u79fb\u3059\u308bT5\u30e2\u30c7\u30eb\u3092\u63d0\u6848\uff0e\u5b66\u7fd2\u30b3\u30fc\u30d1\u30b9\u30fb\u5b66\u7fd2\u6e08\u30e2\u30c7\u30eb\u3092\u516c\u958b\uff0e(Super)GLUE, SQuAD1.1, CNN/DM\u7b49\u3067SOTA https://t.co/pknzcrowCl", "followers": "2,439", "datetime": "2019-10-27 04:55:18", "author": "@kyoun"}, "1187564179164499968": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "209", "datetime": "2019-10-25 02:58:58", "author": "@s3works"}, "1254252620094377984": {"content_summary": "Just finished reading the T5 paper and I'm quite confident that text-to-text objectives with encoder-decoder architectures can be the \"unifying\" framework of NLP. So glad to be a practitioner in this exciting field! \ud83d\ude00 https://t.co/koBTir4k1J", "followers": "551", "datetime": "2020-04-26 03:35:02", "author": "@AND__SO"}, "1202468324753518592": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "168", "datetime": "2019-12-05 06:02:44", "author": "@brunoboutteau"}, "1187335390111457280": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "101", "datetime": "2019-10-24 11:49:51", "author": "@dongmds"}, "1189556890759499777": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "116", "datetime": "2019-10-30 14:57:18", "author": "@ben_trevett"}, "1188399715836030981": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "17", "datetime": "2019-10-27 10:19:06", "author": "@ryokkie"}, "1187335865795633155": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "107", "datetime": "2019-10-24 11:51:44", "author": "@MattBMcDermott"}, "1187525196640468992": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "485", "datetime": "2019-10-25 00:24:04", "author": "@eve_yk"}, "1188321000091348993": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "2,945", "datetime": "2019-10-27 05:06:19", "author": "@halhorn"}, "1187386409985835009": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "25", "datetime": "2019-10-24 15:12:35", "author": "@lcl4lnl"}, "1190203117599510530": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "567", "datetime": "2019-11-01 09:45:10", "author": "@kcgcse"}, "1187428428242284545": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,336", "datetime": "2019-10-24 17:59:33", "author": "@jigarkdoshi"}, "1188110350111014912": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "1,727", "datetime": "2019-10-26 15:09:16", "author": "@ChoimiraiHQ"}, "1187290504444014592": {"content_summary": "Very insightful paper with some amazing results!!", "followers": "19,440", "datetime": "2019-10-24 08:51:29", "author": "@Sam_Witteveen"}, "1187249025331761155": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "27", "datetime": "2019-10-24 06:06:40", "author": "@mahesh21aug"}, "1187576009857490944": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "1,297", "datetime": "2019-10-25 03:45:59", "author": "@random_oracle"}, "1187998240853168128": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "277", "datetime": "2019-10-26 07:43:47", "author": "@albincorreya"}, "1187559873321586688": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "253", "datetime": "2019-10-25 02:41:52", "author": "@apastorlm"}, "1227974597527601152": {"content_summary": "@zubinwadia @erikbryn @MSFTResearch @danielrock The returns in terms of test errors are always decreasing. It's easier to go from 10% to 5% than from 5% to 0%. The question is how quickly they decrease. Some recent papers found \"error = (model size)^(-cons", "followers": "76", "datetime": "2020-02-13 15:15:33", "author": "@sorenmind"}, "1187168231607324673": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "319", "datetime": "2019-10-24 00:45:37", "author": "@lucidrains"}, "1187163956726681600": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "63", "datetime": "2019-10-24 00:28:38", "author": "@dave_co_dev"}, "1194368810154893328": {"content_summary": "@unccs NLP/ML group is getting stronger. Consider applying!", "followers": "86", "datetime": "2019-11-12 21:38:09", "author": "@writetoswarna"}, "1199470912560480257": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "294", "datetime": "2019-11-26 23:32:05", "author": "@rose_miura"}, "1187439584814043136": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "70", "datetime": "2019-10-24 18:43:53", "author": "@sumitsethy"}, "1187422474402578432": {"content_summary": "Looks to be a useful resource for exploring the limits of modern DL-based transfer learning approaches for NLP.", "followers": "492", "datetime": "2019-10-24 17:35:53", "author": "@rajhans_samdani"}, "1187172576419876864": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,374", "datetime": "2019-10-24 01:02:53", "author": "@renato_umeton"}, "1187167030732099584": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-24 00:40:51", "author": "@arxiv_cscl"}, "1187163972342026245": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "206", "datetime": "2019-10-24 00:28:42", "author": "@neuroconvergent"}, "1187167047202947072": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,141", "datetime": "2019-10-24 00:40:55", "author": "@arvind_io"}, "1194614858974523393": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "1,821", "datetime": "2019-11-13 13:55:51", "author": "@rosinality"}, "1244652407746785281": {"content_summary": "\ud83d\ude2f", "followers": "49", "datetime": "2020-03-30 15:47:13", "author": "@amsharan"}, "1188813627983220736": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5,718", "datetime": "2019-10-28 13:43:50", "author": "@jasonyo"}, "1194362582754217985": {"content_summary": "Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply to @UNCCS! In addn to Colin doing exciting NLP (e.g., see recent T5 paper: https://t.co/EJfWID3a0q), the awesome @snigdhac25 +@s", "followers": "4,636", "datetime": "2019-11-12 21:13:24", "author": "@mohitban47"}, "1189342801114386432": {"content_summary": "2019/10/23 \u6295\u7a3f 3\u4f4d LG(Machine Learning) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/QUGcgqUh5m 15 Tweets 30 Retweets 88 Favorites", "followers": "694", "datetime": "2019-10-30 00:46:35", "author": "@arxiv_pop"}, "1190050598210523137": {"content_summary": "Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #PyTorch #Python #RStats #TensorFlow #Java #JavaScript #ReactJS #GoLang #CloudComputing #Serverless #DataScientist #Linux", "followers": "87,257", "datetime": "2019-10-31 23:39:07", "author": "@gp_pulipaka"}, "1187326960822829056": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,623", "datetime": "2019-10-24 11:16:21", "author": "@tarantulae"}, "1187448087482130433": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "10,021", "datetime": "2019-10-24 19:17:40", "author": "@prrgutierrez"}, "1187242840667185152": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "357", "datetime": "2019-10-24 05:42:05", "author": "@_florianmai"}, "1187572892701036545": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "759", "datetime": "2019-10-25 03:33:36", "author": "@ichirou84"}, "1200345830869344256": {"content_summary": "RT @AT_Amir: T5 by google explores the field of transfer learning in NLP. Very good systematic study on how to pretrain and transfer transf\u2026", "followers": "1,369", "datetime": "2019-11-29 09:28:42", "author": "@sara_hlt"}, "1187395048679165952": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "51", "datetime": "2019-10-24 15:46:54", "author": "@Shen19589425"}, "1187360194583564289": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,031", "datetime": "2019-10-24 13:28:25", "author": "@PMZepto"}, "1190484758989459456": {"content_summary": "T5\u8ad6\u6587 https://t.co/Av2bomOGpT \u3053\u308c\u306e\u6700\u5f8c\u3060\u3051 pdf \u30b5\u30a4\u30ba\u304c\u3061\u304c\u3046\uff57 #xpaperchallenge", "followers": "764", "datetime": "2019-11-02 04:24:19", "author": "@cfiken"}, "1196326181773217793": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,316", "datetime": "2019-11-18 07:16:03", "author": "@UkIntrapreneur"}, "1187549181281325056": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "43", "datetime": "2019-10-25 01:59:23", "author": "@swapnilpatill22"}, "1189123851256582144": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,780", "datetime": "2019-10-29 10:16:33", "author": "@t2govind"}, "1245518827418312705": {"content_summary": "RT @Thom_Wolf: Notebook with demo: https://t.co/OX1eWMtMvL Release notes for Transformers 2.7.0: https://t.co/JDBmxHFdY8 T5 paper on \"Exp\u2026", "followers": "53", "datetime": "2020-04-02 01:10:03", "author": "@sivastranger"}, "1187197146602315777": {"content_summary": "FYI, 1 trillion tokens and 11 billion parameters.", "followers": "874", "datetime": "2019-10-24 02:40:31", "author": "@mrdrozdov"}, "1187680814529531904": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "1,070", "datetime": "2019-10-25 10:42:26", "author": "@wip_sakai"}, "1187596933889695744": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "1,103", "datetime": "2019-10-25 05:09:08", "author": "@kfuku0502"}, "1188337359831457792": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "364", "datetime": "2019-10-27 06:11:19", "author": "@tkym1220"}, "1187596250595524609": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "0", "datetime": "2019-10-25 05:06:25", "author": "@WalidShalaby9"}, "1187510313375031296": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "190", "datetime": "2019-10-24 23:24:56", "author": "@nskm_m"}, "1187181393069170688": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,739", "datetime": "2019-10-24 01:37:55", "author": "@Mo_Norouzi"}, "1188587266660171776": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "1", "datetime": "2019-10-27 22:44:21", "author": "@haradatm"}, "1187395303151820800": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "51", "datetime": "2019-10-24 15:47:55", "author": "@Shen19589425"}, "1187982440268292097": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "24", "datetime": "2019-10-26 06:41:00", "author": "@samuel_ngambi"}, "1187347020354347008": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "116", "datetime": "2019-10-24 12:36:04", "author": "@winobes"}, "1196165687116550147": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "48", "datetime": "2019-11-17 20:38:18", "author": "@amy030619"}, "1187372853878022144": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "1,543", "datetime": "2019-10-24 14:18:43", "author": "@deadcatbouncepn"}, "1187352731641626624": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "290", "datetime": "2019-10-24 12:58:45", "author": "@dannyehb"}, "1235926815245393921": {"content_summary": "RT @seb_ruder: @zaidalyafeai @omarsar0 Seconding what @omarsar0 said. I've included what I was aware of in my PhD thesis (https://t.co/SViJ\u2026", "followers": "38", "datetime": "2020-03-06 13:54:49", "author": "@experiencor"}, "1194396764318633985": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "2,070", "datetime": "2019-11-12 23:29:14", "author": "@EricSchles"}, "1187535842648064001": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "197", "datetime": "2019-10-25 01:06:22", "author": "@rnalytics"}, "1194250831391932417": {"content_summary": "RT @ada_rob: A few of you at #ismir2019 asked me what I've been up to. Well, I took a sorta-but-not-really-hiatus from Magenta to work on N\u2026", "followers": "456", "datetime": "2019-11-12 13:49:20", "author": "@PerthMLGroup"}, "1187348374883917824": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-24 12:41:27", "author": "@arxiv_cscl"}, "1190052100287401984": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "1,989", "datetime": "2019-10-31 23:45:05", "author": "@wynandbooysen"}, "1187255712575033344": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "42", "datetime": "2019-10-24 06:33:14", "author": "@MishakinSergey"}, "1187411584949379075": {"content_summary": "RT @nikiparmar09: Great work! Text to text Transformer with a masking loss does better than other Transfer learning techniques. https://t.c\u2026", "followers": "203", "datetime": "2019-10-24 16:52:37", "author": "@MiguelISolano"}, "1187221743817154562": {"content_summary": "@Emir93230828 mira", "followers": "394", "datetime": "2019-10-24 04:18:15", "author": "@Daniel_Teran_"}, "1194374900980412416": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "528", "datetime": "2019-11-12 22:02:21", "author": "@FEProctor"}, "1187195550090711040": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "185", "datetime": "2019-10-24 02:34:10", "author": "@bookwormengr"}, "1188773040596893696": {"content_summary": "[1/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 746 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-28 11:02:33", "author": "@PapersTrending"}, "1237467772222877700": {"content_summary": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/o4c7cdF9Xd (submitted by Rouzbeh Afrasiabi) https://t.co/jU8ZEf0jo1", "followers": "907", "datetime": "2020-03-10 19:58:02", "author": "@AISC_TO"}, "1187342962528784385": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,267", "datetime": "2019-10-24 12:19:56", "author": "@azlawal_lawal"}, "1190051337226067968": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "1,020", "datetime": "2019-10-31 23:42:03", "author": "@Jeremy972i"}, "1188410366625271810": {"content_summary": "[1/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 687 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-27 11:01:25", "author": "@PapersTrending"}, "1219575841438474240": {"content_summary": "[10/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,530 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-01-21 11:01:53", "author": "@PapersTrending"}, "1187474087351746562": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "277", "datetime": "2019-10-24 21:00:59", "author": "@ZimMatthias"}, "1187590801741627392": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "12,765", "datetime": "2019-10-25 04:44:46", "author": "@jaguring1"}, "1202019884541964288": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "365", "datetime": "2019-12-04 00:20:47", "author": "@iugoaoj"}, "1187372286342512641": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "391", "datetime": "2019-10-24 14:16:28", "author": "@ColinCherry"}, "1187179362476388353": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "615", "datetime": "2019-10-24 01:29:51", "author": "@KouroshMeshgi"}, "1187576425806589952": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "12", "datetime": "2019-10-25 03:47:38", "author": "@phdax"}, "1187655638073004033": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "1,878", "datetime": "2019-10-25 09:02:24", "author": "@sesquipedale"}, "1187330004734169089": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "42", "datetime": "2019-10-24 11:28:27", "author": "@MishakinSergey"}, "1187194455662579712": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,671", "datetime": "2019-10-24 02:29:49", "author": "@leopd"}, "1187434880470110208": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "22", "datetime": "2019-10-24 18:25:11", "author": "@YungSungChuang"}, "1187164894052765697": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "37", "datetime": "2019-10-24 00:32:21", "author": "@XiangZhou14"}, "1187628351638384642": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "595", "datetime": "2019-10-25 07:13:58", "author": "@popedaniels"}, "1188209724073734144": {"content_summary": "RT @jaguring1: T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/\u2026", "followers": "1,878", "datetime": "2019-10-26 21:44:08", "author": "@sesquipedale"}, "1187556458763698176": {"content_summary": "\u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u3069\u3067SOTA\u3092\u9054\u6210\u3002", "followers": "11,462", "datetime": "2019-10-25 02:28:18", "author": "@icoxfog417"}, "1187382303577690112": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "637", "datetime": "2019-10-24 14:56:16", "author": "@shaunak38"}, "1187250279848300545": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "85", "datetime": "2019-10-24 06:11:39", "author": "@FlorianDreher"}, "1187185340517421061": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "190", "datetime": "2019-10-24 01:53:36", "author": "@lizardlucas42"}, "1189135115454210048": {"content_summary": "[1/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 801 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-29 11:01:19", "author": "@PapersTrending"}, "1187174576842608640": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "265", "datetime": "2019-10-24 01:10:50", "author": "@Swetava"}, "1187333754773213184": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,649", "datetime": "2019-10-24 11:43:21", "author": "@m__dehghani"}, "1187197505122910208": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "0", "datetime": "2019-10-24 02:41:56", "author": "@Sam09lol"}, "1187929625940414466": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/IaOiJYuVSW", "followers": "129", "datetime": "2019-10-26 03:11:08", "author": "@onurgu_ml"}, "1187817328714670081": {"content_summary": "\"For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models.\" \ud83d\ude32 https://t.co/Ghr9ZfwFZx", "followers": "1,165", "datetime": "2019-10-25 19:44:54", "author": "@YangKevinK"}, "1187438965378310144": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "141", "datetime": "2019-10-24 18:41:25", "author": "@waydegilliam"}, "1187770287208550400": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "24", "datetime": "2019-10-25 16:37:58", "author": "@duynht"}, "1187364967386959872": {"content_summary": "Great summary of encoder-decoder text-to-text models by @colinraffel and authors. Also, a new dataset based on a cleaned common crawl (C4). Particularly interesting reflection as well as new SOTA on several tasks.", "followers": "2,415", "datetime": "2019-10-24 13:47:23", "author": "@JeffD"}, "1187163531805749248": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "81,490", "datetime": "2019-10-24 00:26:57", "author": "@hardmaru"}, "1187714284140130305": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "341", "datetime": "2019-10-25 12:55:26", "author": "@romanov_wizard"}, "1187218719807852544": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "242", "datetime": "2019-10-24 04:06:14", "author": "@m_aggrey"}, "1245555643139772418": {"content_summary": "RT @icoxfog417: Hugging Face\u306eTransformer\u306b\u8907\u6570\u30bf\u30b9\u30af\u3092\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u5b66\u7fd2\u3057\u305fT5\u306e\u30e2\u30c7\u30eb\u304c\u642d\u8f09\u3002\u30b5\u30f3\u30d7\u30eb\u306eNotebook\u3082\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u3002 https://t.co/j9rGGB31WH", "followers": "1,878", "datetime": "2020-04-02 03:36:21", "author": "@sesquipedale"}, "1187543819748700162": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "51", "datetime": "2019-10-25 01:38:04", "author": "@yaelanya_ml"}, "1188341261406597121": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "824", "datetime": "2019-10-27 06:26:49", "author": "@morioka"}, "1194455275257913344": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "126", "datetime": "2019-11-13 03:21:44", "author": "@i812ht"}, "1187414978703384578": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "73", "datetime": "2019-10-24 17:06:06", "author": "@rdlfSchneider"}, "1187517569609150464": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,574", "datetime": "2019-10-24 23:53:46", "author": "@nasrinmmm"}, "1187754188350070785": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "771", "datetime": "2019-10-25 15:34:00", "author": "@HundertSteine"}, "1187362188358569984": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "466", "datetime": "2019-10-24 13:36:20", "author": "@yagwar183"}, "1245346370275295233": {"content_summary": "RT @Thom_Wolf: Notebook with demo: https://t.co/OX1eWMtMvL Release notes for Transformers 2.7.0: https://t.co/JDBmxHFdY8 T5 paper on \"Exp\u2026", "followers": "327", "datetime": "2020-04-01 13:44:46", "author": "@aditya_soni2k17"}, "1187355721349947392": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,709", "datetime": "2019-10-24 13:10:38", "author": "@alexey_r"}, "1187682660493086720": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,070", "datetime": "2019-10-25 10:49:46", "author": "@wip_sakai"}, "1187298400565387265": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "114", "datetime": "2019-10-24 09:22:52", "author": "@BismarckBamfo"}, "1187704183983628288": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "1,044", "datetime": "2019-10-25 12:15:18", "author": "@ysaito8015"}, "1188109484326973441": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "669", "datetime": "2019-10-26 15:05:49", "author": "@arunkumar_bvr"}, "1187164043330670592": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "449", "datetime": "2019-10-24 00:28:59", "author": "@michalwols"}, "1187344425396998145": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "484", "datetime": "2019-10-24 12:25:45", "author": "@timothy_lkh_"}, "1187433683759910912": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "185", "datetime": "2019-10-24 18:20:26", "author": "@algope_"}, "1190068724704907264": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "4,591", "datetime": "2019-11-01 00:51:09", "author": "@TheCuriousLuke"}, "1187193547373633542": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "19", "datetime": "2019-10-24 02:26:13", "author": "@MassBassLol"}, "1187664204498530304": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "2,848", "datetime": "2019-10-25 09:36:26", "author": "@Rain2And"}, "1187515963270238208": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "518", "datetime": "2019-10-24 23:47:23", "author": "@pijili"}, "1187203297528074240": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu https://t.co/CHrMWCSLwT", "followers": "311", "datetime": "2019-10-24 03:04:57", "author": "@arxiv_cs_LG"}, "1187388759748182016": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "871", "datetime": "2019-10-24 15:21:55", "author": "@mihail_eric"}, "1187377839617785858": {"content_summary": "Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u4e0a\u56de\u3063\u3066\u3001\u4eba\u9593\u3068\u306e\u5dee\u306f\u308f\u305a\u304b 0.9\u30dd\u30a4\u30f3\u30c8\u3002\u5e74\u5185\u306b\u4eba\u9593\u3092\u8d85\u3048\u308b\u3068\u601d\u3046\ud83e\udd16 https://t.co/KBDiyR67FC", "followers": "19,021", "datetime": "2019-10-24 14:38:32", "author": "@gijigae"}, "1187435197178011649": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "683", "datetime": "2019-10-24 18:26:27", "author": "@katherine1ee"}, "1187364776151801857": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "270", "datetime": "2019-10-24 13:46:37", "author": "@kotti_sasikanth"}, "1187216384348114944": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "996", "datetime": "2019-10-24 03:56:58", "author": "@ialuronico"}, "1207756659897974784": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4,574", "datetime": "2019-12-19 20:16:41", "author": "@asifrazzaq1988"}, "1209993100002840577": {"content_summary": "Transfer Learning with a Unified Text-to-Text Transformer (T5) (https://t.co/gh8Ko3eTPK). Couldn't really connect to the sentence - \"Finally, we also push the field forward by training larger models than have been previously considered (up to 11 billion pa", "followers": "424", "datetime": "2019-12-26 00:23:30", "author": "@debanjanbhucs"}, "1187211492350840832": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "48", "datetime": "2019-10-24 03:37:31", "author": "@sharan0909"}, "1187589766935539717": {"content_summary": "RT @jmhessel: Table 15 from T5 might be the most computationally expensive table ever constructed in the history of natural language proces\u2026", "followers": "10", "datetime": "2019-10-25 04:40:39", "author": "@JatavVishaal"}, "1187467647312465931": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,170", "datetime": "2019-10-24 20:35:23", "author": "@iskander"}, "1187342208187490304": {"content_summary": "Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study!", "followers": "12,363", "datetime": "2019-10-24 12:16:56", "author": "@sleepinyourhat"}, "1187549556419940353": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "129", "datetime": "2019-10-25 02:00:52", "author": "@Lovely_LoveLive"}, "1187182838887649280": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "157", "datetime": "2019-10-24 01:43:40", "author": "@ankurbpn"}, "1187598365984468992": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "649", "datetime": "2019-10-25 05:14:49", "author": "@cortexelation"}, "1187395102718562305": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "25", "datetime": "2019-10-24 15:47:07", "author": "@lcl4lnl"}, "1187385821080510464": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "6", "datetime": "2019-10-24 15:10:14", "author": "@siddhadev"}, "1188772630251352064": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "304", "datetime": "2019-10-28 11:00:56", "author": "@libre_ai"}, "1190068562712334337": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "468", "datetime": "2019-11-01 00:50:30", "author": "@udayadampage"}, "1187330668226863104": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "75", "datetime": "2019-10-24 11:31:05", "author": "@farhadnz"}, "1187757510238838785": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "50", "datetime": "2019-10-25 15:47:12", "author": "@kushalchauhan98"}, "1187588162878504960": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "2,615", "datetime": "2019-10-25 04:34:17", "author": "@erukiti"}, "1187844921538957313": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "26", "datetime": "2019-10-25 21:34:33", "author": "@ravi220644"}, "1187217337939685378": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "48", "datetime": "2019-10-24 04:00:45", "author": "@donovanOng_"}, "1187479239739600900": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/wwImPFSyOc", "followers": "4,391", "datetime": "2019-10-24 21:21:27", "author": "@rjurney"}, "1220402874594689025": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "47", "datetime": "2020-01-23 17:48:13", "author": "@tasserg"}, "1187507817164337152": {"content_summary": "\u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 https://t.co/UFwHmtmNx1", "followers": "18,231", "datetime": "2019-10-24 23:15:01", "author": "@hillbig"}, "1187588919786786818": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "371", "datetime": "2019-10-25 04:37:17", "author": "@_udoppi_"}, "1252659646210478080": {"content_summary": "Next AUEB NLP Group meeting, Tue *May 5*, *17:00-18:30*: \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)\", Raffel et al. (https://t.co/a4qE1enUql). Study the paper before the meeting. All welcome (but max capacity 10", "followers": "505", "datetime": "2020-04-21 18:05:07", "author": "@AUEBNLPGroup"}, "1188198397896994816": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "322", "datetime": "2019-10-26 20:59:08", "author": "@letranger14"}, "1187711552314466305": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "48", "datetime": "2019-10-25 12:44:35", "author": "@s888mak888s"}, "1187617396477091841": {"content_summary": "Google \u53c8\u653e\u5927\u7d55\u4e86\u22ef\u22ef GitHub \u93c8\u63a5\uff1a https://t.co/HhaGjADfMN \u8ad6\u6587\uff1a https://t.co/UrHxxLVLLD https://t.co/2wCZMis1sY", "followers": "640", "datetime": "2019-10-25 06:30:26", "author": "@strategist922"}, "1187532742684073984": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "91", "datetime": "2019-10-25 00:54:03", "author": "@DrJimFan"}, "1187418057196605443": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,165", "datetime": "2019-10-24 17:18:20", "author": "@YangKevinK"}, "1190500534148403200": {"content_summary": "RT @yohei_kikuta: \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u307e\u3068\u3081\u3001\u610f\u5916\u3068\u77e5\u3063\u3066\u3044\u308b\u3082\u306e\u304c\u591a\u304b\u3063\u305f\u304c\u826f\u3044\u6574\u7406\u306b\u306a\u3063\u305f\u3002 \u6700\u8fd1\u51fa\u305f T5 \u3068\u3044\u3046\u3001encoder-decoder \u306e seq2seq \u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3059\u308c\u3070\u5206\u985e\u3068\u304b QA \u304c\u540c\u3058\u3088\u3046\u306b\u6271\u3048\u308b\u306e\u3067\u305d\u308c\u3092\u89e3\u3044\u3066\u826f\u3044\u4e8b\u524d\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u3001\u2026", "followers": "824", "datetime": "2019-11-02 05:27:00", "author": "@morioka"}, "1187846738666643456": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "1,878", "datetime": "2019-10-25 21:41:46", "author": "@sesquipedale"}, "1187470410813595648": {"content_summary": "Great thread & paper about NLP transfer learning!", "followers": "76", "datetime": "2019-10-24 20:46:22", "author": "@sorenmind"}, "1187387521434435584": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "258", "datetime": "2019-10-24 15:17:00", "author": "@arezae"}, "1203938574665682945": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "326", "datetime": "2019-12-09 07:24:58", "author": "@majortal"}, "1194034220089384961": {"content_summary": "A few of you at #ismir2019 asked me what I've been up to. Well, I took a sorta-but-not-really-hiatus from Magenta to work on NLP! The result was T5, which has been a very rewarding experience. Now, I'm looking forward to bringing some new insights back t", "followers": "4,460", "datetime": "2019-11-11 23:28:36", "author": "@ada_rob"}, "1187236832406208514": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "827", "datetime": "2019-10-24 05:18:13", "author": "@rahuldave"}, "1187387222577831936": {"content_summary": "The public code and models, the huge increase on SuperGLUE, and the universal approach... this seems like a big deal. Kind of interested in these types of models for generative tasks.", "followers": "50", "datetime": "2019-10-24 15:15:49", "author": "@adamsears"}, "1187348426616389633": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "122", "datetime": "2019-10-24 12:41:39", "author": "@TusharJain_007"}, "1187410540412276736": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "93", "datetime": "2019-10-24 16:48:28", "author": "@0xhexhex"}, "1187353529020796928": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "21", "datetime": "2019-10-24 13:01:55", "author": "@ChiMaBa1"}, "1229737404061216773": {"content_summary": "@royschwartz02 @nlpnoah @AliFarhadi @JesseDodge @gabriel_ilharco #enough2skim Also has some recent general suggestions (and also enough to skim) https://t.co/n8KvUBzT3r", "followers": "111", "datetime": "2020-02-18 12:00:19", "author": "@LChoshen"}, "1205148376141979649": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "302", "datetime": "2019-12-12 15:32:18", "author": "@douchi_kamiya"}, "1194610580398600192": {"content_summary": "\uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640 \uad6c\ud604\uc774 \uc798 \ubd84\ub9ac\ub418\uc5b4 \uc788\ub2e4. https://t.co/GOxDMDVvn6", "followers": "8,544", "datetime": "2019-11-13 13:38:51", "author": "@d_ijk_stra"}, "1188104218948694017": {"content_summary": "RT @jaguring1: T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/\u2026", "followers": "2,861", "datetime": "2019-10-26 14:44:54", "author": "@Tarpon_red2"}, "1200114351803371520": {"content_summary": "RT @AT_Amir: T5 by google explores the field of transfer learning in NLP. Very good systematic study on how to pretrain and transfer transf\u2026", "followers": "367", "datetime": "2019-11-28 18:08:53", "author": "@mdigangiPA"}, "1245496510327844864": {"content_summary": "Hugging Face\u306eTransformer\u306b\u8907\u6570\u30bf\u30b9\u30af\u3092\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u5b66\u7fd2\u3057\u305fT5\u306e\u30e2\u30c7\u30eb\u304c\u642d\u8f09\u3002\u30b5\u30f3\u30d7\u30eb\u306eNotebook\u3082\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u3002", "followers": "11,462", "datetime": "2020-04-01 23:41:22", "author": "@icoxfog417"}, "1187447054521421824": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "25,578", "datetime": "2019-10-24 19:13:34", "author": "@Miles_Brundage"}, "1187726081928912897": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "84", "datetime": "2019-10-25 13:42:19", "author": "@shahrzad_naseri"}, "1244648894060994563": {"content_summary": "Hey Now! #NLProc #AI #ArtificialIntelligence #DeepLearning", "followers": "917", "datetime": "2020-03-30 15:33:15", "author": "@Quantum_Stat"}, "1187356999903797248": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "183", "datetime": "2019-10-24 13:15:43", "author": "@msobrevillac"}, "1187180241195696129": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,344", "datetime": "2019-10-24 01:33:20", "author": "@jesse_vig"}, "1187850730809348096": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,237", "datetime": "2019-10-25 21:57:38", "author": "@samgreydanus"}, "1187537421958647809": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "106", "datetime": "2019-10-25 01:12:39", "author": "@mo_rafaie"}, "1187219584522129408": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "12,511", "datetime": "2019-10-24 04:09:41", "author": "@suzatweet"}, "1187412007579815937": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "332", "datetime": "2019-10-24 16:54:18", "author": "@vsinghalus"}, "1187418120635465730": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "1,106", "datetime": "2019-10-24 17:18:35", "author": "@permutans"}, "1187243630077120512": {"content_summary": "RT @roadrunning01: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer pdf: https://t.co/hECWfJh0Ny abs: http\u2026", "followers": "1", "datetime": "2019-10-24 05:45:13", "author": "@Pol09122455"}, "1187592116140032000": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "302", "datetime": "2019-10-25 04:49:59", "author": "@subhobrata1"}, "1187485385103937537": {"content_summary": "RT @gdm3000: T5 stands for their Text-to-Text Transfer Transformer model. It is pre-trained on the new \"Colossal Clean Crawled Corpus\", and\u2026", "followers": "228", "datetime": "2019-10-24 21:45:52", "author": "@arademaker"}, "1195878861164072960": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "192", "datetime": "2019-11-17 01:38:33", "author": "@achowdhery"}, "1187541830109401089": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "27", "datetime": "2019-10-25 01:30:10", "author": "@mahesh21aug"}, "1187409429664124928": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "273", "datetime": "2019-10-24 16:44:03", "author": "@morinosuke__p"}, "1187269620303978496": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "115", "datetime": "2019-10-24 07:28:30", "author": "@viktor_m81"}, "1187510367179509760": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "168", "datetime": "2019-10-24 23:25:09", "author": "@informaru"}, "1187479867769675776": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "589", "datetime": "2019-10-24 21:23:57", "author": "@neilhoulsby"}, "1187307918447636481": {"content_summary": "Promising!", "followers": "163", "datetime": "2019-10-24 10:00:41", "author": "@reachtarunhere"}, "1187247408326987778": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "36", "datetime": "2019-10-24 06:00:14", "author": "@Anko1418"}, "1187377509156999169": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "15", "datetime": "2019-10-24 14:37:13", "author": "@VictorGarritan3"}, "1187260993979797508": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "221", "datetime": "2019-10-24 06:54:13", "author": "@fdasilva59fr"}, "1187329828934078464": {"content_summary": "The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape in NLP. Check out page 33 of the paper or below for the main takeaways. https://t.co/nVUYePuglv https://t.co/8p2oZ3q8uf", "followers": "44,898", "datetime": "2019-10-24 11:27:45", "author": "@seb_ruder"}, "1187257629564964864": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,551", "datetime": "2019-10-24 06:40:51", "author": "@sandeshr"}, "1187548781455142912": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "26,503", "datetime": "2019-10-25 01:57:47", "author": "@mnishi41"}, "1187559105558278145": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "2,442", "datetime": "2019-10-25 02:38:49", "author": "@538355"}, "1188162272788508672": {"content_summary": "RT @jaguring1: T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/\u2026", "followers": "97", "datetime": "2019-10-26 18:35:35", "author": "@tamanoaraiguma"}, "1187275672814866432": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "170", "datetime": "2019-10-24 07:52:33", "author": "@jctestud"}, "1187465125579542528": {"content_summary": "RT @jmhessel: Table 15 from T5 might be the most computationally expensive table ever constructed in the history of natural language proces\u2026", "followers": "36", "datetime": "2019-10-24 20:25:22", "author": "@aurko_roy"}, "1187344470133526528": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "2,070", "datetime": "2019-10-24 12:25:56", "author": "@EricSchles"}, "1187420348515602434": {"content_summary": "RT @JeffD: Great summary of encoder-decoder text-to-text models by @colinraffel and authors. Also, a new dataset based on a cleaned common\u2026", "followers": "255", "datetime": "2019-10-24 17:27:26", "author": "@andrewyates"}, "1190506559761088512": {"content_summary": "RT @cfiken: T5\u8ad6\u6587 https://t.co/Av2bomOGpT \u3053\u308c\u306e\u6700\u5f8c\u3060\u3051 pdf \u30b5\u30a4\u30ba\u304c\u3061\u304c\u3046\uff57 #xpaperchallenge", "followers": "225", "datetime": "2019-11-02 05:50:57", "author": "@ElectronNest"}, "1187206086413041664": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "89", "datetime": "2019-10-24 03:16:02", "author": "@_4mol"}, "1187409805167611904": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "589", "datetime": "2019-10-24 16:45:33", "author": "@fjord41"}, "1188164788964098049": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "99,929", "datetime": "2019-10-26 18:45:35", "author": "@jeremyphoward"}, "1196104318727925767": {"content_summary": "@oliverguhr ..... something for you?", "followers": "109", "datetime": "2019-11-17 16:34:26", "author": "@KlingnerMathias"}, "1190618649209405440": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "302", "datetime": "2019-11-02 13:16:21", "author": "@subhobrata1"}, "1187432272510668800": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "304", "datetime": "2019-10-24 18:14:49", "author": "@metah_ch"}, "1187176498882498560": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "134", "datetime": "2019-10-24 01:18:28", "author": "@AnantPAwasthi"}, "1187936276596457472": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2", "datetime": "2019-10-26 03:37:33", "author": "@xin_josh"}, "1187399002972512257": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "3", "datetime": "2019-10-24 16:02:37", "author": "@bellar3464"}, "1188227880473358336": {"content_summary": "@h_orses @Plinz @GaryMarcus @AdamDanielKing WSC : 93.8% (accuracy) WNLI : 93.2% (accuracy) T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/IPrPEFOxtb https://t.co/ylSgQq2W0Y", "followers": "45", "datetime": "2019-10-26 22:56:17", "author": "@yowaijibunntot1"}, "1187454908276051968": {"content_summary": "RT @jmhessel: Table 15 from T5 might be the most computationally expensive table ever constructed in the history of natural language proces\u2026", "followers": "2,048", "datetime": "2019-10-24 19:44:46", "author": "@vnfrombucharest"}, "1187729248666046464": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "205", "datetime": "2019-10-25 13:54:54", "author": "@a_random_obsrvr"}, "1227910697935425537": {"content_summary": "[6/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,671 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-13 11:01:38", "author": "@PapersTrending"}, "1187406597519216641": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "397", "datetime": "2019-10-24 16:32:48", "author": "@KordoniN"}, "1187266453243748352": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "265", "datetime": "2019-10-24 07:15:55", "author": "@akashjainx"}, "1190053722749120512": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "130", "datetime": "2019-10-31 23:51:32", "author": "@Vectorbottress"}, "1206487325083127808": {"content_summary": "excuse me stupidness, is there a detailed comparison b/w T5 and ALBERT. I want a TL;DR which should I pick...", "followers": "10", "datetime": "2019-12-16 08:12:48", "author": "@blogdeanushk"}, "1187253502990381064": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "137", "datetime": "2019-10-24 06:24:27", "author": "@TinDan_"}, "1187481731839848449": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "202", "datetime": "2019-10-24 21:31:21", "author": "@Ixmt7M7Xrrny1dN"}, "1187398957821038598": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "962", "datetime": "2019-10-24 16:02:27", "author": "@bgalbraith"}, "1194220203460005888": {"content_summary": "RT @ada_rob: A few of you at #ismir2019 asked me what I've been up to. Well, I took a sorta-but-not-really-hiatus from Magenta to work on N\u2026", "followers": "218", "datetime": "2019-11-12 11:47:38", "author": "@AssistedEvolve"}, "1187585011169427456": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "157", "datetime": "2019-10-25 04:21:45", "author": "@altescy"}, "1187354432796708864": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "428", "datetime": "2019-10-24 13:05:31", "author": "@yashk2810"}, "1200202023414771714": {"content_summary": "Our pick of the week: Raffel et al. paper on \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\". By @at_amir #nlproc #deeplearning @colinraffel @ada_rob @katherine1ee @sharan0909 @zhouyanqi30 @kongkonglli @peterjliu", "followers": "858", "datetime": "2019-11-28 23:57:15", "author": "@fbk_mt"}, "1187373476610314240": {"content_summary": "Everything you ever wanted to know but never dared to ask about your friendly (monstrously huge) neighborhood pretrained Transformer! Amazing read. Answers qs like: train lm-style or denoising? bigger better? fine tune what? Congrats @colinraffel Shazeer @", "followers": "674", "datetime": "2019-10-24 14:21:11", "author": "@jonathanrraiman"}, "1187409330364178432": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "48", "datetime": "2019-10-24 16:43:40", "author": "@mzadrogaPL"}, "1187283477772697600": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "745", "datetime": "2019-10-24 08:23:34", "author": "@pabloduboue"}, "1187452042630979584": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "710", "datetime": "2019-10-24 19:33:23", "author": "@lfschiavo"}, "1194036301412085760": {"content_summary": "RT @ada_rob: A few of you at #ismir2019 asked me what I've been up to. Well, I took a sorta-but-not-really-hiatus from Magenta to work on N\u2026", "followers": "868", "datetime": "2019-11-11 23:36:53", "author": "@cinjoncin"}, "1187636022600843264": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "181", "datetime": "2019-10-25 07:44:27", "author": "@atsushi1104ak"}, "1187264906871148544": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "62", "datetime": "2019-10-24 07:09:46", "author": "@eDezhic"}, "1187321754378784770": {"content_summary": "New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the limits to achieve SoTA on GLUE, SuperGLUE, CNN/DM, and SQuAD. Paper: https://t.co/uA7dJ6l4UL  Code/models/data/etc: https://t.co/6ASZU1", "followers": "2,342", "datetime": "2019-10-24 10:55:40", "author": "@DataScienceFr"}, "1187381965781110785": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "736", "datetime": "2019-10-24 14:54:55", "author": "@unsorsodicorda"}, "1228997820037914625": {"content_summary": "[9/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,743 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-16 11:01:28", "author": "@PapersTrending"}, "1245496896245792769": {"content_summary": "RT @Thom_Wolf: Notebook with demo: https://t.co/OX1eWMtMvL Release notes for Transformers 2.7.0: https://t.co/JDBmxHFdY8 T5 paper on \"Exp\u2026", "followers": "1,203", "datetime": "2020-04-01 23:42:54", "author": "@reonaarticle"}, "1187367017915015168": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "34", "datetime": "2019-10-24 13:55:31", "author": "@jaspreetPhD"}, "1187226684803543040": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "123", "datetime": "2019-10-24 04:37:53", "author": "@TheRealRPuri"}, "1187184816770433025": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer pdf: https://t.co/hECWfJh0Ny abs: https://t.co/9N06pdkQjI github: https://t.co/MK54PLzFBg https://t.co/zbYUbPFYII", "followers": "8,314", "datetime": "2019-10-24 01:51:31", "author": "@roadrunning01"}, "1200331241981124608": {"content_summary": "RT @fbk_mt: Our pick of the week: Raffel et al. paper on \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u2026", "followers": "149", "datetime": "2019-11-29 08:30:43", "author": "@karakanta"}, "1187449558487519236": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "4,019", "datetime": "2019-10-24 19:23:31", "author": "@jessyseonoob"}, "1232108986897485824": {"content_summary": "Google's T5 looks very interesting as does the corpus used to train it, the Colossal Clean Crawled Corpus (C4). Overview post at https://t.co/cj1zByZvgU, details on arXiv at https://t.co/pxIUBaFNl6 https://t.co/kozupqIPSi", "followers": "551", "datetime": "2020-02-25 01:04:08", "author": "@timFinin"}, "1202159299939713025": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "122", "datetime": "2019-12-04 09:34:46", "author": "@TusharJain_007"}, "1235925238048854016": {"content_summary": "@zaidalyafeai @omarsar0 Seconding what @omarsar0 said. I've included what I was aware of in my PhD thesis (https://t.co/SViJymosRO), which includes multi-task, sequential, cross-lingual and domain transfer. The T5 paper (https://t.co/nVUYePuglv ) is a good", "followers": "44,898", "datetime": "2020-03-06 13:48:33", "author": "@seb_ruder"}, "1188345967377928192": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4", "datetime": "2019-10-27 06:45:31", "author": "@FeiHao88"}, "1187698179309015041": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "4,643", "datetime": "2019-10-25 11:51:26", "author": "@walterdebrouwer"}, "1187200906279845888": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - https://t.co/9D1COPFWPY https://t.co/vFwTGYyI9o", "followers": "195", "datetime": "2019-10-24 02:55:27", "author": "@hereticreader"}, "1245599354741600258": {"content_summary": "RT @Thom_Wolf: Notebook with demo: https://t.co/OX1eWMtMvL Release notes for Transformers 2.7.0: https://t.co/JDBmxHFdY8 T5 paper on \"Exp\u2026", "followers": "72", "datetime": "2020-04-02 06:30:02", "author": "@r_jesuraj"}, "1187330139262054400": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "28", "datetime": "2019-10-24 11:28:59", "author": "@masti_123"}, "1187564516667617281": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "350", "datetime": "2019-10-25 03:00:19", "author": "@vochicong"}, "1187668157567139840": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1", "datetime": "2019-10-25 09:52:09", "author": "@chengadian"}, "1187400194813550592": {"content_summary": "https://t.co/PmAkwWI2BU - woooooooow", "followers": "14", "datetime": "2019-10-24 16:07:21", "author": "@y_honcharenko"}, "1187373509057699840": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "148", "datetime": "2019-10-24 14:21:19", "author": "@bisioid"}, "1189075066970984449": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "2,478", "datetime": "2019-10-29 07:02:42", "author": "@shion_honda"}, "1200451134801866753": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "91", "datetime": "2019-11-29 16:27:08", "author": "@Ohnyx2"}, "1187267317140512768": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "40", "datetime": "2019-10-24 07:19:21", "author": "@Thunderdrums69"}, "1187873447692521473": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "26", "datetime": "2019-10-25 23:27:54", "author": "@jnywk"}, "1187519558690566144": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "40", "datetime": "2019-10-25 00:01:40", "author": "@linbo_pythoner"}, "1187379449450373122": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "1,218", "datetime": "2019-10-24 14:44:55", "author": "@Homocriticos"}, "1203432186411548672": {"content_summary": "RT @agatan_: text-to-text transfer transformer \u3081\u3063\u3061\u3083\u826f\u3044\u3002\u5922\u304c\u5e83\u304c\u308b\u3057\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5e83\u304c\u308b\u3002BART \u3082\u4f3c\u305f\u3088\u3046\u306a\u8a71\u3060\u3051\u3069 T5 \u306f\u8ad6\u6587\u81ea\u4f53\u304c\u3081\u3061\u3083\u304f\u3061\u3083\u9577\u3044\u5206\u3001\u6bd4\u8f03\u3068\u304b\u8003\u5bdf\u306b\u5bcc\u3093\u3067\u3066\u304b\u306a\u308a\u8aad\u307f\u5fdc\u3048\u3042\u308b\u3002 https://t.co\u2026", "followers": "161", "datetime": "2019-12-07 21:52:46", "author": "@nightwalker"}, "1187404262361329665": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "4", "datetime": "2019-10-24 16:23:31", "author": "@shfaithy"}, "1187725917608710144": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-25 13:41:40", "author": "@arxiv_cscl"}, "1188173184056446976": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "48", "datetime": "2019-10-26 19:18:56", "author": "@cyanamous"}, "1202415670174113793": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "290", "datetime": "2019-12-05 02:33:30", "author": "@dannyehb"}, "1187338208406888449": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "66", "datetime": "2019-10-24 12:01:03", "author": "@battle8500"}, "1187400709915807747": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "282", "datetime": "2019-10-24 16:09:24", "author": "@dunce_ml"}, "1188099395092267008": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,907", "datetime": "2019-10-26 14:25:44", "author": "@TheKanter"}, "1187396466035195904": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "127", "datetime": "2019-10-24 15:52:32", "author": "@abhish_eksharma"}, "1187547701614743553": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "2,479", "datetime": "2019-10-25 01:53:30", "author": "@gravitino"}, "1187168573292068864": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "10", "datetime": "2019-10-24 00:46:59", "author": "@mattbakerSB"}, "1187412227998924801": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5", "datetime": "2019-10-24 16:55:10", "author": "@yuhang_dl"}, "1187450263289958400": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "1,942", "datetime": "2019-10-24 19:26:19", "author": "@GiorgioPatrini"}, "1187405810273570818": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "771", "datetime": "2019-10-24 16:29:40", "author": "@aCraigPfeifer"}, "1187469162618052610": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm2MEjw", "followers": "3,501", "datetime": "2019-10-24 20:41:25", "author": "@arxiv_cscl"}, "1187801875497791490": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "25", "datetime": "2019-10-25 18:43:30", "author": "@Rebotlucion"}, "1187247469803065345": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "62", "datetime": "2019-10-24 06:00:29", "author": "@Marchemjor"}, "1187388432319778817": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "536", "datetime": "2019-10-24 15:20:37", "author": "@sangha_deb"}, "1187252920552546304": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "98", "datetime": "2019-10-24 06:22:09", "author": "@justinkhup"}, "1196949470547591170": {"content_summary": "@adityakusupati @suriyagnskr In terms of modeling, I listed some influential papers below Transformers: https://t.co/UikGf9IHf1 (also recommend this blog post: https://t.co/SNeIWirZKN) BERT: https://t.co/AJ39d0n2Bb RoBERTa: https://t.co/92pdjp5UYi T5: htt", "followers": "366", "datetime": "2019-11-20 00:32:46", "author": "@gabriel_ilharco"}, "1187422885633118208": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "125", "datetime": "2019-10-24 17:37:31", "author": "@nickdaleburns"}, "1187770995047690241": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "54", "datetime": "2019-10-25 16:40:47", "author": "@ainewsantenna"}, "1187546237022531585": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "260", "datetime": "2019-10-25 01:47:41", "author": "@hyohyi"}, "1200024108379054081": {"content_summary": "RT @remilouf: You can read the paper here \ud83d\udc49 https://t.co/IDK1MOAufz", "followers": "432", "datetime": "2019-11-28 12:10:17", "author": "@bhargavbardipur"}, "1187403331389423616": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "2,670", "datetime": "2019-10-24 16:19:49", "author": "@ayirpelle"}, "1187691251140677637": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "302", "datetime": "2019-10-25 11:23:55", "author": "@subhobrata1"}, "1187267477002162176": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "13,229", "datetime": "2019-10-24 07:19:59", "author": "@arnicas"}, "1188543705164107776": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5", "datetime": "2019-10-27 19:51:16", "author": "@Hepeng2012"}, "1187281189926850560": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "30,548", "datetime": "2019-10-24 08:14:28", "author": "@NalKalchbrenner"}, "1188979182224707584": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "572", "datetime": "2019-10-29 00:41:41", "author": "@ebetica"}, "1195407426817642496": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "463", "datetime": "2019-11-15 18:25:14", "author": "@snigdhac25"}, "1190256724046950400": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "1,681", "datetime": "2019-11-01 13:18:11", "author": "@ApsisInc"}, "1187471534308253699": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "625", "datetime": "2019-10-24 20:50:50", "author": "@nova77t"}, "1228635362932731904": {"content_summary": "[4/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,738 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-15 11:01:12", "author": "@PapersTrending"}, "1187556227338797056": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "287", "datetime": "2019-10-25 02:27:22", "author": "@akisehiro"}, "1187275691760336896": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "165", "datetime": "2019-10-24 07:52:38", "author": "@RamanarayanM"}, "1203372212956221442": {"content_summary": "RT @agatan_: text-to-text transfer transformer \u3081\u3063\u3061\u3083\u826f\u3044\u3002\u5922\u304c\u5e83\u304c\u308b\u3057\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5e83\u304c\u308b\u3002BART \u3082\u4f3c\u305f\u3088\u3046\u306a\u8a71\u3060\u3051\u3069 T5 \u306f\u8ad6\u6587\u81ea\u4f53\u304c\u3081\u3061\u3083\u304f\u3061\u3083\u9577\u3044\u5206\u3001\u6bd4\u8f03\u3068\u304b\u8003\u5bdf\u306b\u5bcc\u3093\u3067\u3066\u304b\u306a\u308a\u8aad\u307f\u5fdc\u3048\u3042\u308b\u3002 https://t.co\u2026", "followers": "578", "datetime": "2019-12-07 17:54:27", "author": "@usagisan2020"}, "1187595771740377088": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "83", "datetime": "2019-10-25 05:04:31", "author": "@AthulMul"}, "1187559080136544256": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,418", "datetime": "2019-10-25 02:38:43", "author": "@sguada"}, "1187196771530731522": {"content_summary": "it's about a method to perform how to learn a language from the perspective of human, that's should be the rightful way.", "followers": "51", "datetime": "2019-10-24 02:39:02", "author": "@ravend27"}, "1187183479835914240": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "91", "datetime": "2019-10-24 01:46:13", "author": "@ChadElRicardo"}, "1203385849523531776": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "10", "datetime": "2019-12-07 18:48:39", "author": "@iamyehai"}, "1187346602291159040": {"content_summary": "My god this paper... 34 pages of goodness. Also, 93.2 on WNLI (2 point better than previous SOTA) *and* 90.06 on SQuAD EM (over 1 point better). 92.5% accuracy on ReCoRD (human performance is 91.3)...", "followers": "898", "datetime": "2019-10-24 12:34:24", "author": "@nlothian"}, "1235947902025031680": {"content_summary": "RT @seb_ruder: @zaidalyafeai @omarsar0 Seconding what @omarsar0 said. I've included what I was aware of in my PhD thesis (https://t.co/SViJ\u2026", "followers": "818", "datetime": "2020-03-06 15:18:37", "author": "@deepgradient"}, "1187506831003914241": {"content_summary": "Interesting study on Transfer Learning in NLP.", "followers": "30", "datetime": "2019-10-24 23:11:05", "author": "@allardmaxime079"}, "1187321603501121537": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "107", "datetime": "2019-10-24 10:55:04", "author": "@merajatgupta"}, "1187356952109535233": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "518", "datetime": "2019-10-24 13:15:32", "author": "@313V"}, "1193797417151238144": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "615", "datetime": "2019-11-11 07:47:38", "author": "@KouroshMeshgi"}, "1187762764208640002": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "163", "datetime": "2019-10-25 16:08:05", "author": "@HouhouinK"}, "1187482940323913729": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,944", "datetime": "2019-10-24 21:36:09", "author": "@suzan"}, "1187203223049900032": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "97", "datetime": "2019-10-24 03:04:40", "author": "@vin_mathur"}, "1194465492687187969": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "194", "datetime": "2019-11-13 04:02:20", "author": "@hikushalhere"}, "1187250315688591360": {"content_summary": "Very cool", "followers": "1,049", "datetime": "2019-10-24 06:11:47", "author": "@debayan"}, "1188458647753515013": {"content_summary": "Good survey paper on transfer learning by Google", "followers": "738", "datetime": "2019-10-27 14:13:16", "author": "@faizanj"}, "1190055251577098247": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "11,225", "datetime": "2019-10-31 23:57:36", "author": "@silentseawolf"}, "1232599885100322816": {"content_summary": "The AI won 5-3 https://t.co/zqsWkBpzCT Background: https://t.co/3ElcFGAwkz https://t.co/J65CC7Nivy", "followers": "1,188", "datetime": "2020-02-26 09:34:47", "author": "@fnielsen"}, "1187388373440303104": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "43", "datetime": "2019-10-24 15:20:23", "author": "@criz5Castro"}, "1187379929920528384": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "456", "datetime": "2019-10-24 14:46:50", "author": "@PerthMLGroup"}, "1187246589523480576": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "18,442", "datetime": "2019-10-24 05:56:59", "author": "@Thom_Wolf"}, "1187547447930675200": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "2,479", "datetime": "2019-10-25 01:52:29", "author": "@gravitino"}, "1187361997807128576": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "54", "datetime": "2019-10-24 13:35:35", "author": "@Jo14Robert"}, "1187519662713491456": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "40", "datetime": "2019-10-25 00:02:05", "author": "@linbo_pythoner"}, "1187172261943484416": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "20,608", "datetime": "2019-10-24 01:01:38", "author": "@paperswithcode"}, "1187622746131517440": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "325", "datetime": "2019-10-25 06:51:42", "author": "@MatteoPalmonari"}, "1187719009371852802": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "167", "datetime": "2019-10-25 13:14:13", "author": "@Shaojie_Jiang"}, "1197692301079764992": {"content_summary": "RT @ada_rob: A few of you at #ismir2019 asked me what I've been up to. Well, I took a sorta-but-not-really-hiatus from Magenta to work on N\u2026", "followers": "1,834", "datetime": "2019-11-22 01:44:31", "author": "@m_nabu55"}, "1187688672797216768": {"content_summary": "Google's T5 (Text-To-Text Transfer Transformer) language model set new record and gets very close to human on SuperGLUE benchmark. https://t.co/kBOqWxFmEK Paper: https://t.co/mI6BqAgj0e Code: https://t.co/01qZWrxbqS https://t.co/8SRJmoiaw6", "followers": "45", "datetime": "2019-10-25 11:13:40", "author": "@yoquankara"}, "1187366087077322757": {"content_summary": "T5 stands for their Text-to-Text Transfer Transformer model. It is pre-trained on the new \"Colossal Clean Crawled Corpus\", and every task is considered as a sequence-to-sequence problem. https://t.co/WQa0loPahb", "followers": "1,975", "datetime": "2019-10-24 13:51:50", "author": "@gdm3000"}, "1252712082778267650": {"content_summary": "RT @AUEBNLPGroup: Next AUEB NLP Group meeting, Tue *May 5*, *17:00-18:30*: \"Exploring the Limits of Transfer Learning with a Unified Text-t\u2026", "followers": "73", "datetime": "2020-04-21 21:33:29", "author": "@PepFriday"}, "1187232338482102272": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,284", "datetime": "2019-10-24 05:00:21", "author": "@A_K_Nain"}, "1187547895035129856": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "1,394", "datetime": "2019-10-25 01:54:16", "author": "@Chica_Chubb"}, "1187324789284917248": {"content_summary": "A new paper by Google, \"Text-to-Text Transfer Transformer (T5)\", claiming new SOTA in summarization, question answering, text classification etc. https://t.co/IdtEnK82bA #DeepLearning #MachineLearning #NLProc #Google #NeuralNetworks #ArtificialIntelligenc", "followers": "76", "datetime": "2019-10-24 11:07:43", "author": "@MishraAmogh"}, "1227083277799804929": {"content_summary": "@mark_riedl @MSFTResearch That figure seems to leave out last year\u2019s T5 model ( https://t.co/e5P5k3eOO0 ), which has 11 billion parameters. Probably because that makes the plot less impressive :)", "followers": "556", "datetime": "2020-02-11 04:13:46", "author": "@santiontanon"}, "1188206642141257728": {"content_summary": "@dawnieando @fighto BTW, this is a freaking killer paper if ya'll are interested: https://t.co/wd6lZDDeyL", "followers": "5,189", "datetime": "2019-10-26 21:31:53", "author": "@jroakes"}, "1187299312264458241": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "299", "datetime": "2019-10-24 09:26:29", "author": "@westis96"}, "1237483386928840705": {"content_summary": "T5\uff1a\u4f7f\u7528\u7edf\u4e00\u7684\u6587\u672c\u5230\u6587\u672c\u8f6c\u6362\u5668\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u7684\u5c40\u9650\u6027https://t.co/ogH6qvlLEc\uff08subm\u2026https://t.co/1At0mn53wi |\u4f5c\u8005\uff1a@AISC_TO https://t.co/dgSP0SboJ5", "followers": "11", "datetime": "2020-03-10 21:00:05", "author": "@UnitInorganic"}, "1187621803662938114": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "436", "datetime": "2019-10-25 06:47:57", "author": "@krtk"}, "1190068946805940225": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "258", "datetime": "2019-11-01 00:52:01", "author": "@botgopher"}, "1187683205249241089": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "783", "datetime": "2019-10-25 10:51:56", "author": "@muktabh"}, "1187427541952409601": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "216", "datetime": "2019-10-24 17:56:01", "author": "@udaykovur"}, "1187228232829018112": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "43", "datetime": "2019-10-24 04:44:02", "author": "@santhoshkolloju"}, "1202017610902032384": {"content_summary": "I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to discuss research, eat vegan food, and/or go bouldering. [1] https://t.co/UNPL7mP1Re [2] https://t.co/UJKhZJkffv", "followers": "12,611", "datetime": "2019-12-04 00:11:45", "author": "@colinraffel"}, "1201057225810169856": {"content_summary": "\uff11\u30f6\u6708\u524d\u4ee5\u4e0a\u306b\u8ad6\u6587\u304c\u51fa\u3066\u3044\u305f\u3002\u5168\u304f\u30d5\u30a9\u30ed\u30fc\u3067\u304d\u3066\u3044\u306a\u3044\u3002\u3084\u308b\u3053\u3068\u3092\u7d5e\u3089\u306a\u3044\u3068\u99c4\u76ee\u3060\u306a\u3002 https://t.co/0PyoTd0bqB", "followers": "56", "datetime": "2019-12-01 08:35:31", "author": "@SatouTatsurou"}, "1187250978262638597": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2", "datetime": "2019-10-24 06:14:25", "author": "@QingyangWu1"}, "1232984207737212928": {"content_summary": "[5/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,927 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-27 11:01:57", "author": "@PapersTrending"}, "1187344589834768384": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "302", "datetime": "2019-10-24 12:26:24", "author": "@subhobrata1"}, "1256584371059990530": {"content_summary": "Great paper and even better talk. The first 5-10 minutes are the best explanation what happened in NLP in the last 2-3 years, I have seen. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/6Xp6wGXlvH https://t", "followers": "324", "datetime": "2020-05-02 14:00:34", "author": "@simecek"}, "1187643631248654336": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "1,766", "datetime": "2019-10-25 08:14:41", "author": "@jaialkdanel"}, "1198280044830834690": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "13", "datetime": "2019-11-23 16:40:00", "author": "@abdullah_nits"}, "1190556087159808000": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "72", "datetime": "2019-11-02 09:07:45", "author": "@jonasrbati"}, "1187267606681702405": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,740", "datetime": "2019-10-24 07:20:30", "author": "@JeffreyDeFauw"}, "1187414345120137217": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,454", "datetime": "2019-10-24 17:03:35", "author": "@jussikarlgren"}, "1187214263888179201": {"content_summary": "[R] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer: submitted by /u/hardmaru [visit reddit] [comments] https://t.co/IqfMOtU7C9", "followers": "1,186", "datetime": "2019-10-24 03:48:32", "author": "@CarlRioux"}, "1187248400644923392": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "11,930", "datetime": "2019-10-24 06:04:11", "author": "@Rosenchild"}, "1187919612358168576": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "44", "datetime": "2019-10-26 02:31:20", "author": "@amtprs"}, "1187172298589003776": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,875", "datetime": "2019-10-24 01:01:47", "author": "@sschoenholz"}, "1237483382562529280": {"content_summary": "T5: Explorando los l\u00edmites del aprendizaje de transferencia con un transformador de texto a texto unificado https://t.co/ogH6qvlLEc (subm\u2026 https://t.co/1At0mn53wi | Autor: @AISC_TO https://t.co/dgSP0SboJ5", "followers": "11", "datetime": "2020-03-10 21:00:04", "author": "@UnitInorganic"}, "1187513616355880961": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "1,164", "datetime": "2019-10-24 23:38:03", "author": "@hrtmnr"}, "1187250125011374080": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,325", "datetime": "2019-10-24 06:11:02", "author": "@PMinervini"}, "1187586857434935297": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "140", "datetime": "2019-10-25 04:29:05", "author": "@balaprasannav"}, "1189497728499355648": {"content_summary": "[8/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 846 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-30 11:02:12", "author": "@PapersTrending"}, "1203372365440094208": {"content_summary": "RT @agatan_: text-to-text transfer transformer \u3081\u3063\u3061\u3083\u826f\u3044\u3002\u5922\u304c\u5e83\u304c\u308b\u3057\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5e83\u304c\u308b\u3002BART \u3082\u4f3c\u305f\u3088\u3046\u306a\u8a71\u3060\u3051\u3069 T5 \u306f\u8ad6\u6587\u81ea\u4f53\u304c\u3081\u3061\u3083\u304f\u3061\u3083\u9577\u3044\u5206\u3001\u6bd4\u8f03\u3068\u304b\u8003\u5bdf\u306b\u5bcc\u3093\u3067\u3066\u304b\u306a\u308a\u8aad\u307f\u5fdc\u3048\u3042\u308b\u3002 https://t.co\u2026", "followers": "360", "datetime": "2019-12-07 17:55:04", "author": "@nanay_desu"}, "1187186997711917056": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "116", "datetime": "2019-10-24 02:00:11", "author": "@TeddyAmpian"}, "1200321833045778432": {"content_summary": "RT @AT_Amir: T5 by google explores the field of transfer learning in NLP. Very good systematic study on how to pretrain and transfer transf\u2026", "followers": "366", "datetime": "2019-11-29 07:53:20", "author": "@surafelml"}, "1187246539091132416": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "781", "datetime": "2019-10-24 05:56:47", "author": "@HrSaghir"}, "1187393579989245952": {"content_summary": "Really extensive study. #MachineLearning #DeepLearning", "followers": "102", "datetime": "2019-10-24 15:41:04", "author": "@drChromiak"}, "1187331430478692353": {"content_summary": "Weekend reading. And my weekend is starting tonight", "followers": "4,525", "datetime": "2019-10-24 11:34:07", "author": "@vukosi"}, "1192768629877510144": {"content_summary": "\u3044\u307e\u3001\u3053\u3044\u3064\u304c\u3059\u3054\u3044\u3089\u3057\u3044\uff01 T5\u306f\u300cText-To-Text Transfer Transformer\u300d\u306e\u982d\u6587\u5b57\u304b\u3002 https://t.co/Yblf975RYm", "followers": "527", "datetime": "2019-11-08 11:39:36", "author": "@paco_itengineer"}, "1190889123738816512": {"content_summary": "@tsuchm \u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\uff0e\u4e8b\u524d\u5b66\u7fd2\u306b\u4f7f\u3063\u305f\u30c7\u30fc\u30bf\u3068\uff0c\u8a55\u4fa1\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c9\u30e1\u30a4\u30f3\u306b\u95a2\u3057\u3066\u306f\uff0cT5\u8ad6\u6587 https://t.co/d4Nu1b14WT \u306e\u88688\u3068\u305d\u308c\u306b\u95a2\u3059\u308b\u8b70\u8ad6\uff083.4.1\u7bc0\uff09\u304c\u8208\u5473\u6df1\u3044\u7d50\u679c\u3067\u3059\u306e\u3067\u305c\u3072\u3054\u78ba\u8a8d\u304f\u3060\u3055\u3044\uff0e", "followers": "2,439", "datetime": "2019-11-03 07:11:07", "author": "@kyoun"}, "1187168799826432000": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "7,580", "datetime": "2019-10-24 00:47:53", "author": "@poolio"}, "1187525351318011905": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "18", "datetime": "2019-10-25 00:24:41", "author": "@phiedulxp"}, "1204514002048450560": {"content_summary": "RT @hillbig: Many NLP tasks can be represented as a uniform text-to-text problem, (even the task specification is just a prefix of the inpu\u2026", "followers": "478", "datetime": "2019-12-10 21:31:31", "author": "@yasuokajihei"}, "1187170927668105216": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "109", "datetime": "2019-10-24 00:56:20", "author": "@Charles9n"}, "1187180125911052289": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4,460", "datetime": "2019-10-24 01:32:53", "author": "@ada_rob"}, "1187277041852223488": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "166", "datetime": "2019-10-24 07:57:59", "author": "@wannabe_ary"}, "1187514549731090433": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "3,100", "datetime": "2019-10-24 23:41:46", "author": "@takeda25"}, "1245505060995395586": {"content_summary": "RT @icoxfog417: Hugging Face\u306eTransformer\u306b\u8907\u6570\u30bf\u30b9\u30af\u3092\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u5b66\u7fd2\u3057\u305fT5\u306e\u30e2\u30c7\u30eb\u304c\u642d\u8f09\u3002\u30b5\u30f3\u30d7\u30eb\u306eNotebook\u3082\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u3002", "followers": "165", "datetime": "2020-04-02 00:15:21", "author": "@samurairodeo"}, "1198551968530104320": {"content_summary": "T5(Text-to-Text Transfer Transformer)\u3063\u3066\u5168\u90e8\u540c\u3058Encoder\u3068Decoder\u3067Transfer Learning\u3084\u308b\u306e\u304b\u30fc\u3002 SotA\u8ffd\u3046\u306e\u3063\u3066\u5fc5\u305a\u3057\u3082\u672c\u8cea\u7684\u3067\u306f\u306a\u3044\u3068\u601d\u3046\u3051\u3069\u3001\u307e\u3060\u3057\u3070\u3089\u304f\u306f\u3069\u3093\u3069\u3093\u30a2\u30a4\u30c7\u30a2\u51fa\u3066\u308b\u3088\u3046\u306a\u306e\u3067\u3053\u306e\u8fba\u8ffd\u3044\u304b\u3051\u308b\u3060\u3051\u3067\u3082\u53c2\u8003\u306b\u306a\u308b\u5370\u8c61\u3002 https://t.co/XQ828uuinu", "followers": "145", "datetime": "2019-11-24 10:40:31", "author": "@arts_lib"}, "1187209385891811329": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "209", "datetime": "2019-10-24 03:29:09", "author": "@nfiedel"}, "1187849574917922818": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "104", "datetime": "2019-10-25 21:53:02", "author": "@vibhavagarwal5"}, "1187397242010787840": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "41", "datetime": "2019-10-24 15:55:37", "author": "@rormandi"}, "1187330314277982208": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,217", "datetime": "2019-10-24 11:29:41", "author": "@pragmaticml"}, "1187248276447223809": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,414", "datetime": "2019-10-24 06:03:41", "author": "@alxndrkalinin"}, "1187404347698597889": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "4", "datetime": "2019-10-24 16:23:52", "author": "@shfaithy"}, "1188422476482711553": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "259", "datetime": "2019-10-27 11:49:32", "author": "@brdskggs"}, "1187360983603265541": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "323", "datetime": "2019-10-24 13:31:33", "author": "@vodkamomo"}, "1194633458053574656": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "17", "datetime": "2019-11-13 15:09:46", "author": "@IanonsI"}, "1187753075475267585": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "12,765", "datetime": "2019-10-25 15:29:35", "author": "@jaguring1"}, "1190511197352710146": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "27", "datetime": "2019-11-02 06:09:22", "author": "@mahesh21aug"}, "1188343236227547141": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "1,151", "datetime": "2019-10-27 06:34:40", "author": "@jd_mashiro"}, "1232171724860542976": {"content_summary": "RT @timFinin: Google's T5 looks very interesting as does the corpus used to train it, the Colossal Clean Crawled Corpus (C4). Overview post\u2026", "followers": "348", "datetime": "2020-02-25 05:13:26", "author": "@sha_hsg"}, "1219213513354334209": {"content_summary": "[10/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,515 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-01-20 11:02:08", "author": "@PapersTrending"}, "1187162787476316160": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,651", "datetime": "2019-10-24 00:23:59", "author": "@adjiboussodieng"}, "1237550084889251840": {"content_summary": "RT @AISC_TO: T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/o4c7cdF9Xd (submitted by Ro\u2026", "followers": "818", "datetime": "2020-03-11 01:25:07", "author": "@deepgradient"}, "1187509570333052929": {"content_summary": "Many NLP tasks can be represented as a uniform text-to-text problem, (even the task specification is just a prefix of the input), and many techniques and ideas can be compared directly. Combining the findings, they achieved new SOTA on many tasks. https:", "followers": "18,231", "datetime": "2019-10-24 23:21:59", "author": "@hillbig"}, "1187225011347607553": {"content_summary": "Very impressive work in #NLP", "followers": "4", "datetime": "2019-10-24 04:31:14", "author": "@cenyk1230"}, "1187879054973665281": {"content_summary": "RT @yoquankara: Google's T5 (Text-To-Text Transfer Transformer) language model set new record and gets very close to human on SuperGLUE ben\u2026", "followers": "1,151", "datetime": "2019-10-25 23:50:11", "author": "@jd_mashiro"}, "1187377821066379264": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,024", "datetime": "2019-10-24 14:38:27", "author": "@desertnaut"}, "1190060499720904705": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "216", "datetime": "2019-11-01 00:18:28", "author": "@ShyBOT7"}, "1191672020079120385": {"content_summary": "[9/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 954 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-11-05 11:02:04", "author": "@PapersTrending"}, "1194386467105189888": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "1,763", "datetime": "2019-11-12 22:48:19", "author": "@KrisJordan"}, "1202313732296069120": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "1,613", "datetime": "2019-12-04 19:48:26", "author": "@D_Berthelot_ML"}, "1187698995117744128": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "169", "datetime": "2019-10-25 11:54:41", "author": "@stevenhoi"}, "1187712182793846784": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "252", "datetime": "2019-10-25 12:47:05", "author": "@mtorresruiz"}, "1187349660987469826": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,102", "datetime": "2019-10-24 12:46:33", "author": "@kastnerkyle"}, "1187179611722878976": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "66", "datetime": "2019-10-24 01:30:50", "author": "@tenzinchang"}, "1187653227740717056": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "554", "datetime": "2019-10-25 08:52:49", "author": "@FBWM8888"}, "1187220618976071685": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "303", "datetime": "2019-10-24 04:13:47", "author": "@cepera_ang"}, "1187480426626961408": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "62", "datetime": "2019-10-24 21:26:10", "author": "@timothyslau"}, "1206502187322363904": {"content_summary": "@sandeepkaushik @huggingface Google has published another transformer model called T5 which has upto 11B parameters. Page 3, lines 5, 6. The model itself is 50GB in size. Paper here https://t.co/B7WrJso1Ph", "followers": "193", "datetime": "2019-12-16 09:11:51", "author": "@saradhix"}, "1188166390911365121": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "331", "datetime": "2019-10-26 18:51:57", "author": "@sai_prasanna"}, "1187763999569518592": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "1,517", "datetime": "2019-10-25 16:12:59", "author": "@ludwigschubert"}, "1187199596877045760": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5,218", "datetime": "2019-10-24 02:50:15", "author": "@mousstra1512"}, "1245502143475273728": {"content_summary": "RT @icoxfog417: Hugging Face\u306eTransformer\u306b\u8907\u6570\u30bf\u30b9\u30af\u3092\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u5b66\u7fd2\u3057\u305fT5\u306e\u30e2\u30c7\u30eb\u304c\u642d\u8f09\u3002\u30b5\u30f3\u30d7\u30eb\u306eNotebook\u3082\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u3002", "followers": "1,151", "datetime": "2020-04-02 00:03:45", "author": "@jd_mashiro"}, "1187315302729687041": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "401", "datetime": "2019-10-24 10:30:02", "author": "@rbaruah"}, "1188335034585014272": {"content_summary": "#Google T5 NLP #framework. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. https://t.co/wnjNYZv0HE #machinelearning #NLP #algorithms https://t.co/OHytR9tlKe", "followers": "940", "datetime": "2019-10-27 06:02:05", "author": "@job_santana"}, "1233709130956886016": {"content_summary": "[8/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,959 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-29 11:02:32", "author": "@PapersTrending"}, "1187345850969669633": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "819", "datetime": "2019-10-24 12:31:25", "author": "@akf"}, "1189094244796919808": {"content_summary": "RT @fhiyo_: GLUE, SuperGLUE\u3068\u304b\u3067\u65b0\u3057\u3044SoTA\u306e\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u3044\u308b https://t.co/MfeP9G5QVj", "followers": "12,765", "datetime": "2019-10-29 08:18:54", "author": "@jaguring1"}, "1188162516024561664": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "119", "datetime": "2019-10-26 18:36:33", "author": "@shang_vikas"}, "1187363401774841858": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "24", "datetime": "2019-10-24 13:41:09", "author": "@rewreu"}, "1187456546537529344": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "634", "datetime": "2019-10-24 19:51:17", "author": "@mandarjoshi_"}, "1187322380563156993": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "66", "datetime": "2019-10-24 10:58:09", "author": "@paradeSuburbia"}, "1187324943484309504": {"content_summary": "RT @DataScienceFr: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the\u2026", "followers": "2,352", "datetime": "2019-10-24 11:08:20", "author": "@FranceDevOps"}, "1187731015323791360": {"content_summary": "RT @nikiparmar09: Great work! Text to text Transformer with a masking loss does better than other Transfer learning techniques. https://t.c\u2026", "followers": "194", "datetime": "2019-10-25 14:01:55", "author": "@jastner109"}, "1188032821283213312": {"content_summary": "[1/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 633 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-26 10:01:11", "author": "@PapersTrending"}, "1187348470304301057": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "2,193", "datetime": "2019-10-24 12:41:49", "author": "@michh_31415"}, "1187549738096152576": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "426", "datetime": "2019-10-25 02:01:35", "author": "@okamoto0409"}, "1187570677366149120": {"content_summary": "Super rich supports super research...", "followers": "22", "datetime": "2019-10-25 03:24:48", "author": "@ShilinHe"}, "1187556330443116544": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "155", "datetime": "2019-10-25 02:27:47", "author": "@Trtd6Trtd"}, "1192419002255073281": {"content_summary": "New SOTA in NLP by @Google: \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" https://t.co/DKuTOmUonC This is the Formula 1 of NLP, if you blink you'll miss it #nlp #deeplearning #machinelearning https://t.co/y0oIPVFbh7", "followers": "900", "datetime": "2019-11-07 12:30:18", "author": "@miguelgfierro"}, "1194104226932285440": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "93", "datetime": "2019-11-12 04:06:47", "author": "@0xhexhex"}, "1187424952493826048": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "91", "datetime": "2019-10-24 17:45:44", "author": "@Ohnyx2"}, "1187380477231058944": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "21", "datetime": "2019-10-24 14:49:00", "author": "@sbmaruf"}, "1187361660643622913": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "432", "datetime": "2019-10-24 13:34:14", "author": "@bhargavbardipur"}, "1200438985333657600": {"content_summary": "For the curious \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" https://t.co/Nvre7cafBt", "followers": "518", "datetime": "2019-11-29 15:38:51", "author": "@AndrewCutler13"}, "1257226496222965761": {"content_summary": "@StephenLLH @voxmenthe @huggingface Depends which version. The largest version of T5 beats BERT and friends in quite a few tasks \ud83d\ude00 You can find this in the paper. https://t.co/LTVXBTQsLP", "followers": "551", "datetime": "2020-05-04 08:32:09", "author": "@AND__SO"}, "1187167306062929920": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4,636", "datetime": "2019-10-24 00:41:56", "author": "@mohitban47"}, "1187247342275006464": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "90,659", "datetime": "2019-10-24 05:59:59", "author": "@stanfordnlp"}, "1187450660662366208": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "230", "datetime": "2019-10-24 19:27:53", "author": "@shashank_bits"}, "1187471293815259139": {"content_summary": "RT @jmhessel: Table 15 from T5 might be the most computationally expensive table ever constructed in the history of natural language proces\u2026", "followers": "85", "datetime": "2019-10-24 20:49:53", "author": "@FlorianDreher"}, "1187167352837660672": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,613", "datetime": "2019-10-24 00:42:08", "author": "@D_Berthelot_ML"}, "1187162202471448576": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4,620", "datetime": "2019-10-24 00:21:40", "author": "@Luke_Metz"}, "1187249043556225025": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "245", "datetime": "2019-10-24 06:06:44", "author": "@Rovio_Red"}, "1187475254148354048": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "444", "datetime": "2019-10-24 21:05:37", "author": "@designMoreData"}, "1188088101295419393": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "651", "datetime": "2019-10-26 13:40:51", "author": "@iamsidd"}, "1188411298729488384": {"content_summary": "RT @jaguring1: T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/\u2026", "followers": "516", "datetime": "2019-10-27 11:05:07", "author": "@risounomoewaifu"}, "1229772951974625280": {"content_summary": "RT @LChoshen: @royschwartz02 @nlpnoah @AliFarhadi @JesseDodge @gabriel_ilharco #enough2skim Also has some recent general suggestions (and a\u2026", "followers": "0", "datetime": "2020-02-18 14:21:34", "author": "@Sam09lol"}, "1188749791599022080": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,169", "datetime": "2019-10-28 09:30:10", "author": "@zaoriku0"}, "1187590554810339334": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,114", "datetime": "2019-10-25 04:43:47", "author": "@vanessa_murdock"}, "1187177118523772929": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "399", "datetime": "2019-10-24 01:20:56", "author": "@uetschy"}, "1187453999659782144": {"content_summary": "Table 15 from T5 might be the most computationally expensive table ever constructed in the history of natural language processing \ud83d\ude40 https://t.co/Xwxw1rGxOQ https://t.co/wv40rj1MPs", "followers": "1,254", "datetime": "2019-10-24 19:41:10", "author": "@jmhessel"}, "1187198197220835329": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "130", "datetime": "2019-10-24 02:44:41", "author": "@krunal_wrote"}, "1187249447148896256": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "721", "datetime": "2019-10-24 06:08:20", "author": "@MyLinguistics"}, "1195265647317356544": {"content_summary": "10\u6708\u4e0b\u65ec\u306bGoogle\u304b\u3089\u6700\u65b0NLP\u30e2\u30c7\u30eb\u201dT5\u201d\u304c\u767a\u8868\u3055\u308c\u3001GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u30671\u4f4d\u3092\u7372\u5f97\u3002\u30e2\u30c7\u30eb\u540d\u306f\u8a2d\u8a08\u30b3\u30f3\u30bb\u30d7\u30c8\u3067\u3042\u308b\u201dText-to-Text Transfer Transformer\u201d\u306b\u7740\u60f3\u3057\u3066\u3044\u308b\u3002\u8ee2\u79fb\u5b66\u7fd2\u3068T2T\u51e6\u7406\u304c\u809d\u3089\u3057\u3044\u3002 #NLP #Google https://t.co/GShH0BggLM", "followers": "86", "datetime": "2019-11-15 09:01:51", "author": "@kohkiyoshi"}, "1187331118422548480": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "236", "datetime": "2019-10-24 11:32:52", "author": "@flowing"}, "1187222131295313922": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "7", "datetime": "2019-10-24 04:19:48", "author": "@panteon_college"}, "1187612599225069568": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "44", "datetime": "2019-10-25 06:11:23", "author": "@pratsbhatt"}, "1187467674336448513": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "713", "datetime": "2019-10-24 20:35:30", "author": "@eagirre"}, "1187272455666176000": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "20", "datetime": "2019-10-24 07:39:46", "author": "@Boristream"}, "1187369711597699073": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "58", "datetime": "2019-10-24 14:06:14", "author": "@ikanez"}, "1187243674796617728": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,128", "datetime": "2019-10-24 05:45:24", "author": "@LeoKTam"}, "1187252099442196481": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "257", "datetime": "2019-10-24 06:18:53", "author": "@milangritta"}, "1227973182637957121": {"content_summary": "@erikbryn @MSFTResearch @danielrock This graph from Microsoft about who has the biggest transformer skips the second biggest one, which is from Google: https://t.co/e5sifjj3Nb (11B parameters). Weird.", "followers": "76", "datetime": "2020-02-13 15:09:56", "author": "@sorenmind"}, "1188395419216146432": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "12,765", "datetime": "2019-10-27 10:02:01", "author": "@jaguring1"}, "1187273279850991616": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "302", "datetime": "2019-10-24 07:43:03", "author": "@subhobrata1"}, "1188200155381669894": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "608", "datetime": "2019-10-26 21:06:07", "author": "@patternproject"}, "1187385632974262273": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "4,327", "datetime": "2019-10-24 15:09:30", "author": "@jekbradbury"}, "1188329871660863488": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "425", "datetime": "2019-10-27 05:41:34", "author": "@ryo_masumura"}, "1187495613392244736": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "54", "datetime": "2019-10-24 22:26:31", "author": "@Surreabral"}, "1187404711814729728": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "75", "datetime": "2019-10-24 16:25:18", "author": "@emillinhone"}, "1187353418467332098": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "874", "datetime": "2019-10-24 13:01:29", "author": "@mrdrozdov"}, "1187391061863350272": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "89", "datetime": "2019-10-24 15:31:04", "author": "@singularpattern"}, "1187258918268915714": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "62,400", "datetime": "2019-10-24 06:45:58", "author": "@RichardSocher"}, "1187607288472854528": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "27", "datetime": "2019-10-25 05:50:16", "author": "@dhruvkr1993"}, "1187352263603433472": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "29", "datetime": "2019-10-24 12:56:54", "author": "@n0shadow"}, "1187333084087308291": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "2,195", "datetime": "2019-10-24 11:40:41", "author": "@baykenney"}, "1187763883588640768": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,517", "datetime": "2019-10-25 16:12:32", "author": "@ludwigschubert"}, "1187314147614175232": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "172", "datetime": "2019-10-24 10:25:26", "author": "@ggdupont"}, "1190053808312897536": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "6,847", "datetime": "2019-10-31 23:51:52", "author": "@rstatstweet"}, "1190615869748940800": {"content_summary": "RT @yohei_kikuta: \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u307e\u3068\u3081\u3001\u610f\u5916\u3068\u77e5\u3063\u3066\u3044\u308b\u3082\u306e\u304c\u591a\u304b\u3063\u305f\u304c\u826f\u3044\u6574\u7406\u306b\u306a\u3063\u305f\u3002 \u6700\u8fd1\u51fa\u305f T5 \u3068\u3044\u3046\u3001encoder-decoder \u306e seq2seq \u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3059\u308c\u3070\u5206\u985e\u3068\u304b QA \u304c\u540c\u3058\u3088\u3046\u306b\u6271\u3048\u308b\u306e\u3067\u305d\u308c\u3092\u89e3\u3044\u3066\u826f\u3044\u4e8b\u524d\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u3001\u2026", "followers": "225", "datetime": "2019-11-02 13:05:18", "author": "@ElectronNest"}, "1202317579131351040": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "2,670", "datetime": "2019-12-04 20:03:43", "author": "@ayirpelle"}, "1187340088591769600": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "35", "datetime": "2019-10-24 12:08:31", "author": "@tianshi5940"}, "1187366017003147265": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "237", "datetime": "2019-10-24 13:51:33", "author": "@veydpz_public"}, "1188467546132901888": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "162", "datetime": "2019-10-27 14:48:38", "author": "@kurama554101"}, "1187439118684311552": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "70", "datetime": "2019-10-24 18:42:02", "author": "@sumitsethy"}, "1187673022393548801": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "19,021", "datetime": "2019-10-25 10:11:29", "author": "@gijigae"}, "1187587663387189248": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "478", "datetime": "2019-10-25 04:32:17", "author": "@yasuokajihei"}, "1188813900000530432": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "739", "datetime": "2019-10-28 13:44:55", "author": "@philip368320"}, "1187690553108058112": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "10", "datetime": "2019-10-25 11:21:08", "author": "@Nik_the_M"}, "1194702731161718785": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "1,160", "datetime": "2019-11-13 19:45:02", "author": "@summerlight00"}, "1187693154050461696": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,214", "datetime": "2019-10-25 11:31:28", "author": "@tingchenai"}, "1190126832273805312": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "5,603", "datetime": "2019-11-01 04:42:02", "author": "@gdorn1"}, "1187967550338093058": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "63", "datetime": "2019-10-26 05:41:50", "author": "@TachyHealth"}, "1187343254632701953": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "476", "datetime": "2019-10-24 12:21:06", "author": "@juliaroz_ml"}, "1256573573457743873": {"content_summary": "Brno Reading Group is not taking place due COVID-19. If you want to read ML paper together, you can take advantage of DairAI Zoom paper reading session. Starting right now. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "followers": "504", "datetime": "2020-05-02 13:17:40", "author": "@mlmucz"}, "1190053179766931457": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "33,095", "datetime": "2019-10-31 23:49:22", "author": "@javascriptd"}, "1187233185509199872": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "62", "datetime": "2019-10-24 05:03:43", "author": "@samhardyhey"}, "1187910113207734273": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "14", "datetime": "2019-10-26 01:53:35", "author": "@ProjectDomani"}, "1200327189582888960": {"content_summary": "RT @AT_Amir: T5 by google explores the field of transfer learning in NLP. Very good systematic study on how to pretrain and transfer transf\u2026", "followers": "181", "datetime": "2019-11-29 08:14:37", "author": "@chstoneliu"}, "1187740501459513345": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,387", "datetime": "2019-10-25 14:39:37", "author": "@cedric_chee"}, "1187733991018979329": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "365", "datetime": "2019-10-25 14:13:45", "author": "@iugoaoj"}, "1187746738200817666": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "26", "datetime": "2019-10-25 15:04:24", "author": "@oskaus"}, "1187229524481544193": {"content_summary": "Turn it up to 11 (billion parameters)", "followers": "3,096", "datetime": "2019-10-24 04:49:10", "author": "@ivan_bezdomny"}, "1187219020203876352": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "65", "datetime": "2019-10-24 04:07:26", "author": "@Dhruvrnaik"}, "1187167478125858818": {"content_summary": "https://t.co/C0AZqP0ap2 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v1 [cs.LG]) #NLProc", "followers": "4,200", "datetime": "2019-10-24 00:42:37", "author": "@arxiv_cs_cl"}, "1188791472876732416": {"content_summary": "GLUE, SuperGLUE\u3068\u304b\u3067\u65b0\u3057\u3044SoTA\u306e\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u3044\u308b", "followers": "67", "datetime": "2019-10-28 12:15:48", "author": "@fhiyo_"}, "1187473733650272257": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "406", "datetime": "2019-10-24 20:59:34", "author": "@DieEmme"}, "1187387086921326598": {"content_summary": "Want to know more about how machine learning and AI process content? You should because they are customers too. I'm examining this and so much more @UXSofia in a few weeks. Hope to see you there. It's important. #machineexperiencemx https://t.co/lqIfhs01", "followers": "2,337", "datetime": "2019-10-24 15:15:16", "author": "@msweeny"}, "1237483384739373060": {"content_summary": "T5: Explorando os limites do aprendizado de transfer\u00eancia com um transformador unificado de texto para texto https://t.co/ogH6qvlLEc (subm\u2026 https://t.co/1At0mn53wi | Autor: @AISC_TO https://t.co/dgSP0SboJ5", "followers": "11", "datetime": "2020-03-10 21:00:04", "author": "@UnitInorganic"}, "1187471495347359746": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "26", "datetime": "2019-10-24 20:50:41", "author": "@Tshiviah"}, "1187502616575504384": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "470", "datetime": "2019-10-24 22:54:21", "author": "@john_c_hawkins"}, "1187418912184356865": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "414", "datetime": "2019-10-24 17:21:44", "author": "@NonLocalityGuy"}, "1187577858895560704": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "248", "datetime": "2019-10-25 03:53:20", "author": "@gsksantosh"}, "1187510661565206528": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "824", "datetime": "2019-10-24 23:26:19", "author": "@morioka"}, "1187438519066619905": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "19,072", "datetime": "2019-10-24 18:39:39", "author": "@xamat"}, "1187558849441497088": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "23,865", "datetime": "2019-10-25 02:37:48", "author": "@tanji_y"}, "1192193847826112512": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "111", "datetime": "2019-11-06 21:35:37", "author": "@nimblel"}, "1200380310443020288": {"content_summary": "RT @fbk_mt: Our pick of the week: Raffel et al. paper on \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u2026", "followers": "181", "datetime": "2019-11-29 11:45:42", "author": "@chstoneliu"}, "1187364045382402054": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "12", "datetime": "2019-10-24 13:43:43", "author": "@JimmyCh60659590"}, "1187176917499297794": {"content_summary": "Holy shit", "followers": "1,411", "datetime": "2019-10-24 01:20:08", "author": "@RexDouglass"}, "1187511092265668608": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "993", "datetime": "2019-10-24 23:28:01", "author": "@taniokah"}, "1187413208761536513": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "157", "datetime": "2019-10-24 16:59:04", "author": "@bobb_robinson"}, "1187204210393268224": {"content_summary": "RT @mrdrozdov: FYI, 1 trillion tokens and 11 billion parameters. https://t.co/IGYmThfVct", "followers": "97", "datetime": "2019-10-24 03:08:35", "author": "@vin_mathur"}, "1187289283310342144": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "19", "datetime": "2019-10-24 08:46:38", "author": "@vonum123"}, "1224873855006568449": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "375", "datetime": "2020-02-05 01:54:18", "author": "@remykarem"}, "1232621784941441025": {"content_summary": "[10/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,890 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-26 11:01:49", "author": "@PapersTrending"}, "1190418798333087745": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "377", "datetime": "2019-11-02 00:02:13", "author": "@captainheidi"}, "1187219608563933184": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "30", "datetime": "2019-10-24 04:09:46", "author": "@Tn23008624"}, "1190919369246810112": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "1,938", "datetime": "2019-11-03 09:11:18", "author": "@EldarSilver"}, "1188843138481377280": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "201", "datetime": "2019-10-28 15:41:06", "author": "@ZetaVector"}, "1187261782420791296": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "393", "datetime": "2019-10-24 06:57:21", "author": "@leggendario12"}, "1187345451252621313": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "13", "datetime": "2019-10-24 12:29:50", "author": "@dill_sunnyb11"}, "1187257944267804673": {"content_summary": "[Paper Reading List]", "followers": "686", "datetime": "2019-10-24 06:42:06", "author": "@maver"}, "1187647404171911168": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "8,478", "datetime": "2019-10-25 08:29:41", "author": "@ai_news_jp"}, "1187183522932387840": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "5,508", "datetime": "2019-10-24 01:46:23", "author": "@jesseengel"}, "1194566686575341568": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,231", "datetime": "2019-11-13 10:44:26", "author": "@jackiefloyd"}, "1187340093541048322": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "148", "datetime": "2019-10-24 12:08:32", "author": "@officialtaran"}, "1187416744605782016": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "30,369", "datetime": "2019-10-24 17:13:07", "author": "@notwaldorf"}, "1187606509510742016": {"content_summary": "Happy that I thot of masking the inputs ( and implemented ) in seq2seq using @OpenAI gpt2 , unlike the typical LM loss. Cz I thot we have more visibility. Now, @colinraffel mentioned this as prefix LM in 3.2.1 in https://t.co/wYj7bKBGHD Glad that my draw w", "followers": "43", "datetime": "2019-10-25 05:47:11", "author": "@iamsarath"}, "1187349138188443648": {"content_summary": "@sleepinyourhat I'm a big fan of Google's Limit Exploration Group https://t.co/8xOksUUssw https://t.co/1nvgIBe0sb", "followers": "6,307", "datetime": "2019-10-24 12:44:29", "author": "@tallinzen"}, "1187352324513112064": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "136", "datetime": "2019-10-24 12:57:08", "author": "@FaisalMaqbool94"}, "1187599336235909121": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "595", "datetime": "2019-10-25 05:18:40", "author": "@cddadr"}, "1212456481490030593": {"content_summary": "@zacharylipton Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - For it's exhaustive study https://t.co/DqrKn9elL3", "followers": "331", "datetime": "2020-01-01 19:32:06", "author": "@sai_prasanna"}, "1188403825520054273": {"content_summary": "RT @arxiv_cscl: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "11,930", "datetime": "2019-10-27 10:35:26", "author": "@Rosenchild"}, "1190050638039642112": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "5,058", "datetime": "2019-10-31 23:39:16", "author": "@DeepSingularity"}, "1190531412341882881": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "470", "datetime": "2019-11-02 07:29:42", "author": "@SrirangaTarun"}, "1187705384187891713": {"content_summary": "THE END OF SESAME STREET: Google's T5 Transformer released yesterday. Achieves SOTA results on SuperGlue, 0.9 away from human performance! #AI #ArtificialIntelligence #NLP #NLProc #MachineLearning #DeepLearning Paper: https://t.co/9gVUqyQldM Code: https:", "followers": "917", "datetime": "2019-10-25 12:20:04", "author": "@Quantum_Stat"}, "1187336822805934081": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "3,219", "datetime": "2019-10-24 11:55:32", "author": "@mrt1nz"}, "1187332164108636162": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "160,790", "datetime": "2019-10-24 11:37:02", "author": "@Quebec_AI"}, "1187807372456017920": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "12", "datetime": "2019-10-25 19:05:20", "author": "@2000Qiu"}, "1187270568191680515": {"content_summary": "#DataScience #Analytics https://t.co/QbhBy9EtUG", "followers": "309", "datetime": "2019-10-24 07:32:16", "author": "@MichaelMallari"}, "1187358846681006082": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "164,079", "datetime": "2019-10-24 13:23:03", "author": "@ceobillionaire"}, "1187339407877001216": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "17", "datetime": "2019-10-24 12:05:49", "author": "@Emir93230828"}, "1187188753233797122": {"content_summary": "RT @roadrunning01: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer pdf: https://t.co/hECWfJh0Ny abs: http\u2026", "followers": "2,070", "datetime": "2019-10-24 02:07:10", "author": "@EricSchles"}, "1187831695736786944": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "58", "datetime": "2019-10-25 20:41:59", "author": "@motabuto"}, "1187186842262589440": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,670", "datetime": "2019-10-24 01:59:34", "author": "@ayirpelle"}, "1187495332067729408": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "216", "datetime": "2019-10-24 22:25:24", "author": "@KSKSKSKS2"}, "1187568787450515456": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "514", "datetime": "2019-10-25 03:17:17", "author": "@wkwkrnht"}, "1190484692014747649": {"content_summary": "\u5b9f\u9a13\u306e\u91cf\u304c\u3084\u3070\u3044 \u8a08\u7b97\u8cc7\u6e90\u304c\u3084\u3070\u3044 #xpaperchallenge", "followers": "3,265", "datetime": "2019-11-02 04:24:03", "author": "@ymym3412"}, "1188097676522348545": {"content_summary": "T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/1b00tIjxqE", "followers": "12,765", "datetime": "2019-10-26 14:18:54", "author": "@jaguring1"}, "1187161963727446016": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "287", "datetime": "2019-10-24 00:20:43", "author": "@peterjliu"}, "1187400022956085248": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "43", "datetime": "2019-10-24 16:06:40", "author": "@jeandut14000"}, "1202957761929080837": {"content_summary": "T5\u30e2\u30c7\u30eb\u306e\u30b9\u30b1\u30fc\u30eb\u304c\u304a\u304b\u3057\u304f\u3066\u9762\u767d\u3044 https://t.co/Xmd6gX8gZ8", "followers": "198", "datetime": "2019-12-06 14:27:34", "author": "@LetItBe_Or_Not"}, "1201634915688673280": {"content_summary": "A few cool examples: - Baseline (row 1 of table 8) from \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" (https://t.co/vhqCZrwuiL): https://t.co/njhGfHiqzn - BERT model training from https://t.co/h50ZffM8Zm: https://t.c", "followers": "237", "datetime": "2019-12-02 22:51:03", "author": "@GalOshri"}, "1187449749399408640": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "2,927", "datetime": "2019-10-24 19:24:16", "author": "@evolvingstuff"}, "1187185902805803009": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "449", "datetime": "2019-10-24 01:55:50", "author": "@iwontbecreative"}, "1187667661163057152": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "2,404", "datetime": "2019-10-25 09:50:10", "author": "@jiboncom"}, "1187227509886029825": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm2MEjw", "followers": "3,501", "datetime": "2019-10-24 04:41:10", "author": "@arxiv_cscl"}, "1187636536701018113": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "54", "datetime": "2019-10-25 07:46:30", "author": "@AleMas2100"}, "1187345206070435846": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "265", "datetime": "2019-10-24 12:28:51", "author": "@HoanNguyen88"}, "1190495826981769216": {"content_summary": "\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u307e\u3068\u3081\u3001\u610f\u5916\u3068\u77e5\u3063\u3066\u3044\u308b\u3082\u306e\u304c\u591a\u304b\u3063\u305f\u304c\u826f\u3044\u6574\u7406\u306b\u306a\u3063\u305f\u3002 \u6700\u8fd1\u51fa\u305f T5 \u3068\u3044\u3046\u3001encoder-decoder \u306e seq2seq \u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3059\u308c\u3070\u5206\u985e\u3068\u304b QA \u304c\u540c\u3058\u3088\u3046\u306b\u6271\u3048\u308b\u306e\u3067\u305d\u308c\u3092\u89e3\u3044\u3066\u826f\u3044\u4e8b\u524d\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u3001\u3068\u3044\u3046\u306e\u306f\u628a\u63e1\u3057\u3066\u306a\u304b\u3063\u305f\u304c\u306a\u304b\u306a\u304b\u9762\u767d\u305d\u3046\u3002 https://t.co/JrErPqoJnH", "followers": "1,703", "datetime": "2019-11-02 05:08:18", "author": "@yohei_kikuta"}, "1187515520926404609": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "518", "datetime": "2019-10-24 23:45:37", "author": "@pijili"}, "1187256932878340097": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "21", "datetime": "2019-10-24 06:38:05", "author": "@algo_diver"}, "1187372937575190528": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "727", "datetime": "2019-10-24 14:19:03", "author": "@wenmingye"}, "1192071229164998657": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "689", "datetime": "2019-11-06 13:28:23", "author": "@nomadics"}, "1187871625737170945": {"content_summary": "So I looked into the new Text-To-Text Transfer Transformer stuff today - seems neat, even just for the big new accompanying corpus https://t.co/KxmObmqT5V", "followers": "170", "datetime": "2019-10-25 23:20:39", "author": "@harry_caufield"}, "1187441850254315521": {"content_summary": "Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more affect downstream task performance, and how well can we do on when we combine these insights & scale. It was amazing to collaborat", "followers": "683", "datetime": "2019-10-24 18:52:53", "author": "@katherine1ee"}, "1187679788737142784": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "57", "datetime": "2019-10-25 10:38:22", "author": "@timohear"}, "1244765732103208963": {"content_summary": "Great!", "followers": "5", "datetime": "2020-03-30 23:17:31", "author": "@PierreLePire"}, "1202142610808135680": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "609", "datetime": "2019-12-04 08:28:27", "author": "@kurianbenoy2"}, "1187187810152931329": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "160", "datetime": "2019-10-24 02:03:25", "author": "@tuvuumass"}, "1187371506017460225": {"content_summary": "\u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWrQzxq https://t.co/bS9SqlBFEz", "followers": "12,765", "datetime": "2019-10-24 14:13:21", "author": "@jaguring1"}, "1188335253712187393": {"content_summary": "RT @job_santana: #Google T5 NLP #framework. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. https://t.co\u2026", "followers": "1,877", "datetime": "2019-10-27 06:02:57", "author": "@gdprAI"}, "1190427902690512896": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "19", "datetime": "2019-11-02 00:38:23", "author": "@RoshniAnalytics"}, "1187568730978439169": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,543", "datetime": "2019-10-25 03:17:04", "author": "@KrutPatel2257"}, "1188510955753398274": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "3,501", "datetime": "2019-10-27 17:41:07", "author": "@arxiv_cscl"}, "1187708254081581056": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "425", "datetime": "2019-10-25 12:31:28", "author": "@ryo_masumura"}, "1202459098664464386": {"content_summary": "RT @colinraffel: I will be at @NeurIPSConf next week to present MixMatch [1] and give a T5 demo [2]! Please get in touch if you want to dis\u2026", "followers": "302", "datetime": "2019-12-05 05:26:04", "author": "@subhobrata1"}, "1187319086826622977": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "21", "datetime": "2019-10-24 10:45:04", "author": "@IndexFziQ"}, "1187248755956948992": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "34", "datetime": "2019-10-24 06:05:36", "author": "@DeepMatrixCloud"}, "1187482060400844802": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "0", "datetime": "2019-10-24 21:32:40", "author": "@AnnaC50348062"}, "1187168445495799808": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "508", "datetime": "2019-10-24 00:46:28", "author": "@NickFisherAU"}, "1194367290998808577": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "2,670", "datetime": "2019-11-12 21:32:07", "author": "@ayirpelle"}, "1187403479758753792": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "331", "datetime": "2019-10-24 16:20:25", "author": "@sai_prasanna"}, "1187167505988505600": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "392", "datetime": "2019-10-24 00:42:44", "author": "@HanGuo97"}, "1187171298449022976": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "494", "datetime": "2019-10-24 00:57:48", "author": "@jbohnslav"}, "1187793204118249473": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/pi2AUzgJZ4 https://t.co/loJRHx74xc", "followers": "907", "datetime": "2019-10-25 18:09:02", "author": "@AISC_TO"}, "1187428548593602560": {"content_summary": "[R] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/FvToWUG8Bs #MachineLearning", "followers": "731", "datetime": "2019-10-24 18:00:01", "author": "@therealjpittman"}, "1187907749113450497": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "53", "datetime": "2019-10-26 01:44:12", "author": "@alejandro56664"}, "1187563952672133120": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "180", "datetime": "2019-10-25 02:58:04", "author": "@sriharshams"}, "1187637701429911557": {"content_summary": "One of the best work in NLP research recently. A number of ablations, from datasets, to architectures, to pretraining objectives and much more. Love when we correctly identify where we get the gains from.", "followers": "506", "datetime": "2019-10-25 07:51:07", "author": "@sksq96"}, "1187547752789491712": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "1,394", "datetime": "2019-10-25 01:53:42", "author": "@Chica_Chubb"}, "1188371380716355584": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "116", "datetime": "2019-10-27 08:26:30", "author": "@arnaudmiribel"}, "1190050670952345600": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "93", "datetime": "2019-10-31 23:39:24", "author": "@RaymondWSA460"}, "1187161460033458177": {"content_summary": "New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the limits to achieve SoTA on GLUE, SuperGLUE, CNN/DM, and SQuAD. Paper: https://t.co/UJKhZJkffv Code/models/data/etc: https://t.co/IiktjHrr", "followers": "12,611", "datetime": "2019-10-24 00:18:43", "author": "@colinraffel"}, "1187579109578133506": {"content_summary": "The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape in NLP. Check out page 33 of the paper or below for the main takeaways. https://t.co/HeOqcAv1aw https://t.co/nPHy4YA4wd", "followers": "50", "datetime": "2019-10-25 03:58:18", "author": "@Pearl_Sonali"}, "1194495216893468672": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,001", "datetime": "2019-11-13 06:00:27", "author": "@SassonMargaliot"}, "1187653762044846080": {"content_summary": "'Pinoystartup/data-science' Top: [1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/Xbv2aYbZ4n, see more https://t.co/LmVgIMJS6a", "followers": "71", "datetime": "2019-10-25 08:54:57", "author": "@willynguen"}, "1187530295546769409": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "143", "datetime": "2019-10-25 00:44:20", "author": "@n_kats_"}, "1187476428213735424": {"content_summary": "RT @katherine1ee: Curious about the state of NLP? We explore how different pre-training objectives, datasets, training strategies, and more\u2026", "followers": "566", "datetime": "2019-10-24 21:10:17", "author": "@BaselessPursuit"}, "1187270565549301760": {"content_summary": "#DataScience #Analytics https://t.co/knSVsI7Bo5", "followers": "309", "datetime": "2019-10-24 07:32:15", "author": "@MichaelMallari"}, "1187323982002970625": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1", "datetime": "2019-10-24 11:04:31", "author": "@Pol09122455"}, "1187332585053130752": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "136", "datetime": "2019-10-24 11:38:42", "author": "@Mirko_Torrisi"}, "1187567372359495680": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "1,377", "datetime": "2019-10-25 03:11:40", "author": "@kaineko"}, "1194244964189790214": {"content_summary": "Thanks @Sam_Witteveen for summarising the T5 paper (https://t.co/T6qnMEAznr)! Learnt a ton without digging through 53 pages! @DevSpaceSG @GoogleAI https://t.co/bmhyotXxNK", "followers": "336", "datetime": "2019-11-12 13:26:02", "author": "@DerekChia"}, "1187525342635806720": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,145", "datetime": "2019-10-25 00:24:39", "author": "@niku9Tenhou"}, "1187206849092734976": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "484", "datetime": "2019-10-24 03:19:04", "author": "@timothy_lkh_"}, "1187538352871858177": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "177", "datetime": "2019-10-25 01:16:21", "author": "@meshidenn"}, "1187643866330959872": {"content_summary": "RT @jaguring1: \u304a\u3001\u3059\u3054\u3044\u3002\u30b0\u30fc\u30b0\u30eb\u306e\u30e2\u30c7\u30eb\u300cT5\u300d\u304c17\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\u3092\u66f4\u65b0\u3002\u7279\u306b\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306b\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u300cSuperGLUE\u300d\u3067\u6210\u679c\u3002\u5e38\u8b58\u304c\u306a\u3044\u3068\u89e3\u3051\u306a\u3044\u3068\u8a00\u308f\u308c\u3066\u305fWSC\u3084WNLI\u3067\u3082\u5411\u4e0a\u3002 Exploring the Limits of Tr\u2026", "followers": "2,464", "datetime": "2019-10-25 08:15:37", "author": "@func_hs"}, "1237878029801545734": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "15", "datetime": "2020-03-11 23:08:15", "author": "@Vladimi80535966"}, "1193158060413505536": {"content_summary": "RT @carsondahlberg: #goodread Google's releases new #transferlearning research on it's #NLP #T5 #model is pretty sweet #ai #datascience #\u2026", "followers": "4,124", "datetime": "2019-11-09 13:27:04", "author": "@WIOMAX_PA"}, "1187256971310727168": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "68", "datetime": "2019-10-24 06:38:14", "author": "@NoppadonKoo"}, "1187414778408570880": {"content_summary": "Big week for @GoogleAI! \u2705Quantum supremacy achieved: https://t.co/hdrmUaT7OB \u2705Massive text-to-text neural net beats state of the art: https://t.co/E2Z8lRaOmp \u2705Gmail autocomplete writes 50% of my emails \ud83d\ude1c", "followers": "809", "datetime": "2019-10-24 17:05:18", "author": "@lukeinusa"}, "1188265437433253888": {"content_summary": "RT @jaguring1: T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/\u2026", "followers": "824", "datetime": "2019-10-27 01:25:31", "author": "@morioka"}, "1187567595815223298": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "1,151", "datetime": "2019-10-25 03:12:33", "author": "@jd_mashiro"}, "1190053142957658113": {"content_summary": "RT @gp_pulipaka: Exploring the Limits of #TransferLearning with Transformer. #BigData #Analytics #DataScience #AI #MachineLearning #NLProc\u2026", "followers": "6,714", "datetime": "2019-10-31 23:49:14", "author": "@chidambara09"}, "1187274918641029120": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "46", "datetime": "2019-10-24 07:49:33", "author": "@Sarantium_"}, "1187623204891721728": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "137", "datetime": "2019-10-25 06:53:31", "author": "@TinDan_"}, "1235967995777712129": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/IgAD3rPsBG", "followers": "52", "datetime": "2020-03-06 16:38:28", "author": "@agesmundo"}, "1188327959888707584": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "1,250", "datetime": "2019-10-27 05:33:58", "author": "@FujitaAtsunori"}, "1190482702983675904": {"content_summary": "RT @ryan_chesler: https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A l\u2026", "followers": "42", "datetime": "2019-11-02 04:16:09", "author": "@MishakinSergey"}, "1187317593448894464": {"content_summary": "SOTA on NLP", "followers": "1,348", "datetime": "2019-10-24 10:39:08", "author": "@Lidinwise"}, "1188329887204925442": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "643", "datetime": "2019-10-27 05:41:37", "author": "@ospf_area0"}, "1187354531237109760": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "86", "datetime": "2019-10-24 13:05:54", "author": "@writetoswarna"}, "1187590277327806464": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "12,765", "datetime": "2019-10-25 04:42:41", "author": "@jaguring1"}, "1188402269726871553": {"content_summary": "RT @arxiv_cscl: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/5oURm34fI6", "followers": "11,930", "datetime": "2019-10-27 10:29:15", "author": "@Rosenchild"}, "1188586009652551680": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "0", "datetime": "2019-10-27 22:39:22", "author": "@benbenfiol"}, "1194730077486927872": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "1,893", "datetime": "2019-11-13 21:33:42", "author": "@hannal"}, "1187401070424788994": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "128", "datetime": "2019-10-24 16:10:50", "author": "@ADaiminger"}, "1187250636393336832": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "181", "datetime": "2019-10-24 06:13:04", "author": "@lpadukana"}, "1187670498626461696": {"content_summary": "[1/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 469 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-25 10:01:27", "author": "@PapersTrending"}, "1194637703322599424": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "695", "datetime": "2019-11-13 15:26:38", "author": "@cooker_libre"}, "1188409835022405633": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "143", "datetime": "2019-10-27 10:59:18", "author": "@larosaandrea"}, "1187587997153165312": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "52", "datetime": "2019-10-25 04:33:37", "author": "@ariaryan"}, "1187386688500199426": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "7,556", "datetime": "2019-10-24 15:13:41", "author": "@brandondamos"}, "1187549981286100992": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "45", "datetime": "2019-10-25 02:02:33", "author": "@yoquankara"}, "1187372743811047427": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,191", "datetime": "2019-10-24 14:18:17", "author": "@Zhou_Yu_AI"}, "1187597456160313350": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "7", "datetime": "2019-10-25 05:11:12", "author": "@XingLuxi"}, "1187339579592003584": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "159", "datetime": "2019-10-24 12:06:30", "author": "@JScelza"}, "1187614221556539393": {"content_summary": "https://t.co/vlXTg3e99c Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Solid experiments with systematic views.", "followers": "1", "datetime": "2019-10-25 06:17:49", "author": "@hhwpku"}, "1194364180947562496": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "1,047", "datetime": "2019-11-12 21:19:45", "author": "@uncnlp"}, "1187514617309696002": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "8", "datetime": "2019-10-24 23:42:02", "author": "@airborne2009"}, "1187391411978756096": {"content_summary": "RT @sleepinyourhat: Major progress on our SuperGLUE benchmark from Brain, plus a really extensive ablation study! https://t.co/rRX0ZFPWr0", "followers": "164,079", "datetime": "2019-10-24 15:32:27", "author": "@ceobillionaire"}, "1187630773559447552": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "219", "datetime": "2019-10-25 07:23:36", "author": "@rhea6969"}, "1187183041736773638": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "564", "datetime": "2019-10-24 01:44:28", "author": "@kalpeshk2011"}, "1187296234752491521": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "212", "datetime": "2019-10-24 09:14:15", "author": "@jungchern"}, "1187467528353677312": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,180", "datetime": "2019-10-24 20:34:55", "author": "@dadabots"}, "1187340226336907265": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "35", "datetime": "2019-10-24 12:09:04", "author": "@tianshi5940"}, "1187398502445453312": {"content_summary": "Great work by my colleagues on the Google Research Brain Team.", "followers": "9,989", "datetime": "2019-10-24 16:00:38", "author": "@douglas_eck"}, "1187419548443529217": {"content_summary": "RT @deliprao: \ud83d\udea8\ud83d\udea8Big #nlproc claim from Google: \"we .. [introduce] a unified framework that converts every language problem into a text-to-\u2026", "followers": "425", "datetime": "2019-10-24 17:24:16", "author": "@iamknighton"}, "1187262948680568836": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "93", "datetime": "2019-10-24 07:01:59", "author": "@harmanani"}, "1187932895299551237": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "509", "datetime": "2019-10-26 03:24:07", "author": "@itomoki6"}, "1187245440774766592": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Create 750GB Colossal Clean Crawled Corpus and achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.", "followers": "338", "datetime": "2019-10-24 05:52:25", "author": "@arankomatsuzaki"}, "1187259378497392640": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "168", "datetime": "2019-10-24 06:47:48", "author": "@brunoboutteau"}, "1187259857495351297": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "133", "datetime": "2019-10-24 06:49:42", "author": "@theo_matussiere"}, "1187763568185348097": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "219", "datetime": "2019-10-25 16:11:16", "author": "@tobigue_"}, "1187396502236196864": {"content_summary": "RT @nikiparmar09: Great work! Text to text Transformer with a masking loss does better than other Transfer learning techniques. https://t.c\u2026", "followers": "168", "datetime": "2019-10-24 15:52:41", "author": "@ShumonLahiri"}, "1187357877905674241": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "392", "datetime": "2019-10-24 13:19:12", "author": "@HanGuo97"}, "1187181130027421696": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "567", "datetime": "2019-10-24 01:36:52", "author": "@arnaudbenard"}, "1187548702510071808": {"content_summary": "Superb new SOTA from the Google T5 team, but I guess only big companies can afford such massive computing power.", "followers": "30", "datetime": "2019-10-25 01:57:28", "author": "@system_nlp"}, "1187166462101938177": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "295", "datetime": "2019-10-24 00:38:35", "author": "@balicea1"}, "1187341516609474560": {"content_summary": "This is frustrating. Magic sword has no edge. In the era of overwhelming computational power, every AI engineer will have to reflect on the meaning of his work.", "followers": "7", "datetime": "2019-10-24 12:14:11", "author": "@mi3fa5sol4mi2"}, "1194696631767093248": {"content_summary": "RT @d_ijk_stra: \uc774 \ub17c\ubb38\uc744 \uc77d\uc73c\uba74\uc11c \ub2e4\uc2dc \uc0dd\uac01\ud55c \uac74\ub370, \uc774\ucbe4\ub418\uba74 seq2seq\uc744 \"\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\"\ub77c\uace0 \uc0dd\uac01\ud574\ub3c4 \ub418\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc744 \uc885\uc885 \ud55c\ub2e4. \uadf8\ub9cc\ud07c \uc815\ub9d0 \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \uc77c\uad00\ub41c \ubc29\ubc95\uc73c\ub85c \ud480 \uc218 \uc788\ub294 \ucd94\uc0c1\uc801 \ud504\ub808\uc784\uc6cc\ud06c\uace0 \uc778\ud130\ud398\uc774\uc2a4\uc640\u2026", "followers": "163", "datetime": "2019-11-13 19:20:48", "author": "@nospace_plz"}, "1200461488365932544": {"content_summary": "RT @fbk_mt: Our pick of the week: Raffel et al. paper on \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u2026", "followers": "463", "datetime": "2019-11-29 17:08:16", "author": "@Turchi_Marco"}, "1233346522869313537": {"content_summary": "[6/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 1,953 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2020-02-28 11:01:40", "author": "@PapersTrending"}, "1200190251999879168": {"content_summary": "RT @AT_Amir: T5 by google explores the field of transfer learning in NLP. Very good systematic study on how to pretrain and transfer transf\u2026", "followers": "463", "datetime": "2019-11-28 23:10:29", "author": "@Turchi_Marco"}, "1187374731307167744": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "4", "datetime": "2019-10-24 14:26:10", "author": "@EricL75804054"}, "1209926299143299073": {"content_summary": "@rctatman https://t.co/y242hPCuAB suggestion for @kaggle reading group (paper by @colinraffel et al) if you haven\u2019t had it already", "followers": "3,617", "datetime": "2019-12-25 19:58:03", "author": "@AndyHarless"}, "1187859020943511553": {"content_summary": "RT @yoquankara: Google's T5 (Text-To-Text Transfer Transformer) language model set new record and gets very close to human on SuperGLUE ben\u2026", "followers": "1,673", "datetime": "2019-10-25 22:30:34", "author": "@keigohtr"}, "1187550166456270848": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "352", "datetime": "2019-10-25 02:03:17", "author": "@141923"}, "1187253755823218688": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "611", "datetime": "2019-10-24 06:25:28", "author": "@smart_grid_fr"}, "1188331847203246080": {"content_summary": "RT @kyoun: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Google) https://t.co/d4Nu1b14WT \u30bf\u30b9\u30af\u3092\u5168\u3066Text-to\u2026", "followers": "144", "datetime": "2019-10-27 05:49:25", "author": "@Jack_hibi"}, "1187352162483003392": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "184", "datetime": "2019-10-24 12:56:30", "author": "@luk_augustyniak"}, "1190495632256946176": {"content_summary": "RT @ymym3412: \u5b9f\u9a13\u306e\u91cf\u304c\u3084\u3070\u3044 \u8a08\u7b97\u8cc7\u6e90\u304c\u3084\u3070\u3044 #xpaperchallenge https://t.co/Auz04Pk4IQ", "followers": "824", "datetime": "2019-11-02 05:07:31", "author": "@morioka"}, "1187368212062769153": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "66", "datetime": "2019-10-24 14:00:16", "author": "@diegovogeid"}, "1227167012264108032": {"content_summary": "https://t.co/Pbqq6UCAIK \u5927\u5909\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u30fb\u30fb\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\uff01", "followers": "162", "datetime": "2020-02-11 09:46:30", "author": "@kurama554101"}, "1187336761262792704": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "7", "datetime": "2019-10-24 11:55:18", "author": "@mi3fa5sol4mi2"}, "1190737179183476736": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "239", "datetime": "2019-11-02 21:07:21", "author": "@ico_chico"}, "1188102905909235712": {"content_summary": "RT @jaguring1: T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/oxrxWryY8Q https://t.co/\u2026", "followers": "45", "datetime": "2019-10-26 14:39:41", "author": "@pesasape"}, "1189859882473861121": {"content_summary": "[1/10] \ud83d\udcc8 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 875 \u2b50 - \ud83d\udcc4 https://t.co/vNIC44CsIX - \ud83d\udd17 https://t.co/YmMUyl74bo", "followers": "222", "datetime": "2019-10-31 11:01:17", "author": "@PapersTrending"}, "1190852767927197697": {"content_summary": "My favourite type of papers are ones that approach architectures/concepts with strong design of experiments that can be reproduced like this one. I always try to collate \"highlights\" from any client projects we do in this format to share them internally wi", "followers": "184", "datetime": "2019-11-03 04:46:39", "author": "@SiddKotwal"}, "1187290200784945152": {"content_summary": "Wow, most of the headroom gone on SuperGLUE already.", "followers": "374", "datetime": "2019-10-24 08:50:17", "author": "@anotherjohng"}, "1187333067733471232": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "0", "datetime": "2019-10-24 11:40:37", "author": "@DoqterS"}, "1187761445422764032": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "39", "datetime": "2019-10-25 16:02:50", "author": "@sagor_sarker"}, "1187377982748401664": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "84", "datetime": "2019-10-24 14:39:06", "author": "@matsu_mire"}, "1204513986953134080": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "478", "datetime": "2019-12-10 21:31:27", "author": "@yasuokajihei"}, "1187549644059840512": {"content_summary": "RT @gijigae: Google Brain\u304c\u6700\u65b0\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30e2\u30c7\u30eb\uff08Text-to-Text Transfer Transformer\u3001\u7565\u3057\u3066 \"T5\"\uff09\u3092\u767a\u8868\u3002AI\u306e\u8a00\u8a9e\u7406\u89e3\u80fd\u529b\u3092\u6e2c\u308b\u76ee\u7684\u3067\u4f5c\u3089\u308c\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3001SuperGLUE \u3067\u4eca\u307e\u3067\u306e\u8a18\u9332\u30924.3\u30dd\u30a4\u30f3\u30c8\u3082\u2026", "followers": "525", "datetime": "2019-10-25 02:01:13", "author": "@Qbps"}, "1187167009160749058": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "66", "datetime": "2019-10-24 00:40:46", "author": "@diegovogeid"}, "1245501585968361472": {"content_summary": "RT @icoxfog417: Hugging Face\u306eTransformer\u306b\u8907\u6570\u30bf\u30b9\u30af\u3092\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u5b66\u7fd2\u3057\u305fT5\u306e\u30e2\u30c7\u30eb\u304c\u642d\u8f09\u3002\u30b5\u30f3\u30d7\u30eb\u306eNotebook\u3082\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u3002 https://t.co/j9rGGB31WH", "followers": "554", "datetime": "2020-04-02 00:01:33", "author": "@FBWM8888"}, "1187335823294779395": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "107", "datetime": "2019-10-24 11:51:34", "author": "@merajatgupta"}, "1187343569373270017": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "134", "datetime": "2019-10-24 12:22:21", "author": "@RobHarrigan89"}, "1187385238827212800": {"content_summary": "RT @JeffD: Great summary of encoder-decoder text-to-text models by @colinraffel and authors. Also, a new dataset based on a cleaned common\u2026", "followers": "209", "datetime": "2019-10-24 15:07:56", "author": "@ir_glasgow"}, "1188319497758728194": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "39", "datetime": "2019-10-27 05:00:20", "author": "@nisar_3s"}, "1187250544034926592": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "217", "datetime": "2019-10-24 06:12:42", "author": "@mrhctr"}, "1187365741877702656": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "342", "datetime": "2019-10-24 13:50:27", "author": "@SalomonKabongo1"}, "1187858447812792320": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/U2b7205COA", "followers": "595", "datetime": "2019-10-25 22:28:17", "author": "@pinoystartup"}, "1203321923016351745": {"content_summary": "RT @agatan_: text-to-text transfer transformer \u3081\u3063\u3061\u3083\u826f\u3044\u3002\u5922\u304c\u5e83\u304c\u308b\u3057\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5e83\u304c\u308b\u3002BART \u3082\u4f3c\u305f\u3088\u3046\u306a\u8a71\u3060\u3051\u3069 T5 \u306f\u8ad6\u6587\u81ea\u4f53\u304c\u3081\u3061\u3083\u304f\u3061\u3083\u9577\u3044\u5206\u3001\u6bd4\u8f03\u3068\u304b\u8003\u5bdf\u306b\u5bcc\u3093\u3067\u3066\u304b\u306a\u308a\u8aad\u307f\u5fdc\u3048\u3042\u308b\u3002 https://t.co\u2026", "followers": "199", "datetime": "2019-12-07 14:34:37", "author": "@khmlpy"}, "1187253319707709440": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,393", "datetime": "2019-10-24 06:23:44", "author": "@huangcza"}, "1187340329478959105": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "1,602", "datetime": "2019-10-24 12:09:28", "author": "@alxcnwy"}, "1190427633328087042": {"content_summary": "https://t.co/xzczkfWSHZ Read through the new T5 paper from google. Major conclusions: Does pretraining data matter? A little. Does pretraining task matter? A little. Does model architecture matter? A little. Does model size matter? A lot. Training", "followers": "586", "datetime": "2019-11-02 00:37:19", "author": "@ryan_chesler"}, "1187600070390108160": {"content_summary": "RT @icoxfog417: \u5206\u985e\u3084\u8981\u7d04\u3001\u95a2\u4fc2\u63a8\u5b9a\u3068\u3044\u3063\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30bf\u30b9\u30af\u3092\u5168\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u5f62\u5f0f\u3067\u89e3\u3053\u3046\u3068\u3044\u3046\u7814\u7a76\u3002Transformer\u3092\u30d9\u30fc\u30b9\u306b\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u5f62\u5f0f(Attention\u306e\u304b\u3051\u65b9/\u69cb\u6210)\u3001\u76ee\u7684\u95a2\u6570\u306a\u3069\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u8a66\u3057\u3001\u7d50\u679c\u3068\u3057\u3066GLUE/SuperGLUE\u306a\u2026", "followers": "787", "datetime": "2019-10-25 05:21:35", "author": "@thatsdone"}, "1187297193260208134": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "4,021", "datetime": "2019-10-24 09:18:04", "author": "@DanHLawReporter"}, "1187251622709288960": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "2,985", "datetime": "2019-10-24 06:16:59", "author": "@solyarisoftware"}, "1187451838355906568": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "69", "datetime": "2019-10-24 19:32:34", "author": "@v_trokhymenko"}, "1187822694387073024": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "42", "datetime": "2019-10-25 20:06:13", "author": "@Oscar74176026"}, "1187202486832156673": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "1,064", "datetime": "2019-10-24 03:01:44", "author": "@djoerd"}, "1187343095723106304": {"content_summary": "RT @seb_ruder: The new study by @colinraffel et al. provides a great overview of best practices in the current transfer learning landscape\u2026", "followers": "321", "datetime": "2019-10-24 12:20:28", "author": "@gamino"}, "1187588032591007752": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "100", "datetime": "2019-10-25 04:33:45", "author": "@KarimiRabeeh"}, "1187419678869614592": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "425", "datetime": "2019-10-24 17:24:47", "author": "@iamknighton"}, "1187302595947302912": {"content_summary": "RT @colinraffel: New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the li\u2026", "followers": "164,079", "datetime": "2019-10-24 09:39:32", "author": "@ceobillionaire"}, "1250324532130283524": {"content_summary": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://t.co/0Cm45v6FvL", "followers": "15", "datetime": "2020-04-15 07:26:13", "author": "@matejkvassay"}, "1194371550713122822": {"content_summary": "RT @mohitban47: Welcome again, @ColinRaffel! Looking fwd to having u here soon \ud83d\ude00; & NLP folks applying for PhD this year, definitely apply\u2026", "followers": "239", "datetime": "2019-11-12 21:49:02", "author": "@ramakanth1729"}, "1199814678551154688": {"content_summary": "You can read the paper here \ud83d\udc49 https://t.co/IDK1MOAufz", "followers": "1,287", "datetime": "2019-11-27 22:18:05", "author": "@remilouf"}, "1187595548007682048": {"content_summary": "RT @hillbig: \u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3092\u7d71\u4e00\u7684\u306b\u6587\u304b\u3089\u6587\u3078\u306e\u5909\u63db\u30bf\u30b9\u30af\u3068\u307f\u306a\u3057\u3001\u30bf\u30b9\u30af\u306e\u7a2e\u985e\u3084\u6761\u4ef6\u3082\u5165\u529b\u6587\u306e\u5148\u982d\u306b\u5358\u8a9e\u3068\u3057\u3066\u5165\u308c\u3001\u30b3\u30fc\u30d1\u30b9\u3001\u4e8b\u524d\u5b66\u7fd2\u624b\u6cd5\u30e2\u30c7\u30eb\u3092\u7d71\u4e00\u7684\u306b\u8a55\u4fa1\u3002\u898b\u3064\u304b\u3063\u305f\u77e5\u898b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6700\u5927\u7d1a\u306e\u30c7\u30fc\u30bf\uff08C4\uff09\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u591a\u304f\u306eSOTA\u3092\u66f4\u65b0 http\u2026", "followers": "2,067", "datetime": "2019-10-25 05:03:37", "author": "@yo_ehara"}, "1187352743385563136": {"content_summary": "Exciting! Among other results, new system performs almost as well as humans on recently created benchmark that was designed to be hard for computers. All code and models are available.", "followers": "6,525", "datetime": "2019-10-24 12:58:48", "author": "@barneyp"}, "1223002068459380736": {"content_summary": "T5 \u201cText-to-Text Transfer Transformer\u201d https://t.co/X3unpbsHZs", "followers": "3,083", "datetime": "2020-01-30 21:56:30", "author": "@MonaJalal_"}}}