{"twitter": {"1088290952454664192": {"author": "@StephenPiment", "datetime": "2019-01-24 04:22:37", "content_summary": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks https://t.co/I91AibjEAs", "followers": "6,125"}, "1088104206127501313": {"author": "@Charbel_Sakr", "datetime": "2019-01-23 16:00:33", "content_summary": "My second #ICLR2019 paper now posted on #arXiv https://t.co/SjSMGSX9IX", "followers": "19"}, "1127592564410990594": {"author": "@SalimChemlal", "datetime": "2019-05-12 15:13:11", "content_summary": "A good hardware design consideration for FPUs. Faster computation of dot product by reducing the accumulation precision. Intrigued this has not been tested until now... Their ICLR 2019 paper at https://t.co/2v56wN3dZO", "followers": "1,328"}, "1088159722732220416": {"author": "@arXiv__ml", "datetime": "2019-01-23 19:41:09", "content_summary": "#arXiv #machinelearning [cs.LG] Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks. (arXiv:1901.06588v1 [cs.LG]) https://t.co/ehISQutlhx Efforts to reduce the numerical precision of computations in deep learning training have", "followers": "1,708"}, "1087981086636982272": {"author": "@arxivml", "datetime": "2019-01-23 07:51:19", "content_summary": "\"Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks\", Charbel Sakr, Naigang Wang, Chi\u2026 https://t.co/GLHgJ7aGKr", "followers": "773"}, "1088157059504373760": {"author": "@ham_gretsky", "datetime": "2019-01-23 19:30:34", "content_summary": "RT @mlmemoirs: #arXiv #machinelearning [cs.LG] Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks. (arXiv:190\u2026", "followers": "19,048"}, "1088155026516774912": {"author": "@mlmemoirs", "datetime": "2019-01-23 19:22:29", "content_summary": "#arXiv #machinelearning [cs.LG] Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks. (arXiv:1901.06588v1 [cs.LG]) https://t.co/fGxmLRXrZ6 Efforts to reduce the numerical precision of computations in deep learning training have", "followers": "1,247"}, "1087898747613446144": {"author": "@StatMLPapers", "datetime": "2019-01-23 02:24:08", "content_summary": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks. (arXiv:1901.06588v1 [cs.LG]) https://t.co/Beny7mi2gl", "followers": "9,685"}, "1087901580379205632": {"author": "@BrundageBot", "datetime": "2019-01-23 02:35:23", "content_summary": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks. Charbel Sakr, Naigang Wang, Chia-Yu Chen, Jungwook Choi, Ankur Agrawal, Naresh Shanbhag, and Kailash Gopalakrishnan https://t.co/CcIqGHHEq0", "followers": "3,878"}, "1093411834537734144": {"author": "@DSPonFPGA", "datetime": "2019-02-07 07:31:10", "content_summary": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks https://t.co/w6jZdjDOuy", "followers": "403"}, "1087919118160846849": {"author": "@StatsPapers", "datetime": "2019-01-23 03:45:04", "content_summary": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks. https://t.co/2R4ICjHypc", "followers": "5,454"}}, "queriedAt": "2020-05-21 19:44:32", "completed": "1", "citation_id": "54305260", "tab": "twitter"}