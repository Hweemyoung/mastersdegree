{"twitter": {"1096070576257540097": {"content_summary": "RT @HazanPrinceton: Courtesy of your local Google AI Lab part 2: The multiplicative weights update and (stochastic) gradient descent are t\u2026", "datetime": "2019-02-14 15:36:04", "followers": "685", "author": "@FScammells"}, "1093495200318513152": {"content_summary": "RT @arxiv_org: Exponentiated Gradient Meets Gradient Descent. https://t.co/8Nf96WnJHf https://t.co/nAc3kWjOaB", "datetime": "2019-02-07 13:02:26", "followers": "305", "author": "@subhobrata1"}, "1093325680547368960": {"content_summary": "Exponentiated Gradient Meets Gradient Descent. (arXiv:1902.01903v1 [cs.LG]) https://t.co/Z8OmEchwSQ", "datetime": "2019-02-07 01:48:49", "followers": "9,721", "author": "@StatMLPapers"}, "1093417055087484929": {"content_summary": "\"Exponentiated Gradient Meets Gradient Descent\", Udaya Ghai, Elad Hazan, Yoram Singer https://t.co/7AJn2Sl4jX", "datetime": "2019-02-07 07:51:55", "followers": "780", "author": "@arxivml"}, "1093434721772564480": {"content_summary": "Exponentiated Gradient Meets Gradient Descent. https://t.co/8Nf96WnJHf https://t.co/nAc3kWjOaB", "datetime": "2019-02-07 09:02:07", "followers": "12,761", "author": "@arxiv_org"}, "1096447860344786945": {"content_summary": "RT @HazanPrinceton: Courtesy of your local Google AI Lab part 2: The multiplicative weights update and (stochastic) gradient descent are t\u2026", "datetime": "2019-02-15 16:35:15", "followers": "82", "author": "@nsdgpn"}, "1093335444597673984": {"content_summary": "Exponentiated Gradient Meets Gradient Descent. (arXiv:1902.01903v1 [cs.LG]) https://t.co/1LzWbyADNq The (stochastic) gradient descent and the multiplicative update method are probably the most popular algorithms in machine learning. We introduce and study", "datetime": "2019-02-07 02:27:37", "followers": "50", "author": "@yapp1e"}, "1093690935320358914": {"content_summary": "Exponentiated Gradient Meets Gradient Descent. (arXiv:1902.01903v1 [cs.LG]) https://t.co/4L1qY7MjCn", "datetime": "2019-02-08 02:00:13", "followers": "45", "author": "@owltrainlab"}, "1096069088164675584": {"content_summary": "Courtesy of your local Google AI Lab part 2: The multiplicative weights update and (stochastic) gradient descent are two of the most common ML methods. Interpolating between the two as an instantiation of a single algorithm (+ a matrix version): https://t", "datetime": "2019-02-14 15:30:09", "followers": "3,328", "author": "@HazanPrinceton"}, "1093526649344208896": {"content_summary": "Exponentiated Gradient Meets Gradient Descent. (arXiv:1902.01903v1 [cs.LG]) https://t.co/CrsashVNZl", "datetime": "2019-02-07 15:07:24", "followers": "151", "author": "@y_yammt"}}, "queriedAt": "2020-06-03 22:39:04", "tab": "twitter", "completed": "1", "citation_id": "55055238"}