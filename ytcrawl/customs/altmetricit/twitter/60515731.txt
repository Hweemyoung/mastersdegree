{"citation_id": "60515731", "queriedAt": "2020-05-09 13:21:38", "completed": "0", "twitter": {"1233373248219336704": {"followers": "211", "content_summary": "RT @DCasBol: @DeepMind Batch Normalization should be used like BN-conv-ReLU, but in the paper it's used like BN-ReLU-conv, which makes the\u2026", "author": "@AssistedEvolve", "datetime": "2020-02-28 12:47:51"}, "1232983940442607620": {"followers": "149", "content_summary": "@DeepMind Batch Normalization should be used like BN-conv-ReLU, but in the paper it's used like BN-ReLU-conv, which makes the centering effect of BN useless. https://t.co/0HtUmfQ4lF", "author": "@DCasBol", "datetime": "2020-02-27 11:00:53"}, "1191261820830531584": {"followers": "149", "content_summary": "@amArunava @jeremyphoward @rasbt Sure! This is a very interesting one :) https://t.co/0HtUmfQ4lF", "author": "@DCasBol", "datetime": "2019-11-04 07:52:05"}, "1187183984964329473": {"followers": "1,752", "content_summary": "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks https://t.co/cyyw4QPdGR", "author": "@whisponchan", "datetime": "2019-10-24 01:48:13"}}, "tab": "twitter"}