{"queriedAt": "2020-06-03 15:12:26", "completed": "1", "citation_id": "55007417", "tab": "twitter", "twitter": {"1120913806178308096": {"author": "@TheMarcelSchulz", "datetime": "2019-04-24 04:54:11", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "434"}, "1120611203158552576": {"author": "@Coryandar", "datetime": "2019-04-23 08:51:45", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "264"}, "1120550683789414400": {"author": "@olethros", "datetime": "2019-04-23 04:51:16", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "159"}, "1143926542377967616": {"author": "@gmravi2003", "datetime": "2019-06-26 16:58:35", "content_summary": "RT @bremen79: If you are at #COLT2019, don't miss @kwangsungjun's talk and poster today on parameter-free noisy online convex optimization.\u2026", "followers": "143"}, "1122190920584052736": {"author": "@ergodicwalk", "datetime": "2019-04-27 17:28:59", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "754"}, "1120541690387939329": {"author": "@alsombra7", "datetime": "2019-04-23 04:15:32", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "102"}, "1120526398123278338": {"author": "@kamalikac", "datetime": "2019-04-23 03:14:46", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "2,133"}, "1092978257782337537": {"author": "@BrundageBot", "datetime": "2019-02-06 02:48:17", "content_summary": "Parameter-free Online Convex Optimization with Sub-Exponential Noise. Kwang-Sung Jun and Francesco Orabona https://t.co/Axkv61F3ac", "followers": "3,797"}, "1120579529896890368": {"author": "@anshulkundaje", "datetime": "2019-04-23 06:45:53", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "6,791"}, "1120525981482201088": {"author": "@bremen79", "datetime": "2019-04-23 03:13:07", "content_summary": "Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Song, @kamalikac, and @ergodicwalk, with a SGD-like procedure that does not have any learning rate! From @kwangsungjun and yours truly", "followers": "1,180"}, "1122542342924394497": {"author": "@gergelyacs", "datetime": "2019-04-28 16:45:25", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "38"}, "1144320377608269824": {"author": "@kwangsungjun", "datetime": "2019-06-27 19:03:33", "content_summary": "RT @bremen79: If you are at #COLT2019, don't miss @kwangsungjun's talk and poster today on parameter-free noisy online convex optimization.\u2026", "followers": "107"}, "1144134690565062657": {"author": "@djhsu", "datetime": "2019-06-27 06:45:41", "content_summary": "RT @bremen79: If you are at #COLT2019, don't miss @kwangsungjun's talk and poster today on parameter-free noisy online convex optimization.\u2026", "followers": "598"}, "1120517167001288704": {"author": "@kwangsungjun", "datetime": "2019-04-23 02:38:05", "content_summary": "Online learning with noisy gradients with applications in differentially-private parameter-free SGD and concentration inequalities. The paper with Francesco @bremen79 is accepted at #COLT2019! https://t.co/auJq3Q379d (updates coming soon)", "followers": "107"}, "1120731500092301318": {"author": "@NuitBlog", "datetime": "2019-04-23 16:49:46", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "261"}, "1120540318636625920": {"author": "@bipr", "datetime": "2019-04-23 04:10:05", "content_summary": "RT @bremen79: Latest chapter in the saga of SGD algorithms *without* learning rates: We obtain the same guarantee of the private SGD of Son\u2026", "followers": "1,252"}, "1143894585225039873": {"author": "@bremen79", "datetime": "2019-06-26 14:51:36", "content_summary": "If you are at #COLT2019, don't miss @kwangsungjun's talk and poster today on parameter-free noisy online convex optimization. As an application, he will show how to do differentially-private SGD without learning rates. Paper here: https://t.co/kdTiCNsiNL", "followers": "1,180"}, "1143921947400708096": {"author": "@jeandut14000", "datetime": "2019-06-26 16:40:19", "content_summary": "RT @bremen79: If you are at #COLT2019, don't miss @kwangsungjun's talk and poster today on parameter-free noisy online convex optimization.\u2026", "followers": "37"}, "1092963405646852096": {"author": "@StatMLPapers", "datetime": "2019-02-06 01:49:16", "content_summary": "Parameter-free Online Convex Optimization with Sub-Exponential Noise. (arXiv:1902.01500v1 [cs.LG]) https://t.co/Sr3aykfuT8", "followers": "9,490"}}}