{"citation_id": "71174559", "completed": "1", "queriedAt": "2020-05-14 11:10:05", "tab": "twitter", "twitter": {"1200332651384754176": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "80", "datetime": "2019-11-29 08:36:19", "author": "@ricklentz"}, "1200220126454280198": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "674", "datetime": "2019-11-29 01:09:11", "author": "@hidemotoNakada"}, "1199495782417752066": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "159", "datetime": "2019-11-27 01:10:54", "author": "@matasramon"}, "1200239295946088449": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "456", "datetime": "2019-11-29 02:25:22", "author": "@PerthMLGroup"}, "1199755578450567169": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "183", "datetime": "2019-11-27 18:23:14", "author": "@dkastaniotis"}, "1199787639223791617": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "75", "datetime": "2019-11-27 20:30:38", "author": "@gevero"}, "1199514420545765376": {"content_summary": "RT @pcastr: \ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0fmake everyone a lottery winner\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0f train sparse networks (with a randomly initialized topology) end-to-end without sacrific\u2026", "followers": "1,063", "datetime": "2019-11-27 02:24:58", "author": "@miguelalonsojr"}, "1199563231733305344": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "12", "datetime": "2019-11-27 05:38:55", "author": "@robsrana"}, "1199564904518311936": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "821", "datetime": "2019-11-27 05:45:34", "author": "@deepgradient"}, "1199494004196462592": {"content_summary": "Exciting work\ud83c\udf89", "followers": "30", "datetime": "2019-11-27 01:03:50", "author": "@allardmaxime079"}, "1199495728239919105": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "5,817", "datetime": "2019-11-27 01:10:41", "author": "@pfau"}, "1199531817092567040": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "66", "datetime": "2019-11-27 03:34:06", "author": "@russell_lliu"}, "1199550665405554689": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "114", "datetime": "2019-11-27 04:48:59", "author": "@rajpratim"}, "1201773551063371776": {"content_summary": "Congratulations @utkuevci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5076 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/9fLK4ckjoQ", "followers": "182", "datetime": "2019-12-03 08:01:57", "author": "@BotLottery"}, "1199530820349779968": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "824", "datetime": "2019-11-27 03:30:08", "author": "@morioka"}, "1199910798501408768": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "5,566", "datetime": "2019-11-28 04:40:02", "author": "@Tim_Dettmers"}, "1199830305709969409": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "499", "datetime": "2019-11-27 23:20:11", "author": "@utkuevci"}, "1199158377034567681": {"content_summary": "Rigging the Lottery: Making All Tickets Winners \u5b9d\u304f\u3058\u306e\u30ea\u30ae\u30f3\u30b0\uff1a\u3059\u3079\u3066\u306e\u30c1\u30b1\u30c3\u30c8\u3092\u52dd\u8005\u306b\u3059\u308b 2019-11-25T18:58:53+00:00 arXiv: https://t.co/LbnI3gP7uo summary: https://t.co/Lbj1EG8RXj", "followers": "131", "datetime": "2019-11-26 02:50:11", "author": "@arXiv_reaDer"}, "1199765712555986944": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "523", "datetime": "2019-11-27 19:03:31", "author": "@bensprecher"}, "1204473369661456386": {"content_summary": "My one paper for yesterday: \u201cRigging the Lottery, making all tickets winner\u201d Really good paper, though long and a bit mathy for some to fully grasp. Have wanted to get into sparsity, good one to get started. Might write a blog on it. https://t.co/NXkC", "followers": "3,075", "datetime": "2019-12-10 18:50:04", "author": "@ivan_bezdomny"}, "1199402628238626816": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "424", "datetime": "2019-11-26 19:00:45", "author": "@iamknighton"}, "1199519262483705856": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "408", "datetime": "2019-11-27 02:44:12", "author": "@jainnitk"}, "1199400165033070592": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "20", "datetime": "2019-11-26 18:50:57", "author": "@polceanum"}, "1199639818654928896": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "92", "datetime": "2019-11-27 10:43:15", "author": "@morgan94088660"}, "1199582821322559490": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "117", "datetime": "2019-11-27 06:56:46", "author": "@jinyeom95"}, "1199391069676306432": {"content_summary": "Earlier #FAIR came up with this lottery ticket hypothesis now #DeepMind have come up with a technique which need 'no luck required, all initialization \u201ctickets\u201d are winners' #AIWar https://t.co/Ia8u1ckDI8 https://t.co/lGpJqPTkPv", "followers": "808", "datetime": "2019-11-26 18:14:49", "author": "@crazykesh"}, "1199259550915989504": {"content_summary": "RT @BrundageBot: Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Else\u2026", "followers": "240", "datetime": "2019-11-26 09:32:12", "author": "@blauigris"}, "1199533061303488515": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "184", "datetime": "2019-11-27 03:39:02", "author": "@SiavashSakhavi"}, "1200194286643372033": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "824", "datetime": "2019-11-28 23:26:31", "author": "@morioka"}, "1199371197412069381": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "167", "datetime": "2019-11-26 16:55:51", "author": "@Planning203"}, "1199398353760595968": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "899", "datetime": "2019-11-26 18:43:45", "author": "@kylewadegrove"}, "1199492152138579968": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "2,763", "datetime": "2019-11-27 00:56:29", "author": "@CSProfKGD"}, "1201397657375166464": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-02 07:08:17", "author": "@mayur_shingote"}, "1200326744424759296": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "52", "datetime": "2019-11-29 08:12:51", "author": "@hakanardo"}, "1199449851114786816": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "33", "datetime": "2019-11-26 22:08:23", "author": "@OndrejKlejch"}, "1199262495568875521": {"content_summary": "RT @BrundageBot: Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Else\u2026", "followers": "215", "datetime": "2019-11-26 09:43:54", "author": "@AssistedEvolve"}, "1204016884116414465": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB \"\u5b9d\u304f\u3058\u7406\u8ad6\"\u3088\u308d\u3057\u304f\u826f\u3044\u521d\u671f\u5024\u3092\u9078\u3076\u306e\u3067\u306f\u306a\u304f\u3001\u3069\u306e\u3088\u3046\u306a\u521d\u671f\u5024\u3067\u3082\u6700\u521d\u304b\u3089\u758e\u3067\u9ad8\u7cbe\u5ea6\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u7fd2\u3055\u305b\u308bRigL\u3092\u63d0\u6848\u3002\u300e\u758e\u306aNN\u5b66\u7fd2\u2192\u30d1\u30e9\u30e1\u30fc\u30bf\u5c0f\u306a\u90e8\u5206\u3092\u524a\u9664\u2192\u52fe\u914d\u5927\u540c\u58eb\u3092\u7d50\u5408\u300f\u3092\u7e70\u308a\u8fd4\u2026", "followers": "373", "datetime": "2019-12-09 12:36:09", "author": "@mikame_univ"}, "1199503434237349888": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "98", "datetime": "2019-11-27 01:41:19", "author": "@treasured_write"}, "1202724894313734153": {"content_summary": "Congratulations @hasdid! Because you tweeted about a lottery, your tweet has been randomly selected out of 5079 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/q4C1JkCabq", "followers": "182", "datetime": "2019-12-05 23:02:15", "author": "@BotLottery"}, "1200588046997106690": {"content_summary": "RT @jesseengel: Sparsity is a clear inductive bias for neural nets, but end to end training and efficient inference have always been a chal\u2026", "followers": "52", "datetime": "2019-11-30 01:31:10", "author": "@HengjianJia"}, "1199459889631895552": {"content_summary": "Dur burda", "followers": "110", "datetime": "2019-11-26 22:48:17", "author": "@ceryoncu"}, "1199397723230941189": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "449", "datetime": "2019-11-26 18:41:15", "author": "@iwontbecreative"}, "1235914964126167041": {"content_summary": "\u5b9d\u304f\u3058\u4eee\u8aac\u3092\u30d9\u30fc\u30b9\u306b\u3057\u305f\u300eRigging the Lottery\uff08\u30a4\u30ab\u30b5\u30de\u304f\u3058\uff09\u300f\u3063\u3066\u3044\u3046\u679d\u5208\u308a\u306e\u6c7a\u5b9a\u7248\u307f\u305f\u3044\u306a\u306e\u304c\u3001TF\u306e\u30bd\u30fc\u30b9\u4ed8\u304d\u3067\u51fa\u3066\u308b\u3093\u3060\u3051\u3069\u3001\u3042\u3093\u307e\u3057\u8a71\u984c\u306b\u306a\u3063\u3066\u306a\u3044\u306e\u306a\u3093\u3067\u306a\u3093\uff1f https://t.co/FIfLiRr9HT", "followers": "380", "datetime": "2020-03-06 13:07:44", "author": "@harujoh"}, "1199260592336441345": {"content_summary": "RT @blauigris: Well, we are back to the 2000 with the return of constructive-deconstructive methods. Glad to see this. Instead of finding\u2026", "followers": "291", "datetime": "2019-11-26 09:36:21", "author": "@_lpag"}, "1199907792418525184": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "84", "datetime": "2019-11-28 04:28:05", "author": "@rahulmeetu"}, "1200758612789850112": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "501", "datetime": "2019-11-30 12:48:56", "author": "@maxvipworld"}, "1229813411413929984": {"content_summary": "Congratulations @fdasilva59fr! Because you tweeted about a lottery, your tweet has been randomly selected out of 4420 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/N11g4yvrxA", "followers": "182", "datetime": "2020-02-18 17:02:20", "author": "@BotLottery"}, "1199523931004133376": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "408", "datetime": "2019-11-27 03:02:45", "author": "@jainnitk"}, "1199497057083502598": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "152", "datetime": "2019-11-27 01:15:58", "author": "@mattbeach42"}, "1199358973100613632": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,676", "datetime": "2019-11-26 16:07:16", "author": "@allcell9"}, "1199399754221936641": {"content_summary": "RT @pcastr: \ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0fmake everyone a lottery winner\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0f train sparse networks (with a randomly initialized topology) end-to-end without sacrific\u2026", "followers": "154", "datetime": "2019-11-26 18:49:19", "author": "@DCasBol"}, "1199547282938462208": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "1,286", "datetime": "2019-11-27 04:35:33", "author": "@tak_yamm"}, "1200050746449752070": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "22", "datetime": "2019-11-28 13:56:08", "author": "@nandan_cse"}, "1199480730092523520": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "28,418", "datetime": "2019-11-27 00:11:05", "author": "@ogrisel"}, "1199613226092240896": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "172", "datetime": "2019-11-27 08:57:35", "author": "@ggdupont"}, "1199636326326267905": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "416", "datetime": "2019-11-27 10:29:23", "author": "@__nggih"}, "1199498711367176192": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "2,822", "datetime": "2019-11-27 01:22:33", "author": "@WonderMicky"}, "1199568478753050624": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "30", "datetime": "2019-11-27 05:59:46", "author": "@RealTheisen"}, "1202166313201872896": {"content_summary": "Congratulations @BrundageBot! Because you tweeted about a lottery, your tweet has been randomly selected out of 5070 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/mM6iSC8Cdr", "followers": "182", "datetime": "2019-12-04 10:02:38", "author": "@BotLottery"}, "1200202615780364288": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "1,834", "datetime": "2019-11-28 23:59:36", "author": "@m_nabu55"}, "1199497427452977154": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1", "datetime": "2019-11-27 01:17:26", "author": "@asterisk_0x2a"}, "1200038791643357184": {"content_summary": "RT @jacobmenick: New work by Utku Evci et al. on sparse training. My contribution was helping with the RNN experiments. Fun collaborating w\u2026", "followers": "61", "datetime": "2019-11-28 13:08:38", "author": "@albelwu"}, "1199431775115591680": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "0", "datetime": "2019-11-26 20:56:34", "author": "@Sam09lol"}, "1199538609013108736": {"content_summary": "RT @sarahookr: What differs in this paper is how the connections are grown after pruning for the most important weights. I think this is pa\u2026", "followers": "2,894", "datetime": "2019-11-27 04:01:05", "author": "@nsaphra"}, "1199251492269699073": {"content_summary": "Well, we are back to the 2000 with the return of constructive-deconstructive methods. Glad to see this. Instead of finding the correct initialization they add and remove units according to the gradient.", "followers": "240", "datetime": "2019-11-26 09:00:11", "author": "@blauigris"}, "1199521302144405504": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "36", "datetime": "2019-11-27 02:52:19", "author": "@vasan_ashwin"}, "1199360921501519872": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "13", "datetime": "2019-11-26 16:15:01", "author": "@Bharath09095865"}, "1199720804742660097": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "266", "datetime": "2019-11-27 16:05:04", "author": "@sandeepsign"}, "1199647110527799297": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "215", "datetime": "2019-11-27 11:12:14", "author": "@AssistedEvolve"}, "1199493723429724166": {"content_summary": "Congratulations @BrundageBot! Because you tweeted about a lottery, your tweet has been randomly selected out of 4761 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/mM6iSC8Cdr", "followers": "182", "datetime": "2019-11-27 01:02:43", "author": "@BotLottery"}, "1200095089982230533": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-11-28 16:52:20", "author": "@mayur_shingote"}, "1199525933058674688": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "404", "datetime": "2019-11-27 03:10:43", "author": "@tmramalho"}, "1223759659611365380": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "142", "datetime": "2020-02-02 00:06:53", "author": "@petrini_linda"}, "1199698819413237760": {"content_summary": "RT @kzykmyzw: dense\u306a\u30e2\u30c7\u30eb\u3092sparse\u5316\u3001\u3068\u3044\u3046\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u306a\u304f\u3001sparse\u306a\u30e2\u30c7\u30eb\u3092e2e\u3067\u5b66\u7fd2\u3059\u308b\u624b\u6cd5\u3002\u5b66\u7fd2\u4e2d\u306b\u4e0d\u8981\u306a\u63a5\u7d9a\u306e\u524a\u9664\u3068\u65b0\u898f\u63a5\u7d9a\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u6700\u9069\u5316\u3057\u3066\u3044\u304f\u3002\u63a5\u7d9a\u524a\u9664\u306fweight\u3001\u65b0\u898f\u63a5\u7d9a\u306fgradient\u306b\u57fa\u3065\u3044\u3066\u6c7a\u5b9a\u3002Re\u2026", "followers": "1,788", "datetime": "2019-11-27 14:37:42", "author": "@sotongshi"}, "1199272704622202880": {"content_summary": "RT @BrundageBot: Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Else\u2026", "followers": "28", "datetime": "2019-11-26 10:24:28", "author": "@radenmuaz"}, "1199370052673388551": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "203", "datetime": "2019-11-26 16:51:18", "author": "@TotalSecurily"}, "1199507178270277632": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1,708", "datetime": "2019-11-27 01:56:11", "author": "@superbradyon"}, "1200411799226597378": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "179,056", "datetime": "2019-11-29 13:50:50", "author": "@Montreal_AI"}, "1199464401289568257": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "2,071", "datetime": "2019-11-26 23:06:12", "author": "@EricSchles"}, "1199536093739339777": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "33", "datetime": "2019-11-27 03:51:05", "author": "@ruben_rrf"}, "1199633630777069569": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "228", "datetime": "2019-11-27 10:18:40", "author": "@sabarinathan_7"}, "1199357805955821568": {"content_summary": "We also introduce a technique [https://t.co/52dt8KBSBL] for training neural networks that are sparse throughout training from a random initialization - no luck required, all initialization \u201ctickets\u201d are winners. https://t.co/uS6Ot8cWso", "followers": "196", "datetime": "2019-11-26 16:02:38", "author": "@luckflow"}, "1200391579925504000": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "1,708", "datetime": "2019-11-29 12:30:29", "author": "@superbradyon"}, "1247322335025086464": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1", "datetime": "2020-04-07 00:36:33", "author": "@sjyfelouce"}, "1204017056158367746": {"content_summary": "https://t.co/HyOygUfALB Instead of choosing good initial values in favor of \"Lottery Theory\", they propose RigL to train a sparse and accurate network from any initial value.The learning time does not increase greatly, inference speed is improved with th", "followers": "1,292", "datetime": "2019-12-09 12:36:50", "author": "@AkiraTOSEI"}, "1199929773763551232": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "11,914", "datetime": "2019-11-28 05:55:26", "author": "@Rosenchild"}, "1200142910076776448": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "5,718", "datetime": "2019-11-28 20:02:21", "author": "@jasonyo"}, "1199427462259707905": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "411", "datetime": "2019-11-26 20:39:25", "author": "@michalwols"}, "1201291809173000192": {"content_summary": "RT @HotCompScience: Most popular computer science paper of the day: \"Rigging the Lottery: Making All Tickets Winners\" https://t.co/0mK2TOat\u2026", "followers": "287", "datetime": "2019-12-02 00:07:40", "author": "@dannyehb"}, "1199928406017638400": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "109", "datetime": "2019-11-28 05:50:00", "author": "@JamesONeil21"}, "1202213109970038786": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-04 13:08:36", "author": "@mayur_shingote"}, "1199409699679956992": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "109", "datetime": "2019-11-26 19:28:51", "author": "@Charles9n"}, "1199452326676901892": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "42", "datetime": "2019-11-26 22:18:14", "author": "@A_Mairesse"}, "1199535838461190144": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "143", "datetime": "2019-11-27 03:50:04", "author": "@raviprasadmkini"}, "1199623247312105472": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "967", "datetime": "2019-11-27 09:37:24", "author": "@KerenGu"}, "1199533089015443456": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "4,144", "datetime": "2019-11-27 03:39:09", "author": "@laurent_dinh"}, "1201985145785528325": {"content_summary": "Congratulations @utkuevci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5089 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/9fLK4ckjoQ", "followers": "182", "datetime": "2019-12-03 22:02:45", "author": "@BotLottery"}, "1200204524025085953": {"content_summary": "RT @hillbig: RigL trains sparse NNs from scratch; regularly drops the edges with the smallest magnitude, computes the gradients wrt virtual\u2026", "followers": "98", "datetime": "2019-11-29 00:07:11", "author": "@treasured_write"}, "1200009399676391426": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "5,309", "datetime": "2019-11-28 11:11:50", "author": "@HubBucket"}, "1200324793482072064": {"content_summary": "RT @hillbig: RigL trains sparse NNs from scratch; regularly drops the edges with the smallest magnitude, computes the gradients wrt virtual\u2026", "followers": "821", "datetime": "2019-11-29 08:05:06", "author": "@deepgradient"}, "1199624149582274560": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "60", "datetime": "2019-11-27 09:40:59", "author": "@SanghyukChun"}, "1199562373192966144": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "49", "datetime": "2019-11-27 05:35:31", "author": "@LittleFunnyGeek"}, "1199724001641152513": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "3,452", "datetime": "2019-11-27 16:17:46", "author": "@LiamFedus"}, "1199393146360279044": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "41", "datetime": "2019-11-26 18:23:04", "author": "@LeoHeinsaar"}, "1199492826830163968": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "759", "datetime": "2019-11-27 00:59:10", "author": "@suneelmarthi"}, "1199160265159217153": {"content_summary": "Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen https://t.co/xOT0trUmL1", "followers": "308", "datetime": "2019-11-26 02:57:41", "author": "@arxiv_cs_LG"}, "1199352447707353088": {"content_summary": "Rigging the Lottery: Making All Tickets Winners https://t.co/FTlfXHHje1", "followers": "4,087", "datetime": "2019-11-26 15:41:21", "author": "@arxiv_cscv"}, "1202394315713998849": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-05 01:08:38", "author": "@mayur_shingote"}, "1204022433449070592": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB \"\u5b9d\u304f\u3058\u7406\u8ad6\"\u3088\u308d\u3057\u304f\u826f\u3044\u521d\u671f\u5024\u3092\u9078\u3076\u306e\u3067\u306f\u306a\u304f\u3001\u3069\u306e\u3088\u3046\u306a\u521d\u671f\u5024\u3067\u3082\u6700\u521d\u304b\u3089\u758e\u3067\u9ad8\u7cbe\u5ea6\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u7fd2\u3055\u305b\u308bRigL\u3092\u63d0\u6848\u3002\u300e\u758e\u306aNN\u5b66\u7fd2\u2192\u30d1\u30e9\u30e1\u30fc\u30bf\u5c0f\u306a\u90e8\u5206\u3092\u524a\u9664\u2192\u52fe\u914d\u5927\u540c\u58eb\u3092\u7d50\u5408\u300f\u3092\u7e70\u308a\u8fd4\u2026", "followers": "1,882", "datetime": "2019-12-09 12:58:12", "author": "@sesquipedale"}, "1199495294439714816": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "161", "datetime": "2019-11-27 01:08:58", "author": "@nightwalker"}, "1199564486207668225": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "2,673", "datetime": "2019-11-27 05:43:54", "author": "@ayirpelle"}, "1200912784889769984": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "234", "datetime": "2019-11-30 23:01:34", "author": "@quantumbtc"}, "1199392094286176256": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "62", "datetime": "2019-11-26 18:18:53", "author": "@dave_co_dev"}, "1199551913576062976": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "410", "datetime": "2019-11-27 04:53:57", "author": "@JuanpaMF"}, "1199529758616276992": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "408", "datetime": "2019-11-27 03:25:55", "author": "@jainnitk"}, "1201231626857938944": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-01 20:08:32", "author": "@mayur_shingote"}, "1200193930458882048": {"content_summary": "\u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u3088\u3046\u306a\u3082\u306ehttps://t.co/DRwXtTXdOf", "followers": "18,217", "datetime": "2019-11-28 23:25:06", "author": "@hillbig"}, "1199573518125395968": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "47", "datetime": "2019-11-27 06:19:48", "author": "@mpsampat"}, "1202690862146342920": {"content_summary": "#DeepLearning #AI #Automated | Rigging the Lottery: Making All Tickets Winners https://t.co/vEv1ecAa6K", "followers": "3,639", "datetime": "2019-12-05 20:47:01", "author": "@hasdid"}, "1199671910658801665": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "141", "datetime": "2019-11-27 12:50:46", "author": "@FermionicSoup"}, "1199706661260271616": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "274", "datetime": "2019-11-27 15:08:52", "author": "@cighos"}, "1201301605041106944": {"content_summary": "2019/11/25 \u6295\u7a3f 3\u4f4d LG(Machine Learning) Rigging the Lottery: Making All Tickets Winners https://t.co/IIidFOAJHK 7 Tweets 19 Retweets 134 Favorites", "followers": "691", "datetime": "2019-12-02 00:46:36", "author": "@arxiv_pop"}, "1200101690445316097": {"content_summary": "A very interesting work indeed! https://t.co/FaSs9MpROl using @Tensorflow Model Optimization Toolkit \ud83d\udc49 Train sparse NN with fixed parameter count & computational cost throughout training, without sacrificing accuracy. Start from any initialization, no", "followers": "221", "datetime": "2019-11-28 17:18:34", "author": "@fdasilva59fr"}, "1199356740690558976": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "105", "datetime": "2019-11-26 15:58:24", "author": "@tetsu_nakamura8"}, "1199500087606554625": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "252", "datetime": "2019-11-27 01:28:01", "author": "@rogerluorl18"}, "1200483715278753792": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "1,538", "datetime": "2019-11-29 18:36:36", "author": "@minemaz"}, "1201881305836859394": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-03 15:10:07", "author": "@mayur_shingote"}, "1199681117692915715": {"content_summary": "RT @kzykmyzw: dense\u306a\u30e2\u30c7\u30eb\u3092sparse\u5316\u3001\u3068\u3044\u3046\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u306a\u304f\u3001sparse\u306a\u30e2\u30c7\u30eb\u3092e2e\u3067\u5b66\u7fd2\u3059\u308b\u624b\u6cd5\u3002\u5b66\u7fd2\u4e2d\u306b\u4e0d\u8981\u306a\u63a5\u7d9a\u306e\u524a\u9664\u3068\u65b0\u898f\u63a5\u7d9a\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u6700\u9069\u5316\u3057\u3066\u3044\u304f\u3002\u63a5\u7d9a\u524a\u9664\u306fweight\u3001\u65b0\u898f\u63a5\u7d9a\u306fgradient\u306b\u57fa\u3065\u3044\u3066\u6c7a\u5b9a\u3002Re\u2026", "followers": "903", "datetime": "2019-11-27 13:27:22", "author": "@wk77"}, "1199913633586851840": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "3", "datetime": "2019-11-28 04:51:18", "author": "@EGenus7"}, "1199527793639342080": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "8,627", "datetime": "2019-11-27 03:18:06", "author": "@Jack_Burdick"}, "1199511418241990656": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "3,105", "datetime": "2019-11-27 02:13:02", "author": "@kastnerkyle"}, "1202690895545536513": {"content_summary": "RT @hasdid: #DeepLearning #AI #Automated | Rigging the Lottery: Making All Tickets Winners https://t.co/vEv1ecAa6K", "followers": "91", "datetime": "2019-12-05 20:47:09", "author": "@imabit_inc"}, "1199606620889567232": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "597", "datetime": "2019-11-27 08:31:20", "author": "@2prime_PKU"}, "1199652087400947712": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "240", "datetime": "2019-11-27 11:32:00", "author": "@blauigris"}, "1199501454244888578": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "70", "datetime": "2019-11-27 01:33:27", "author": "@anirudhgj"}, "1199422174743977985": {"content_summary": "RT @pcastr: \ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0fmake everyone a lottery winner\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0f train sparse networks (with a randomly initialized topology) end-to-end without sacrific\u2026", "followers": "240", "datetime": "2019-11-26 20:18:25", "author": "@blauigris"}, "1199390750116601857": {"content_summary": "\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0fmake everyone a lottery winner\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0f train sparse networks (with a randomly initialized topology) end-to-end without sacrificing (much) accuracy! joint work with @utkuevci @Tgale96 @jacobmenick and @erich_elsen \ud83d\udc47\ud83c\udffe\ud83c\udf9f\ufe0f\ud83d\udc47\ud83c\udffe\ud83c\udf9f\ufe0f\ud83d\udc47\ud83c\udffe", "followers": "3,247", "datetime": "2019-11-26 18:13:33", "author": "@pcastr"}, "1199499548688879618": {"content_summary": "Now included in my top 10 ML paper titles of 2019 list", "followers": "421", "datetime": "2019-11-27 01:25:52", "author": "@zacharynado"}, "1199840334785630208": {"content_summary": "[R] Rigging the Lottery: Making All Tickets Winners https://t.co/9pQ01IqcjA #MachineLearning", "followers": "731", "datetime": "2019-11-28 00:00:02", "author": "@therealjpittman"}, "1204023176784535552": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB \"\u5b9d\u304f\u3058\u7406\u8ad6\"\u3088\u308d\u3057\u304f\u826f\u3044\u521d\u671f\u5024\u3092\u9078\u3076\u306e\u3067\u306f\u306a\u304f\u3001\u3069\u306e\u3088\u3046\u306a\u521d\u671f\u5024\u3067\u3082\u6700\u521d\u304b\u3089\u758e\u3067\u9ad8\u7cbe\u5ea6\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u7fd2\u3055\u305b\u308bRigL\u3092\u63d0\u6848\u3002\u300e\u758e\u306aNN\u5b66\u7fd2\u2192\u30d1\u30e9\u30e1\u30fc\u30bf\u5c0f\u306a\u90e8\u5206\u3092\u524a\u9664\u2192\u52fe\u914d\u5927\u540c\u58eb\u3092\u7d50\u5408\u300f\u3092\u7e70\u308a\u8fd4\u2026", "followers": "494", "datetime": "2019-12-09 13:01:09", "author": "@nagakagachi"}, "1200207604850364416": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "12", "datetime": "2019-11-29 00:19:26", "author": "@AIUEO_Rhythm"}, "1199396030648737792": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "11", "datetime": "2019-11-26 18:34:32", "author": "@is8ac"}, "1199864368516591617": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "145", "datetime": "2019-11-28 01:35:32", "author": "@vigi99"}, "1199613849831387136": {"content_summary": "accompanying 45 https://t.co/ZIGcSP0N9t", "followers": "567", "datetime": "2019-11-27 09:00:04", "author": "@jacobmenick"}, "1199606360566042635": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "299", "datetime": "2019-11-27 08:30:18", "author": "@500zainraza"}, "1199368046839615493": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "15", "datetime": "2019-11-26 16:43:20", "author": "@vijaycivs"}, "1199626938261950466": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "283", "datetime": "2019-11-27 09:52:04", "author": "@kowalskithomas"}, "1199867141861912576": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "341", "datetime": "2019-11-28 01:46:33", "author": "@lgraesser3"}, "1200943057794928641": {"content_summary": "Congratulations @data4sci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5000 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/yP8UVR8S7r", "followers": "182", "datetime": "2019-12-01 01:01:52", "author": "@BotLottery"}, "1199399570637090816": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "61", "datetime": "2019-11-26 18:48:36", "author": "@Laxya16"}, "1200007470325870592": {"content_summary": "[9/10] \ud83d\udcc8 - Rigging the Lottery: Making All Tickets Winners - 20 \u2b50 - \ud83d\udcc4 https://t.co/fe8mwligHm - \ud83d\udd17 https://t.co/tKt48fjVyn", "followers": "220", "datetime": "2019-11-28 11:04:10", "author": "@PapersTrending"}, "1204212526868598784": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB Instead of choosing good initial values in favor of \"Lottery Theory\", they propose RigL to train\u2026", "followers": "1,708", "datetime": "2019-12-10 01:33:34", "author": "@superbradyon"}, "1200206348354387968": {"content_summary": "RT @hillbig: RigL trains sparse NNs from scratch; regularly drops the edges with the smallest magnitude, computes the gradients wrt virtual\u2026", "followers": "240", "datetime": "2019-11-29 00:14:26", "author": "@mshero_y"}, "1200216245217329152": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "211", "datetime": "2019-11-29 00:53:46", "author": "@NHigashino"}, "1200584626353385472": {"content_summary": "@ATrueJameson @ID_AA_Carmack Hot off the press: https://t.co/s5zip0t045 Also there's plenty work on architecture search, combining Genetic Programming with Deep Learning e.g. https://t.co/uI5MomU5ds", "followers": "371", "datetime": "2019-11-30 01:17:35", "author": "@richardassar"}, "1199356536100917248": {"content_summary": "We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training from a random initialization - no luck required, all initialization \u201ctickets\u201d are winners. https://t.co/fA7VmXrj20", "followers": "295,998", "datetime": "2019-11-26 15:57:35", "author": "@DeepMindAI"}, "1202001788351569920": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-03 23:08:53", "author": "@mayur_shingote"}, "1199432323508293632": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "0", "datetime": "2019-11-26 20:58:44", "author": "@Sam09lol"}, "1199501605973848064": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "1,316", "datetime": "2019-11-27 01:34:03", "author": "@KageKirin"}, "1200059496892182532": {"content_summary": "RT @PapersTrending: [9/10] \ud83d\udcc8 - Rigging the Lottery: Making All Tickets Winners - 20 \u2b50 - \ud83d\udcc4 https://t.co/fe8mwligHm - \ud83d\udd17 https://t.co/tKt48fjV\u2026", "followers": "291", "datetime": "2019-11-28 14:30:54", "author": "@Shujian_Liu"}, "1199533466418847746": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "423", "datetime": "2019-11-27 03:40:39", "author": "@BetoOchoa13"}, "1200406617747005441": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1,118", "datetime": "2019-11-29 13:30:14", "author": "@ziyuwang"}, "1199560362288386048": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "22", "datetime": "2019-11-27 05:27:31", "author": "@Tsingggg"}, "1200378913819635713": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "219", "datetime": "2019-11-29 11:40:09", "author": "@ShirotaShin"}, "1199493898474786816": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "30", "datetime": "2019-11-27 01:03:25", "author": "@allardmaxime079"}, "1199268229857697794": {"content_summary": "Rigging the Lottery: Making All Tickets Winners. https://t.co/shsyJsL5XL", "followers": "5,454", "datetime": "2019-11-26 10:06:41", "author": "@StatsPapers"}, "1199495011559063553": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "0", "datetime": "2019-11-27 01:07:50", "author": "@Krishna91083813"}, "1201699805333868544": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-03 03:08:54", "author": "@mayur_shingote"}, "1199883077591425024": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "3", "datetime": "2019-11-28 02:49:53", "author": "@EGenus7"}, "1199665019383758848": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "8", "datetime": "2019-11-27 12:23:23", "author": "@pieorpi1"}, "1199392887328276480": {"content_summary": "Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can train 75% sparse MobileNets with almost no drop in accuracy.\u201d \ud83d\ude31\ud83d\ude31", "followers": "12,911", "datetime": "2019-11-26 18:22:02", "author": "@deliprao"}, "1201425523257348096": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "11,914", "datetime": "2019-12-02 08:59:00", "author": "@Rosenchild"}, "1199756053698641921": {"content_summary": "RT @sarahookr: What differs in this paper is how the connections are grown after pruning for the most important weights. I think this is pa\u2026", "followers": "533", "datetime": "2019-11-27 18:25:08", "author": "@URengaraju"}, "1200238866978017281": {"content_summary": "Sparsity is a clear inductive bias for neural nets, but end to end training and efficient inference have always been a challenge. I know @erich_elsen has been thinking about this for a long time, and seems like they've made some real progress!", "followers": "5,508", "datetime": "2019-11-29 02:23:39", "author": "@jesseengel"}, "1200042136957145090": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "215", "datetime": "2019-11-28 13:21:55", "author": "@AssistedEvolve"}, "1199173889206685696": {"content_summary": "\"Rigging the Lottery: Making All Tickets Winners\", Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich\u2026 https://t.co/Lyvhatyh7N", "followers": "767", "datetime": "2019-11-26 03:51:49", "author": "@arxivml"}, "1199378917179691011": {"content_summary": "End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the Lottery: Making All Tickets Winners\u201d (RigL\ud83d\udc47) ! \ud83d\udcc3 https://t.co/ZkD09vcZix \ud83d\udcc1 https://t.co/6ONkN1Mzwo with @Tgale96 @jacobmenick @pc", "followers": "499", "datetime": "2019-11-26 17:26:31", "author": "@utkuevci"}, "1199512532760825856": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "52", "datetime": "2019-11-27 02:17:28", "author": "@HengjianJia"}, "1199356858143784960": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "62", "datetime": "2019-11-26 15:58:52", "author": "@dave_co_dev"}, "1199481428762693633": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "5,545", "datetime": "2019-11-27 00:13:52", "author": "@__MLT__"}, "1199359870660677632": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "297", "datetime": "2019-11-26 16:10:50", "author": "@westis96"}, "1201904208473018371": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "669", "datetime": "2019-12-03 16:41:08", "author": "@arunkumar_bvr"}, "1200252972166348800": {"content_summary": "\u3064\u307e\u308a\u751f\u80b2\u9014\u4e2d\u306e\u5208\u308a\u8fbc\u307f\u304c\u826f\u3044\u3068\u3044\u3046\u8a71\u306a\u306e\u304b\u306a\uff1f\u7a4d\u8aad\u30ea\u30b9\u30c8\u306b\u5165\u308c\u3068\u3053\u3046", "followers": "251", "datetime": "2019-11-29 03:19:42", "author": "@hal9801"}, "1200651445151731712": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-11-30 05:43:06", "author": "@mayur_shingote"}, "1199514298428428288": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "613", "datetime": "2019-11-27 02:24:29", "author": "@KouroshMeshgi"}, "1199543782016004097": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "191", "datetime": "2019-11-27 04:21:38", "author": "@ikdeepl"}, "1200239082732838912": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "456", "datetime": "2019-11-29 02:24:31", "author": "@PerthMLGroup"}, "1199512657591701504": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "68", "datetime": "2019-11-27 02:17:58", "author": "@diegovogeid"}, "1199491484208119808": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "419", "datetime": "2019-11-27 00:53:49", "author": "@nik0spapp"}, "1200157206991319058": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "118", "datetime": "2019-11-28 20:59:10", "author": "@matthewopala"}, "1199565765591982080": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "5,545", "datetime": "2019-11-27 05:49:00", "author": "@__MLT__"}, "1199506100606734336": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "109", "datetime": "2019-11-27 01:51:54", "author": "@miguelsvasco"}, "1199486345439252480": {"content_summary": "More lottery ticket research in DL (now from Google) https://t.co/qeV7Prabhf", "followers": "4,802", "datetime": "2019-11-27 00:33:24", "author": "@IntuitMachine"}, "1200161785766195202": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "322", "datetime": "2019-11-28 21:17:22", "author": "@letranger14"}, "1199550744363331585": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "149", "datetime": "2019-11-27 04:49:18", "author": "@krunal_wrote"}, "1199682218559508483": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "96", "datetime": "2019-11-27 13:31:44", "author": "@oum_hou"}, "1199795135044866048": {"content_summary": "[1911.11134] Rigging the Lottery: Making All Tickets Winners https://t.co/Dwz5z1LqLQ", "followers": "4,070", "datetime": "2019-11-27 21:00:25", "author": "@bgoncalves"}, "1199530937131786241": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "824", "datetime": "2019-11-27 03:30:36", "author": "@morioka"}, "1199464537008803846": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "44", "datetime": "2019-11-26 23:06:45", "author": "@tingwuc"}, "1200839121414918144": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-11-30 18:08:51", "author": "@mayur_shingote"}, "1199716535415164928": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "22", "datetime": "2019-11-27 15:48:06", "author": "@OctothorpeVoid"}, "1199643133627240449": {"content_summary": "RT @jacobmenick: New work by Utku Evci et al. on sparse training. My contribution was helping with the RNN experiments. Fun collaborating w\u2026", "followers": "532", "datetime": "2019-11-27 10:56:26", "author": "@sidfix"}, "1224512895117078528": {"content_summary": "@bartoldson I don't think I got your point fully, but checkout Figure 5-right in our preprint. The best rate is not the smallest one (at least in our sweep). So I suspect it is just about the rate. https://t.co/cxdiImNBxj", "followers": "499", "datetime": "2020-02-04 01:59:59", "author": "@utkuevci"}, "1201112202964537344": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "165", "datetime": "2019-12-01 12:13:59", "author": "@micokoch"}, "1228068062307454976": {"content_summary": "@owulveryck @arxiv - https://t.co/sGSDqoUP57 Rigging the Lottery: Making All Tickets Winners - https://t.co/9YWrX1FOMr Self-training with Noisy Student improves ImageNet classification - https://t.co/fmH7W1nbzd Using Local Knowledge Graph Construction to S", "followers": "221", "datetime": "2020-02-13 21:26:57", "author": "@fdasilva59fr"}, "1199508388490752002": {"content_summary": "dense\u306a\u30e2\u30c7\u30eb\u3092sparse\u5316\u3001\u3068\u3044\u3046\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u306a\u304f\u3001sparse\u306a\u30e2\u30c7\u30eb\u3092e2e\u3067\u5b66\u7fd2\u3059\u308b\u624b\u6cd5\u3002\u5b66\u7fd2\u4e2d\u306b\u4e0d\u8981\u306a\u63a5\u7d9a\u306e\u524a\u9664\u3068\u65b0\u898f\u63a5\u7d9a\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u6700\u9069\u5316\u3057\u3066\u3044\u304f\u3002\u63a5\u7d9a\u524a\u9664\u306fweight\u3001\u65b0\u898f\u63a5\u7d9a\u306fgradient\u306b\u57fa\u3065\u3044\u3066\u6c7a\u5b9a\u3002ResNet-50\u3084MobilieNet\u3067\u5b9f\u9a13\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u657080%\u6e1b\u3067\u3082\u307b\u307c\u6027\u80fd\u52a3\u5316\u306a\u3057", "followers": "1,021", "datetime": "2019-11-27 02:01:00", "author": "@kzykmyzw"}, "1199448784654258178": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1,286", "datetime": "2019-11-26 22:04:09", "author": "@heghbalz"}, "1199514019356217344": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "563", "datetime": "2019-11-27 02:23:22", "author": "@bcmcmahan"}, "1199485367268737025": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "296", "datetime": "2019-11-27 00:29:31", "author": "@AlmanafSivi"}, "1201963378450800642": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "151", "datetime": "2019-12-03 20:36:15", "author": "@AiResearcher1"}, "1199494758562816000": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "847", "datetime": "2019-11-27 01:06:50", "author": "@t_Signull"}, "1201094126848090115": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "52", "datetime": "2019-12-01 11:02:09", "author": "@hakanardo"}, "1199776332273504256": {"content_summary": "Easy money", "followers": "1,189", "datetime": "2019-11-27 19:45:43", "author": "@johndpope"}, "1199472134831124485": {"content_summary": "RT @pcastr: \ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0fmake everyone a lottery winner\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0f train sparse networks (with a randomly initialized topology) end-to-end without sacrific\u2026", "followers": "201", "datetime": "2019-11-26 23:36:56", "author": "@aymanshams__"}, "1199574618224431104": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "73", "datetime": "2019-11-27 06:24:10", "author": "@MatthieuPerrot"}, "1199596371008987136": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "45", "datetime": "2019-11-27 07:50:36", "author": "@jeandut14000"}, "1199584228477931520": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "69", "datetime": "2019-11-27 07:02:21", "author": "@tomsthom"}, "1201035389177171969": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-01 07:08:45", "author": "@mayur_shingote"}, "1200945380692840449": {"content_summary": "Most popular computer science paper of the day: \"Rigging the Lottery: Making All Tickets Winners\" https://t.co/0mK2TOatLR https://t.co/BKAsbswobi", "followers": "280", "datetime": "2019-12-01 01:11:05", "author": "@HotCompScience"}, "1199494154776170497": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "21", "datetime": "2019-11-27 01:04:26", "author": "@VictorElkjaer"}, "1199564862571139073": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "821", "datetime": "2019-11-27 05:45:24", "author": "@deepgradient"}, "1199578186171895808": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "1,251", "datetime": "2019-11-27 06:38:21", "author": "@biggiobattista"}, "1199580318962872320": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "782", "datetime": "2019-11-27 06:46:49", "author": "@muktabh"}, "1199542865090625536": {"content_summary": "RT @kzykmyzw: dense\u306a\u30e2\u30c7\u30eb\u3092sparse\u5316\u3001\u3068\u3044\u3046\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u306a\u304f\u3001sparse\u306a\u30e2\u30c7\u30eb\u3092e2e\u3067\u5b66\u7fd2\u3059\u308b\u624b\u6cd5\u3002\u5b66\u7fd2\u4e2d\u306b\u4e0d\u8981\u306a\u63a5\u7d9a\u306e\u524a\u9664\u3068\u65b0\u898f\u63a5\u7d9a\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u6700\u9069\u5316\u3057\u3066\u3044\u304f\u3002\u63a5\u7d9a\u524a\u9664\u306fweight\u3001\u65b0\u898f\u63a5\u7d9a\u306fgradient\u306b\u57fa\u3065\u3044\u3066\u6c7a\u5b9a\u3002Re\u2026", "followers": "196", "datetime": "2019-11-27 04:18:00", "author": "@ogiek_amukawi"}, "1199548947766894592": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "4", "datetime": "2019-11-27 04:42:10", "author": "@schurholt"}, "1199168321913946112": {"content_summary": "RT @BrundageBot: Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Else\u2026", "followers": "98", "datetime": "2019-11-26 03:29:42", "author": "@treasured_write"}, "1200603153449664513": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "1,087", "datetime": "2019-11-30 02:31:12", "author": "@knottyknot"}, "1199392906756280320": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "416", "datetime": "2019-11-26 18:22:07", "author": "@orf_bnw"}, "1199814943219970048": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "186", "datetime": "2019-11-27 22:19:08", "author": "@surangasms01"}, "1199540364195123200": {"content_summary": "RT @SmallerNNsPls: Really cool improvements on Tim Dettmer's work; now sparse networks really can be trained from scratch using less GPU me\u2026", "followers": "113", "datetime": "2019-11-27 04:08:03", "author": "@fsign"}, "1200024115832217601": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "130", "datetime": "2019-11-28 12:10:19", "author": "@rhira2016"}, "1204512566233296896": {"content_summary": "RT @hillbig: RigL trains sparse NNs from scratch; regularly drops the edges with the smallest magnitude, computes the gradients wrt virtual\u2026", "followers": "472", "datetime": "2019-12-10 21:25:49", "author": "@yasuokajihei"}, "1200460185355128834": {"content_summary": "Congratulations @utkuevci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5000 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/9fLK4ckjoQ", "followers": "182", "datetime": "2019-11-29 17:03:06", "author": "@BotLottery"}, "1199694794466844672": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "94", "datetime": "2019-11-27 14:21:42", "author": "@DeVlugtIsaac"}, "1199411960938467328": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "880", "datetime": "2019-11-26 19:37:50", "author": "@RichmanRonald"}, "1199736250799804416": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "206", "datetime": "2019-11-27 17:06:26", "author": "@bnjasim"}, "1200238679832182784": {"content_summary": "RT @BrundageBot: Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Else\u2026", "followers": "456", "datetime": "2019-11-29 02:22:55", "author": "@PerthMLGroup"}, "1199618133130133509": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "88", "datetime": "2019-11-27 09:17:05", "author": "@ML_Milano"}, "1202574003296243713": {"content_summary": "Congratulations @utkuevci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5072 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/9fLK4ckjoQ", "followers": "182", "datetime": "2019-12-05 13:02:39", "author": "@BotLottery"}, "1200042722805125122": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "19", "datetime": "2019-11-28 13:24:15", "author": "@MassBassLol"}, "1200360633058787328": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "47", "datetime": "2019-11-29 10:27:31", "author": "@OCTOPON"}, "1199457325230231553": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1,100", "datetime": "2019-11-26 22:38:05", "author": "@permutans"}, "1199448653791805440": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "2,995", "datetime": "2019-11-26 22:03:38", "author": "@arimorcos"}, "1199681571009093632": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "690", "datetime": "2019-11-27 13:29:10", "author": "@SythonUK"}, "1204016483635871744": {"content_summary": "https://t.co/HyOygUfALB \"\u5b9d\u304f\u3058\u7406\u8ad6\"\u3088\u308d\u3057\u304f\u826f\u3044\u521d\u671f\u5024\u3092\u9078\u3076\u306e\u3067\u306f\u306a\u304f\u3001\u3069\u306e\u3088\u3046\u306a\u521d\u671f\u5024\u3067\u3082\u6700\u521d\u304b\u3089\u758e\u3067\u9ad8\u7cbe\u5ea6\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u7fd2\u3055\u305b\u308bRigL\u3092\u63d0\u6848\u3002\u300e\u758e\u306aNN\u5b66\u7fd2\u2192\u30d1\u30e9\u30e1\u30fc\u30bf\u5c0f\u306a\u90e8\u5206\u3092\u524a\u9664\u2192\u52fe\u914d\u5927\u540c\u58eb\u3092\u7d50\u5408\u300f\u3092\u7e70\u308a\u8fd4\u3057\u3066\u5b66\u7fd2\u3002\u5b66\u7fd2\u6642\u9593\u3082\u5927\u304d\u304f\u4f38\u3073\u305a\u3001\u7cbe\u5ea6\u305d\u306e\u307e\u307e\u3067\u63a8\u8ad6\u901f\u5ea6\u5411\u4e0a\u304c\u78ba\u8a8d https://t.co/9XY8Lb2wxa", "followers": "1,292", "datetime": "2019-12-09 12:34:33", "author": "@AkiraTOSEI"}, "1199644392169979904": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "5", "datetime": "2019-11-27 11:01:26", "author": "@MyungsubChoi"}, "1199537851798446080": {"content_summary": "What differs in this paper is how the connections are grown after pruning for the most important weights. I think this is part of a very interesting direction of research, amplifying the role of weights estimated to be important (in addition to removing th", "followers": "10,074", "datetime": "2019-11-27 03:58:04", "author": "@sarahookr"}, "1199512883866034176": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1,063", "datetime": "2019-11-27 02:18:52", "author": "@miguelalonsojr"}, "1199392082894475264": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "81,524", "datetime": "2019-11-26 18:18:50", "author": "@hugo_larochelle"}, "1199722754972491778": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "284", "datetime": "2019-11-27 16:12:49", "author": "@Ar_Douillard"}, "1204235294209216512": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB Instead of choosing good initial values in favor of \"Lottery Theory\", they propose RigL to train\u2026", "followers": "3,398", "datetime": "2019-12-10 03:04:02", "author": "@rinatie_ceo"}, "1199589957683294209": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "172", "datetime": "2019-11-27 07:25:07", "author": "@WeikangGong"}, "1199799714650177536": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "12", "datetime": "2019-11-27 21:18:37", "author": "@manolis_1979"}, "1200199874567909376": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "37,494", "datetime": "2019-11-28 23:48:43", "author": "@sedielem"}, "1199490507606355968": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "2,925", "datetime": "2019-11-27 00:49:57", "author": "@evolvingstuff"}, "1199841738053038080": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "11", "datetime": "2019-11-28 00:05:36", "author": "@Didar_Olmez"}, "1199385838347407360": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "142", "datetime": "2019-11-26 17:54:02", "author": "@n_kats_"}, "1199642299774439424": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "497", "datetime": "2019-11-27 10:53:07", "author": "@gauthier_gidel"}, "1199560206646165504": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "1,935", "datetime": "2019-11-27 05:26:54", "author": "@EldarSilver"}, "1199359077849161729": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "2,600", "datetime": "2019-11-26 16:07:41", "author": "@AdamMarblestone"}, "1199697263997702151": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "2,145", "datetime": "2019-11-27 14:31:31", "author": "@iskander"}, "1199755764471959553": {"content_summary": "@RUSH1L follow up with this one. You dont need lucky tickets. \"Rigged Lottery\". https://t.co/Gz86i2ulaq", "followers": "65", "datetime": "2019-11-27 18:23:59", "author": "@jjayaram7"}, "1199357415352725505": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-11-26 16:01:05", "author": "@mayur_shingote"}, "1200483589898436610": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "6,491", "datetime": "2019-11-29 18:36:06", "author": "@jingbay"}, "1199643384861921283": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "30", "datetime": "2019-11-27 10:57:25", "author": "@NikolausWest"}, "1199494114569441280": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "0", "datetime": "2019-11-27 01:04:17", "author": "@ftritanaka"}, "1201563938112114688": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1,401", "datetime": "2019-12-02 18:09:01", "author": "@mayur_shingote"}, "1200200191887978496": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "768", "datetime": "2019-11-28 23:49:59", "author": "@karlhigley"}, "1200240587720531968": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "7,541", "datetime": "2019-11-29 02:30:30", "author": "@brandondamos"}, "1199616376438214656": {"content_summary": "New work by Utku Evci et al. on sparse training. My contribution was helping with the RNN experiments. Fun collaborating with @utkuevci and getting involved in sparse man @erich_elsen's sweeping sparsity research programme.", "followers": "567", "datetime": "2019-11-27 09:10:06", "author": "@jacobmenick"}, "1199597210310184962": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "34", "datetime": "2019-11-27 07:53:57", "author": "@therealaseifert"}, "1199491855597084672": {"content_summary": "@jefrankle", "followers": "68", "datetime": "2019-11-27 00:55:18", "author": "@Bschulz5"}, "1199422583931899906": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "240", "datetime": "2019-11-26 20:20:02", "author": "@blauigris"}, "1199868759709601792": {"content_summary": "\u307e\u305f\u9762\u767d\u305d\u3046\u306aLottery tickets\u30da\u30fc\u30d1\u30fc\u304c\u51fa\u305f\u3002\u3053\u306e\u3088\u3046\u306a\u8efd\u91cf\u5316\u306epruning\u7cfb\u3082\u4eca\u5f8cCV\u304b\u3089NLP\u306b\u6d41\u308c\u3066\u6765\u308b\u3060\u308d\u3046\u3002\u5f8c\u3067\u8aad\u3080\u3002", "followers": "45", "datetime": "2019-11-28 01:52:59", "author": "@yoquankara"}, "1199677252973268992": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "2", "datetime": "2019-11-27 13:12:00", "author": "@sndnyang"}, "1199437424553652226": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "113", "datetime": "2019-11-26 21:19:01", "author": "@linco69"}, "1199398327747514368": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "45", "datetime": "2019-11-26 18:43:39", "author": "@jeandut14000"}, "1204177095020072961": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB \"\u5b9d\u304f\u3058\u7406\u8ad6\"\u3088\u308d\u3057\u304f\u826f\u3044\u521d\u671f\u5024\u3092\u9078\u3076\u306e\u3067\u306f\u306a\u304f\u3001\u3069\u306e\u3088\u3046\u306a\u521d\u671f\u5024\u3067\u3082\u6700\u521d\u304b\u3089\u758e\u3067\u9ad8\u7cbe\u5ea6\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u7fd2\u3055\u305b\u308bRigL\u3092\u63d0\u6848\u3002\u300e\u758e\u306aNN\u5b66\u7fd2\u2192\u30d1\u30e9\u30e1\u30fc\u30bf\u5c0f\u306a\u90e8\u5206\u3092\u524a\u9664\u2192\u52fe\u914d\u5927\u540c\u58eb\u3092\u7d50\u5408\u300f\u3092\u7e70\u308a\u8fd4\u2026", "followers": "547", "datetime": "2019-12-09 23:12:46", "author": "@FBWM8888"}, "1199155959479836672": {"content_summary": "Rigging the Lottery: Making All Tickets Winners https://t.co/FTlfXHHje1", "followers": "4,087", "datetime": "2019-11-26 02:40:34", "author": "@arxiv_cscv"}, "1199163347364646912": {"content_summary": "Rigging the Lottery: Making All Tickets Winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen https://t.co/LPfDhVsAJR", "followers": "3,858", "datetime": "2019-11-26 03:09:56", "author": "@BrundageBot"}, "1199624397855895555": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "611", "datetime": "2019-11-27 09:41:59", "author": "@tiagotvv"}, "1199431611290251264": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "10", "datetime": "2019-11-26 20:55:55", "author": "@MinhTruonghong"}, "1199701102612140035": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "584", "datetime": "2019-11-27 14:46:46", "author": "@morgangiraud"}, "1200149227550040065": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "149", "datetime": "2019-11-28 20:27:28", "author": "@g3n1uss"}, "1200256368747388933": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "164,152", "datetime": "2019-11-29 03:33:12", "author": "@ceobillionaire"}, "1200341507359809536": {"content_summary": "RT @hillbig: \u6700\u521d\u304b\u3089\u758e\u306aNN\u3067\u5b66\u7fd2\u3057\u305f\u5834\u5408\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u304f\u5c40\u6240\u89e3\u306b\u9665\u308b\u3002RigL\u306f\u758e\u306aNN\u304b\u3089\u958b\u59cb\u3059\u308b\u304c\u3001\u5b9a\u671f\u7684\u306b\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u679d\u3092\u9593\u5f15\u3044\u305f\u5f8c\u306b\u4eee\u60f3\u7684\u306b\u5bc6\u306a\u679d\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3001\u52fe\u914d\u304c\u5927\u304d\u3044\u679d\u3092\u91cd\u307f0\u3067\u5c0e\u5165\u3059\u308b\u3002\u3042\u305f\u304b\u3082\u6700\u9069\u5316\u4e2d\u306b\u672c\u6765\u306a\u3044\u964d\u308a\u5742\u3092\u307f\u3064\u3051\u3066\u305d\u3053\u306b\u9053\u3092\u4f5c\u308b\u2026", "followers": "4,256", "datetime": "2019-11-29 09:11:31", "author": "@ibaibabaibai"}, "1199490826188931072": {"content_summary": "Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/M7W3t50lOf", "followers": "81,077", "datetime": "2019-11-27 00:51:13", "author": "@hardmaru"}, "1199695567561117697": {"content_summary": "RT @pcastr: \ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0fmake everyone a lottery winner\ud83c\udf9f\ufe0f\ud83c\udf9f\ufe0f train sparse networks (with a randomly initialized topology) end-to-end without sacrific\u2026", "followers": "7", "datetime": "2019-11-27 14:24:47", "author": "@RuihanSun1"}, "1200255439901822976": {"content_summary": "RT @deliprao: Great paper title, with results to match. \u201cMobileNets are efficient networks and difficult to sparsify. With RigL we can trai\u2026", "followers": "48", "datetime": "2019-11-29 03:29:31", "author": "@chbalajitilak"}, "1199674506006220800": {"content_summary": "[1911.11134] Rigging the Lottery: Making All Tickets Winners https://t.co/mB1Epng2E2", "followers": "670", "datetime": "2019-11-27 13:01:05", "author": "@data4sci"}, "1201577146361303042": {"content_summary": "Congratulations @utkuevci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5040 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/9fLK4ckjoQ", "followers": "182", "datetime": "2019-12-02 19:01:30", "author": "@BotLottery"}, "1199503464809648128": {"content_summary": "Wow - the advances in lottery ticket hypothesis keep on coming thick and fast.", "followers": "145,487", "datetime": "2019-11-27 01:41:26", "author": "@StartupYou"}, "1199537603353157632": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "32", "datetime": "2019-11-27 03:57:05", "author": "@manish_181192"}, "1199681083526139904": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "903", "datetime": "2019-11-27 13:27:13", "author": "@wk77"}, "1199645528486797312": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "361", "datetime": "2019-11-27 11:05:56", "author": "@iugoaoj"}, "1199491472447270912": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "396", "datetime": "2019-11-27 00:53:47", "author": "@linkoffate"}, "1199431481610756096": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "83", "datetime": "2019-11-26 20:55:24", "author": "@im_5an"}, "1199681385339834368": {"content_summary": "RT @hardmaru: Everyone is a winner \ud83d\udd25 https://t.co/nKUV3zIsJc https://t.co/gh79ZGC04o https://t.co/M7W3t50lOf", "followers": "5,008", "datetime": "2019-11-27 13:28:25", "author": "@btreetaiji"}, "1199375159070806016": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "1", "datetime": "2019-11-26 17:11:35", "author": "@Pol09122455"}, "1199860495395672064": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "37", "datetime": "2019-11-28 01:20:09", "author": "@sk413025"}, "1201845396546498560": {"content_summary": "@blauigris thanks for mentioning our work! i'd be remiss if i didn't mention the other authors: @utkuevci @Tgale96 @jacobmenick @erich_elsen and here's utku's original tweet linking to code and paper: https://t.co/VcDMSkYlMW", "followers": "3,247", "datetime": "2019-12-03 12:47:26", "author": "@pcastr"}, "1200196002130477056": {"content_summary": "RigL trains sparse NNs from scratch; regularly drops the edges with the smallest magnitude, computes the gradients wrt virtual dense edges, and introduces new edges with the largest gradient. Escaping bad local minima by making a new descending direction.", "followers": "18,217", "datetime": "2019-11-28 23:33:20", "author": "@hillbig"}, "1199484797422374913": {"content_summary": "Really cool improvements on Tim Dettmer's work; now sparse networks really can be trained from scratch using less GPU memory!", "followers": "125", "datetime": "2019-11-27 00:27:15", "author": "@SmallerNNsPls"}, "1199393428511088644": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "4,477", "datetime": "2019-11-26 18:24:11", "author": "@CShorten30"}, "1199442407537827840": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "36", "datetime": "2019-11-26 21:38:49", "author": "@kuz44ma69"}, "1199360416670986240": {"content_summary": "RT @DeepMindAI: We also introduce a technique [https://t.co/SFz2vTThTv] for training neural networks that are sparse throughout training fr\u2026", "followers": "712", "datetime": "2019-11-26 16:13:01", "author": "@myusufsarigoz"}, "1199521035839819776": {"content_summary": "RT @SmallerNNsPls: Really cool improvements on Tim Dettmer's work; now sparse networks really can be trained from scratch using less GPU me\u2026", "followers": "108", "datetime": "2019-11-27 02:51:15", "author": "@dam_nlp"}, "1202785259131146240": {"content_summary": "Congratulations @utkuevci! Because you tweeted about a lottery, your tweet has been randomly selected out of 5081 tweets as the winner of a tweet lottery. Your prize is a like and follow. Cool! https://t.co/9fLK4ckjoQ", "followers": "182", "datetime": "2019-12-06 03:02:07", "author": "@BotLottery"}, "1204177196127899648": {"content_summary": "RT @AkiraTOSEI: https://t.co/HyOygUfALB Instead of choosing good initial values in favor of \"Lottery Theory\", they propose RigL to train\u2026", "followers": "547", "datetime": "2019-12-09 23:13:10", "author": "@FBWM8888"}, "1199621592810053632": {"content_summary": "RT @utkuevci: End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: \u201cRigging the\u2026", "followers": "110", "datetime": "2019-11-27 09:30:50", "author": "@enricesena"}}}