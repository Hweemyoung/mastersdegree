{"citation_id": "21021191", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 15:01:47", "twitter": {"874496961260302336": {"followers": "71", "datetime": "2017-06-13 05:21:37", "author": "@hjguyhan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875617454818377728": {"followers": "590", "datetime": "2017-06-16 07:34:04", "author": "@codekee", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "907296377562071040": {"followers": "106", "datetime": "2017-09-11 17:34:47", "author": "@ap229997", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "1013946615659298817": {"followers": "311", "datetime": "2018-07-03 00:44:46", "author": "@moultano", "content_summary": "@davidcrawshaw https://t.co/Nv7vKV3o0M This is the direction everything is going for short (sentence length) text."}, "1117081217818513408": {"followers": "377", "datetime": "2019-04-13 15:04:51", "author": "@alkari", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874611484789276673": {"followers": "177,517", "datetime": "2017-06-13 12:56:42", "author": "@Montreal_AI", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874614343228416001": {"followers": "874", "datetime": "2017-06-13 13:08:03", "author": "@mrdrozdov", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876903123582803968": {"followers": "695", "datetime": "2017-06-19 20:42:51", "author": "@thoma_gu", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874542375686860800": {"followers": "255", "datetime": "2017-06-13 08:22:05", "author": "@josipK", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876826267944976385": {"followers": "247", "datetime": "2017-06-19 15:37:27", "author": "@zxybazh", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "875020295865868288": {"followers": "26", "datetime": "2017-06-14 16:01:10", "author": "@DSM06", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875066578601160704": {"followers": "4,390", "datetime": "2017-06-14 19:05:04", "author": "@GoAbiAryan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1107168970979840000": {"followers": "802", "datetime": "2019-03-17 06:37:07", "author": "@estes_rickey", "content_summary": "Attention Is All You Need https://t.co/ASC8suXl0n"}, "1155195562578370561": {"followers": "1,186", "datetime": "2019-07-27 19:17:39", "author": "@meditationstuff", "content_summary": "@Meaningness This struck me as maybe the first, certainly not \"AI,\" but first non-bullshit \"deep learning\" paper, ever. Curious if you have any thoughts. (1) https://t.co/jtf474iEbq (2) An informal take: https://t.co/jf0e4EIBaL"}, "1064806915451039744": {"followers": "712", "datetime": "2018-11-20 09:05:26", "author": "@xabi_soto", "content_summary": "5 Bestalde, hizkuntzaren prozesamenduan azkenaldian modan dagoen teknika sare neuronaletan oinarritzen da, zehazki Transformer izeneko arkitekturarekin. Hitz gutxitan, Transformer oso mobida totxoa da (erreferentzia: https://t.co/fV3wvb5aYm). #txiotesia4"}, "874447134019510272": {"followers": "208", "datetime": "2017-06-13 02:03:37", "author": "@AdrianoCarmezim", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875008043968155648": {"followers": "54", "datetime": "2017-06-14 15:12:29", "author": "@jsdelfino", "content_summary": "Lots of alternatives to RNNs for machine translation coming up recently... Interesting! https://t.co/uqVUN22rBQ"}, "874517711002513412": {"followers": "282", "datetime": "2017-06-13 06:44:04", "author": "@mizvladimir", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "874898823612694528": {"followers": "524", "datetime": "2017-06-14 07:58:29", "author": "@kareldumon", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874449594872188933": {"followers": "67", "datetime": "2017-06-13 02:13:24", "author": "@elyase", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874529039570149377": {"followers": "435", "datetime": "2017-06-13 07:29:05", "author": "@mcbessega", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "948820441422024704": {"followers": "818", "datetime": "2018-01-04 07:36:36", "author": "@ErmiaBivatan", "content_summary": "RT @hillbig: To collect non-local information in feature map, self-attention is promising because it can learn a flexible connection and ca\u2026"}, "1129114349832998913": {"followers": "426", "datetime": "2019-05-16 20:00:13", "author": "@a__real__dog", "content_summary": "@MoMoButFaster @areitu More recent work drops LSTM's in favor of attention networks*, but this is what I have ready to go right-this-second, so that's what we have! * https://t.co/RIaHMjYG3J"}, "874443991852359682": {"followers": "264", "datetime": "2017-06-13 01:51:08", "author": "@hellorahulk", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "906303152059703296": {"followers": "129", "datetime": "2017-09-08 23:48:04", "author": "@RndWalk", "content_summary": "\"the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\" https://t.co/5X4PMg25Oa"}, "874601103421304836": {"followers": "283", "datetime": "2017-06-13 12:15:27", "author": "@_Guz_", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874811357635293184": {"followers": "4,884", "datetime": "2017-06-14 02:10:55", "author": "@martin_wicke", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "877169766112399360": {"followers": "388", "datetime": "2017-06-20 14:22:23", "author": "@laithshadeed", "content_summary": "[1706.03762] Attention Is All You Need - https://t.co/J7JsTpdocH https://t.co/opnnTEmM9L"}, "876896100229279744": {"followers": "1,587", "datetime": "2017-06-19 20:14:56", "author": "@edublancas", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1058520599025336320": {"followers": "214", "datetime": "2018-11-03 00:45:51", "author": "@WNixalo", "content_summary": "paper: https://t.co/a9OTisN3YU"}, "874455135111348224": {"followers": "432", "datetime": "2017-06-13 02:35:25", "author": "@tsukamakiri", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874583816639311872": {"followers": "361", "datetime": "2017-06-13 11:06:45", "author": "@jadhavamitb", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "876844326545874944": {"followers": "1,627", "datetime": "2017-06-19 16:49:13", "author": "@chris_brockett", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874516234787725312": {"followers": "221", "datetime": "2017-06-13 06:38:12", "author": "@kog", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "878513148596191233": {"followers": "342", "datetime": "2017-06-24 07:20:31", "author": "@ITdevopscloud", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874742904446648320": {"followers": "532", "datetime": "2017-06-13 21:38:55", "author": "@rnella01", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165819169654431745": {"followers": "1,918", "datetime": "2019-08-26 02:52:04", "author": "@DocXavi", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1117064254509678594": {"followers": "177,517", "datetime": "2019-04-13 13:57:26", "author": "@Montreal_AI", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "877873852746838017": {"followers": "1,121", "datetime": "2017-06-22 13:00:11", "author": "@msurd", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874679351899545601": {"followers": "1,587", "datetime": "2017-06-13 17:26:22", "author": "@edublancas", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874472593939615745": {"followers": "1,197", "datetime": "2017-06-13 03:44:47", "author": "@menphim", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1171312896254648321": {"followers": "20", "datetime": "2019-09-10 06:42:11", "author": "@Boristream", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026"}, "874534559328608257": {"followers": "783", "datetime": "2017-06-13 07:51:01", "author": "@muktabh", "content_summary": "RT @arxiv_flying: #NIPS2017 Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/ZoYQ68Vy3l"}, "876828612036485120": {"followers": "650", "datetime": "2017-06-19 15:46:46", "author": "@timwee", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1165867471863042048": {"followers": "50", "datetime": "2019-08-26 06:04:00", "author": "@remiconnesson", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "925278427078451200": {"followers": "3,049", "datetime": "2017-10-31 08:29:02", "author": "@EpicRelevance", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1176438655835000832": {"followers": "8", "datetime": "2019-09-24 10:10:07", "author": "@ShivamChandhok2", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026"}, "945970595577192451": {"followers": "15,143", "datetime": "2017-12-27 10:52:19", "author": "@alevergara78", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1165989021430034434": {"followers": "12", "datetime": "2019-08-26 14:07:00", "author": "@octadero", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1166247800537067521": {"followers": "905", "datetime": "2019-08-27 07:15:18", "author": "@fishtale13", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874622336279789569": {"followers": "90,659", "datetime": "2017-06-13 13:39:49", "author": "@stanfordnlp", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876828073819328512": {"followers": "702", "datetime": "2017-06-19 15:44:38", "author": "@lzamparo", "content_summary": "\"Attention is (not) all you need\" apparently. https://t.co/ujr3EmtkhF"}, "876229339288924160": {"followers": "56,127", "datetime": "2017-06-18 00:05:28", "author": "@IntelligenceTV", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874647318104035334": {"followers": "351", "datetime": "2017-06-13 15:19:05", "author": "@dginev", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "1165891916879269888": {"followers": "4", "datetime": "2019-08-26 07:41:08", "author": "@creatorscan", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165817320729481216": {"followers": "43", "datetime": "2019-08-26 02:44:43", "author": "@BhanujeetC", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874773483330428929": {"followers": "4,144", "datetime": "2017-06-13 23:40:25", "author": "@laurent_dinh", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1117336742980251648": {"followers": "93", "datetime": "2019-04-14 08:00:13", "author": "@poqaa_ai", "content_summary": "Attention Is All You Need https://t.co/0btLvEs3rl (Popularity:14.8) #Natural_language_processing #Machine_Learning"}, "1097837697585070080": {"followers": "10", "datetime": "2019-02-19 12:37:58", "author": "@Akheneiton", "content_summary": "@manolomart1nez @mary_meadow @gonzalotorne Attention is all you need https://t.co/kzHFSbU8QL"}, "874468949777305602": {"followers": "5,410", "datetime": "2017-06-13 03:30:19", "author": "@yu4u", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "881566985619988482": {"followers": "12,547", "datetime": "2017-07-02 17:35:22", "author": "@ml_review", "content_summary": "RT @ml_review: \"Attention Is All You Need\" paper from Google. Achieves a single-model state-of-the-art BLEU score of 41.0. https://t.co/7Xo\u2026"}, "874877270795714561": {"followers": "113", "datetime": "2017-06-14 06:32:50", "author": "@tranlaman", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875928469401513985": {"followers": "1,246", "datetime": "2017-06-17 04:09:55", "author": "@JonClarkSeattle", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874493950291095553": {"followers": "59", "datetime": "2017-06-13 05:09:39", "author": "@liqiangniu", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874451352163762176": {"followers": "7,922", "datetime": "2017-06-13 02:20:23", "author": "@jasonbaldridge", "content_summary": "More than meets the eye! https://t.co/UEmGlMWfsc https://t.co/qvHc1uX8OK"}, "874454866805964800": {"followers": "522", "datetime": "2017-06-13 02:34:21", "author": "@pijili", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165779992447373312": {"followers": "698", "datetime": "2019-08-26 00:16:24", "author": "@AINow6", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874836047913529346": {"followers": "1", "datetime": "2017-06-14 03:49:02", "author": "@deep_blacksky", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "948827513467297792": {"followers": "691", "datetime": "2018-01-04 08:04:42", "author": "@SythonUK", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "1013946343402934274": {"followers": "440", "datetime": "2018-07-03 00:43:41", "author": "@Joe_Meka", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "1117346054184296449": {"followers": "411", "datetime": "2019-04-14 08:37:13", "author": "@taylorhxu", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "975840100322963456": {"followers": "2,026", "datetime": "2018-03-19 21:03:04", "author": "@geoffreyirving", "content_summary": "@jackclarkSF @mer__edith Seems like an oddly specific topic for a Washington DC chat. https://t.co/w43ZFPv1kD"}, "874610273285533696": {"followers": "3,835", "datetime": "2017-06-13 12:51:53", "author": "@autiomaa", "content_summary": "Beautiful diagrams in the research report from Google Brain. https://t.co/lwoMl6ULko"}, "1219647531380105216": {"followers": "304", "datetime": "2020-01-21 15:46:46", "author": "@tae_hwang", "content_summary": "@CancerConnector @ianholmes Once you read all the relevant #DL papers, read this! \"Attention is all you need\" https://t.co/VugUfN4suk This is all you need for the most of your comp bio application!"}, "874506231724949508": {"followers": "1,847", "datetime": "2017-06-13 05:58:27", "author": "@katsuhitosudoh", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874682565956947968": {"followers": "176", "datetime": "2017-06-13 17:39:09", "author": "@KellyJayDavis", "content_summary": "Nice, \"Attention Is All You Need\" https://t.co/xR9cAqkCzU"}, "883251978486386688": {"followers": "336", "datetime": "2017-07-07 09:10:56", "author": "@DeepLearningCor", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874531309493911552": {"followers": "1,310", "datetime": "2017-06-13 07:38:06", "author": "@arxiv_flying", "content_summary": "#NIPS2017 Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/ZoYQ68Vy3l"}, "874665147796553728": {"followers": "85", "datetime": "2017-06-13 16:29:56", "author": "@iPrabakaran", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874592129305034754": {"followers": "2,329", "datetime": "2017-06-13 11:39:47", "author": "@KristinHenry", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117092583816204288": {"followers": "4,812", "datetime": "2019-04-13 15:50:01", "author": "@IntuitMachine", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874499080591986689": {"followers": "63", "datetime": "2017-06-13 05:30:02", "author": "@fkariminejadasl", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877809424886149121": {"followers": "736", "datetime": "2017-06-22 08:44:10", "author": "@TheOrbifold", "content_summary": "Attention is all you need. A milestone in intelligence. #AI https://t.co/FNyCG16TSd By Google of course."}, "1166035791547514880": {"followers": "647", "datetime": "2019-08-26 17:12:51", "author": "@zzprosper", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1115914665228414976": {"followers": "3,455", "datetime": "2019-04-10 09:49:23", "author": "@XOR0sha2wine", "content_summary": "@argent_smith \u041e\u043d\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u043c\u0443\u043b\u044c\u0442\u0438\u044f\u0437\u044b\u0447\u043d\u044b\u0439 BERT \u0434\u043b\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e: https://t.co/hW3vpcreQr https://t.co/aL2R7cqjGR https://t.co/8ZmlLdlJqO \u0415\u0448\u0435 \u0435\u0441\u0442\u044c \u0437\u0430\u0444\u0430\u0439\u0442\u044e\u043d\u0435\u043d\u043d\u044b\u0439 multilingual BERT \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u043e\u0442 iPavlov, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u043b\u0438 \u043d\u0430 \u043d\u043e\u0432\u043e\u0441\u0442\u044f\u0445 \u0438 \u0412\u0438\u043a\u0438\u043f\u0435\u0434\u0438\u0438: https://t"}, "876767878825074689": {"followers": "3,032", "datetime": "2017-06-19 11:45:26", "author": "@jaukia", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1117356014033088513": {"followers": "236", "datetime": "2019-04-14 09:16:47", "author": "@flowing", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "875102154041708544": {"followers": "2,164", "datetime": "2017-06-14 21:26:26", "author": "@alanyttian", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166060794867376128": {"followers": "21", "datetime": "2019-08-26 18:52:12", "author": "@jlgc68", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874728023072927745": {"followers": "33", "datetime": "2017-06-13 20:39:47", "author": "@abhijitambekar", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874548724806111232": {"followers": "826", "datetime": "2017-06-13 08:47:18", "author": "@chiheb_tr", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165709767819456513": {"followers": "107", "datetime": "2019-08-25 19:37:21", "author": "@cmsprs", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1218199760336646145": {"followers": "16,720", "datetime": "2020-01-17 15:53:50", "author": "@samcharrington", "content_summary": "RT @comcomodel: Music & AI plus A Geometric Perspective on RL \u26a0\ufe0fAttention Is All You Need https://t.co/EHmKtIbbKf \ud83d\udce3Unreasonable Effectivene\u2026"}, "1166710991578513408": {"followers": "67", "datetime": "2019-08-28 13:55:51", "author": "@SonOfHills", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876229465088675841": {"followers": "1,039", "datetime": "2017-06-18 00:05:58", "author": "@nonsensews", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "876004463001174016": {"followers": "6,149", "datetime": "2017-06-17 09:11:54", "author": "@MateCat", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "910557347638071296": {"followers": "140", "datetime": "2017-09-20 17:32:43", "author": "@moulla", "content_summary": "@tombrammar 3) New techniques are being tested every day, check out 'Attention is all you need' https://t.co/JSPxzf6dSl. Remarkable progress has been>"}, "874904153218322432": {"followers": "21", "datetime": "2017-06-14 08:19:39", "author": "@luoyuchu", "content_summary": "\u60f3\u4e0d\u5230 attention \u8fd8\u80fd\u8fd9\u4e48\u7528\uff0c\u800c\u4e14\u6548\u679c\u5c45\u7136\u8fd9\u4e48\u597d https://t.co/wcCO8hZ3PW"}, "874493721659621376": {"followers": "1,942", "datetime": "2017-06-13 05:08:45", "author": "@GiorgioPatrini", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875739664052891648": {"followers": "159,742", "datetime": "2017-06-16 15:39:41", "author": "@Montreal_IA", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1226256967221596160": {"followers": "11,390", "datetime": "2020-02-08 21:30:18", "author": "@_brohrer_", "content_summary": "Finally diving into the original paper on transformers, Attention is All You Need, to wrap my head around the method. https://t.co/TjmTi3dkt9"}, "876639575619190784": {"followers": "5,410", "datetime": "2017-06-19 03:15:36", "author": "@yu4u", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874464680093245440": {"followers": "282", "datetime": "2017-06-13 03:13:21", "author": "@vksbhandary", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874582799285600256": {"followers": "6,422", "datetime": "2017-06-13 11:02:42", "author": "@efernandez", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876976522669694977": {"followers": "7,527", "datetime": "2017-06-20 01:34:31", "author": "@u_akihiro", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874928100324376576": {"followers": "44", "datetime": "2017-06-14 09:54:49", "author": "@CharaBentoPapa", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "1021422695324344325": {"followers": "611", "datetime": "2018-07-23 15:52:02", "author": "@lll_anna_lll", "content_summary": "\u6b21\u8aad\u3080 Xiong et al., Dynamic Coattention Networks For Question Answering, https://t.co/OVnGRjNuTD Xiong et al., DCN+: Mixed Objective and Deep Residual Coattention for Question Answering, https://t.co/2WJYXxQ75s Vaswani et al., Attention is All You Need, http"}, "1211603432559325185": {"followers": "222", "datetime": "2019-12-30 11:02:23", "author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - fairseq - 6,575 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "876113256041066496": {"followers": "1,587", "datetime": "2017-06-17 16:24:12", "author": "@edublancas", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "1117067687455334401": {"followers": "1,191", "datetime": "2019-04-13 14:11:05", "author": "@windx0303", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "875312472030408705": {"followers": "163,852", "datetime": "2017-06-15 11:22:10", "author": "@ceobillionaire", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "976792606117154816": {"followers": "216", "datetime": "2018-03-22 12:07:59", "author": "@hokiegeek2", "content_summary": "Interesting paper. Do attention mechanisms themselves obviate the need for RNNs in some cases https://t.co/QhbnQeLwuf"}, "948955192224923648": {"followers": "6,486", "datetime": "2018-01-04 16:32:03", "author": "@jingbay", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874823146334846976": {"followers": "255", "datetime": "2017-06-14 02:57:46", "author": "@dennisobrien", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874522969342652417": {"followers": "2,338", "datetime": "2017-06-13 07:04:58", "author": "@SashaVNovikov", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874649786309365766": {"followers": "311", "datetime": "2017-06-13 15:28:53", "author": "@eraldoluis", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874878694262792192": {"followers": "298", "datetime": "2017-06-14 06:38:29", "author": "@Pranjal_Yadav", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "875093744885878785": {"followers": "3", "datetime": "2017-06-14 20:53:01", "author": "@JieHe8", "content_summary": "RT @iamtrask: in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://\u2026"}, "877129968961978368": {"followers": "131", "datetime": "2017-06-20 11:44:15", "author": "@HogwartsScience", "content_summary": "Harry Potter And The Attention Is All You Need https://t.co/BeM3UTl5XC #harrypotter"}, "874633190991953921": {"followers": "1,135", "datetime": "2017-06-13 14:22:57", "author": "@zkajdan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874905565394337793": {"followers": "43", "datetime": "2017-06-14 08:25:16", "author": "@hot_pinecone", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1116924410420551680": {"followers": "5,219", "datetime": "2019-04-13 04:41:45", "author": "@omarsar0", "content_summary": "What timing! We were just discussing this topic in our lab. We really wanted to understand the purpose of the positional encoding. @aidahalitaj can you please share with lab-mates? Hope it's helpful."}, "948822277059051520": {"followers": "674", "datetime": "2018-01-04 07:43:53", "author": "@hidemotoNakada", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874569178279292928": {"followers": "54", "datetime": "2017-06-13 10:08:35", "author": "@vo_d_p", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874834700686299138": {"followers": "317", "datetime": "2017-06-14 03:43:40", "author": "@Homhospital", "content_summary": "Amazing you new insight. https://t.co/vBgKocOAhL"}, "1212836380558032896": {"followers": "1,838", "datetime": "2020-01-02 20:41:41", "author": "@gringene_bio", "content_summary": "RT @wzuidema: Shock 9 (2017): Know all about LSTMs now? Good, now forget all of it, as Attention Is All You Need (i.e. the Transformer). [\u2026"}, "874474298332684288": {"followers": "876", "datetime": "2017-06-13 03:51:34", "author": "@nanTosaka2", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165748456549474304": {"followers": "8,183", "datetime": "2019-08-25 22:11:05", "author": "@neurobongo", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "919120515914129408": {"followers": "2,861", "datetime": "2017-10-14 08:39:42", "author": "@Tarpon_red2", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "966072445798141952": {"followers": "208", "datetime": "2018-02-20 22:09:54", "author": "@AdrianoCarmezim", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1167395773081948162": {"followers": "163,852", "datetime": "2019-08-30 11:16:56", "author": "@ceobillionaire", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874811114411900928": {"followers": "27", "datetime": "2017-06-14 02:09:57", "author": "@mahesh21aug", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165983849412608000": {"followers": "162", "datetime": "2019-08-26 13:46:27", "author": "@AjayyXc", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1003535949203030017": {"followers": "647", "datetime": "2018-06-04 07:16:29", "author": "@iansimon", "content_summary": "@robertwiblin @sentientist Somehow unrelated: https://t.co/UKxsB7k8LC"}, "874688380792995840": {"followers": "64", "datetime": "2017-06-13 18:02:15", "author": "@supriya_pandhre", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "876609299023376384": {"followers": "471", "datetime": "2017-06-19 01:15:18", "author": "@daytb_twy", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874559624279715840": {"followers": "258", "datetime": "2017-06-13 09:30:37", "author": "@arezae", "content_summary": "RT @deliprao: The new Transformer Network looks deceptively simple for producing SOTA in MT. https://t.co/7otNrERtSH https://t.co/mkGqPin2\u2026"}, "1166016451855228929": {"followers": "9", "datetime": "2019-08-26 15:56:00", "author": "@FroZone_FR", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876948516291194881": {"followers": "163,852", "datetime": "2017-06-19 23:43:13", "author": "@ceobillionaire", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874601879224946691": {"followers": "184", "datetime": "2017-06-13 12:18:31", "author": "@david__kell", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875073563836825601": {"followers": "226", "datetime": "2017-06-14 19:32:50", "author": "@sagarpath", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874668797852098560": {"followers": "82", "datetime": "2017-06-13 16:44:26", "author": "@nsdgpn", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165924231294652416": {"followers": "34", "datetime": "2019-08-26 09:49:33", "author": "@MatRazor", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "877329529030627329": {"followers": "215", "datetime": "2017-06-21 00:57:14", "author": "@rsinghbiz", "content_summary": "RT @v_vashishta: #Google #DeepLearning - Attention Is All You Need https://t.co/Bjxj2dYXxw #MachineLearning"}, "874442883415379968": {"followers": "189", "datetime": "2017-06-13 01:46:44", "author": "@shahrukh_athar", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "874594213085810688": {"followers": "4,056", "datetime": "2017-06-13 11:48:04", "author": "@scottbot_17", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "876824338074173440": {"followers": "631", "datetime": "2017-06-19 15:29:47", "author": "@Alisher_ai_ML", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "948859989166182400": {"followers": "1,448", "datetime": "2018-01-04 10:13:45", "author": "@dicekicker", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "875101195433525248": {"followers": "51", "datetime": "2017-06-14 21:22:38", "author": "@DataScienceToni", "content_summary": "Attention-based translation model without RNNs & CNNs outperforming both: https://t.co/OLmgQkST3z"}, "874451027935801344": {"followers": "155", "datetime": "2017-06-13 02:19:06", "author": "@fitzyfitzyfitzy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966124579017039873": {"followers": "318", "datetime": "2018-02-21 01:37:04", "author": "@nelslindahl", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876801009854689280": {"followers": "56,127", "datetime": "2017-06-19 13:57:05", "author": "@IntelligenceTV", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1117022711769894913": {"followers": "328", "datetime": "2019-04-13 11:12:22", "author": "@JJavierMoralo", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874585735755698176": {"followers": "164", "datetime": "2017-06-13 11:14:23", "author": "@ZachBessinger", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874518360398200834": {"followers": "848", "datetime": "2017-06-13 06:46:39", "author": "@TomLePaine", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874588960021815297": {"followers": "268", "datetime": "2017-06-13 11:27:11", "author": "@yukia_lucy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875736032452456448": {"followers": "1,417", "datetime": "2017-06-16 15:25:15", "author": "@seanmylaw", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "877219618649145345": {"followers": "1", "datetime": "2017-06-20 17:40:29", "author": "@dphilippow", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874476301947617280": {"followers": "1,026", "datetime": "2017-06-13 03:59:32", "author": "@muku_act", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876769891461746688": {"followers": "936", "datetime": "2017-06-19 11:53:26", "author": "@theyblinked", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1171505207630413825": {"followers": "2", "datetime": "2019-09-10 19:26:21", "author": "@CchangJ", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026"}, "927526983151603712": {"followers": "2,058", "datetime": "2017-11-06 13:24:00", "author": "@sei_shinagawa", "content_summary": "RT @mickey24: \u8ad6\u6587 \"Attention Is All You Need\": Attention\u30d9\u30fc\u30b9\u306e\u5358\u7d14\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u300cTransformer\u300d\u3067\u7ffb\u8a33\u30bf\u30b9\u30af\u306e\u7cbe\u5ea6\u5411\u4e0a\u30fb\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3092\u5b9f\u73fe\u3002RNN\u3084CNN\u306f\u4f7f\u308f\u306a\u3044\u3002\u8457\u8005\u306fGoogler https://t.\u2026"}, "874811616960950272": {"followers": "179", "datetime": "2017-06-14 02:11:57", "author": "@rajneeshagrawal", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874598421335879681": {"followers": "194", "datetime": "2017-06-13 12:04:47", "author": "@anupradhan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874506388499595266": {"followers": "235", "datetime": "2017-06-13 05:59:05", "author": "@swapanj162", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165902220317143043": {"followers": "837", "datetime": "2019-08-26 08:22:05", "author": "@MarklDouthwaite", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874510435872948225": {"followers": "291", "datetime": "2017-06-13 06:15:10", "author": "@bistaumanga", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874768409057267712": {"followers": "230", "datetime": "2017-06-13 23:20:15", "author": "@kimbustion", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "876783737933094913": {"followers": "574", "datetime": "2017-06-19 12:48:27", "author": "@LucaAmb", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875674918704361472": {"followers": "78", "datetime": "2017-06-16 11:22:24", "author": "@alirezaasvadi", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874802118590443520": {"followers": "177,517", "datetime": "2017-06-14 01:34:12", "author": "@Montreal_AI", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874788617025069056": {"followers": "1,293", "datetime": "2017-06-14 00:40:33", "author": "@Landa23", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874504119754432512": {"followers": "9,375", "datetime": "2017-06-13 05:50:04", "author": "@odashi_t", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874679293531389952": {"followers": "844", "datetime": "2017-06-13 17:26:08", "author": "@akuyudanta", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876702300529733632": {"followers": "838", "datetime": "2017-06-19 07:24:51", "author": "@joostbastings", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "877139729262759936": {"followers": "278", "datetime": "2017-06-20 12:23:02", "author": "@spurNDlover", "content_summary": "RT @dataandme: So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #mach\u2026"}, "1165797982668767232": {"followers": "309", "datetime": "2019-08-26 01:27:53", "author": "@Daniel_J_Im", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874818755616018432": {"followers": "861", "datetime": "2017-06-14 02:40:19", "author": "@ryf_feed", "content_summary": "Attention is All You Need by Google Brain https://t.co/Gw9Y8ROLRO"}, "874664028529844226": {"followers": "1,418", "datetime": "2017-06-13 16:25:29", "author": "@sguada", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874591546166915072": {"followers": "1,566", "datetime": "2017-06-13 11:37:28", "author": "@IFB_Bioinfo", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874642567521894400": {"followers": "9", "datetime": "2017-06-13 15:00:12", "author": "@ShGupta_", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "1165982038702227457": {"followers": "783", "datetime": "2019-08-26 13:39:15", "author": "@muktabh", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874473719292239873": {"followers": "437", "datetime": "2017-06-13 03:49:16", "author": "@grousselle", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876908104994045952": {"followers": "1,305", "datetime": "2017-06-19 21:02:39", "author": "@m4rkmc", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "950518309564203009": {"followers": "19", "datetime": "2018-01-09 00:03:19", "author": "@chatnoir0522", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "876771230292090881": {"followers": "108", "datetime": "2017-06-19 11:58:45", "author": "@templarpt", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1166470965741666307": {"followers": "66", "datetime": "2019-08-27 22:02:04", "author": "@diegovogeid", "content_summary": "@facebookai @facebookai , you should read this tweet :) https://t.co/Sv58eslHZx"}, "920629091329458176": {"followers": "10", "datetime": "2017-10-18 12:34:14", "author": "@seishin_dev", "content_summary": "Attention Is All You Need https://t.co/G8MRvtO1Lt \u7ffb\u8a33\u306e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066Attention\u3060\u3051\u4f7f\u3048\u3070\u3044\u3044\u3058\u3083\u3093\u3068\u3044\u3046\u767a\u60f3\u3002 \u5b66\u7fd2\u901f\u5ea6\u306e\u5411\u4e0a\uff08RNN\u3092\u4f7f\u308f\u305a\u306b\u4e26\u5217\u8a08\u7b97\u3092\u53ef\u80fd\u306b\u3057\u305f\uff09\u3068\u7cbe\u5ea6\u5411\u4e0a\uff08\u7ffb\u8a33\u30bf\u30b9\u30af\u306b\u9069\u3057\u3066\u3044\u308b\uff09\u304c\u3044\u3044\u3053\u3068\u3002"}, "1116990180638855168": {"followers": "1,838", "datetime": "2019-04-13 09:03:06", "author": "@gringene_bio", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1181734074408300544": {"followers": "1,984", "datetime": "2019-10-09 00:52:13", "author": "@hmCuesta", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "948818800962551810": {"followers": "18,250", "datetime": "2018-01-04 07:30:05", "author": "@hillbig", "content_summary": "\u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://t.co/0g29WJkdtt https://t.co/9wZxjj7Lug"}, "874587844395712512": {"followers": "1,501", "datetime": "2017-06-13 11:22:45", "author": "@DrZeeshanZia", "content_summary": "RT @cto_movidius: Busy week in Deep Networks: attention networks https://t.co/gI4a2k7n61, SELU https://t.co/utp0GPYTB6 & CortexNet https://\u2026"}, "876074882236375042": {"followers": "77", "datetime": "2017-06-17 13:51:43", "author": "@pangeamt", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "874637118945062912": {"followers": "107", "datetime": "2017-06-13 14:38:33", "author": "@marionyk", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875801518716465152": {"followers": "56,127", "datetime": "2017-06-16 19:45:28", "author": "@IntelligenceTV", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165707341658038272": {"followers": "849", "datetime": "2019-08-25 19:27:42", "author": "@JamesWaititu", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "961175694372888577": {"followers": "293", "datetime": "2018-02-07 09:51:57", "author": "@vingovan", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "876801663448887296": {"followers": "33,955", "datetime": "2017-06-19 13:59:41", "author": "@aiprgirl", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874478344221319170": {"followers": "405", "datetime": "2017-06-13 04:07:38", "author": "@DSPonFPGA", "content_summary": "apparently it's not love ... https://t.co/hcJmnEbhYU"}, "1165797549686628352": {"followers": "389", "datetime": "2019-08-26 01:26:09", "author": "@marceloprates_", "content_summary": "nesse caso foi vacilo porque o artigo em quest\u00e3o \u00e9 importante, mas demonstra o qu\u00e3o dif\u00edcil \u00e9 se manter atualizado com o estado-da-arte enlouquecedor de deep learning"}, "874559633121300481": {"followers": "145", "datetime": "2017-06-13 09:30:39", "author": "@esvhd", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874689844990980096": {"followers": "64", "datetime": "2017-06-13 18:08:04", "author": "@supriya_pandhre", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874626654622101505": {"followers": "521", "datetime": "2017-06-13 13:56:58", "author": "@metasemantic", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874586327538446336": {"followers": "103", "datetime": "2017-06-13 11:16:44", "author": "@jongen87", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "972249612382851072": {"followers": "9,670", "datetime": "2018-03-09 23:15:45", "author": "@lintool", "content_summary": "You know that game when you only talk using movie or song titles? I wonder if you can do so using only arXiv paper titles. I'll start: https://t.co/6cV0VnwMhx"}, "1166119231605415939": {"followers": "545", "datetime": "2019-08-26 22:44:24", "author": "@nicolarohrseitz", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165719144693731331": {"followers": "82", "datetime": "2019-08-25 20:14:36", "author": "@t0d3sf4ll", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874565631433101312": {"followers": "21", "datetime": "2017-06-13 09:54:29", "author": "@ChanduZkY", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874546999160262656": {"followers": "210", "datetime": "2017-06-13 08:40:27", "author": "@MihaelFeldman", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "1161593364115886080": {"followers": "656", "datetime": "2019-08-14 11:00:14", "author": "@273sn", "content_summary": "Attention \u3084\u3093\u3051\uff01(\u8208\u596e\u3057\u3066\u3044\u308b\u3002\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306f\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u3069\u3046\u6d3b\u7528\u3059\u308b\u304b\u306f\u6ce8\u610f\u529b\uff01\u3064\u307e\u308a\u3001\u8a8d\u8b58\u3067\u3042\u308a\"re\"-cognition \u3063\u3066\u3053\u3068\u306f\u3001\u4e8c\u6b21\u5143\u306a\u3093\u3060\u3088\u3002\u4e8c\u968e\u5fae\u5206\u3067\u52a0\u901f\u5ea6\u306e\u69d8\u306b\u6b21\u5143\u3092\u5236\u3059\u308b\u4e8b\u304c\u91cd\u8981\u306a\u3093\u3060\u3002\u30a2\u30a1\u30a1 ADHD orz orz) https://t.co/zHfdLh7dC0"}, "874544012698554368": {"followers": "258", "datetime": "2017-06-13 08:28:35", "author": "@arezae", "content_summary": "[1706.03762] Attention Is All You Need - https://t.co/hW7hwAefTH https://t.co/6XBYOMKYDn via @nuzzel thanks @Miles_Brundage"}, "1095892525569593345": {"followers": "200", "datetime": "2019-02-14 03:48:33", "author": "@overleo", "content_summary": "Transformer - Attention Is All You Need \u306e\u6982\u8981\u304c\u7406\u89e3\u3067\u304d\u308b\u30ea\u30f3\u30af - higepon blog: Attention Is All You Need https://t.co/ciRa83MRpO \u3067\u63d0\u6848\u3055\u308c\u305f Transformer \u4ee5\u964d\u306f self-attention \u304c\u4e3b\u6d41\u306a\u5370\u8c61\u3067\u3059\u3002\u5148\u65e5\u304a\u304a\u304d\u306a\u8a71\u984c\u306b\u306a\u3063\u305f BERT \u3082 Transformer\u2026 https://t.co/5E1pVokfQN [ml]"}, "874490956812353536": {"followers": "1,288", "datetime": "2017-06-13 04:57:46", "author": "@heghbalz", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874472293849677824": {"followers": "1,351", "datetime": "2017-06-13 03:43:36", "author": "@datamusing", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966012341660868608": {"followers": "1,061", "datetime": "2018-02-20 18:11:04", "author": "@mvaldenegro", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1167771631466504193": {"followers": "177,517", "datetime": "2019-08-31 12:10:27", "author": "@Montreal_AI", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874688817852162049": {"followers": "820", "datetime": "2017-06-13 18:03:59", "author": "@ramansrivastav", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165749714202980353": {"followers": "117", "datetime": "2019-08-25 22:16:05", "author": "@AnandaBasak", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "877144738524254208": {"followers": "278", "datetime": "2017-06-20 12:42:56", "author": "@HermanKamper", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874860020860686337": {"followers": "4,891", "datetime": "2017-06-14 05:24:17", "author": "@IgorCarron", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1210153713542799360": {"followers": "222", "datetime": "2019-12-26 11:01:43", "author": "@PapersTrending", "content_summary": "[2/10] \ud83d\udcc8 - Attention Is All You Need - 38 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "874542004843159552": {"followers": "2,663", "datetime": "2017-06-13 08:20:36", "author": "@sigitpurnomo", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "876905114136494081": {"followers": "407", "datetime": "2017-06-19 20:50:45", "author": "@_yaowu_", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1028693500290715649": {"followers": "16,522", "datetime": "2018-08-12 17:23:37", "author": "@tkasasagi", "content_summary": "Attention Is All You Need https://t.co/Vuo9FpWexG"}, "966111921152733184": {"followers": "113", "datetime": "2018-02-21 00:46:46", "author": "@tranlaman", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165869025529868288": {"followers": "42", "datetime": "2019-08-26 06:10:11", "author": "@MishakinSergey", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1102707440775249923": {"followers": "2", "datetime": "2019-03-04 23:08:35", "author": "@LetMePetTheDog", "content_summary": "Day 0: Revisiting Transformer architecture again, on the way to understanding the unreasonable effectiveness of BERT [1] https://t.co/7OrXZXR3vU [2] https://t.co/8oHYDf55fU #WholeSemesterInML"}, "874517624578744321": {"followers": "69", "datetime": "2017-06-13 06:43:44", "author": "@ku21fan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "919151241913032704": {"followers": "837", "datetime": "2017-10-14 10:41:47", "author": "@m_hayase256", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874927077421699075": {"followers": "458", "datetime": "2017-06-14 09:50:45", "author": "@kouki_outstand", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "875260368708395009": {"followers": "3,789", "datetime": "2017-06-15 07:55:08", "author": "@AndrewBonello", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876713587930087424": {"followers": "430", "datetime": "2017-06-19 08:09:42", "author": "@recurseparadox", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1165916786639822848": {"followers": "47", "datetime": "2019-08-26 09:19:58", "author": "@jokers_LieMind", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966010622159630336": {"followers": "1", "datetime": "2018-02-20 18:04:14", "author": "@chengadian", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "948819296146268161": {"followers": "433", "datetime": "2018-01-04 07:32:03", "author": "@hs_heddy", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874655938736779265": {"followers": "41", "datetime": "2017-06-13 15:53:20", "author": "@Abaybektursun", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877204748293636096": {"followers": "63", "datetime": "2017-06-20 16:41:24", "author": "@fjnyan__", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "1117097376618033152": {"followers": "1,369", "datetime": "2019-04-13 16:09:03", "author": "@sara_hlt", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1185394747428372481": {"followers": "93", "datetime": "2019-10-19 03:18:26", "author": "@ChaircoChen", "content_summary": "attention is all you need, \u771f\u96e3\u61c2 Q, K, V. qq https://t.co/SlW5VUJX3Q"}, "919146669207273472": {"followers": "6,227", "datetime": "2017-10-14 10:23:37", "author": "@waseda_fablab", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "876958644348358660": {"followers": "309", "datetime": "2017-06-20 00:23:28", "author": "@Daniel_J_Im", "content_summary": "Good Example of proof by self-contradiction. lol https://t.co/gwv53Erbbb"}, "876643743520051200": {"followers": "951", "datetime": "2017-06-19 03:32:10", "author": "@eiennohito", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876826454859948038": {"followers": "381", "datetime": "2017-06-19 15:38:12", "author": "@riceasphait", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874922723327307777": {"followers": "11,476", "datetime": "2017-06-14 09:33:27", "author": "@icoxfog417", "content_summary": "RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089\u540c\u69d8\u306b\u51e6\u7406\u3057\u51fa\u529b\u3057\u3066\u3044\u308b https://t.co/KqsDnLTs2c"}, "882790139772018688": {"followers": "1,052", "datetime": "2017-07-06 02:35:45", "author": "@ngutten", "content_summary": "@mouthofmorrison Based on https://t.co/c80SoBYTE7, which is like a content-based search (each target pixel generates a query, each source a key, dot product)"}, "948886471766818816": {"followers": "216", "datetime": "2018-01-04 11:58:58", "author": "@KSKSKSKS2", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "1116964874842771456": {"followers": "181", "datetime": "2019-04-13 07:22:33", "author": "@shubham_stark", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "875461389099311104": {"followers": "1,720", "datetime": "2017-06-15 21:13:55", "author": "@mooopan", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "874675799571419136": {"followers": "7,376", "datetime": "2017-06-13 17:12:15", "author": "@ericflo", "content_summary": "@olshansky https://t.co/wAYCYD7amg"}, "1065976385334763520": {"followers": "2,481", "datetime": "2018-11-23 14:32:29", "author": "@shion_honda", "content_summary": "Transformer [Vaswani+, 2017, NIPS] Self-Attention\u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u7573\u307f\u8fbc\u307f\u3084\u518d\u5e30\u7121\u3057\u306eNN\u3067\u3082\u7ffb\u8a33\u30bf\u30b9\u30af\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002 Transformer\u306f\u9577\u3044\u6642\u7cfb\u5217\u3092\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u4e0a\u306b\u3001\u4e26\u5217\u5316\u306b\u3088\u308a\u5b66\u7fd2\u3092\u9ad8\u901f\u5316\u3067\u304d\u308b\u3002\u307e\u305f\u3001\u753b\u50cf\u306a\u3069\u4ed6\u306e\u9818\u57df\u3067\u3082\u5fdc\u7528\u53ef\u80fd\u3002 https://t.co/bP4zZrExfF #NowReading https://t.co/rKzbAtx8dE"}, "874892625509330944": {"followers": "954", "datetime": "2017-06-14 07:33:51", "author": "@Roldiesel", "content_summary": "RT @graphific: Attention Is All You Need from @GoogleBrain. No RNNs or CNNs, only Attention = fast training & SOTA on WMT'14 https://t.co/Y\u2026"}, "874638106179358722": {"followers": "14,852", "datetime": "2017-06-13 14:42:29", "author": "@twiecki", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "905040535244513282": {"followers": "4,019", "datetime": "2017-09-05 12:10:53", "author": "@ballforest", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "877700587403354112": {"followers": "43", "datetime": "2017-06-22 01:31:41", "author": "@NonbeSusumu", "content_summary": "RT @kawano_hiroki: Google \u304b\u3089\u65b0\u3057\u3044\u8ad6\u6587\u304c\u3067\u305f\u3002Attention Is All You Need. RNN\u3092\u4f7f\u308f\u305a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3060\u3051\u4f7f\u3063\u3066\u3088\u308a\u9ad8\u901f\u9ad8\u6027\u80fd\u306aNMT\u304c\u3067\u304d\u308b\u3089\u3057\u3044\u3002\u65b0\u3057\u3044\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u540d\u306f \"the Transformer\" https\u2026"}, "874923093680168960": {"followers": "3,862", "datetime": "2017-06-14 09:34:55", "author": "@SUIGADOU", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "874453636352430080": {"followers": "1,627", "datetime": "2017-06-13 02:29:28", "author": "@chris_brockett", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874730438882770944": {"followers": "335", "datetime": "2017-06-13 20:49:22", "author": "@kobi78", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165919665329188866": {"followers": "117", "datetime": "2019-08-26 09:31:24", "author": "@pabaldonedo", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874765581785985024": {"followers": "407", "datetime": "2017-06-13 23:09:01", "author": "@nishantiam", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966029977220239362": {"followers": "145", "datetime": "2018-02-20 19:21:09", "author": "@esvhd", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165882955199328256": {"followers": "118", "datetime": "2019-08-26 07:05:32", "author": "@matthewopala", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "877048554711162880": {"followers": "6", "datetime": "2017-06-20 06:20:44", "author": "@YuBillDian", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "966010594448101377": {"followers": "112", "datetime": "2018-02-20 18:04:07", "author": "@MSripadarao", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874445919952846848": {"followers": "3,291", "datetime": "2017-06-13 01:58:48", "author": "@sonicair", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874754228748771328": {"followers": "1,058", "datetime": "2017-06-13 22:23:54", "author": "@GillesMoyse", "content_summary": "@stjaco After RNN and CNN for #NLP, the Transformer model seems to outperform them for translation tasks...https://t.co/81z8ZWUVQo"}, "1165941391689580547": {"followers": "56", "datetime": "2019-08-26 10:57:44", "author": "@markniesta", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1168543764626853888": {"followers": "2,671", "datetime": "2019-09-02 15:18:38", "author": "@ayirpelle", "content_summary": "RT @erickherring: @MelMitchell1 Not an NLP person, per se, but the paper that introduced the model is https://t.co/pt4meJFPIJ - maybe one o\u2026"}, "874665270786314240": {"followers": "758", "datetime": "2017-06-13 16:30:25", "author": "@shashank27392", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "876779506651942914": {"followers": "2,290", "datetime": "2017-06-19 12:31:38", "author": "@fronori", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874629416831156227": {"followers": "2,671", "datetime": "2017-06-13 14:07:57", "author": "@ayirpelle", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874524632652681218": {"followers": "771", "datetime": "2017-06-13 07:11:34", "author": "@arxivml", "content_summary": "\"Attention Is All You Need\", Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\uff0e Gome\u2026 https://t.co/4vRyMuud33"}, "876768865539178502": {"followers": "546", "datetime": "2017-06-19 11:49:21", "author": "@vdv7", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1198823192661516293": {"followers": "102", "datetime": "2019-11-25 04:38:16", "author": "@quasicoh", "content_summary": "Can someone explain to me the intuition behind the absolutely insane looking \"positional encoding\" that's used in the Transformer model (from \"Attention Is All You Need\" https://t.co/l78LzLfhcQ)? https://t.co/QhFzkIiOh9"}, "1217526381812187136": {"followers": "5,695", "datetime": "2020-01-15 19:18:04", "author": "@aronchick", "content_summary": "Man, I must have gone back and read \"Attention is all you need\" at least 10x - it's so cool when you can read a seminal paper that transforms (no pun intended) and entire industry https://t.co/oWtBkHkaE8"}, "874598560037261313": {"followers": "1,868", "datetime": "2017-06-13 12:05:20", "author": "@junpenglao", "content_summary": "Something I always want to know: what tool do the @GoogleBrain folks use for those crispy acyclic graphs? https://t.co/zCfvAI4J2N"}, "874499293834563585": {"followers": "1,087", "datetime": "2017-06-13 05:30:53", "author": "@ak1010", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "966196242496786432": {"followers": "198", "datetime": "2018-02-21 06:21:49", "author": "@zephyrzilla", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874588588351995904": {"followers": "395", "datetime": "2017-06-13 11:25:43", "author": "@salahuddin517", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "874981917183156225": {"followers": "217", "datetime": "2017-06-14 13:28:40", "author": "@diegogaleano05", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874604134040784896": {"followers": "294", "datetime": "2017-06-13 12:27:29", "author": "@yen_chen_lin", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "877226406161268736": {"followers": "2,282", "datetime": "2017-06-20 18:07:27", "author": "@_Desmoden", "content_summary": "Huh... can't say I fully grasp it yet... but WoW \"Attention is all you need\" https://t.co/NF3WMbgNne thanks to @_brohrer_ for posting!"}, "874589286061948928": {"followers": "1", "datetime": "2017-06-13 11:28:29", "author": "@r5123", "content_summary": "Attention Is All You Need https://t.co/7GjuziTA3x"}, "881639122565308416": {"followers": "340", "datetime": "2017-07-02 22:22:01", "author": "@Ajay_Talati", "content_summary": "@aeronlaffere OK, if your interested in \"somehow\" trying to apply techniques from language modelling, to raw audio and EEG, maybe https://t.co/CaerYT5VsJ"}, "874658679525486592": {"followers": "261", "datetime": "2017-06-13 16:04:14", "author": "@GuptaRajat033", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "877582130288578560": {"followers": "3,500", "datetime": "2017-06-21 17:40:59", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudrC28"}, "948775705340411904": {"followers": "934", "datetime": "2018-01-04 04:38:50", "author": "@4dri4nG4rrigos", "content_summary": "please Google Guys! Slow down with this network architecture you call \"Attention is All You Need\". @elonmusk he has not yet achieved \"Noah's ark\"! https://t.co/KTKKTPWUur #Pytorch https://t.co/QVy7zTVFu6 https://t.co/h6NtqgEMwm"}, "874904727145861120": {"followers": "47", "datetime": "2017-06-14 08:21:56", "author": "@xuechen1994", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1218183805586022400": {"followers": "192", "datetime": "2020-01-17 14:50:26", "author": "@comcomodel", "content_summary": "Music & AI plus A Geometric Perspective on RL \u26a0\ufe0fAttention Is All You Need https://t.co/EHmKtIbbKf \ud83d\udce3Unreasonable Effectiveness of RNN @karpathy \ud83c\udfb6Transformer @huangcza @iansimon @notwaldorf @GoogleMagenta \ud83d\udecb\ufe0fPablo @pcastr \ud83c\udf99\ufe0fSam @samcharrington @twimlai ht"}, "874714573751767040": {"followers": "243", "datetime": "2017-06-13 19:46:20", "author": "@The1Fundamental", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874765286842519553": {"followers": "443", "datetime": "2017-06-13 23:07:51", "author": "@ivenzor", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "874551937236180992": {"followers": "178", "datetime": "2017-06-13 09:00:04", "author": "@namhoonlee09", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1241083386204020736": {"followers": "71", "datetime": "2020-03-20 19:25:12", "author": "@2abstract4me", "content_summary": "you are right vasvani et al. all i need is attention. thanks for proving it with maths https://t.co/ZkndPY8deg"}, "1014019266956447744": {"followers": "1,418", "datetime": "2018-07-03 05:33:27", "author": "@johnolafenwa", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "877149646078005248": {"followers": "395", "datetime": "2017-06-20 13:02:26", "author": "@qiming82", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876916435523686400": {"followers": "754", "datetime": "2017-06-19 21:35:45", "author": "@suneelmarthi", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "875732280777551872": {"followers": "159,742", "datetime": "2017-06-16 15:10:20", "author": "@Montreal_IA", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874729184915931136": {"followers": "116", "datetime": "2017-06-13 20:44:24", "author": "@Discover_Matt", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874597739631431681": {"followers": "2,152", "datetime": "2017-06-13 12:02:05", "author": "@ultimape", "content_summary": "State of the art looks a heck of a lot like using @ViRAms signaling https://t.co/IhwIiWBCXM to generate paths from one language to another. https://t.co/d8fNnPgjCh"}, "876770650806919168": {"followers": "583", "datetime": "2017-06-19 11:56:27", "author": "@_ritviksingh", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874719221313413122": {"followers": "27", "datetime": "2017-06-13 20:04:48", "author": "@JennyLCopara", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "966105824681750528": {"followers": "159,742", "datetime": "2018-02-21 00:22:32", "author": "@Montreal_IA", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "899650995646607361": {"followers": "4,588", "datetime": "2017-08-21 15:14:46", "author": "@justinpickard", "content_summary": "RT @npseaver: Machine learning folks: Can anyone help me grok the current usage of \"attention\" in neural net research like this?: https://t\u2026"}, "1212765644397121536": {"followers": "483", "datetime": "2020-01-02 16:00:36", "author": "@felbalazard", "content_summary": "RT @wzuidema: Shock 9 (2017): Know all about LSTMs now? Good, now forget all of it, as Attention Is All You Need (i.e. the Transformer). [\u2026"}, "876973966681751552": {"followers": "725", "datetime": "2017-06-20 01:24:21", "author": "@yamo_o", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876207833834016768": {"followers": "3,957", "datetime": "2017-06-17 22:40:01", "author": "@Pernelle3", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876729696167247873": {"followers": "375", "datetime": "2017-06-19 09:13:43", "author": "@kano_sawa", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "875132428410650624": {"followers": "100", "datetime": "2017-06-14 23:26:44", "author": "@echoes2099", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877684201356615680": {"followers": "4,945", "datetime": "2017-06-22 00:26:34", "author": "@kanair_jp", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876799640443768834": {"followers": "936", "datetime": "2017-06-19 13:51:39", "author": "@theyblinked", "content_summary": "Transformer: ML attention architecture replacing recurrent and convolutional mapping with intention. https://t.co/tgHg8ce70X @GoogleBrain"}, "874735422546427905": {"followers": "170", "datetime": "2017-06-13 21:09:11", "author": "@harry_caufield", "content_summary": "Digging in to \"Attention Is All You Need\" this afternoon - https://t.co/Lq6kzNoNIG"}, "874911374866087936": {"followers": "780", "datetime": "2017-06-14 08:48:21", "author": "@rebekahwegener", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874500932435943424": {"followers": "1,918", "datetime": "2017-06-13 05:37:24", "author": "@DocXavi", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874781277538455552": {"followers": "317", "datetime": "2017-06-14 00:11:23", "author": "@chrisbot_js", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1165697312401895424": {"followers": "256", "datetime": "2019-08-25 18:47:51", "author": "@jcchinhui", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874632237282611200": {"followers": "404", "datetime": "2017-06-13 14:19:09", "author": "@wubr2000", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1038715889225367552": {"followers": "2,960", "datetime": "2018-09-09 09:09:01", "author": "@jittat", "content_summary": "RT @kobkrit: Attention is all you need. https://t.co/q7tvjCR5zk https://t.co/q7tvjCR5zk"}, "1117051996224282625": {"followers": "66", "datetime": "2019-04-13 13:08:44", "author": "@diegovogeid", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "883087099678924800": {"followers": "205", "datetime": "2017-07-06 22:15:46", "author": "@featurexai", "content_summary": "Is #attention all you need? Finding out at tonight's #FeatureX #MachineLearning seminar. https://t.co/C9E6B1Qsk5 https://t.co/oK4XpgtUHD"}, "874697353894277120": {"followers": "61", "datetime": "2017-06-13 18:37:54", "author": "@albelwu", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "966032984041144320": {"followers": "845", "datetime": "2018-02-20 19:33:06", "author": "@ArkadiuszKula", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874615877102034945": {"followers": "161", "datetime": "2017-06-13 13:14:09", "author": "@farhanhubble", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "877536835286814720": {"followers": "3,500", "datetime": "2017-06-21 14:41:00", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "1166378941164208128": {"followers": "1", "datetime": "2019-08-27 15:56:24", "author": "@Pol09122455", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1070938115739574272": {"followers": "100", "datetime": "2018-12-07 07:08:38", "author": "@hyodo_net", "content_summary": "\u3053\u308c\u304b\u3002 QT [1706.03762] Attention Is All You Need https://t.co/ngpDXHLDWi"}, "874880142090350592": {"followers": "1,938", "datetime": "2017-06-14 06:44:15", "author": "@EldarSilver", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876774178971627521": {"followers": "1,564", "datetime": "2017-06-19 12:10:28", "author": "@migballesteros", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "899562388495540224": {"followers": "1,645", "datetime": "2017-08-21 09:22:41", "author": "@colun", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "876992488254316544": {"followers": "34", "datetime": "2017-06-20 02:37:57", "author": "@genelkim", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "899652178331095041": {"followers": "1,288", "datetime": "2017-08-21 15:19:28", "author": "@heghbalz", "content_summary": "RT @npseaver: Machine learning folks: Can anyone help me grok the current usage of \"attention\" in neural net research like this?: https://t\u2026"}, "874594621833306112": {"followers": "207", "datetime": "2017-06-13 11:49:41", "author": "@eram1205", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165967212584288257": {"followers": "604", "datetime": "2019-08-26 12:40:20", "author": "@rharang", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966008380950228994": {"followers": "562", "datetime": "2018-02-20 17:55:20", "author": "@wwwAIblog", "content_summary": "seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"attention is all you need\" paper (https://t.co/ZGhPh8CKHN) for MT. e.g., recently https://t.co/BckU2zj7gE, https://t.co/VY9B0CGwrm\u2026 h"}, "1064807388941828096": {"followers": "1,882", "datetime": "2018-11-20 09:07:19", "author": "@Uninet_ataria", "content_summary": "RT @xabi_soto: 5 Bestalde, hizkuntzaren prozesamenduan azkenaldian modan dagoen teknika sare neuronaletan oinarritzen da, zehazki Transfor\u2026"}, "1099077605058260993": {"followers": "265", "datetime": "2019-02-22 22:44:55", "author": "@DaniaL_KH", "content_summary": "RT @willkurt: Finally worked through \"Attention is all you need\". My recommended steps: - Skim the paper: https://t.co/iCNGDeJ62q - Step th\u2026"}, "876886001641115648": {"followers": "62,853", "datetime": "2017-06-19 19:34:49", "author": "@chrmanning", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1117247830643445760": {"followers": "66", "datetime": "2019-04-14 02:06:54", "author": "@khuongav", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "876639258693570560": {"followers": "634", "datetime": "2017-06-19 03:14:21", "author": "@toosaka_", "content_summary": "\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02 https://t.co/oq6vGPoZuH"}, "914474990547828736": {"followers": "681", "datetime": "2017-10-01 13:00:02", "author": "@denis_arnaud", "content_summary": "https://t.co/PpDzo3FxcX"}, "874494965056847873": {"followers": "395", "datetime": "2017-06-13 05:13:41", "author": "@linkoffate", "content_summary": "Attention Is All You Need https://t.co/datm5124sF WTF \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b \uc5b4\uc81c \ub098\uc628 \ub17c\ubb38\uc744 \uadf8\ub300\ub85c \uc539\uc5b4\ubc84\ub9ac\ub124 \uc774\uac70 \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b"}, "874452999661391872": {"followers": "1,646", "datetime": "2017-06-13 02:26:56", "author": "@dipanjand", "content_summary": "Check this out: https://t.co/N6hZyklj6G SOTA on several metrics using extremely simple models."}, "966581357194502145": {"followers": "3", "datetime": "2018-02-22 07:52:08", "author": "@r3dapple1", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "877601765872603136": {"followers": "12,188", "datetime": "2017-06-21 18:59:00", "author": "@tryolabs", "content_summary": "Attention Is All You Need https://t.co/8gikylgaqh"}, "1187380860950990853": {"followers": "1,186", "datetime": "2019-10-24 14:50:32", "author": "@CarlRioux", "content_summary": "[D] Layer Complexity of Recurrent NNs in the Transformer Paper: https://t.co/Nn3ZEKm2GA Table 1 of this paper says the layer complexity of self-attention NNs is N^2*d, which I understand.What I dont understand is the complexity of Recurrent NNs, which\u2026 htt"}, "1159808373052850177": {"followers": "237", "datetime": "2019-08-09 12:47:18", "author": "@fluck_5", "content_summary": "The new @PyTorch release includes a Transformer (https://t.co/nZHwLcAtqv) module by default. This is super cool and it's gonna be extremely handy... I have too many projects open, but I really fell like starting a new one right now. https://t.co/syzwuLOgJ"}, "944130444005539840": {"followers": "53", "datetime": "2017-12-22 09:00:13", "author": "@PoQaa_cs", "content_summary": "Attention Is All You Need https://t.co/sf2MJ0D9n0 (Popularity:20.0) #Natural_language_processing #Machine_Learning"}, "1092093969696215042": {"followers": "7,292", "datetime": "2019-02-03 16:14:27", "author": "@MLHispano", "content_summary": "\u00a1Ya lleg\u00f3 el d\u00eda de la lectura del paper semanal! \ud83d\uddde\ud83d\udc9a Attention Is All You Need: https://t.co/AaDE6V6vqY \u00bfOs anim\u00e1is a comentarlo?"}, "1165874624350961664": {"followers": "62", "datetime": "2019-08-26 06:32:26", "author": "@samhardyhey", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1121323167358668800": {"followers": "19", "datetime": "2019-04-25 08:00:50", "author": "@lixx3527", "content_summary": "great paper:https://t.co/b7Cq0PLNXj"}, "1117178158502023168": {"followers": "94", "datetime": "2019-04-13 21:30:03", "author": "@chauhraj", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874885134503358465": {"followers": "189", "datetime": "2017-06-14 07:04:05", "author": "@Lukeaia", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877161334625812480": {"followers": "120", "datetime": "2017-06-20 13:48:53", "author": "@abhijeetgulati", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876681741108264961": {"followers": "522", "datetime": "2017-06-19 06:03:09", "author": "@pijili", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "877224516493205504": {"followers": "220", "datetime": "2017-06-20 17:59:57", "author": "@haromn", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "944130439131815937": {"followers": "93", "datetime": "2017-12-22 09:00:12", "author": "@poqaa_ai", "content_summary": "Attention Is All You Need https://t.co/0btLvEs3rl (Popularity:20.0) #Natural_language_processing #Machine_Learning"}, "875439039922438145": {"followers": "44", "datetime": "2017-06-15 19:45:06", "author": "@RKopetz", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1098953521850974208": {"followers": "4,872", "datetime": "2019-02-22 14:31:51", "author": "@willkurt", "content_summary": "Finally worked through \"Attention is all you need\". My recommended steps: - Skim the paper: https://t.co/iCNGDeJ62q - Step through \"The Annotated Transformer\":https://t.co/TXZVExlXwW - Read \"The Illustrated Transformer\" to fill in gaps:https://t.co/GLsf2A5"}, "876086267804622848": {"followers": "177,517", "datetime": "2017-06-17 14:36:57", "author": "@Montreal_AI", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "966900563584598025": {"followers": "1", "datetime": "2018-02-23 05:00:33", "author": "@DVolobaev", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876645995634987008": {"followers": "3,467", "datetime": "2017-06-19 03:41:07", "author": "@_primenumber", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874444413514252288": {"followers": "166", "datetime": "2017-06-13 01:52:49", "author": "@dparedes28", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165697511740166144": {"followers": "1,411", "datetime": "2019-08-25 18:48:39", "author": "@RexDouglass", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1117040391507927040": {"followers": "101", "datetime": "2019-04-13 12:22:37", "author": "@tomivek", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1116909488999505920": {"followers": "117", "datetime": "2019-04-13 03:42:28", "author": "@hendyfergus", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874912602178060288": {"followers": "22", "datetime": "2017-06-14 08:53:14", "author": "@Tsingggg", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874884603093409792": {"followers": "68", "datetime": "2017-06-14 07:01:58", "author": "@howardzail", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1189753744956637184": {"followers": "464", "datetime": "2019-10-31 03:59:32", "author": "@pthenq1", "content_summary": "@asfilippini @vivichuleta Y aca esta el paper en la que presentaron en sociedad a los Transformers. Es una variante de RRNs https://t.co/vF4EtdwkJD"}, "874466644571115520": {"followers": "728", "datetime": "2017-06-13 03:21:09", "author": "@sarnthil", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877274781044719616": {"followers": "162", "datetime": "2017-06-20 21:19:41", "author": "@neurotalker", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874498483910131712": {"followers": "173,737", "datetime": "2017-06-13 05:27:40", "author": "@goodfellow_ian", "content_summary": "https://t.co/TMieyzfKYx"}, "881750411207520256": {"followers": "131", "datetime": "2017-07-03 05:44:14", "author": "@HogwartsScience", "content_summary": "Harry Potter And The Attention Is All You Need https://t.co/R73shjXUjI #science"}, "876779136907264000": {"followers": "669", "datetime": "2017-06-19 12:30:10", "author": "@razoralign", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1013720866302160896": {"followers": "53", "datetime": "2018-07-02 09:47:43", "author": "@DR_hayorbami", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "875257856605528066": {"followers": "46", "datetime": "2017-06-15 07:45:09", "author": "@davidbarbera9", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874504327125008384": {"followers": "7,137", "datetime": "2017-06-13 05:50:53", "author": "@anshul", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1097807681174138880": {"followers": "1,221", "datetime": "2019-02-19 10:38:42", "author": "@millionsmile", "content_summary": "Attention Is All You Need https://t.co/aCL2bXFF3K"}, "875088664509861888": {"followers": "12", "datetime": "2017-06-14 20:32:50", "author": "@MachineLookup", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1116989670230036480": {"followers": "43", "datetime": "2019-04-13 09:01:04", "author": "@jeandut14000", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874626529371795456": {"followers": "3,104", "datetime": "2017-06-13 13:56:29", "author": "@eturner303", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "879166869772443648": {"followers": "1,527", "datetime": "2017-06-26 02:38:10", "author": "@Pedra999", "content_summary": "RT @Pedra999: All you need is Attention : Training #tensorflow - > https://t.co/fllCNmcQe5"}, "874632634059673600": {"followers": "563", "datetime": "2017-06-13 14:20:44", "author": "@bcmcmahan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877183937587630080": {"followers": "103", "datetime": "2017-06-20 15:18:42", "author": "@spacemanslog", "content_summary": "Look: [1706.03762] Attention Is All You Need https://t.co/bKQkDkveLz https://t.co/iwHNq4Pwi4"}, "1213956436167929857": {"followers": "5,383", "datetime": "2020-01-05 22:52:23", "author": "@Alfons_Valencia", "content_summary": "RT @wzuidema: Shock 9 (2017): Know all about LSTMs now? Good, now forget all of it, as Attention Is All You Need (i.e. the Transformer). [\u2026"}, "874711071210254336": {"followers": "45", "datetime": "2017-06-13 19:32:25", "author": "@davidcittadini", "content_summary": "Attention Is All You Need https://t.co/7p1nULCU4B"}, "877477290002624512": {"followers": "408", "datetime": "2017-06-21 10:44:23", "author": "@A_Ym", "content_summary": "RT @kanair: I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mH\u2026"}, "875149819941195776": {"followers": "18", "datetime": "2017-06-15 00:35:51", "author": "@SladeGrantham", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874603387504951296": {"followers": "1,288", "datetime": "2017-06-13 12:24:31", "author": "@heghbalz", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "984257117023420416": {"followers": "3,449", "datetime": "2018-04-12 02:29:17", "author": "@reiver", "content_summary": "\"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin https://t.co/CKGy7QUmPC (machine learning)"}, "874876155358314498": {"followers": "76,883", "datetime": "2017-06-14 06:28:24", "author": "@NandoDF", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "948820277084868608": {"followers": "391", "datetime": "2018-01-04 07:35:56", "author": "@_ktomoya", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874693148601176064": {"followers": "323", "datetime": "2017-06-13 18:21:12", "author": "@mooitoys", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874961110562201601": {"followers": "561", "datetime": "2017-06-14 12:05:59", "author": "@menace_", "content_summary": "Replace rnns and cnn with parallel attention layers, achieve state of art results in machine translation on WMT: https://t.co/qjeV2HRhAQ https://t.co/c4L8MXHyeX"}, "874897515954540545": {"followers": "13", "datetime": "2017-06-14 07:53:17", "author": "@zjm1126", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877144360235794432": {"followers": "3,500", "datetime": "2017-06-20 12:41:26", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "874670653479174144": {"followers": "20", "datetime": "2017-06-13 16:51:49", "author": "@RT_Luo", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1098242982678020096": {"followers": "1,187", "datetime": "2019-02-20 15:28:26", "author": "@ivanprado", "content_summary": "The Transformer is a trendy network for NLP. Good post to learn how it is implemented: https://t.co/My6AVKlQ7z. I was getting lost by reading the paper https://t.co/aIGbjYloiU. The transformer is behind the impressive results from OpenAI GPT-2 https://t.co"}, "948822943978602497": {"followers": "433", "datetime": "2018-01-04 07:46:32", "author": "@a_maumau_", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874871878569132032": {"followers": "2,416", "datetime": "2017-06-14 06:11:24", "author": "@josephreisinger", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877663256759877633": {"followers": "300", "datetime": "2017-06-21 23:03:21", "author": "@kawano_hiroki", "content_summary": "Google \u304b\u3089\u65b0\u3057\u3044\u8ad6\u6587\u304c\u3067\u305f\u3002Attention Is All You Need. RNN\u3092\u4f7f\u308f\u305a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3060\u3051\u4f7f\u3063\u3066\u3088\u308a\u9ad8\u901f\u9ad8\u6027\u80fd\u306aNMT\u304c\u3067\u304d\u308b\u3089\u3057\u3044\u3002\u65b0\u3057\u3044\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u540d\u306f \"the Transformer\" https://t.co/fYuw5gvYP5"}, "874653921813250049": {"followers": "2,193", "datetime": "2017-06-13 15:45:19", "author": "@frankolken", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "980861973297549313": {"followers": "1,327", "datetime": "2018-04-02 17:38:12", "author": "@nudro", "content_summary": "Attention Is All You Need https://t.co/G3kyL5v7eH by @ilblackdragon \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\" https://t.co/BuUFxtuy2f"}, "948873282102558720": {"followers": "897", "datetime": "2018-01-04 11:06:34", "author": "@phykm", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "1166085420066791431": {"followers": "2,174", "datetime": "2019-08-26 20:30:03", "author": "@WilfriedGibily", "content_summary": "Google's Paper: A business line for #data and #AI systems as neural #networks based on \u201ctransformers\u201d widen their scope of application with #NVidia, #Facebook, #Google, #Amazon, #Apple, #Microsoft. #machinelearning #GPUs #technology https://t.co/PYdXREkjXI"}, "874427878384914432": {"followers": "25,193", "datetime": "2017-06-13 00:47:06", "author": "@Reza_Zadeh", "content_summary": "Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https://t.co/B1URpKZRjC https://t.co/qxd4WUFK5g"}, "874610754539982849": {"followers": "683", "datetime": "2017-06-13 12:53:48", "author": "@analyticsaurabh", "content_summary": "Wait, I'm still digesting SELU! This looks promising though. Would love to see confirmations from NLP folks. https://t.co/1DL9ubOMiw"}, "903463856122429441": {"followers": "12,765", "datetime": "2017-09-01 03:45:43", "author": "@jaguring1", "content_summary": "\u3053\u308c\u3060\u306a\u3002\u305f\u3076\u3093\u3002\u6a5f\u68b0\u7ffb\u8a33\u30bf\u30b9\u30af\u3067state-of-the-art Attention Is All You Need https://t.co/bXqJCb4mlH"}, "874591052598005760": {"followers": "2,048", "datetime": "2017-06-13 11:35:30", "author": "@vnfrombucharest", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "877451692958023680": {"followers": "295", "datetime": "2017-06-21 09:02:40", "author": "@gargs", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874496022797352960": {"followers": "72", "datetime": "2017-06-13 05:17:53", "author": "@gabeibagon", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165731738275983360": {"followers": "43", "datetime": "2019-08-25 21:04:39", "author": "@frabarbuto", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966149135031197697": {"followers": "783", "datetime": "2018-02-21 03:14:38", "author": "@muktabh", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1117579608025489408": {"followers": "456", "datetime": "2019-04-15 00:05:16", "author": "@PerthMLGroup", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "938780564513058816": {"followers": "3,500", "datetime": "2017-12-07 14:41:42", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "875135445876822017": {"followers": "3,632", "datetime": "2017-06-14 23:38:44", "author": "@kvashee", "content_summary": "New GOOG NMT approach = attention + positional encoding = 28.4 BLEU on WMT 2014 EN-to-DE = +2 over previous best https://t.co/gPQwVEFPmt"}, "876962032045641728": {"followers": "160", "datetime": "2017-06-20 00:36:56", "author": "@EgorAnanyev", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "877482819165061121": {"followers": "1,679", "datetime": "2017-06-21 11:06:21", "author": "@rei_akaishi", "content_summary": "Attention Is All You Need https://t.co/lqx5nMgriR"}, "1117336741419945984": {"followers": "24", "datetime": "2019-04-14 08:00:12", "author": "@poqaa_nlp", "content_summary": "Attention Is All You Need https://t.co/Wa4hpI86Qj (Popularity:14.8) #Natural_language_processing #Machine_Learning"}, "1166027350162780161": {"followers": "330", "datetime": "2019-08-26 16:39:18", "author": "@mr_ubik", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876635542028419072": {"followers": "17", "datetime": "2017-06-19 02:59:34", "author": "@andybaoxv", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "961025577674145792": {"followers": "456", "datetime": "2018-02-06 23:55:27", "author": "@PerthMLGroup", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "874871666714791937": {"followers": "193", "datetime": "2017-06-14 06:10:34", "author": "@alexhock", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "877247881727451137": {"followers": "217", "datetime": "2017-06-20 19:32:48", "author": "@cmkumar87", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874750908965339136": {"followers": "454", "datetime": "2017-06-13 22:10:43", "author": "@SirrahChan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117899944977846272": {"followers": "89", "datetime": "2019-04-15 21:18:11", "author": "@singularpattern", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874476084192063489": {"followers": "33", "datetime": "2017-06-13 03:58:40", "author": "@realjmcguigan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "887804123608829953": {"followers": "38,161", "datetime": "2017-07-19 22:39:32", "author": "@mclynd", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "875196510241120256": {"followers": "363", "datetime": "2017-06-15 03:41:23", "author": "@VivienneSAP99", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1098955583254876160": {"followers": "201", "datetime": "2019-02-22 14:40:03", "author": "@dvgodoy", "content_summary": "RT @willkurt: Finally worked through \"Attention is all you need\". My recommended steps: - Skim the paper: https://t.co/iCNGDeJ62q - Step th\u2026"}, "876911439742676993": {"followers": "257", "datetime": "2017-06-19 21:15:54", "author": "@SigP226", "content_summary": "#stanfordnlp RT gneubig: Yesterday \"attention is all you need\" https://t.co/x1CJG4vcmr Today \"you need a bunch of other stuff\" \u2026"}, "874453996227895296": {"followers": "335", "datetime": "2017-06-13 02:30:53", "author": "@kobi78", "content_summary": "RT @jasonbaldridge: A really interesting new model from Google! Not just MT: check out the parsing results & attention viz in appendix. htt\u2026"}, "927531763714351105": {"followers": "487", "datetime": "2017-11-06 13:42:59", "author": "@Quasi_quant2010", "content_summary": "RT @mickey24: \u8ad6\u6587 \"Attention Is All You Need\": Attention\u30d9\u30fc\u30b9\u306e\u5358\u7d14\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u300cTransformer\u300d\u3067\u7ffb\u8a33\u30bf\u30b9\u30af\u306e\u7cbe\u5ea6\u5411\u4e0a\u30fb\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3092\u5b9f\u73fe\u3002RNN\u3084CNN\u306f\u4f7f\u308f\u306a\u3044\u3002\u8457\u8005\u306fGoogler https://t.\u2026"}, "874658665675882497": {"followers": "261", "datetime": "2017-06-13 16:04:10", "author": "@GuptaRajat033", "content_summary": "RT @iamaidang: Our paper \"All You Need Is Attention\". SOTA on WMT EN-DE using only feed forward and attention layers. https://t.co/QmXdfgYo\u2026"}, "877730145871609857": {"followers": "395", "datetime": "2017-06-22 03:29:08", "author": "@zuxfoucault", "content_summary": "RT @kanair: I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mH\u2026"}, "1165783340311441408": {"followers": "0", "datetime": "2019-08-26 00:29:42", "author": "@Sam09lol", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874523545317109760": {"followers": "3,837", "datetime": "2017-06-13 07:07:15", "author": "@zehavoc", "content_summary": "RT @jasonbaldridge: More than meets the eye! https://t.co/UEmGlMWfsc https://t.co/qvHc1uX8OK"}, "874794064331276288": {"followers": "147", "datetime": "2017-06-14 01:02:12", "author": "@bbaugh0", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "891924927812476928": {"followers": "4,531", "datetime": "2017-07-31 07:34:08", "author": "@ChrSzegedy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874577661884076033": {"followers": "46", "datetime": "2017-06-13 10:42:18", "author": "@OriShechter", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1232012564558536707": {"followers": "7,020", "datetime": "2020-02-24 18:40:59", "author": "@andreisavu", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/ZBgjVKnscb explained in a more accessible way here https://t.co/ql2oR2Lx9V -- amazing work!"}, "874748440978259968": {"followers": "152", "datetime": "2017-06-13 22:00:55", "author": "@ZizhaoZhang", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874843982660075520": {"followers": "2,826", "datetime": "2017-06-14 04:20:33", "author": "@VenkatNagaswamy", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874514182728810496": {"followers": "1,938", "datetime": "2017-06-13 06:30:03", "author": "@EldarSilver", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "880527433677582337": {"followers": "317", "datetime": "2017-06-29 20:44:34", "author": "@chrisbot_js", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "874642452782563329": {"followers": "74", "datetime": "2017-06-13 14:59:45", "author": "@WenchenLi", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1166837295548383234": {"followers": "243", "datetime": "2019-08-28 22:17:44", "author": "@neurosp1ke", "content_summary": "oops .. only in v3? .. Will tomorrow take an hour to check if Google really published exactly the same technique as FB in 17. Seems they thought it would not be that good/important - or why was it removed from later paper versions?"}, "966093682364567552": {"followers": "825", "datetime": "2018-02-20 23:34:17", "author": "@fly51fly", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165761050593169408": {"followers": "1,090", "datetime": "2019-08-25 23:01:07", "author": "@GonzaloBarria", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1092665663875760129": {"followers": "874", "datetime": "2019-02-05 06:06:09", "author": "@guicho271828", "content_summary": "Attention Is All You Need (2017) https://t.co/rgkuMX8NVN You May Not Need Attention (2018) https://t.co/tkqkSBsrkG"}, "874810166973054976": {"followers": "27", "datetime": "2017-06-14 02:06:11", "author": "@mahesh21aug", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "875629708972511232": {"followers": "12,679", "datetime": "2017-06-16 08:22:45", "author": "@boredyannlecun", "content_summary": "Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker https://t.co/DUO4aufKFd"}, "878825620565884928": {"followers": "4,755", "datetime": "2017-06-25 04:02:10", "author": "@tianhuil", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "877653335247015936": {"followers": "309", "datetime": "2017-06-21 22:23:55", "author": "@cstonebiz", "content_summary": "RT @v_vashishta: #Google #DeepLearning - Attention Is All You Need https://t.co/Bjxj2dYXxw #MachineLearning"}, "874457338714791936": {"followers": "131", "datetime": "2017-06-13 02:44:10", "author": "@HogwartsScience", "content_summary": "Harry Potter And The Attention Is All You Need https://t.co/XotZIqWl7q #science"}, "874524144057982976": {"followers": "1,032", "datetime": "2017-06-13 07:09:38", "author": "@lazykuna", "content_summary": "RT @linkoffate: Attention Is All You Need https://t.co/datm5124sF WTF \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b \uc5b4\uc81c \ub098\uc628 \ub17c\ubb38\uc744 \uadf8\ub300\ub85c \uc539\uc5b4\ubc84\ub9ac\ub124 \uc774\uac70 \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b"}, "876778166961831937": {"followers": "357", "datetime": "2017-06-19 12:26:19", "author": "@AsymTimeTweet", "content_summary": "Attention Is All You Need https://t.co/aGoRnPRzP7"}, "874642123584393216": {"followers": "395", "datetime": "2017-06-13 14:58:26", "author": "@qiming82", "content_summary": "Attention is all you need https://t.co/agLjoMmAp7"}, "877040033102008320": {"followers": "98", "datetime": "2017-06-20 05:46:53", "author": "@DeepLearningNow", "content_summary": "New #deeplearning paper https://t.co/qwZnM1IVje https://t.co/zQUSKLhk5D"}, "1117106011695796226": {"followers": "251", "datetime": "2019-04-13 16:43:22", "author": "@Tanaygahlot", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874581686641545216": {"followers": "366", "datetime": "2017-06-13 10:58:17", "author": "@ashishawasthi", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "874939041518628864": {"followers": "111", "datetime": "2017-06-14 10:38:17", "author": "@nimblel", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1117050547037712386": {"followers": "31", "datetime": "2019-04-13 13:02:58", "author": "@Greg_Irie", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874484155458834432": {"followers": "962", "datetime": "2017-06-13 04:30:44", "author": "@jnhwkim", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1258762513123950592": {"followers": "4", "datetime": "2020-05-08 14:15:44", "author": "@3XCL4M4T10N", "content_summary": "seq2seq\u306e\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\uff0c\u8a08\u7b97\u91cf\uff0c\u4e26\u5217\u6027\uff0c\u9577\u8ddd\u96e2\u306e\u4f9d\u5b58\u6027\u306a\u3069\u3092\u6539\u5584\u3057\u305f\u30e2\u30c7\u30eb\u306e\u63d0\u6848 leak\u3057\u306a\u3044\u305f\u3081\u306emask\u3084positional encoding\u306a\u3069\u3067seq2seq\u7279\u6709\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066\u3044\u308b https://t.co/HWZAVCcmuC"}, "1165957438291554305": {"followers": "366", "datetime": "2019-08-26 12:01:30", "author": "@iugoaoj", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "948935840377356288": {"followers": "45", "datetime": "2018-01-04 15:15:09", "author": "@nullbytep", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874475902498779136": {"followers": "15,013", "datetime": "2017-06-13 03:57:56", "author": "@littleidea", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "1165885003827490816": {"followers": "2,652", "datetime": "2019-08-26 07:13:40", "author": "@johngod40", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874440257441222656": {"followers": "1,229", "datetime": "2017-06-13 01:36:18", "author": "@Kiikurage", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165736227888406529": {"followers": "168", "datetime": "2019-08-25 21:22:29", "author": "@brunoboutteau", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966010788367556609": {"followers": "177,517", "datetime": "2018-02-20 18:04:54", "author": "@Montreal_AI", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874528661864456192": {"followers": "532", "datetime": "2017-06-13 07:27:35", "author": "@oskarsyahbana", "content_summary": "A new NN architecture based not on recurrence or convolutions, but on, get this, Attention https://t.co/6mnPHmXIjb"}, "1116918785875816448": {"followers": "818", "datetime": "2019-04-13 04:19:24", "author": "@ErmiaBivatan", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1082961598304657409": {"followers": "671", "datetime": "2019-01-09 11:25:40", "author": "@erwtokritos", "content_summary": "RT @AUEBNLPGroup: Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0\u2026"}, "874676712625258497": {"followers": "314", "datetime": "2017-06-13 17:15:53", "author": "@savanvisalpara7", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "967011867406290945": {"followers": "89", "datetime": "2018-02-23 12:22:50", "author": "@cole_gulino", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "877169273835331584": {"followers": "4,673", "datetime": "2017-06-20 14:20:26", "author": "@WordLo", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "966443647603441664": {"followers": "98", "datetime": "2018-02-21 22:44:55", "author": "@Delfox29", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "880527288764317697": {"followers": "683", "datetime": "2017-06-29 20:43:59", "author": "@VidGajsek", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "919659840091418624": {"followers": "250", "datetime": "2017-10-15 20:22:46", "author": "@4sim", "content_summary": "Attention is all you need, my fav. paper of 2017, https://t.co/cYmZo87oKt"}, "874557068488855552": {"followers": "155", "datetime": "2017-06-13 09:20:28", "author": "@Trtd6Trtd", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "968631391553445888": {"followers": "215", "datetime": "2018-02-27 23:38:14", "author": "@hendrabunyamin", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165954313107267584": {"followers": "521", "datetime": "2019-08-26 11:49:05", "author": "@CrosalZareus", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874759690042953728": {"followers": "30", "datetime": "2017-06-13 22:45:37", "author": "@MarchBragagnini", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899652949676142594": {"followers": "4,298", "datetime": "2017-08-21 15:22:32", "author": "@savasavasava", "content_summary": "RT @npseaver: Machine learning folks: Can anyone help me grok the current usage of \"attention\" in neural net research like this?: https://t\u2026"}, "1073527787569455104": {"followers": "33", "datetime": "2018-12-14 10:39:04", "author": "@data_topology", "content_summary": "Attention \u2013 a concept to improve the performance of neural machine translation applications. Transformer \u2013 a LM that uses attention to boost training speed. ? Complexity vs speed ? READ: https://t.co/FMBMj99JWc https://t.co/eLEDWi8GqP #NLP #Language_Mo"}, "877140699338096644": {"followers": "189", "datetime": "2017-06-20 12:26:53", "author": "@shahrukh_athar", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165983767502229509": {"followers": "206", "datetime": "2019-08-26 13:46:07", "author": "@tactiZity_", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874740461071736832": {"followers": "482", "datetime": "2017-06-13 21:29:12", "author": "@LarryASimon", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165826743816450049": {"followers": "407", "datetime": "2019-08-26 03:22:10", "author": "@hugojair", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875971927780622341": {"followers": "786", "datetime": "2017-06-17 07:02:37", "author": "@kotymg", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874604247697981440": {"followers": "144", "datetime": "2017-06-13 12:27:56", "author": "@cyril_anderson", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "991686018041434113": {"followers": "207", "datetime": "2018-05-02 14:29:05", "author": "@TigerScorpior", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "961083941372522496": {"followers": "30", "datetime": "2018-02-07 03:47:22", "author": "@lokeshsonii", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "874786508082999298": {"followers": "103", "datetime": "2017-06-14 00:32:10", "author": "@spacemanslog", "content_summary": "Look: [R] [1706.03762] Attention Is All You Need <-- Sota NMT; less compute https://t.co/bKQkDkveLz https://t.co/Zl4Kiohg47"}, "966010971822215168": {"followers": "675", "datetime": "2018-02-20 18:05:37", "author": "@ehfo0", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1132978643909500928": {"followers": "451", "datetime": "2019-05-27 11:55:33", "author": "@hausenjapan", "content_summary": "https://t.co/QmVpwb50Wt \u306f\u3001\u3069\u306e\u5206\u91ce\u3092\u3084\u308b\u306b\u305b\u3088\u3001\u8aad\u3080\u3053\u3068\u304c\u591a\u3044\u306e\u3067\u3001\u7d50\u5c40\u3001\u82f1\u8a9e\u81ea\u4f53\u3092\u52c9\u5f37\u3057\u305f\u307b\u3046\u304c\u304a\u5f97\u304b\u3082\u3002 \u300cAttention Is All You Need\u300d https://t.co/wDTXcwn1kz Computer Science > Computation and Language"}, "1248660251269836801": {"followers": "4,021", "datetime": "2020-04-10 17:12:57", "author": "@DanHLawReporter", "content_summary": "Attention (BERT\u2019s secret sauce): https://t.co/rXijQ3MKgF"}, "961426314333966336": {"followers": "188", "datetime": "2018-02-08 02:27:50", "author": "@blueviggen", "content_summary": "RT @graphific: Attention Is All You Need from @GoogleBrain. No RNNs or CNNs, only Attention = fast training & SOTA on WMT'14 https://t.co/Y\u2026"}, "1245261065899794432": {"followers": "11", "datetime": "2020-04-01 08:05:48", "author": "@thelordempire_", "content_summary": "@animatedtext @Molevolent oh you want attention? here you go https://t.co/Fbq3IhuF4l"}, "877449088509255680": {"followers": "3,973", "datetime": "2017-06-21 08:52:19", "author": "@kanair", "content_summary": "I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mHvCG8sjhT"}, "876014951353249793": {"followers": "321", "datetime": "2017-06-17 09:53:34", "author": "@Soria_Emilio", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "874944244942811136": {"followers": "109", "datetime": "2017-06-14 10:58:58", "author": "@ish_girwan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874675162221871104": {"followers": "252", "datetime": "2017-06-13 17:09:44", "author": "@CourtSociety", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874624963902291973": {"followers": "173", "datetime": "2017-06-13 13:50:15", "author": "@siddharthm83", "content_summary": "State of the art on machine translation from attention only! https://t.co/sFsIPtkDo2"}, "874751994090708997": {"followers": "4,072", "datetime": "2017-06-13 22:15:02", "author": "@bgoncalves", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/a1M2Mv7sRF"}, "874613626233135104": {"followers": "2,501", "datetime": "2017-06-13 13:05:12", "author": "@_yroy_", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874446118280728576": {"followers": "1,025", "datetime": "2017-06-13 01:59:35", "author": "@desertnaut", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "874659281374777345": {"followers": "2,091", "datetime": "2017-06-13 16:06:37", "author": "@alexis_b_cook", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1116945515214721025": {"followers": "289", "datetime": "2019-04-13 06:05:37", "author": "@MarcoZorzi", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874481163150151680": {"followers": "1,074", "datetime": "2017-06-13 04:18:51", "author": "@shonosuke_i", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874664457846243329": {"followers": "509", "datetime": "2017-06-13 16:27:11", "author": "@mhkoji", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166034462242553862": {"followers": "28", "datetime": "2019-08-26 17:07:34", "author": "@AbhiGoswami5393", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966008703391592448": {"followers": "200", "datetime": "2018-02-20 17:56:37", "author": "@HiteshUVaidya", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "966017104209174540": {"followers": "366", "datetime": "2018-02-20 18:30:00", "author": "@surafelml", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1114026383699664898": {"followers": "640", "datetime": "2019-04-05 04:46:02", "author": "@copasta_", "content_summary": "\u8aad\u307f\u8aad\u307f(\u7406\u89e3\u3067\u304d\u308b\u304b\u2026\uff1f)/Attention Is All You Need https://t.co/wYy4yY989Q"}, "874958136989306880": {"followers": "481", "datetime": "2017-06-14 11:54:10", "author": "@debarko", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874475139655532545": {"followers": "4,327", "datetime": "2017-06-13 03:54:54", "author": "@jekbradbury", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "885121016158179329": {"followers": "46", "datetime": "2017-07-12 12:57:49", "author": "@KittyPryde9", "content_summary": "RT @xyc234: @PyTorch implementation of Attention is all you need https://t.co/9HylJYUMx1 https://t.co/2Z4OFDUXL2"}, "876700729007853568": {"followers": "82", "datetime": "2017-06-19 07:18:36", "author": "@mario_y_c", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874561748761706497": {"followers": "331", "datetime": "2017-06-13 09:39:04", "author": "@Rohitpatil5", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874693859355308038": {"followers": "21", "datetime": "2017-06-13 18:24:01", "author": "@ys1045097987", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "880343492660256768": {"followers": "247", "datetime": "2017-06-29 08:33:39", "author": "@kdubovikov", "content_summary": "It is fascinating that the new state of the art in machine translation has been achieved by dropping LSTMs out https://t.co/9ggqVPSnxr"}, "874783134901981184": {"followers": "416", "datetime": "2017-06-14 00:18:46", "author": "@ryanmalia", "content_summary": "This is beyond my reach but I think this equates to adderall for machine learning https://t.co/srUoXgHZn1"}, "874677386410729472": {"followers": "319", "datetime": "2017-06-13 17:18:34", "author": "@eywalker", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "894763688569131008": {"followers": "26", "datetime": "2017-08-08 03:34:21", "author": "@tokoton01", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874588955227824129": {"followers": "1,627", "datetime": "2017-06-13 11:27:10", "author": "@chris_brockett", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874477256021639168": {"followers": "1,231", "datetime": "2017-06-13 04:03:19", "author": "@odan3240", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877208806245015553": {"followers": "2,663", "datetime": "2017-06-20 16:57:31", "author": "@sigitpurnomo", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1116986316816814080": {"followers": "210", "datetime": "2019-04-13 08:47:45", "author": "@daniwi79", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874598746134257665": {"followers": "440", "datetime": "2017-06-13 12:06:04", "author": "@fabmilo", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1182598631636926464": {"followers": "63", "datetime": "2019-10-11 10:07:40", "author": "@Ronmemo1", "content_summary": "ABST100\u30e1\u30e2\uff1a58 Attention\uff082017\uff09\u2192https://t.co/uZP17phR2H\u3000\uff0c\u300c~the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely\u300d\u3064\u307e\u308aRNN\u3067\u3082CNN\u3067\u3082\u306a\u3044\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u3068\u3044\u3046\u3053\u3068"}, "996668648294141953": {"followers": "1,202", "datetime": "2018-05-16 08:28:17", "author": "@StephenEglen", "content_summary": "State of the art in neural machine translation https://t.co/4M3WkyxfnV #ccbi18"}, "874664968590897152": {"followers": "66", "datetime": "2017-06-13 16:29:13", "author": "@rubentous1", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "1165938900293976064": {"followers": "100", "datetime": "2019-08-26 10:47:50", "author": "@winnietaech", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874438907781193728": {"followers": "81,553", "datetime": "2017-06-13 01:30:56", "author": "@hardmaru", "content_summary": "Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t.co/ohJJLfemIC https://t.co/iBxfDJygZJ"}, "1013704958900924417": {"followers": "36", "datetime": "2018-07-02 08:44:30", "author": "@StatshubNgr", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "874699577764360192": {"followers": "1,135", "datetime": "2017-06-13 18:46:45", "author": "@socialnews_nr", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/fEgNSDGnDC"}, "876603239252213760": {"followers": "1,126", "datetime": "2017-06-19 00:51:13", "author": "@innocent_zero", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874594359718662144": {"followers": "733", "datetime": "2017-06-13 11:48:39", "author": "@SethHWeidman", "content_summary": "Well that changes everything https://t.co/YGjDLfsPcU"}, "874638499693158400": {"followers": "148", "datetime": "2017-06-13 14:44:02", "author": "@sBalaneshin", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "966364000979210240": {"followers": "63", "datetime": "2018-02-21 17:28:26", "author": "@fkariminejadasl", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "966029653138984965": {"followers": "19", "datetime": "2018-02-20 19:19:51", "author": "@AVSave", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874607623789826048": {"followers": "3,500", "datetime": "2017-06-13 12:41:21", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "874625782248648704": {"followers": "245", "datetime": "2017-06-13 13:53:30", "author": "@__jm", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874481953927548928": {"followers": "4,804", "datetime": "2017-06-13 04:21:59", "author": "@cto_movidius", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1116522463461085184": {"followers": "95", "datetime": "2019-04-12 02:04:33", "author": "@boubou_101", "content_summary": "@lizardlucas42 I mean, Attention Is All You Need in that case ( https://t.co/HgsAojET4w )"}, "874439565754458114": {"followers": "1,143", "datetime": "2017-06-13 01:33:33", "author": "@pmawhorter", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "944130435650424834": {"followers": "24", "datetime": "2017-12-22 09:00:11", "author": "@poqaa_nlp", "content_summary": "Attention Is All You Need https://t.co/Wa4hpI86Qj (Popularity:20.0) #Natural_language_processing #Machine_Learning"}, "874702664109621248": {"followers": "227", "datetime": "2017-06-13 18:59:00", "author": "@Ukerzel", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874455047194542080": {"followers": "190", "datetime": "2017-06-13 02:35:04", "author": "@pri_rao", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117066067103236096": {"followers": "218", "datetime": "2019-04-13 14:04:39", "author": "@AssistedEvolve", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874458280487079936": {"followers": "115", "datetime": "2017-06-13 02:47:55", "author": "@viirya", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874511800741527552": {"followers": "2,282", "datetime": "2017-06-13 06:20:35", "author": "@eoinbrazil", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874839585607622657": {"followers": "15,143", "datetime": "2017-06-14 04:03:05", "author": "@alevergara78", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1166416199548379149": {"followers": "212", "datetime": "2019-08-27 18:24:27", "author": "@manuel_lmartin", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1231534507179827200": {"followers": "222", "datetime": "2020-02-23 11:01:21", "author": "@PapersTrending", "content_summary": "[2/10] \ud83d\udcc8 - Attention Is All You Need - 286 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "1166020129097834496": {"followers": "509", "datetime": "2019-08-26 16:10:37", "author": "@bhagirathl", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875792686703411200": {"followers": "159,742", "datetime": "2017-06-16 19:10:22", "author": "@Montreal_IA", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874496511790374912": {"followers": "289", "datetime": "2017-06-13 05:19:50", "author": "@dfarmer", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "877042302891446272": {"followers": "1,354", "datetime": "2017-06-20 05:55:54", "author": "@weehyong", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874511720265416705": {"followers": "76", "datetime": "2017-06-13 06:20:16", "author": "@alapite", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877079303053246465": {"followers": "870", "datetime": "2017-06-20 08:22:55", "author": "@eisenzopf", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/iSxYeyOzc0"}, "874500304720482304": {"followers": "1,058", "datetime": "2017-06-13 05:34:54", "author": "@BigsnarfDude", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "1256417708679335937": {"followers": "1,215", "datetime": "2020-05-02 02:58:19", "author": "@naranjandra", "content_summary": "RT @renolarana: Attention is all you need . Todas mis publicaciones en redes sociales se producen por mi necesidad de atenci\u00f3n\ud83d\ude09 \ud83e\uddd0El art\u00edcu\u2026"}, "874926559865581569": {"followers": "299", "datetime": "2017-06-14 09:48:41", "author": "@madrugad0", "content_summary": "fresh Google paper: fully connected net with two layers of attention gives SOTA in NMT; also curious that... https://t.co/56wWmC1IOM"}, "875229368045379584": {"followers": "94", "datetime": "2017-06-15 05:51:56", "author": "@garethseneque", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165878657404952576": {"followers": "753", "datetime": "2019-08-26 06:48:27", "author": "@minsuk_chang", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "905104910173184000": {"followers": "444", "datetime": "2017-09-05 16:26:41", "author": "@dakuton", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "874764050348990466": {"followers": "485", "datetime": "2017-06-13 23:02:56", "author": "@eve_yk", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874665530946277376": {"followers": "739", "datetime": "2017-06-13 16:31:27", "author": "@ajinkyakale", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874481178417475584": {"followers": "1,081", "datetime": "2017-06-13 04:18:54", "author": "@roeeaharoni", "content_summary": "RT @arxiv_cs_cl: Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/8ovOiC3bdC #NLProc"}, "875238323882786816": {"followers": "193", "datetime": "2017-06-15 06:27:32", "author": "@alexhock", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "966096512492109827": {"followers": "55", "datetime": "2018-02-20 23:45:32", "author": "@piushvaish", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874473128633671680": {"followers": "6,525", "datetime": "2017-06-13 03:46:55", "author": "@barneyp", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "1165867042639175682": {"followers": "42", "datetime": "2019-08-26 06:02:18", "author": "@emsi_kil3r", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874465459877199873": {"followers": "783", "datetime": "2017-06-13 03:16:27", "author": "@muktabh", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165750045133721600": {"followers": "9", "datetime": "2019-08-25 22:17:24", "author": "@mbenhamdtw", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874517162530131969": {"followers": "95", "datetime": "2017-06-13 06:41:53", "author": "@Lefiish", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875467816043823104": {"followers": "1", "datetime": "2017-06-15 21:39:27", "author": "@zhongkeli", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874696578380034051": {"followers": "281", "datetime": "2017-06-13 18:34:50", "author": "@adkatrit", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "966010980319813633": {"followers": "163,852", "datetime": "2018-02-20 18:05:39", "author": "@ceobillionaire", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876784639955914752": {"followers": "344", "datetime": "2017-06-19 12:52:02", "author": "@trylks", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876830292505489408": {"followers": "107", "datetime": "2017-06-19 15:53:27", "author": "@marionyk", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1120593375831437312": {"followers": "31", "datetime": "2019-04-23 07:40:55", "author": "@VladimirRybako9", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874680717334032384": {"followers": "10,282", "datetime": "2017-06-13 17:31:48", "author": "@golbin", "content_summary": "RNN\uacfc CNN \uc5c6\uc774 \uc5b4\ud150\uc158 \uba54\uce74\ub2c8\uc998\ub9cc\uc73c\ub85c Seq2Seq\ub97c \uad6c\ud604\ud588\ub294\ub370, \uc18d\ub3c4\ub3c4 \uac81\ub098 \ube60\ub974\uace0 \uacb0\uacfc\ub3c4 \uc88b\uc558\ub2e4\uace0. \uc73c\uc74c.. @_@;; https://t.co/Uuy5uJ3PZR"}, "874654074515316736": {"followers": "175", "datetime": "2017-06-13 15:45:56", "author": "@Gangadhar_P", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875249146889986048": {"followers": "861", "datetime": "2017-06-15 07:10:32", "author": "@ryf_feed", "content_summary": "Attention Is All You Need https://t.co/Gw9Y8ROLRO"}, "1165840927753007104": {"followers": "46", "datetime": "2019-08-26 04:18:32", "author": "@DeepakB94018371", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874457370142822400": {"followers": "2,043", "datetime": "2017-06-13 02:44:18", "author": "@dosei_sanga", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "881673974056460290": {"followers": "3,500", "datetime": "2017-07-03 00:40:30", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "966010400343982081": {"followers": "409", "datetime": "2018-02-20 18:03:21", "author": "@RonInDune", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876558094515437569": {"followers": "58", "datetime": "2017-06-18 21:51:50", "author": "@_WizDom13_", "content_summary": "Attention Is All You Need Attention mechanisms only without any RNN and CNN / ConvNet. https://t.co/f4554bygku"}, "1141046112142471169": {"followers": "5,538", "datetime": "2019-06-18 18:12:47", "author": "@cackerman1", "content_summary": "Google\u2019s Evolved Transformer translation tasks https://t.co/xEaoMXAkBO https://t.co/HugWwSLSx2 https://t.co/oumkpVC09y https://t.co/LOvjBiNjYV"}, "874672695245275136": {"followers": "57", "datetime": "2017-06-13 16:59:55", "author": "@rahdirsrahonam", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874786330193952768": {"followers": "1,132", "datetime": "2017-06-14 00:31:28", "author": "@vsatayamas", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165799582581829633": {"followers": "11,647", "datetime": "2019-08-26 01:34:14", "author": "@Shpantzer", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876886476864327680": {"followers": "64", "datetime": "2017-06-19 19:36:42", "author": "@dolorousrtur", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874508201227468800": {"followers": "2,043", "datetime": "2017-06-13 06:06:17", "author": "@dosei_sanga", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874578481425915904": {"followers": "393", "datetime": "2017-06-13 10:45:33", "author": "@ChrisDansereau", "content_summary": "Attention is all you need!! https://t.co/K1DhLsIB9s"}, "875035141583892480": {"followers": "1,649", "datetime": "2017-06-14 17:00:09", "author": "@alexk_z", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "876886365690048513": {"followers": "276", "datetime": "2017-06-19 19:36:15", "author": "@arturbekasov", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874679188808122369": {"followers": "12,904", "datetime": "2017-06-13 17:25:44", "author": "@DrBeef_", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875214494296809472": {"followers": "4,744", "datetime": "2017-06-15 04:52:50", "author": "@ArnoCandel", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874422398346571776": {"followers": "1,815", "datetime": "2017-06-13 00:25:20", "author": "@iamaidang", "content_summary": "Our paper \"All You Need Is Attention\". SOTA on WMT EN-DE using only feed forward and attention layers. https://t.co/QmXdfgYojW"}, "875313863897513985": {"followers": "33,955", "datetime": "2017-06-15 11:27:42", "author": "@aiprgirl", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1077430541429788674": {"followers": "672", "datetime": "2018-12-25 05:07:13", "author": "@Oezy_k", "content_summary": "@physics303 \u9045\u304f\u306a\u308a\u307e\u3057\u3066\u3059\u307f\u307e\u305b\u3093\u304c\u3001\u8fd4\u4fe1\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3059\u3002 \u79c1\u3001\u3082\u3063\u3071\u3089\u5b9f\u52d9\u306e\u5074\u3067\u3059\u306e\u3067\u3001\u5c02\u9580\u306e\u65b9\u306b\u304a\u6559\u3048\u3067\u304d\u308b\u307b\u3069\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u304c\u306a\u304f\u3001\u9006\u306b\u7814\u7a76\u52d5\u5411\u3092\u304a\u6559\u3048\u3044\u305f\u3060\u304d\u305f\u3044\u304f\u3089\u3044\u3067\u3059\u304c\u3001\u4e0b\u8a18\u306a\u3093\u3066\u3044\u304b\u304c\u3067\u3057\u3087\u3046\u304b\uff1f https://t.co/JgJPVlVbG3"}, "874976672230981632": {"followers": "756", "datetime": "2017-06-14 13:07:49", "author": "@noodlefrenzy", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874622682398167040": {"followers": "876", "datetime": "2017-06-13 13:41:11", "author": "@jonintweet", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "877253359341883392": {"followers": "33", "datetime": "2017-06-20 19:54:34", "author": "@_prasenjitgiri_", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874445240899391489": {"followers": "25,741", "datetime": "2017-06-13 01:56:06", "author": "@Miles_Brundage", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899764105824710656": {"followers": "12,671", "datetime": "2017-08-21 22:44:14", "author": "@genkuroki", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "876766718756089856": {"followers": "11,390", "datetime": "2017-06-19 11:40:49", "author": "@_brohrer_", "content_summary": "Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874502354883584000": {"followers": "130", "datetime": "2017-06-13 05:43:03", "author": "@prafull7", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "874952396623011841": {"followers": "413", "datetime": "2017-06-14 11:31:21", "author": "@ThomasBoquet", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1233607109574627328": {"followers": "155", "datetime": "2020-02-29 04:17:08", "author": "@Richard60943814", "content_summary": "RT @DimejiMudele: \"Attention is all you need\" is probably the most important scientific publication in Deep learning since 2017. Here you\u2026"}, "1165862240815386625": {"followers": "2,293", "datetime": "2019-08-26 05:43:13", "author": "@ImNickHuber", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874678879977435136": {"followers": "350", "datetime": "2017-06-13 17:24:30", "author": "@0_rishabh", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874472180163235842": {"followers": "4,331", "datetime": "2017-06-13 03:43:09", "author": "@korymath", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1171228039620907009": {"followers": "203", "datetime": "2019-09-10 01:04:59", "author": "@vikasbahirwani", "content_summary": "Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LUftwroVOp) 2) Quickly review CoVe, ELMo, GPT and GPT2 via. https://t.co/CzHQjBlCTT. Thanx @lilianweng. 3) Read BERT https://t.co/ZnrRHD"}, "875801133679337483": {"followers": "160,790", "datetime": "2017-06-16 19:43:56", "author": "@Quebec_AI", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165712315263660033": {"followers": "275", "datetime": "2019-08-25 19:47:28", "author": "@EricOndenyi", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875156037464072193": {"followers": "4,225", "datetime": "2017-06-15 01:00:33", "author": "@mathpunk", "content_summary": "RT @gigasquid: Looking forward to the code examples being released for \"Attention is All You Need\". See you RNN/ LSTM. https://t.co/LUuZ6oZ\u2026"}, "874612637996261377": {"followers": "68", "datetime": "2017-06-13 13:01:17", "author": "@shinryu_rk", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874446593755185152": {"followers": "847", "datetime": "2017-06-13 02:01:29", "author": "@t_Signull", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874817458875408384": {"followers": "194", "datetime": "2017-06-14 02:35:10", "author": "@gopalanj", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875223509982617602": {"followers": "329", "datetime": "2017-06-15 05:28:40", "author": "@balajinix", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "948873221935255552": {"followers": "235", "datetime": "2018-01-04 11:06:19", "author": "@bamboo4031", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874926664098226176": {"followers": "484", "datetime": "2017-06-14 09:49:06", "author": "@snneko", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "874660879039705088": {"followers": "62", "datetime": "2017-06-13 16:12:58", "author": "@JyotiAneja", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166281136764440584": {"followers": "1,377", "datetime": "2019-08-27 09:27:46", "author": "@kaineko", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874595466176024576": {"followers": "151", "datetime": "2017-06-13 11:53:02", "author": "@heilman13", "content_summary": "RT @dipanjand: Check this out: https://t.co/N6hZyklj6G SOTA on several metrics using extremely simple models."}, "874617186706309121": {"followers": "2,050", "datetime": "2017-06-13 13:19:21", "author": "@shunk031", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/AkEw7JLyJr"}, "876636833014194177": {"followers": "347", "datetime": "2017-06-19 03:04:42", "author": "@hikomimo", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874516082811260928": {"followers": "53", "datetime": "2017-06-13 06:37:36", "author": "@sjobeek", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1043239429912113152": {"followers": "473", "datetime": "2018-09-21 20:43:57", "author": "@MShahriariNia", "content_summary": "Intuition behind why multi-head attention is better than simple attention: \u201cmulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions\" https://t.co/Hwnvpwl5b6"}, "938637885380608000": {"followers": "783", "datetime": "2017-12-07 05:14:45", "author": "@muktabh", "content_summary": "New update https://t.co/XQKWqv0X0T"}, "876713707182710784": {"followers": "275", "datetime": "2017-06-19 08:10:11", "author": "@danieldekok", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1117217919035789312": {"followers": "111", "datetime": "2019-04-14 00:08:03", "author": "@richardvenusfo", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "875631519175286785": {"followers": "843", "datetime": "2017-06-16 08:29:57", "author": "@rbstojnic", "content_summary": "Cannot argue with this! https://t.co/R1Pcartkm9"}, "874664844019900416": {"followers": "129", "datetime": "2017-06-13 16:28:43", "author": "@swat946", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "1165721731555889152": {"followers": "67", "datetime": "2019-08-25 20:24:53", "author": "@gbcolborne", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1013684007987490816": {"followers": "835", "datetime": "2018-07-02 07:21:15", "author": "@kantologist", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "1151037693738065921": {"followers": "6,857", "datetime": "2019-07-16 07:55:46", "author": "@rstatstweet", "content_summary": "RT @ivivek87: Interesting feed on my iPhone chrome page on #NLP based code generator using Transformer. Tool to improve my #Python skills,\u2026"}, "1165900148679094273": {"followers": "170", "datetime": "2019-08-26 08:13:51", "author": "@karsi14_", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874738712193757185": {"followers": "625", "datetime": "2017-06-13 21:22:15", "author": "@nova77t", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877170330728636417": {"followers": "13", "datetime": "2017-06-20 14:24:38", "author": "@xyc234", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "874678781201469440": {"followers": "269,218", "datetime": "2017-06-13 17:24:06", "author": "@karpathy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "948923610948763653": {"followers": "456", "datetime": "2018-01-04 14:26:33", "author": "@PerthMLGroup", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874534735653031936": {"followers": "409", "datetime": "2017-06-13 07:51:43", "author": "@AmirSaffari", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874796315871387648": {"followers": "361", "datetime": "2017-06-14 01:11:09", "author": "@jadhavamitb", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874438831738572801": {"followers": "1,627", "datetime": "2017-06-13 01:30:38", "author": "@chris_brockett", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "874440362869141505": {"followers": "7,922", "datetime": "2017-06-13 01:36:43", "author": "@jasonbaldridge", "content_summary": "A really interesting new model from Google! Not just MT: check out the parsing results & attention viz in appendix. https://t.co/mcDCEV0DU9"}, "876949740000292865": {"followers": "160,790", "datetime": "2017-06-19 23:48:05", "author": "@Quebec_AI", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874589899579572224": {"followers": "707", "datetime": "2017-06-13 11:30:55", "author": "@deco_flight", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966009212886282246": {"followers": "554", "datetime": "2018-02-20 17:58:38", "author": "@Ghosteen2", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876957029700706305": {"followers": "346", "datetime": "2017-06-20 00:17:03", "author": "@lhenault", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874595374203379712": {"followers": "11,002", "datetime": "2017-06-13 11:52:41", "author": "@data_hpz", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "875008111500644352": {"followers": "257", "datetime": "2017-06-14 15:12:45", "author": "@chajeongwon", "content_summary": "RT @golbin: RNN\uacfc CNN \uc5c6\uc774 \uc5b4\ud150\uc158 \uba54\uce74\ub2c8\uc998\ub9cc\uc73c\ub85c Seq2Seq\ub97c \uad6c\ud604\ud588\ub294\ub370, \uc18d\ub3c4\ub3c4 \uac81\ub098 \ube60\ub974\uace0 \uacb0\uacfc\ub3c4 \uc88b\uc558\ub2e4\uace0. \uc73c\uc74c.. @_@;; https://t.co/Uuy5uJ3PZR"}, "1165814162955497472": {"followers": "30", "datetime": "2019-08-26 02:32:10", "author": "@gitmurali", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1212727371142455302": {"followers": "929", "datetime": "2020-01-02 13:28:31", "author": "@wzuidema", "content_summary": "Shock 9 (2017): Know all about LSTMs now? Good, now forget all of it, as Attention Is All You Need (i.e. the Transformer). [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin https:"}, "899748997610352640": {"followers": "707", "datetime": "2017-08-21 21:44:12", "author": "@Amboinensis", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "877946874589827073": {"followers": "368", "datetime": "2017-06-22 17:50:21", "author": "@buss_jan", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "986243284568768512": {"followers": "11,406", "datetime": "2018-04-17 14:01:37", "author": "@beenwrekt", "content_summary": "Original paper for reference: https://t.co/3kJLaMM6r3"}, "1165889082288955392": {"followers": "36", "datetime": "2019-08-26 07:29:53", "author": "@gregoirebasset", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966015872195354624": {"followers": "99,939", "datetime": "2018-02-20 18:25:06", "author": "@jeremyphoward", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874663744302796801": {"followers": "58", "datetime": "2017-06-13 16:24:21", "author": "@shuvendu1roy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165966369898151937": {"followers": "12,511", "datetime": "2019-08-26 12:36:59", "author": "@suzatweet", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "903472481800826880": {"followers": "12,765", "datetime": "2017-09-01 04:20:00", "author": "@jaguring1", "content_summary": "RT @_Ryobot: \u81ea\u5df1\u6ce8\u610f\u3067\u5165\u51fa\u529b\u6587\u3092\u9023\u7d9a\u8868\u73fe\u306b\u7b26\u53f7\u5316\u3057\uff0c\u65e2\u8a33\u306e\u51fa\u529b\u6587\u3067\u30de\u30b9\u30ad\u30f3\u30b0\u3057\u306a\u304c\u3089\u7ffb\u8a33\u3059\u308b\u624b\u6cd5\u3067WMT\u82f1\u72ec/\u82f1\u4ecf\u3067SOTA\uff0e \u81ea\u5df1\u6ce8\u610f\u306e\u5165\u529bQ,K,V\u306f\u524d\u5c64\u306e\u540c\u3058\u51fa\u529b\uff08\u7ffb\u8a33\u5c64\u306fK,V\u304cEnc\u306e\u51fa\u529b\uff09\uff0ed\u3068softmax\u3067\u5185\u7a4d\u306e\u5024\u7206\u767a\u3092\u6291\u6b62\uff0e https://t.\u2026"}, "874519464452214784": {"followers": "323", "datetime": "2017-06-13 06:51:02", "author": "@dnlcrl", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "878981539282989057": {"followers": "177", "datetime": "2017-06-25 14:21:44", "author": "@3pletDad", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "877991116385144832": {"followers": "33", "datetime": "2017-06-22 20:46:09", "author": "@dimichtw", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "882871760986660864": {"followers": "181", "datetime": "2017-07-06 08:00:05", "author": "@lpadukana", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876776072842539009": {"followers": "771", "datetime": "2017-06-19 12:18:00", "author": "@aCraigPfeifer", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874799331580280833": {"followers": "25", "datetime": "2017-06-14 01:23:08", "author": "@jiahui_du", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874951537017180161": {"followers": "66", "datetime": "2017-06-14 11:27:56", "author": "@vadim__uvarov", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874968899959181313": {"followers": "1,667", "datetime": "2017-06-14 12:36:56", "author": "@Scaled_Wurm", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "876635317486313472": {"followers": "4,019", "datetime": "2017-06-19 02:58:41", "author": "@ballforest", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876623900930592769": {"followers": "2,673", "datetime": "2017-06-19 02:13:19", "author": "@mosko_mule", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "875378355708637185": {"followers": "4,643", "datetime": "2017-06-15 15:43:58", "author": "@walterdebrouwer", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876606477162565633": {"followers": "2,067", "datetime": "2017-06-19 01:04:05", "author": "@yo_ehara", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874500236307189760": {"followers": "12,955", "datetime": "2017-06-13 05:34:38", "author": "@deliprao", "content_summary": "The new Transformer Network looks deceptively simple for producing SOTA in MT. https://t.co/7otNrERtSH https://t.co/mkGqPin21l"}, "874515646129647616": {"followers": "88", "datetime": "2017-06-13 06:35:52", "author": "@hugeiezzy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1215007211635630081": {"followers": "45", "datetime": "2020-01-08 20:27:47", "author": "@iamshnik", "content_summary": "Today, I got a chance to teach my fellow mates the transformer architecture and the attention mechanisms mentioned in the paper https://t.co/sH6rbmhFV9 and I think I did it pretty well. The feeling of satisfaction I had was amazing. Teaching is such a joy!"}, "874638570690138116": {"followers": "227", "datetime": "2017-06-13 14:44:19", "author": "@dpvalente", "content_summary": "This is really interesting: https://t.co/3cHQ1lS76s"}, "874680504334921728": {"followers": "408", "datetime": "2017-06-13 17:30:57", "author": "@barselona_59", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874495215314255872": {"followers": "2", "datetime": "2017-06-13 05:14:41", "author": "@_jonmorton", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877001941204729856": {"followers": "123", "datetime": "2017-06-20 03:15:31", "author": "@Schrater_Lab", "content_summary": "Attention alone without encoder decoder https://t.co/oAHAfNJDGp"}, "899923311513096192": {"followers": "2,456", "datetime": "2017-08-22 09:16:51", "author": "@jinbeizame007", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "878044036019306496": {"followers": "4,894", "datetime": "2017-06-23 00:16:26", "author": "@nakaZAWAHIDEKI", "content_summary": "RT @kanair: I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mH\u2026"}, "874596610264649728": {"followers": "317", "datetime": "2017-06-13 11:57:35", "author": "@MathWei", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117012099379671040": {"followers": "340", "datetime": "2019-04-13 10:30:12", "author": "@Cedias_", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874710841756483584": {"followers": "289", "datetime": "2017-06-13 19:31:30", "author": "@444shimo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874609834351403009": {"followers": "7,376", "datetime": "2017-06-13 12:50:08", "author": "@ericflo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "880355605243990016": {"followers": "1,626", "datetime": "2017-06-29 09:21:47", "author": "@bneyshabur", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874503480085491713": {"followers": "61", "datetime": "2017-06-13 05:47:31", "author": "@albelwu", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "876755562242859008": {"followers": "1,073", "datetime": "2017-06-19 10:56:30", "author": "@terashimahiroki", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "875012019195224065": {"followers": "86", "datetime": "2017-06-14 15:28:16", "author": "@JADORE801120", "content_summary": "Position Encoding dimension values in \u300aAttention is All You Need\u300b https://t.co/4OjKRhhY7L https://t.co/7Px0bH60yt"}, "966058811982319618": {"followers": "1,280", "datetime": "2018-02-20 21:15:43", "author": "@deeppomf", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "948819930719256577": {"followers": "18,250", "datetime": "2018-01-04 07:34:34", "author": "@hillbig", "content_summary": "To collect non-local information in feature map, self-attention is promising because it can learn a flexible connection and can be performed in parallel. Non-local NN, which is a generalization of self-attention is also promising. https://t.co/0g29WJkdtt h"}, "899628823662141442": {"followers": "286", "datetime": "2017-08-21 13:46:40", "author": "@shunsuke_sasaki", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "874671097404370945": {"followers": "301", "datetime": "2017-06-13 16:53:34", "author": "@relopezbriega", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874499034215571457": {"followers": "412", "datetime": "2017-06-13 05:29:51", "author": "@deanofthewebb", "content_summary": "\ud83d\ude31 I just LEARNED recurrence \ud83d\ude02 \"the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\" https://t.co/lUXb9xY86O"}, "877384970968469505": {"followers": "1,416", "datetime": "2017-06-21 04:37:32", "author": "@Rajesh23MD", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874571051988070401": {"followers": "76", "datetime": "2017-06-13 10:16:02", "author": "@undo76", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1000268531311751168": {"followers": "24", "datetime": "2018-05-26 06:52:56", "author": "@megulo25", "content_summary": "https://t.co/6adT1oqt9y"}, "876886557487058944": {"followers": "1,627", "datetime": "2017-06-19 19:37:01", "author": "@chris_brockett", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876060887030800384": {"followers": "330", "datetime": "2017-06-17 12:56:06", "author": "@piyushkaul2", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "877388794667954177": {"followers": "103", "datetime": "2017-06-21 04:52:44", "author": "@avmoldovan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874606900918145026": {"followers": "227", "datetime": "2017-06-13 12:38:29", "author": "@hhsecond", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1166046927873441792": {"followers": "28", "datetime": "2019-08-26 17:57:06", "author": "@iriya07", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874602163116408832": {"followers": "27", "datetime": "2017-06-13 12:19:39", "author": "@vcrsoft", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "902456840319107072": {"followers": "1,055", "datetime": "2017-08-29 09:04:12", "author": "@KassandraPari", "content_summary": "#LINGUISTICS Attention Is All You Need https://t.co/0kf3trVPuK https://t.co/QBkfkAz2E4"}, "966100730959429633": {"followers": "115", "datetime": "2018-02-21 00:02:18", "author": "@viirya", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "899759371952963584": {"followers": "1,448", "datetime": "2017-08-21 22:25:25", "author": "@dicekicker", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "874825696157331456": {"followers": "65", "datetime": "2017-06-14 03:07:54", "author": "@orsonady", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874974638459297793": {"followers": "301", "datetime": "2017-06-14 12:59:44", "author": "@relopezbriega", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877340594594557952": {"followers": "3,500", "datetime": "2017-06-21 01:41:12", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "874594350092738565": {"followers": "1,719", "datetime": "2017-06-13 11:48:36", "author": "@bjh_ip", "content_summary": "Just when I was beginning to get to grips w/ seq2seq models, turns out you can possibly drop the RNNs for attention layers. Need to digest. https://t.co/5zTvjsHNrO"}, "874690384114352128": {"followers": "861", "datetime": "2017-06-13 18:10:13", "author": "@ryf_feed", "content_summary": "Attention Is All You Need https://t.co/Gw9Y8ROLRO"}, "903492675806011398": {"followers": "44,640", "datetime": "2017-09-01 05:40:14", "author": "@bigdata", "content_summary": "Transformer from @googleresearch outperforms both RNN & CNN models on English\u21e2German & English\u21e2French translation https://t.co/ptKU46GAvB https://t.co/3IZmGWIcWy"}, "1121772207175163905": {"followers": "1,398", "datetime": "2019-04-26 13:45:10", "author": "@mei_tokuta", "content_summary": "\u6700\u8fd1\u77e5\u3063\u305f\u8ad6\u6587(\u767a\u8868\u306f2\u5e74\u3082\u524d\u306a\u3093\u3060\u3051\u3069)\u3001\u30bf\u30a4\u30c8\u30eb\u304c\u6700\u9ad8\u3059\u304e\u3066\u7b11\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/mR7kst88HD"}, "874953826247290880": {"followers": "53", "datetime": "2017-06-14 11:37:02", "author": "@lerouxrgd", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1085136798051913728": {"followers": "112", "datetime": "2019-01-15 11:29:08", "author": "@hajjihi", "content_summary": "RT @data_topology: Attention \u2013 a concept to improve the performance of neural machine translation applications. Transformer \u2013 a LM that us\u2026"}, "875218739867795457": {"followers": "366", "datetime": "2017-06-15 05:09:43", "author": "@therevoltingx", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "966110861646471175": {"followers": "664", "datetime": "2018-02-21 00:42:33", "author": "@crude2refined", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876867383620775939": {"followers": "561", "datetime": "2017-06-19 18:20:50", "author": "@menace_", "content_summary": "I read this paper yesterday. https://t.co/IVCa3CxdKI Then read twitter comments about it today....Wut? TBH I was just thinking matrices... https://t.co/qQEGKeT1yb"}, "876772797200162816": {"followers": "52", "datetime": "2017-06-19 12:04:59", "author": "@hoangcuong0605", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874662458807521285": {"followers": "734", "datetime": "2017-06-13 16:19:15", "author": "@FsMatt", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877335858004672513": {"followers": "395", "datetime": "2017-06-21 01:22:23", "author": "@zuxfoucault", "content_summary": "RT @v_vashishta: #Google #DeepLearning - Attention Is All You Need https://t.co/Bjxj2dYXxw #MachineLearning"}, "874547789212123137": {"followers": "4,255", "datetime": "2017-06-13 08:43:35", "author": "@weballergy", "content_summary": "The Transformer network architecture, based on attention mechanisms. #deeplearning https://t.co/GOhXoQ1QDG"}, "1249170083110412289": {"followers": "396", "datetime": "2020-04-12 02:58:50", "author": "@tamara_bozovic", "content_summary": "Idea: using #MachineLearning to analyse people's perceptions of their streets post #lockdown from tweets. Challenge: understanding *how* we get computers to understand subtleties of language. My brain hurts. https://t.co/dHzm5yyu8P https://t.co/MzYam6rSZ"}, "876954653178683394": {"followers": "13", "datetime": "2017-06-20 00:07:37", "author": "@xyc234", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876784005127032832": {"followers": "79", "datetime": "2017-06-19 12:49:31", "author": "@systemdirector", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "875553878795665408": {"followers": "146", "datetime": "2017-06-16 03:21:26", "author": "@datagrokker", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "887559229653037056": {"followers": "1,938", "datetime": "2017-07-19 06:26:25", "author": "@EldarSilver", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874443154807603200": {"followers": "1,336", "datetime": "2017-06-13 01:47:49", "author": "@jigarkdoshi", "content_summary": "No more RNN or CNN only attention. SOTA on machine translation. The game got spicy! https://t.co/wib3PdV36E"}, "876774946990690305": {"followers": "292", "datetime": "2017-06-19 12:13:31", "author": "@sam_remedios", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966010025255645184": {"followers": "48", "datetime": "2018-02-20 18:01:52", "author": "@hagellar", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874637937257975808": {"followers": "593", "datetime": "2017-06-13 14:41:48", "author": "@rasmusbergpalm", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874761019184340992": {"followers": "778", "datetime": "2017-06-13 22:50:53", "author": "@pommedeterre33", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874521583234170880": {"followers": "298", "datetime": "2017-06-13 06:59:27", "author": "@arishabh8", "content_summary": "Amazing !!!! https://t.co/RMndpjOwoz"}, "1098872148632920066": {"followers": "6,190", "datetime": "2019-02-22 09:08:30", "author": "@abigail_e_see", "content_summary": "@abhish_eksharma @ashVaswani @stanfordnlp I agree that Ashish is great but he is just one of 8 equal-contribution authors listed on the original Transformers paper! https://t.co/ls1LTVh1M6"}, "1165702603134722052": {"followers": "112", "datetime": "2019-08-25 19:08:52", "author": "@gauravontwit", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165862045100589057": {"followers": "22", "datetime": "2019-08-26 05:42:26", "author": "@midin", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874504165652910080": {"followers": "1,186", "datetime": "2017-06-13 05:50:15", "author": "@Tbeltramelli", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874727514467319808": {"followers": "632", "datetime": "2017-06-13 20:37:45", "author": "@vambati", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "1038689830794743808": {"followers": "150,393", "datetime": "2018-09-09 07:25:28", "author": "@bact", "content_summary": "RT @kobkrit: Attention is all you need. https://t.co/q7tvjCR5zk https://t.co/q7tvjCR5zk"}, "1040256677298286592": {"followers": "1,088", "datetime": "2018-09-13 15:11:33", "author": "@rosstaylor90", "content_summary": "@richardcraib Attention is All You Need https://t.co/RM8hDs4NRG"}, "874638321397485569": {"followers": "395", "datetime": "2017-06-13 14:43:20", "author": "@qiming82", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874633972608839682": {"followers": "443", "datetime": "2017-06-13 14:26:03", "author": "@ivenzor", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876922825105432578": {"followers": "1,325", "datetime": "2017-06-19 22:01:08", "author": "@rogerkmoore", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874731935708270592": {"followers": "335", "datetime": "2017-06-13 20:55:19", "author": "@kobi78", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1014019831744663553": {"followers": "720", "datetime": "2018-07-03 05:35:42", "author": "@Ait360", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "874542336931487744": {"followers": "431", "datetime": "2017-06-13 08:21:55", "author": "@blauigris", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876608441501982720": {"followers": "15,247", "datetime": "2017-06-19 01:11:53", "author": "@noriomurakami", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874784701646422016": {"followers": "124", "datetime": "2017-06-14 00:25:00", "author": "@ragi256", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "907213495997026304": {"followers": "222", "datetime": "2017-09-11 12:05:27", "author": "@Urk0", "content_summary": "I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "1184535950669635585": {"followers": "186", "datetime": "2019-10-16 18:25:52", "author": "@anyanikd", "content_summary": "The inflections that brought #NLP out of winter. #attention is all you need. #context #Transformers https://t.co/kCXjsluFgk"}, "876823251384623104": {"followers": "11,787", "datetime": "2017-06-19 15:25:28", "author": "@yutakashino", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874625848283983872": {"followers": "3,414", "datetime": "2017-06-13 13:53:46", "author": "@alxndrkalinin", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876773185613701121": {"followers": "84", "datetime": "2017-06-19 12:06:31", "author": "@UrbanProtag", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "875701586932125696": {"followers": "221", "datetime": "2017-06-16 13:08:22", "author": "@ScriptShade", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874578178651672576": {"followers": "1,777", "datetime": "2017-06-13 10:44:21", "author": "@AngelRapsody", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874679812933255174": {"followers": "1,666", "datetime": "2017-06-13 17:28:12", "author": "@victorgreiff", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874799176537657344": {"followers": "63", "datetime": "2017-06-14 01:22:31", "author": "@kuanchen22", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874494556313600002": {"followers": "59", "datetime": "2017-06-13 05:12:04", "author": "@liqiangniu", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874839692457410560": {"followers": "183", "datetime": "2017-06-14 04:03:31", "author": "@Shuping_Ruan", "content_summary": "\ud83d\ude02 https://t.co/1tN90R3zYE"}, "966177940751953920": {"followers": "37", "datetime": "2018-02-21 05:09:06", "author": "@krishnkant98", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "924104057731420160": {"followers": "76", "datetime": "2017-10-28 02:42:31", "author": "@jithsjoy", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874679037821505536": {"followers": "8,130", "datetime": "2017-06-13 17:25:08", "author": "@lurino", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876895153180508164": {"followers": "16", "datetime": "2017-06-19 20:11:11", "author": "@AI_News_Today", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874778013895802880": {"followers": "8", "datetime": "2017-06-13 23:58:25", "author": "@mh_ha_soar", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "948875132482723841": {"followers": "1,134", "datetime": "2018-01-04 11:13:55", "author": "@koburouze845", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874866266820968448": {"followers": "63", "datetime": "2017-06-14 05:49:06", "author": "@fkariminejadasl", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874614100860571648": {"followers": "238", "datetime": "2017-06-13 13:07:05", "author": "@xavysp", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966008549724733440": {"followers": "913", "datetime": "2018-02-20 17:56:00", "author": "@SpaceAnubis", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874950346086510592": {"followers": "126", "datetime": "2017-06-14 11:23:12", "author": "@truskovskiy", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876890592499912704": {"followers": "88", "datetime": "2017-06-19 19:53:03", "author": "@hugeiezzy", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874434336484884480": {"followers": "1,025", "datetime": "2017-06-13 01:12:46", "author": "@desertnaut", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "874745911230025728": {"followers": "74", "datetime": "2017-06-13 21:50:51", "author": "@ljastrzebski", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1166001804250931200": {"followers": "76", "datetime": "2019-08-26 14:57:48", "author": "@me7mmad", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966008654888611841": {"followers": "161", "datetime": "2018-02-20 17:56:25", "author": "@farhanhubble", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "924681833433976836": {"followers": "12", "datetime": "2017-10-29 16:58:23", "author": "@A__Kruglov", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "907214101897732096": {"followers": "1,341", "datetime": "2017-09-11 12:07:51", "author": "@emnlp2017", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "876643743620710400": {"followers": "149", "datetime": "2017-06-19 03:32:10", "author": "@to_aruchan", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874541500545323008": {"followers": "1,876", "datetime": "2017-06-13 08:18:36", "author": "@MannyKayy", "content_summary": "RT @arxiv_flying: #NIPS2017 Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/ZoYQ68Vy3l"}, "874557420835545088": {"followers": "155", "datetime": "2017-06-13 09:21:52", "author": "@Trtd6Trtd", "content_summary": "RT @_Ryobot: \u81ea\u5df1\u6ce8\u610f\u3067\u5165\u51fa\u529b\u6587\u3092\u9023\u7d9a\u8868\u73fe\u306b\u7b26\u53f7\u5316\u3057\uff0c\u65e2\u8a33\u306e\u51fa\u529b\u6587\u3067\u30de\u30b9\u30ad\u30f3\u30b0\u3057\u306a\u304c\u3089\u7ffb\u8a33\u3059\u308b\u624b\u6cd5\u3067WMT\u82f1\u72ec/\u82f1\u4ecf\u3067SOTA\uff0e \u81ea\u5df1\u6ce8\u610f\u306e\u5165\u529bQ,K,V\u306f\u524d\u5c64\u306e\u540c\u3058\u51fa\u529b\uff08\u7ffb\u8a33\u5c64\u306fK,V\u304cEnc\u306e\u51fa\u529b\uff09\uff0ed\u3068softmax\u3067\u5185\u7a4d\u306e\u5024\u7206\u767a\u3092\u6291\u6b62\uff0e https://t.\u2026"}, "1165978635792199680": {"followers": "66", "datetime": "2019-08-26 13:25:44", "author": "@diegovogeid", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874494103869825025": {"followers": "4,078", "datetime": "2017-06-13 05:10:16", "author": "@blackenedgold", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165771444791304192": {"followers": "165", "datetime": "2019-08-25 23:42:26", "author": "@plsang", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874568793732853760": {"followers": "419", "datetime": "2017-06-13 10:07:03", "author": "@nik0spapp", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875173777042137088": {"followers": "2,290", "datetime": "2017-06-15 02:11:03", "author": "@fronori", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874489431868096513": {"followers": "361", "datetime": "2017-06-13 04:51:42", "author": "@jadhavamitb", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874508965895299076": {"followers": "2,176", "datetime": "2017-06-13 06:09:19", "author": "@wednesdaymuse", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "1165254684162043906": {"followers": "103", "datetime": "2019-08-24 13:29:00", "author": "@M_Sherafati", "content_summary": "\u062f\u0631 \u0645\u0648\u0631\u062f \u062a\u0631\u0646\u0633\u0641\u0648\u0631\u0645\u0631 \u0648 \u0627\u062a\u0646\u0634\u0646 \u062e\u0648\u0646\u062f\u0645 \u0648 \u062d\u0642\u06cc\u0642\u062a\u0627 \u0628\u0631\u06af\u0627\u0645! \u0686\u0647 \u0645\u06cc\u200c\u06a9\u0646\u0646\u062f \u062e\u062f\u0627\u06cc\u06cc! https://t.co/cvYfk45E94"}, "875059118805053440": {"followers": "5,813", "datetime": "2017-06-14 18:35:26", "author": "@LiltHQ", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "907234303662284800": {"followers": "126", "datetime": "2017-09-11 13:28:08", "author": "@pedroKarudoso", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "949097904924315648": {"followers": "174", "datetime": "2018-01-05 01:59:08", "author": "@takabsd", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "876632204004139008": {"followers": "36", "datetime": "2017-06-19 02:46:19", "author": "@aimpast", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "1117122118829785088": {"followers": "111", "datetime": "2019-04-13 17:47:22", "author": "@marcoleewow", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874436639464058880": {"followers": "733", "datetime": "2017-06-13 01:21:55", "author": "@Pythonner", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "875192937591734276": {"followers": "159", "datetime": "2017-06-15 03:27:11", "author": "@rgem52", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874633917332045824": {"followers": "218", "datetime": "2017-06-13 14:25:50", "author": "@AssistedEvolve", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1117110376741601280": {"followers": "955", "datetime": "2019-04-13 17:00:43", "author": "@NicolasGUTOWSKI", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "877459473857732610": {"followers": "2,048", "datetime": "2017-06-21 09:33:35", "author": "@LuisB", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874443752470855681": {"followers": "13", "datetime": "2017-06-13 01:50:11", "author": "@Lawrencelj", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899563069302489088": {"followers": "1,647", "datetime": "2017-08-21 09:25:23", "author": "@Qhapaq_49", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "876901392782204928": {"followers": "186", "datetime": "2017-06-19 20:35:58", "author": "@nkmry_", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1232621739890352129": {"followers": "222", "datetime": "2020-02-26 11:01:38", "author": "@PapersTrending", "content_summary": "[8/10] \ud83d\udcc8 - Attention Is All You Need - 386 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "874815148036247552": {"followers": "2,442", "datetime": "2017-06-14 02:25:59", "author": "@nicklovescode", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "877383614022819840": {"followers": "353", "datetime": "2017-06-21 04:32:09", "author": "@sadiqmmm", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874609282599223296": {"followers": "1,608", "datetime": "2017-06-13 12:47:57", "author": "@dartdog", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876949939946835968": {"followers": "4", "datetime": "2017-06-19 23:48:53", "author": "@lonnyxiang", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "966124996039909376": {"followers": "216", "datetime": "2018-02-21 01:38:43", "author": "@uk1889", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874658755912155136": {"followers": "261", "datetime": "2017-06-13 16:04:32", "author": "@GuptaRajat033", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1166205595109773317": {"followers": "287", "datetime": "2019-08-27 04:27:35", "author": "@SPHIX_1", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "882965546496434178": {"followers": "858", "datetime": "2017-07-06 14:12:45", "author": "@panderson_me", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874994488866783237": {"followers": "139", "datetime": "2017-06-14 14:18:37", "author": "@kanugeete", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "899758810645934080": {"followers": "458", "datetime": "2017-08-21 22:23:11", "author": "@kouki_outstand", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "875671505434234880": {"followers": "173", "datetime": "2017-06-16 11:08:50", "author": "@matt_corkum", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874984496910565377": {"followers": "216", "datetime": "2017-06-14 13:38:55", "author": "@KSKSKSKS2", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "987197682627293184": {"followers": "409", "datetime": "2018-04-20 05:14:03", "author": "@AlexanderBresk", "content_summary": "RT @MachineRockstar: Ein Paper zum Thema Autoencoder im Bereich #NLP? Lest \"Attention Is All You Need\" von Ashish Vaswani et al. Viel Spa\u00df\u2026"}, "876781011408793600": {"followers": "143", "datetime": "2017-06-19 12:37:37", "author": "@snehaSM", "content_summary": "RT @memotv: @_vade @kcimc @quasimondo @genekogan @pkmital V interesting. Just last week same authors said \"attention is all you need\"! :)\u2026"}, "1138863539261755392": {"followers": "992", "datetime": "2019-06-12 17:40:01", "author": "@datahack_", "content_summary": "@JJavierMoralo, de @datahack_, en el #InnodataBCN. El modelo de speech to text de datahack, se basa en el uso de la arquitectura \u201cthe Tranformer\u201d, desarrollada por Google en 2017 https://t.co/VJqZH04Tbb https://t.co/BRXpZDwQwz"}, "874428300654923776": {"followers": "322", "datetime": "2017-06-13 00:48:47", "author": "@letranger14", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "1098956702139875328": {"followers": "180", "datetime": "2019-02-22 14:44:30", "author": "@sriharshams", "content_summary": "RT @willkurt: Finally worked through \"Attention is all you need\". My recommended steps: - Skim the paper: https://t.co/iCNGDeJ62q - Step th\u2026"}, "874635840114147329": {"followers": "205", "datetime": "2017-06-13 14:33:28", "author": "@chitchattoe", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165961242550444032": {"followers": "1,151", "datetime": "2019-08-26 12:16:37", "author": "@williamxlr", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "894715901408288768": {"followers": "3,897", "datetime": "2017-08-08 00:24:28", "author": "@chakapokochin", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876894632541474817": {"followers": "43", "datetime": "2017-06-19 20:09:06", "author": "@amitnavindgi", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "966218805817966592": {"followers": "30", "datetime": "2018-02-21 07:51:29", "author": "@lokeshsonii", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1116901612969054209": {"followers": "13,638", "datetime": "2019-04-13 03:11:10", "author": "@aureliengeron", "content_summary": "In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding. This allows the model to know the word positions, both absolute and relative. Here's a diagram to understand how it works. \ud83e\udd8e https://t.c"}, "875141193092317184": {"followers": "7,942", "datetime": "2017-06-15 00:01:34", "author": "@angsuman", "content_summary": "Deep Neural Networks \u2013 Attention is all you need (No recurrence or convolutions) https://t.co/Tfs992stGA"}, "880533202376458240": {"followers": "3", "datetime": "2017-06-29 21:07:29", "author": "@galactica147", "content_summary": "lol https://t.co/AGS376hxAf"}, "875130478709227520": {"followers": "139", "datetime": "2017-06-14 23:18:59", "author": "@ox0spy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874495025987461120": {"followers": "592", "datetime": "2017-06-13 05:13:56", "author": "@pablete", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874531701518729216": {"followers": "4,804", "datetime": "2017-06-13 07:39:40", "author": "@cto_movidius", "content_summary": "@Miles_Brundage @hardmaru @GoogleBrain Busy week between attention networks https://t.co/gI4a2k7n61, SELU https://t.co/utp0GPYTB6 and CortexNet https://t.co/f9lJVESYkS"}, "874510313302794240": {"followers": "103", "datetime": "2017-06-13 06:14:40", "author": "@RahulKumaresan", "content_summary": "Really good paper. https://t.co/gHqbfEyfW1"}, "1232259299998674946": {"followers": "222", "datetime": "2020-02-25 11:01:26", "author": "@PapersTrending", "content_summary": "[2/10] \ud83d\udcc8 - Attention Is All You Need - 364 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "877325486824710145": {"followers": "3,500", "datetime": "2017-06-21 00:41:10", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudrC28"}, "876939503293784065": {"followers": "563", "datetime": "2017-06-19 23:07:24", "author": "@Omair28Khan", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874781918461435904": {"followers": "4,054", "datetime": "2017-06-14 00:13:56", "author": "@chezou", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874703158500630528": {"followers": "160,790", "datetime": "2017-06-13 19:00:58", "author": "@Quebec_AI", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "967404882507345920": {"followers": "4,314", "datetime": "2018-02-24 14:24:32", "author": "@srisatish", "content_summary": "Attention Is All You Need (ie, all you need is love) https://t.co/PavOPPZvdp"}, "1168029071953092610": {"followers": "24", "datetime": "2019-09-01 05:13:26", "author": "@KamiCule", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875523311706316800": {"followers": "34", "datetime": "2017-06-16 01:19:58", "author": "@reegansri", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "899555820311937024": {"followers": "2,043", "datetime": "2017-08-21 08:56:35", "author": "@dosei_sanga", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "877143343544897536": {"followers": "165", "datetime": "2017-06-20 12:37:24", "author": "@Yakima_TUG", "content_summary": "RT @dataandme: So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #mach\u2026"}, "874979533765693440": {"followers": "56,127", "datetime": "2017-06-14 13:19:11", "author": "@IntelligenceTV", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874607734477529088": {"followers": "1,328", "datetime": "2017-06-13 12:41:47", "author": "@asayeed", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1117917265075343361": {"followers": "490", "datetime": "2019-04-15 22:27:00", "author": "@ST4Good", "content_summary": "If you are into #MachineLearning and #NLP you have necessarily heard about Transformers. Here is a nice explanation about how it actually workers."}, "877213593174802433": {"followers": "1,239", "datetime": "2017-06-20 17:16:33", "author": "@ipnosimmia", "content_summary": "RT @dataandme: So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #mach\u2026"}, "874600462447767552": {"followers": "139", "datetime": "2017-06-13 12:12:54", "author": "@JoshuahTouyz", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "885141024976248833": {"followers": "471", "datetime": "2017-07-12 14:17:20", "author": "@daytb_twy", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "877040119617921024": {"followers": "15,645", "datetime": "2017-06-20 05:47:13", "author": "@pmedina", "content_summary": "RT @DeepLearningNow: New #deeplearning paper https://t.co/qwZnM1IVje https://t.co/zQUSKLhk5D"}, "875005628657397760": {"followers": "25,311", "datetime": "2017-06-14 15:02:53", "author": "@deeplearning4j", "content_summary": "RT @iamtrask: in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://\u2026"}, "876729268549677056": {"followers": "145", "datetime": "2017-06-19 09:12:01", "author": "@esvhd", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1233584260587704320": {"followers": "45,424", "datetime": "2020-02-29 02:46:21", "author": "@VanRijmenam", "content_summary": "RT @DimejiMudele: \"Attention is all you need\" is probably the most important scientific publication in Deep learning since 2017. Here you\u2026"}, "908095390037278720": {"followers": "1,871", "datetime": "2017-09-13 22:29:47", "author": "@born2data", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "879271798080143361": {"followers": "6,289", "datetime": "2017-06-26 09:35:07", "author": "@avtosamara", "content_summary": "#\u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0442\u043e\u0440\u0441\u043a\u043e\u0435 Attention Is All You Need https://t.co/dH4VoS0QJZ"}, "874596471496073216": {"followers": "1,667", "datetime": "2017-06-13 11:57:02", "author": "@Scaled_Wurm", "content_summary": "RT @joyport: \u672c\u6765RNN\u3084CNN\u306a\u3069\u304c\u5fc5\u8981\u3068\u601d\u308f\u308c\u3066\u304d\u305f\u30bf\u30b9\u30af\u304cAttention\u306e\u307f\u3067\u89e3\u6c7a\u3067\u304d\u308b\uff1f\u7d50\u5c40\u4f55\u3092\u3084\u3063\u3066\u3044\u308b\u3093\u3060\u308d\u3046\u30fb\u30fb\u30fb\u3002 https://t.co/DCe0je3Qxh"}, "993377391421640704": {"followers": "8,952", "datetime": "2018-05-07 06:30:00", "author": "@Deep_In_Depth", "content_summary": "arXiv - Attention Is All You Need https://t.co/9ixw38rf7T"}, "874730659318710272": {"followers": "335", "datetime": "2017-06-13 20:50:15", "author": "@kobi78", "content_summary": "RT @golbin: RNN\uacfc CNN \uc5c6\uc774 \uc5b4\ud150\uc158 \uba54\uce74\ub2c8\uc998\ub9cc\uc73c\ub85c Seq2Seq\ub97c \uad6c\ud604\ud588\ub294\ub370, \uc18d\ub3c4\ub3c4 \uac81\ub098 \ube60\ub974\uace0 \uacb0\uacfc\ub3c4 \uc88b\uc558\ub2e4\uace0. \uc73c\uc74c.. @_@;; https://t.co/Uuy5uJ3PZR"}, "877691520161169409": {"followers": "8", "datetime": "2017-06-22 00:55:39", "author": "@GeorgeZouSQ", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165817641899982848": {"followers": "99", "datetime": "2019-08-26 02:46:00", "author": "@rabikspace", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165864309630676992": {"followers": "467", "datetime": "2019-08-26 05:51:26", "author": "@CarmenVdeC", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1166032564437409792": {"followers": "2,283", "datetime": "2019-08-26 17:00:01", "author": "@govindsyadav", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874888832679903234": {"followers": "65", "datetime": "2017-06-14 07:18:47", "author": "@jabot", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875059055819202560": {"followers": "3,596", "datetime": "2017-06-14 18:35:11", "author": "@BBVAData", "content_summary": "Attention is all you need \u2014 https://t.co/RHNZ1Pgloq #ComputerScience"}, "874747213691445248": {"followers": "594", "datetime": "2017-06-13 21:56:02", "author": "@jorgelbgm", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "891793851928559616": {"followers": "322", "datetime": "2017-07-30 22:53:17", "author": "@letranger14", "content_summary": "RT @jekbradbury: @haldaume3 residual layers (b/c https://t.co/68EzsEnOs3), self-attention (b/c https://t.co/R9WHZokiRI) and define-by-run (\u2026"}, "1165732186907004928": {"followers": "4,712", "datetime": "2019-08-25 21:06:26", "author": "@OsowoIAM", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1167127611081801729": {"followers": "302", "datetime": "2019-08-29 17:31:21", "author": "@subhobrata1", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "948850603093979136": {"followers": "477", "datetime": "2018-01-04 09:36:27", "author": "@yasuokajihei", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "1117094204218331136": {"followers": "113", "datetime": "2019-04-13 15:56:27", "author": "@isaiahtaguibao", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "938588966382481408": {"followers": "755", "datetime": "2017-12-07 02:00:22", "author": "@M157q_News_RSS", "content_summary": "Attention Is All You Need. (arXiv:1706.03762v5 [cs.CL] UPDATED) https://t.co/XJCK9sHpZ4 The dominant sequence transduction models are based"}, "1176223193566261250": {"followers": "694", "datetime": "2019-09-23 19:53:57", "author": "@santty128", "content_summary": "@ParcolletT @_josh_meyer_ For more ref: Vaswani et al. 2017 (https://t.co/L3jcINM4Tk) Table 1 layer complexity of self-attention."}, "1116201212586577920": {"followers": "72", "datetime": "2019-04-11 04:48:01", "author": "@tono46420310", "content_summary": "Transformer\u306e\u3081\u3063\u3061\u3083\u826f\u3044\u89e3\u8aac\u3002 https://t.co/pkj6seUEJc \u8ad6\u6587( https://t.co/KPaaQ0kG90 )\u306e\u524d\u306b\u3053\u306e\u8a18\u4e8b\u304b\u3089\u8aad\u3080\u306e\u304c\u304a\u3059\u3059\u3081\u3002"}, "1165698801132343297": {"followers": "1,263", "datetime": "2019-08-25 18:53:46", "author": "@sudodoki", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874582342777671680": {"followers": "487", "datetime": "2017-06-13 11:00:54", "author": "@mmourafiq", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1093538324181073920": {"followers": "992", "datetime": "2019-02-07 15:53:48", "author": "@datahack_", "content_summary": "@JJavierMoralo, en el #CBigData. El modelo de speech to text de @datahack_, se basa en el uso de la arquitectura \u201cthe Tranformer\u201d, desarrollada por Google en 2017. M\u00e1s info: https://t.co/VJqZH04Tbb https://t.co/9H7TaMKHNj"}, "874558934819807232": {"followers": "3,005", "datetime": "2017-06-13 09:27:53", "author": "@syoyo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165717902911299584": {"followers": "446", "datetime": "2019-08-25 20:09:40", "author": "@benrozemberczki", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966125180237082629": {"followers": "12", "datetime": "2018-02-21 01:39:27", "author": "@bartoldson", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876640668092727297": {"followers": "1,942", "datetime": "2017-06-19 03:19:57", "author": "@GiorgioPatrini", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1155835449199349762": {"followers": "143", "datetime": "2019-07-29 13:40:20", "author": "@machine_neko", "content_summary": "#100DaysOfCode Day74 - 1h / Total 171.5h \u27a1\ufe0f\u3084\u3063\u305f\u4e8b \u3000\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u6d41\u884c\u308a\u7406\u89e3\uff1aAttention\u3084Transformer\u3001BERT\u306a\u3069 https://t.co/hBbRF6fvKv \u27a1\ufe0f\u3084\u308b\u4e8b \u30fbKaggle(\u505c\u6ede\u4e2d) \u30fb\u81ea\u7136\u8a00\u8a9e\u51e6\u7406"}, "1116904671480877057": {"followers": "81", "datetime": "2019-04-13 03:23:19", "author": "@Hansatyam", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874455404260057093": {"followers": "4,944", "datetime": "2017-06-13 02:36:29", "author": "@shakamunyi", "content_summary": "RT @EKplugin: \"[R] [1706.03762] Attention Is All You Need <-- Sota NMT; less compute\". Detail:https://t.co/FaydVlDRRK #MachineLearning http\u2026"}, "874426320410865664": {"followers": "3,500", "datetime": "2017-06-13 00:40:55", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "1171237995917131777": {"followers": "137", "datetime": "2019-09-10 01:44:33", "author": "@TinDan_", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026"}, "876963085076004864": {"followers": "3,500", "datetime": "2017-06-20 00:41:07", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "876645177594568704": {"followers": "1,969", "datetime": "2017-06-19 03:37:52", "author": "@kivantium", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1165725088471109632": {"followers": "6,390", "datetime": "2019-08-25 20:38:13", "author": "@mtyka", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1117148921166483458": {"followers": "456", "datetime": "2019-04-13 19:33:53", "author": "@johnny_kelsey", "content_summary": "Interesting thread on neural network architectures..."}, "874666975196102656": {"followers": "1,418", "datetime": "2017-06-13 16:37:12", "author": "@sguada", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874783474808500226": {"followers": "103", "datetime": "2017-06-14 00:20:07", "author": "@spacemanslog", "content_summary": "[R] [1706.03762] Attention Is All You Need <-- Sota NMT; less compute https://t.co/bKQkDkveLz https://t.co/8YZDpl8OXi"}, "1117336744683114496": {"followers": "53", "datetime": "2019-04-14 08:00:13", "author": "@PoQaa_cs", "content_summary": "Attention Is All You Need https://t.co/sf2MJ0D9n0 (Popularity:14.8) #Natural_language_processing #Machine_Learning"}, "966009215126032385": {"followers": "160,790", "datetime": "2018-02-20 17:58:39", "author": "@Quebec_AI", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "966187580235624448": {"followers": "1,288", "datetime": "2018-02-21 05:47:24", "author": "@heghbalz", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1177349158883082240": {"followers": "1,080", "datetime": "2019-09-26 22:28:08", "author": "@_josh_meyer_", "content_summary": "RT @_Gooofy_: The Zamia Brain project provides infrastructure useful to create natural language processing systems based on transformer net\u2026"}, "874442105854230528": {"followers": "340", "datetime": "2017-06-13 01:43:39", "author": "@Ajay_Talati", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1207277685493268480": {"followers": "656", "datetime": "2019-12-18 12:33:25", "author": "@273sn", "content_summary": "BERT\u306b\u51fa\u4f1a\u3063\u3066\u3001\u5185\u5fc3\u5c11\u3057\u99ac\u9e7f\u306b\u3057\u3066\u3044\u305fkaggle\u306e\u9762\u767d\u3055\u3082\u77e5\u308c\u305f\u3002 https://t.co/ZWFIOdEU18 <pad>\u3092mask\u3059\u308b\u306e\u304b\u3001position\u3082\u5909\u6570\u306b\u3059\u308b\u306e\u304b\uff08BERT\uff09\u5206\u304b\u3089\u306a\u3059\u304e\u308b\u304c\u9762\u767d\u3044\u3002 \u524d\u9014\u3058\u3083\u306a\u304f\u3066\u73fe\u5728\u591a\u96e3\u3002\u7d50\u8ad6\u3092\u6025\u3050\u3068\u90fd\u5e02\u3092\u8131\u51fa\u3057\u305f\u3044\u304c\u3001\u8db3\u9996\u3082\u75db\u3044\u3057\u8272\u3005\u3068\u75db\u3044orz \u6700\u9069\u5316\u3068\u77ed\u7d61\u306e\u9055\u3044\u306f\u9577\u3044\u76ee"}, "874567775678844929": {"followers": "1,581", "datetime": "2017-06-13 10:03:01", "author": "@joavanschoren", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1066269138941702144": {"followers": "824", "datetime": "2018-11-24 09:55:47", "author": "@morioka", "content_summary": "RT @shion_honda: Transformer [Vaswani+, 2017, NIPS] Self-Attention\u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u7573\u307f\u8fbc\u307f\u3084\u518d\u5e30\u7121\u3057\u306eNN\u3067\u3082\u7ffb\u8a33\u30bf\u30b9\u30af\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002 Transformer\u306f\u9577\u3044\u6642\u7cfb\u5217\u3092\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u4e0a\u306b\u3001\u4e26\u5217\u5316\u306b\u2026"}, "1117105749933662209": {"followers": "416", "datetime": "2019-04-13 16:42:20", "author": "@evan_cofer", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1165858461059403776": {"followers": "2,618", "datetime": "2019-08-26 05:28:12", "author": "@ryanpwakefield", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874609881201987584": {"followers": "175", "datetime": "2017-06-13 12:50:19", "author": "@PIlIGRIM", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874822994228637700": {"followers": "781", "datetime": "2017-06-14 02:57:09", "author": "@HrSaghir", "content_summary": "RT @graphific: Attention Is All You Need from @GoogleBrain. No RNNs or CNNs, only Attention = fast training & SOTA on WMT'14 https://t.co/Y\u2026"}, "876795560929488896": {"followers": "62", "datetime": "2017-06-19 13:35:26", "author": "@timstae_c", "content_summary": "Attention is all you need: https://t.co/ezlw18LmmF"}, "876603881098166272": {"followers": "1,667", "datetime": "2017-06-19 00:53:46", "author": "@Scaled_Wurm", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "881672636815863810": {"followers": "4,200", "datetime": "2017-07-03 00:35:11", "author": "@arxiv_cs_cl", "content_summary": "Attention Is All You Need. (arXiv:1706.03762v4 [cs.CL] UPDATED) https://t.co/8ovOiC3bdC #NLProc"}, "874425969842499585": {"followers": "4,200", "datetime": "2017-06-13 00:39:31", "author": "@arxiv_cs_cl", "content_summary": "Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/8ovOiC3bdC #NLProc"}, "876893779206307840": {"followers": "13", "datetime": "2017-06-19 20:05:43", "author": "@suesunss", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1102820805354897409": {"followers": "6", "datetime": "2019-03-05 06:39:04", "author": "@KNSI_Golem", "content_summary": "RT @LetMePetTheDog: Day 0: Revisiting Transformer architecture again, on the way to understanding the unreasonable effectiveness of BERT [\u2026"}, "874754269102166021": {"followers": "469", "datetime": "2017-06-13 22:24:04", "author": "@Ozan__Caglayan", "content_summary": "I am not sure about this type of block visualization. I find it hard to grasp at first look. Am I the only one? https://t.co/NXxH4mqcx8"}, "874444128553242627": {"followers": "146", "datetime": "2017-06-13 01:51:41", "author": "@ctphoenix", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874480487301042176": {"followers": "3,314", "datetime": "2017-06-13 04:16:09", "author": "@AndySugs", "content_summary": "RT: (ilblackdragon)Paper \"Attention Is All You Need\" is now out - https://t.co/GrjCnDPNeE #MachineLearning #NLU #machinetranslation #dee\u2026"}, "874449580586401792": {"followers": "17", "datetime": "2017-06-13 02:13:21", "author": "@andybaoxv", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874658731446882304": {"followers": "261", "datetime": "2017-06-13 16:04:26", "author": "@GuptaRajat033", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117111103270588417": {"followers": "77", "datetime": "2019-04-13 17:03:36", "author": "@epictwitty", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874822204843855872": {"followers": "7,590", "datetime": "2017-06-14 02:54:01", "author": "@graphific", "content_summary": "Attention Is All You Need from @GoogleBrain. No RNNs or CNNs, only Attention = fast training & SOTA on WMT'14 https://t.co/Y9diEXBcTE #arxiv https://t.co/lhmoQ8xgsy"}, "1165767524048363520": {"followers": "595", "datetime": "2019-08-25 23:26:51", "author": "@popedaniels", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876736124588347392": {"followers": "229", "datetime": "2017-06-19 09:39:15", "author": "@ilker_kesen", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874802620350779392": {"followers": "6,824", "datetime": "2017-06-14 01:36:12", "author": "@MeganRisdal", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874506398838644736": {"followers": "3,372", "datetime": "2017-06-13 05:59:07", "author": "@optionseveryday", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877488278164865024": {"followers": "1,633", "datetime": "2017-06-21 11:28:03", "author": "@jkbren", "content_summary": "RT @kanair: I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mH\u2026"}, "1165839455447441408": {"followers": "591", "datetime": "2019-08-26 04:12:41", "author": "@ActivePort", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "881792784117760000": {"followers": "1,348", "datetime": "2017-07-03 08:32:37", "author": "@udmrzn", "content_summary": "RT @arxiv_cs_cl: Attention Is All You Need. (arXiv:1706.03762v4 [cs.CL] UPDATED) https://t.co/8ovOiC3bdC #NLProc"}, "878572554209959936": {"followers": "3,500", "datetime": "2017-06-24 11:16:34", "author": "@EnrikeLopez", "content_summary": "RT @Pedra999: All you need is Attention : Training #tensorflow - > https://t.co/fllCNmcQe5"}, "874494818033909761": {"followers": "16,454", "datetime": "2017-06-13 05:13:06", "author": "@earnmyturns", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874467189809704960": {"followers": "2,379", "datetime": "2017-06-13 03:23:19", "author": "@TerryUm_ML", "content_summary": "All you need is not love, but attention. https://t.co/ydwf2U3CdY"}, "874625782374694912": {"followers": "79,158", "datetime": "2017-06-13 13:53:30", "author": "@machinelearnbot", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1116903304188051456": {"followers": "1,182", "datetime": "2019-04-13 03:17:53", "author": "@jokoegwale", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1177337715412602886": {"followers": "210", "datetime": "2019-09-26 21:42:39", "author": "@_Gooofy_", "content_summary": "The Zamia Brain project provides infrastructure useful to create natural language processing systems based on transformer networks (see https://t.co/2D6mR6FhKp). Source code and documentation is now available here: https://t.co/VCJjBtxXIa"}, "876920633140985857": {"followers": "3,642", "datetime": "2017-06-19 21:52:25", "author": "@losnuevetoros", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "877460134619889665": {"followers": "60", "datetime": "2017-06-21 09:36:13", "author": "@friend1_ws", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165941371716493312": {"followers": "422", "datetime": "2019-08-26 10:57:39", "author": "@jcvasquezc1", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165872020288135168": {"followers": "615", "datetime": "2019-08-26 06:22:05", "author": "@mattmcd", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874859598787825669": {"followers": "125", "datetime": "2017-06-14 05:22:37", "author": "@HaesunRickyPark", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874554225946001408": {"followers": "1,055", "datetime": "2017-06-13 09:09:10", "author": "@stealthinu", "content_summary": "RT @_Ryobot: \u81ea\u5df1\u6ce8\u610f\u3067\u5165\u51fa\u529b\u6587\u3092\u9023\u7d9a\u8868\u73fe\u306b\u7b26\u53f7\u5316\u3057\uff0c\u65e2\u8a33\u306e\u51fa\u529b\u6587\u3067\u30de\u30b9\u30ad\u30f3\u30b0\u3057\u306a\u304c\u3089\u7ffb\u8a33\u3059\u308b\u624b\u6cd5\u3067WMT\u82f1\u72ec/\u82f1\u4ecf\u3067SOTA\uff0e \u81ea\u5df1\u6ce8\u610f\u306e\u5165\u529bQ,K,V\u306f\u524d\u5c64\u306e\u540c\u3058\u51fa\u529b\uff08\u7ffb\u8a33\u5c64\u306fK,V\u304cEnc\u306e\u51fa\u529b\uff09\uff0ed\u3068softmax\u3067\u5185\u7a4d\u306e\u5024\u7206\u767a\u3092\u6291\u6b62\uff0e https://t.\u2026"}, "874578451436634112": {"followers": "128", "datetime": "2017-06-13 10:45:26", "author": "@goloskokovic", "content_summary": "RT @cto_movidius: Busy week in Deep Networks: attention networks https://t.co/gI4a2k7n61, SELU https://t.co/utp0GPYTB6 & CortexNet https://\u2026"}, "874483964857114624": {"followers": "5,752", "datetime": "2017-06-13 04:29:59", "author": "@ejiwarp", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899837060680343554": {"followers": "635", "datetime": "2017-08-22 03:34:08", "author": "@nixeneko", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "907225166022930432": {"followers": "1,564", "datetime": "2017-09-11 12:51:49", "author": "@migballesteros", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "1165699921980350464": {"followers": "818", "datetime": "2019-08-25 18:58:13", "author": "@deepgradient", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "912590664768122880": {"followers": "28,418", "datetime": "2017-09-26 08:12:24", "author": "@ogrisel", "content_summary": "@F_Vaggi @cangermueller https://t.co/GswxoT0arh"}, "875799215858999296": {"followers": "163,852", "datetime": "2017-06-16 19:36:19", "author": "@ceobillionaire", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874593629532180482": {"followers": "16", "datetime": "2017-06-13 11:45:45", "author": "@LUDOPLEX_INC", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "876952227155496962": {"followers": "13", "datetime": "2017-06-19 23:57:58", "author": "@xyc234", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966274392614371329": {"followers": "38", "datetime": "2018-02-21 11:32:22", "author": "@cirhuzalain", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874769429866962945": {"followers": "4,812", "datetime": "2017-06-13 23:24:19", "author": "@IntuitMachine", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874928638814298113": {"followers": "419", "datetime": "2017-06-14 09:56:57", "author": "@nexttechlab", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874484789859844101": {"followers": "38", "datetime": "2017-06-13 04:33:15", "author": "@experiencor", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "879137769485991936": {"followers": "1,168", "datetime": "2017-06-26 00:42:32", "author": "@ApacheOpennlp", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874606480573345793": {"followers": "161", "datetime": "2017-06-13 12:36:49", "author": "@LovedeepSG", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874563798677557249": {"followers": "3,205", "datetime": "2017-06-13 09:47:12", "author": "@iatitov", "content_summary": "RT @dipanjand: Check this out: https://t.co/N6hZyklj6G SOTA on several metrics using extremely simple models."}, "1203065666925817857": {"followers": "7,011", "datetime": "2019-12-06 21:36:21", "author": "@martin_gorner", "content_summary": "It does not go all the way to Transformer. The paper for that is here: https://t.co/8hGgiIHYrX"}, "874669026181623809": {"followers": "232", "datetime": "2017-06-13 16:45:21", "author": "@fabiofumarola", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874476740738822144": {"followers": "1,528", "datetime": "2017-06-13 04:01:16", "author": "@ilblackdragon", "content_summary": "Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeplearning #TensorFlow https://t.co/pZYPgDS84c"}, "1117058048730619907": {"followers": "61", "datetime": "2019-04-13 13:32:47", "author": "@bwrado1", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "877448375976681473": {"followers": "1,978", "datetime": "2017-06-21 08:49:29", "author": "@ZibbyZ", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165954555286364160": {"followers": "2,891", "datetime": "2019-08-26 11:50:03", "author": "@angelicabedoy16", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875481988995481600": {"followers": "231", "datetime": "2017-06-15 22:35:46", "author": "@sughimsi", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1121654728067174400": {"followers": "232", "datetime": "2019-04-26 05:58:21", "author": "@augeas", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874529458090369024": {"followers": "296", "datetime": "2017-06-13 07:30:45", "author": "@MarcRomeyn", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966060830096789504": {"followers": "718", "datetime": "2018-02-20 21:23:45", "author": "@timebenezer", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876577245170733056": {"followers": "210", "datetime": "2017-06-18 23:07:55", "author": "@MihaelFeldman", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874611667996295168": {"followers": "13", "datetime": "2017-06-13 12:57:25", "author": "@TheEcolss", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "905042216367702016": {"followers": "353", "datetime": "2017-09-05 12:17:33", "author": "@K3nt0W", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "877039431831748608": {"followers": "728", "datetime": "2017-06-20 05:44:29", "author": "@sarnthil", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874626458978848768": {"followers": "86", "datetime": "2017-06-13 13:56:12", "author": "@tiagomluis", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874704981907169280": {"followers": "642", "datetime": "2017-06-13 19:08:13", "author": "@_AntreasAntonio", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877501777616269313": {"followers": "963", "datetime": "2017-06-21 12:21:41", "author": "@FlorianWilhelm", "content_summary": "Attention is all you need! Better and faster, yet simpler sequence to sequence #deeplearning https://t.co/evSed6PRNp"}, "1166011442929242112": {"followers": "166", "datetime": "2019-08-26 15:36:06", "author": "@dbparedes", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874473662157471744": {"followers": "1,573", "datetime": "2017-06-13 03:49:02", "author": "@coffeephoenix", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1220364101475545088": {"followers": "8,940", "datetime": "2020-01-23 15:14:09", "author": "@danbri", "content_summary": "@ImageSnippets not ringing any bells, sorry! well it word-association reminded me of https://t.co/TrRjkLHNn0 but I don't think that's what you're after"}, "874549945394757633": {"followers": "9,122", "datetime": "2017-06-13 08:52:09", "author": "@elluba", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874923496677273601": {"followers": "824", "datetime": "2017-06-14 09:36:31", "author": "@morioka", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "875477542790103040": {"followers": "301", "datetime": "2017-06-15 22:18:06", "author": "@relopezbriega", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "1116974975716679680": {"followers": "4,891", "datetime": "2019-04-13 08:02:41", "author": "@IgorCarron", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874889516108132352": {"followers": "207", "datetime": "2017-06-14 07:21:29", "author": "@Weenkus", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165934631033724928": {"followers": "1", "datetime": "2019-08-26 10:30:52", "author": "@robert_lundberg", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "967616875025776640": {"followers": "14", "datetime": "2018-02-25 04:26:55", "author": "@LvguanInn", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165976446004862977": {"followers": "74", "datetime": "2019-08-26 13:17:02", "author": "@luisfredgs", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876623440492417024": {"followers": "96", "datetime": "2017-06-19 02:11:29", "author": "@vincentzlt", "content_summary": "\ud83d\ude02 https://t.co/gLJ6Ll896Z"}, "874594124917129216": {"followers": "19", "datetime": "2017-06-13 11:47:43", "author": "@VenkataNagaRavi", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "876917030435401728": {"followers": "79,158", "datetime": "2017-06-19 21:38:07", "author": "@machinelearnbot", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874446867152515076": {"followers": "2,671", "datetime": "2017-06-13 02:02:34", "author": "@ayirpelle", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874494646340157440": {"followers": "274", "datetime": "2017-06-13 05:12:25", "author": "@mir_k", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "948789664881790977": {"followers": "934", "datetime": "2018-01-04 05:34:18", "author": "@4dri4nG4rrigos", "content_summary": "I enjoyed reading it. Because I encountered \"Attention is all you need\" here: https://t.co/KTKKTPWUur So sad... https://t.co/dorMQoaNvX"}, "874629624600317952": {"followers": "664", "datetime": "2017-06-13 14:08:46", "author": "@crude2refined", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874759816933027842": {"followers": "205", "datetime": "2017-06-13 22:46:07", "author": "@smilejx1", "content_summary": "Attention Is All You Need\uff0cfrom Googlebrain \uff0cno rnn \uff0cno cnn https://t.co/gkCTZ1UbM8"}, "875822067807072260": {"followers": "2", "datetime": "2017-06-16 21:07:07", "author": "@filippowskii", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874704852147949568": {"followers": "195", "datetime": "2017-06-13 19:07:42", "author": "@hereticreader", "content_summary": "[1706.03762] Attention Is All You Need - https://t.co/9D1COPolYq https://t.co/jDy8wnsgL5"}, "987023709142593536": {"followers": "229", "datetime": "2018-04-19 17:42:44", "author": "@vivek531", "content_summary": "RT @beenwrekt: Original paper for reference: https://t.co/3kJLaMM6r3"}, "876705628881788928": {"followers": "2,164", "datetime": "2017-06-19 07:38:04", "author": "@alanyttian", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874610332991455232": {"followers": "493", "datetime": "2017-06-13 12:52:07", "author": "@dpwiz", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899650456548290564": {"followers": "9,264", "datetime": "2017-08-21 15:12:38", "author": "@npseaver", "content_summary": "Machine learning folks: Can anyone help me grok the current usage of \"attention\" in neural net research like this?: https://t.co/rYhBljczVS"}, "1233114821429776387": {"followers": "1,810", "datetime": "2020-02-27 19:40:58", "author": "@jaldeko", "content_summary": "- Recursos AI: https://t.co/fI4pd8vt1Z - Blog ai.facebook: https://t.co/DjQ3MtzToc - Blog Data Science: https://t.co/Rmr4snvu5v - Proyecto Curso Intro ML (DotCSV): https://t.co/pJwEe6JPTF - Paper Attention is all you need: https://t.co/V3PEiGjiul"}, "1165698898880552961": {"followers": "98", "datetime": "2019-08-25 18:54:09", "author": "@shpotes", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1116983610995826688": {"followers": "5,017", "datetime": "2019-04-13 08:37:00", "author": "@Clive_G_Brown", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "880207628013420544": {"followers": "604", "datetime": "2017-06-28 23:33:46", "author": "@seth_stafford", "content_summary": "Good argument that arxiv papers with empirical claims involving code really should link to that code. https://t.co/DRnRlObBsz"}, "1166175413439094786": {"followers": "63", "datetime": "2019-08-27 02:27:39", "author": "@saqibns", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874468475447660544": {"followers": "2,438", "datetime": "2017-06-13 03:28:26", "author": "@amit_pande", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966008142940131328": {"followers": "269,218", "datetime": "2018-02-20 17:54:23", "author": "@karpathy", "content_summary": "seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"attention is all you need\" paper (https://t.co/df3wrUYi37) for MT. e.g., recently https://t.co/pDfQwTtT2h, https://t.co/1csB3sLfag et"}, "875481855126102017": {"followers": "218", "datetime": "2017-06-15 22:35:14", "author": "@ZacharyMayer", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "1013670920819560449": {"followers": "17,007", "datetime": "2018-07-02 06:29:15", "author": "@DataScienceNIG", "content_summary": "ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling dependencies +how Transformer learn using the attention mechanism,instead of encoding using the hidden state of the encoder Click to read h"}, "876926568563658752": {"followers": "2,121", "datetime": "2017-06-19 22:16:01", "author": "@PabloRedux", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874602223413731329": {"followers": "16", "datetime": "2017-06-13 12:19:54", "author": "@DateLirondoWaxu", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874482934077685762": {"followers": "575", "datetime": "2017-06-13 04:25:53", "author": "@oh3kqd", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1038607858185658369": {"followers": "517", "datetime": "2018-09-09 01:59:44", "author": "@kobkrit", "content_summary": "Attention is all you need. https://t.co/q7tvjCR5zk https://t.co/q7tvjCR5zk"}, "874504705803657217": {"followers": "86", "datetime": "2017-06-13 05:52:24", "author": "@JADORE801120", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "938556231563776000": {"followers": "864", "datetime": "2017-12-06 23:50:17", "author": "@anca_dmtrch", "content_summary": "Attention is all you need! Sequence modelling without convolution or recurrence from @GoogleBrain - what kind of sorcery is this?! \ud83d\udd2e #NIPS2017 https://t.co/4L4BSemxOn"}, "876643172952285186": {"followers": "1,006", "datetime": "2017-06-19 03:29:54", "author": "@alvations", "content_summary": "#neuralempty ... https://t.co/AZaFie4xdF"}, "876825078503059456": {"followers": "76", "datetime": "2017-06-19 15:32:43", "author": "@X_rayAI", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "876896741320249344": {"followers": "443", "datetime": "2017-06-19 20:17:29", "author": "@ivenzor", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "878023968145293313": {"followers": "307", "datetime": "2017-06-22 22:56:41", "author": "@tschwartzbiz", "content_summary": "RT @v_vashishta: #Google #DeepLearning - Attention Is All You Need https://t.co/Bjxj2dYXxw #MachineLearning"}, "1014417041992794112": {"followers": "1,871", "datetime": "2018-07-04 07:54:04", "author": "@KRiver1", "content_summary": "@Ishotihadus https://t.co/E7fSrVvxh2 https://t.co/MIEjXnY9kJ https://t.co/aj2HJUIKct \u3044\u304f\u3064\u304b\u898b\u3066\u307f\u305f\u3051\u3069\u6a5f\u68b0\u7ffb\u8a33\u307e\u3067\u9061\u3063\u3066\u8a00\u53ca\u3057\u3066\u308b\u4eba\u306f\u3044\u306a\u304f\u3066\u3001\u305b\u3044\u305c\u3044NMT\u3063\u3066\u3082\u306e\u304c\u3042\u3063\u3066\u306d\uff08\u3053\u3053\u306b\u6700\u8fd1\u306e\u7814\u7a76\u3092\u5f15\u304f\uff09\u3063\u3066\u611f\u3058\u3063\u307d\u3044"}, "874476032098578432": {"followers": "596", "datetime": "2017-06-13 03:58:27", "author": "@joyport", "content_summary": "\u672c\u6765RNN\u3084CNN\u306a\u3069\u304c\u5fc5\u8981\u3068\u601d\u308f\u308c\u3066\u304d\u305f\u30bf\u30b9\u30af\u304cAttention\u306e\u307f\u3067\u89e3\u6c7a\u3067\u304d\u308b\uff1f\u7d50\u5c40\u4f55\u3092\u3084\u3063\u3066\u3044\u308b\u3093\u3060\u308d\u3046\u30fb\u30fb\u30fb\u3002 https://t.co/DCe0je3Qxh"}, "874547051773779968": {"followers": "243", "datetime": "2017-06-13 08:40:40", "author": "@cosminstamate", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1166989618648178688": {"followers": "1,136", "datetime": "2019-08-29 08:23:01", "author": "@ozaiane", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "938907223387807744": {"followers": "1,411", "datetime": "2017-12-07 23:05:00", "author": "@RexDouglass", "content_summary": "RT @arxiv_cs_cl: https://t.co/8ovOiC3bdC Attention Is All You Need. (arXiv:1706.03762v5 [cs.CL] UPDATED) #NLProc"}, "874758275241304064": {"followers": "52", "datetime": "2017-06-13 22:39:59", "author": "@PredictivaA", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874449377745448962": {"followers": "6,390", "datetime": "2017-06-13 02:12:32", "author": "@mtyka", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966502605244411904": {"followers": "165", "datetime": "2018-02-22 02:39:12", "author": "@RamanarayanM", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "911199114951626752": {"followers": "13", "datetime": "2017-09-22 12:02:52", "author": "@MaksymDel", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "907298538870136833": {"followers": "394", "datetime": "2017-09-11 17:43:23", "author": "@srivatsanlaxman", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "875421171151458304": {"followers": "148", "datetime": "2017-06-15 18:34:06", "author": "@BartSchuller", "content_summary": "Rediscovered the browser tab with https://t.co/xK4CASkkeG \u201cAttention Is All You Need\u201d Irony\u2019s not dead yet."}, "966015388411600896": {"followers": "4,977", "datetime": "2018-02-20 18:23:10", "author": "@ishiid", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874449446905364480": {"followers": "189", "datetime": "2017-06-13 02:12:49", "author": "@shahrukh_athar", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "877745365847490560": {"followers": "62", "datetime": "2017-06-22 04:29:37", "author": "@danielnjoo", "content_summary": "breakthru in NLP w/ Transformer arch (vs. R/CNN) beating other models by ~10% in perf w/ 1-2 magnitude less training https://t.co/RZ4f08ikMR"}, "1234563797903454208": {"followers": "215", "datetime": "2020-03-02 19:38:41", "author": "@jbowayles", "content_summary": "@thorstenball the transformer paper (natural language processing) is up there in terms of impact: https://t.co/whJAkce9FD also, the word2vec paper (nlp again): https://t.co/hzEilllsyC"}, "874593760474152962": {"followers": "1,448", "datetime": "2017-06-13 11:46:16", "author": "@dicekicker", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165778435840462848": {"followers": "2", "datetime": "2019-08-26 00:10:12", "author": "@xin_josh", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1027490961553805314": {"followers": "222", "datetime": "2018-08-09 09:45:09", "author": "@bzamecnik", "content_summary": "https://t.co/o0yzoZoIcD https://t.co/zpMt5pYDkj"}, "877314792821858308": {"followers": "345", "datetime": "2017-06-20 23:58:40", "author": "@AlexEdGibson", "content_summary": "RT @timstae_c: Attention is all you need: https://t.co/ezlw18LmmF"}, "874595714797633536": {"followers": "363", "datetime": "2017-06-13 11:54:02", "author": "@aachigorin", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "966159158205669376": {"followers": "753", "datetime": "2018-02-21 03:54:28", "author": "@tahantech", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165932660272881664": {"followers": "14", "datetime": "2019-08-26 10:23:02", "author": "@BLJ11232689", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874590269244530688": {"followers": "589", "datetime": "2017-06-13 11:32:23", "author": "@immsrini", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "885366576152793088": {"followers": "1,860", "datetime": "2017-07-13 05:13:35", "author": "@Ritmonegro", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165955945765228547": {"followers": "565", "datetime": "2019-08-26 11:55:34", "author": "@BluFlame", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875994930476462080": {"followers": "548", "datetime": "2017-06-17 08:34:01", "author": "@ImSoErgodic", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "1213802462773153796": {"followers": "3,321", "datetime": "2020-01-05 12:40:33", "author": "@juantomas", "content_summary": "RT @wzuidema: Shock 9 (2017): Know all about LSTMs now? Good, now forget all of it, as Attention Is All You Need (i.e. the Transformer). [\u2026"}, "879231584225898496": {"followers": "209", "datetime": "2017-06-26 06:55:19", "author": "@rhodriev", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874858626518790145": {"followers": "4,795", "datetime": "2017-06-14 05:18:45", "author": "@__ice9", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "875011295698759684": {"followers": "224", "datetime": "2017-06-14 15:25:24", "author": "@ElectronNest", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "951236544601272321": {"followers": "175", "datetime": "2018-01-10 23:37:20", "author": "@sappy_and_sappy", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "876557984691884032": {"followers": "46", "datetime": "2017-06-18 21:51:23", "author": "@vahidpartovinia", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "874559080068763648": {"followers": "850", "datetime": "2017-06-13 09:28:27", "author": "@AymericDamien", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899759744092487680": {"followers": "1,365", "datetime": "2017-08-21 22:26:54", "author": "@akinori_ito", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "876640563122102272": {"followers": "1,231", "datetime": "2017-06-19 03:19:32", "author": "@odan3240", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874500396248580096": {"followers": "425", "datetime": "2017-06-13 05:35:16", "author": "@iamknighton", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "1038008425781682176": {"followers": "229", "datetime": "2018-09-07 10:17:48", "author": "@fujifog", "content_summary": "Attention https://t.co/1Z5XL4ayFM Diversity https://t.co/a92N2Y9kUy Wasserstein https://t.co/UrQrZ7hXd6 CNN https://t.co/ndnfWuYGxm is all you need. \u7d50\u5c40\u5168\u90e8\u5fc5\u8981\u306a\u306e\u3067\u306f\uff0e\uff0e\uff0e"}, "875040020578566144": {"followers": "229", "datetime": "2017-06-14 17:19:33", "author": "@hengcherkeng", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874596733308739587": {"followers": "180", "datetime": "2017-06-13 11:58:05", "author": "@pmarelas", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875358799305072640": {"followers": "51,260", "datetime": "2017-06-15 14:26:15", "author": "@ML_NLP", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874644836585947136": {"followers": "1", "datetime": "2017-06-13 15:09:13", "author": "@Yongjin_Cho_", "content_summary": "RT @iamtrask: in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://\u2026"}, "877673880940298240": {"followers": "824", "datetime": "2017-06-21 23:45:34", "author": "@morioka", "content_summary": "RT @kawano_hiroki: Google \u304b\u3089\u65b0\u3057\u3044\u8ad6\u6587\u304c\u3067\u305f\u3002Attention Is All You Need. RNN\u3092\u4f7f\u308f\u305a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3060\u3051\u4f7f\u3063\u3066\u3088\u308a\u9ad8\u901f\u9ad8\u6027\u80fd\u306aNMT\u304c\u3067\u304d\u308b\u3089\u3057\u3044\u3002\u65b0\u3057\u3044\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u540d\u306f \"the Transformer\" https\u2026"}, "876006444264501248": {"followers": "1,602", "datetime": "2017-06-17 09:19:46", "author": "@alxcnwy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874732780869083137": {"followers": "43", "datetime": "2017-06-13 20:58:41", "author": "@jeandut14000", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "899608902026711041": {"followers": "1,691", "datetime": "2017-08-21 12:27:30", "author": "@ytsmiling", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "966212172996075520": {"followers": "38", "datetime": "2018-02-21 07:25:08", "author": "@jonnazar", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874539020503601152": {"followers": "54", "datetime": "2017-06-13 08:08:45", "author": "@625", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "877300593190084608": {"followers": "1,286", "datetime": "2017-06-20 23:02:15", "author": "@yitabashi", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874663522046689283": {"followers": "805", "datetime": "2017-06-13 16:23:28", "author": "@TomKenter", "content_summary": "Attention is all you need. Really cool paper with my former supervisor @googleresearch as one of the authors! https://t.co/wk03CUpFhK"}, "874539078779260929": {"followers": "4,884", "datetime": "2017-06-13 08:08:59", "author": "@martin_wicke", "content_summary": "This makes sense for translation tasks. And thus, RNNs retreat to conversation. https://t.co/OQDRQ5iSiX"}, "874467853096820736": {"followers": "169", "datetime": "2017-06-13 03:25:57", "author": "@AnirbanSantara", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166151820940460032": {"followers": "5", "datetime": "2019-08-27 00:53:54", "author": "@elvcastelo", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1196949470547591170": {"followers": "366", "datetime": "2019-11-20 00:32:46", "author": "@gabriel_ilharco", "content_summary": "@adityakusupati @suriyagnskr In terms of modeling, I listed some influential papers below Transformers: https://t.co/UikGf9IHf1 (also recommend this blog post: https://t.co/SNeIWirZKN) BERT: https://t.co/AJ39d0n2Bb RoBERTa: https://t.co/92pdjp5UYi T5: htt"}, "875495758300164096": {"followers": "6", "datetime": "2017-06-15 23:30:29", "author": "@dev_juice", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876953594989355008": {"followers": "368", "datetime": "2017-06-20 00:03:24", "author": "@buss_jan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875074810702958594": {"followers": "823", "datetime": "2017-06-14 19:37:47", "author": "@gumarten", "content_summary": "@jokedaems https://t.co/Ht0KZwQWGv"}, "874640611718164480": {"followers": "101", "datetime": "2017-06-13 14:52:26", "author": "@hector_sab", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "966269704036913152": {"followers": "75", "datetime": "2018-02-21 11:13:44", "author": "@adityachivu", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165852829552009216": {"followers": "155", "datetime": "2019-08-26 05:05:49", "author": "@keylinker", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "948854391582289920": {"followers": "118", "datetime": "2018-01-04 09:51:30", "author": "@cre_pero", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "899810636674113536": {"followers": "876", "datetime": "2017-08-22 01:49:08", "author": "@nanTosaka2", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "874458238284046336": {"followers": "40", "datetime": "2017-06-13 02:47:45", "author": "@dmikeando", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874528280875061248": {"followers": "149", "datetime": "2017-06-13 07:26:04", "author": "@tulkenss", "content_summary": "https://t.co/0FuTVZKm8O I feel like this title could've been a lot punnier."}, "876803524885196801": {"followers": "329", "datetime": "2017-06-19 14:07:05", "author": "@PetrusLundqvist", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "875490083981320199": {"followers": "425", "datetime": "2017-06-15 23:07:56", "author": "@iamknighton", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "1185498596382314496": {"followers": "1", "datetime": "2019-10-19 10:11:05", "author": "@DataSciencePics", "content_summary": "Transformer model architecture. From: https://t.co/q94qIOXjOp #ml #ai https://t.co/2Qn4gaIi12"}, "874542686002454528": {"followers": "1,876", "datetime": "2017-06-13 08:23:19", "author": "@MannyKayy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875160351129587712": {"followers": "1,946", "datetime": "2017-06-15 01:17:42", "author": "@mhiramat", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874534127248257024": {"followers": "418", "datetime": "2017-06-13 07:49:18", "author": "@dantkz", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "878897674711445504": {"followers": "15,143", "datetime": "2017-06-25 08:48:29", "author": "@alevergara78", "content_summary": "RT @Pedra999: All you need is Attention : Training #tensorflow - > https://t.co/fllCNmcQe5"}, "1060519561491996673": {"followers": "510", "datetime": "2018-11-08 13:09:01", "author": "@fpocket", "content_summary": "\u3053\u306e\u8ad6\u6587\u306d\u3002 https://t.co/dpUEJ3Mq5u"}, "1189848546431897602": {"followers": "1,833", "datetime": "2019-10-31 10:16:14", "author": "@msarozz", "content_summary": "RT @NicholaStott: The T in BERT=Transformer. This paper was the original work that proposed an attention based transformer model, thereby c\u2026"}, "874579305904439296": {"followers": "1,752", "datetime": "2017-06-13 10:48:50", "author": "@vincent_spruyt", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165888505945440256": {"followers": "312", "datetime": "2019-08-26 07:27:35", "author": "@asilazo99", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1166003967257731073": {"followers": "194", "datetime": "2019-08-26 15:06:23", "author": "@jastner109", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874640898419642368": {"followers": "317", "datetime": "2017-06-13 14:53:34", "author": "@Homhospital", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876624158880399361": {"followers": "183", "datetime": "2017-06-19 02:14:21", "author": "@Shuping_Ruan", "content_summary": "\ud83e\udd23 https://t.co/nOuPHwZ8Z4"}, "874517990615654401": {"followers": "374", "datetime": "2017-06-13 06:45:11", "author": "@Jintai100", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874465564718112768": {"followers": "8", "datetime": "2017-06-13 03:16:52", "author": "@JunhoCho10", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117015176241246209": {"followers": "669", "datetime": "2019-04-13 10:42:25", "author": "@razoralign", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "926513665095979009": {"followers": "1,848", "datetime": "2017-11-03 18:17:26", "author": "@coastalcph", "content_summary": "RT @dipanjand: Check this out: https://t.co/N6hZyklj6G SOTA on several metrics using extremely simple models."}, "874502052755324928": {"followers": "36", "datetime": "2017-06-13 05:41:51", "author": "@jinyungHong", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "877216039628668928": {"followers": "156", "datetime": "2017-06-20 17:26:16", "author": "@jethro_sun", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1059642083495952387": {"followers": "1,905", "datetime": "2018-11-06 03:02:14", "author": "@anoushnajarian", "content_summary": "Reading about Transformers in Attention Is All You Need https://t.co/c4K7PRjBRe"}, "875182100219129856": {"followers": "285", "datetime": "2017-06-15 02:44:07", "author": "@shakthydoss", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1184839470664699905": {"followers": "1,461", "datetime": "2019-10-17 14:31:57", "author": "@rfrsarmiento", "content_summary": "RT @AndrewLBeam: @MaartenvSmeden There is a very famous ML paper where all 8 authors are \u201cfirst\u201d authors: https://t.co/lHqFvaefWs"}, "876889198912995328": {"followers": "937", "datetime": "2017-06-19 19:47:31", "author": "@iiacobac", "content_summary": "Indecision #NMT #NLProc https://t.co/w5AMIdTcmY"}, "874773630609227776": {"followers": "241", "datetime": "2017-06-13 23:41:00", "author": "@_torontoai", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1165728959801118720": {"followers": "3,772", "datetime": "2019-08-25 20:53:36", "author": "@volkuleshov", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876929464034185216": {"followers": "336", "datetime": "2017-06-19 22:27:31", "author": "@ljupc0", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "964437068607733762": {"followers": "457", "datetime": "2018-02-16 09:51:30", "author": "@peroxycarbonate", "content_summary": "@admittedlyhuman @Meaningness @slatestarcodex are you, now? what are your comments on https://t.co/sFjTkkkqfu ?"}, "1165923728175390720": {"followers": "1,210", "datetime": "2019-08-26 09:47:33", "author": "@Mallamzee", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876826251905736705": {"followers": "1,106", "datetime": "2017-06-19 15:37:23", "author": "@permutans", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "966009567527165952": {"followers": "54", "datetime": "2018-02-20 18:00:03", "author": "@ainewsantenna", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "966011953654181888": {"followers": "408", "datetime": "2018-02-20 18:09:32", "author": "@barselona_59", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1096411542998994944": {"followers": "1,224", "datetime": "2019-02-15 14:10:56", "author": "@pozorvlak", "content_summary": "@Meaningness @drossbucket @DRMacIver I haven't read the actual paper, but AIUI: kinda, but not really? The difference is that Transformer networks (https://t.co/214ls0TBCF) can look back arbitrarily far in the text-so-far, whereas Markov chains only condit"}, "876406051356246016": {"followers": "1", "datetime": "2017-06-18 11:47:40", "author": "@r_grimov", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "966011888931889152": {"followers": "354", "datetime": "2018-02-20 18:09:16", "author": "@kartik9", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "899653071667515393": {"followers": "126", "datetime": "2017-08-21 15:23:01", "author": "@wataru_murata", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "874590099618582528": {"followers": "238", "datetime": "2017-06-13 11:31:43", "author": "@burakisikli", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165900937682137088": {"followers": "881", "datetime": "2019-08-26 08:16:59", "author": "@jimaids", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874508806121676800": {"followers": "57", "datetime": "2017-06-13 06:08:41", "author": "@toycube_pf", "content_summary": "RT @linkoffate: Attention Is All You Need https://t.co/datm5124sF WTF \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b \uc5b4\uc81c \ub098\uc628 \ub17c\ubb38\uc744 \uadf8\ub300\ub85c \uc539\uc5b4\ubc84\ub9ac\ub124 \uc774\uac70 \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b"}, "874602629187457024": {"followers": "218", "datetime": "2017-06-13 12:21:30", "author": "@ZacharyMayer", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "876973372688564224": {"followers": "274", "datetime": "2017-06-20 01:22:00", "author": "@RamtinMehdizade", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874581682925576192": {"followers": "585", "datetime": "2017-06-13 10:58:16", "author": "@efc_Students", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117097512924585990": {"followers": "19", "datetime": "2019-04-13 16:09:36", "author": "@MassBassLol", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "912926324624429056": {"followers": "103", "datetime": "2017-09-27 06:26:11", "author": "@agibiansky", "content_summary": "Awesome plot. If you want, you can skip the initial phase by giving it a good bias with position encodings, like in https://t.co/ceEQ2vPPZU https://t.co/rPRWIxe1XG"}, "874994301570240512": {"followers": "1,769", "datetime": "2017-06-14 14:17:52", "author": "@jaialkdanel", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "1033080960143839232": {"followers": "1,023", "datetime": "2018-08-24 19:57:49", "author": "@EPfromthe501", "content_summary": "Attention Is All You Need (Submitted on 12 Jun 2017 (v1), last revised 6 Dec 2017 (this version, v5)) https://t.co/i07IDvFdWA"}, "1013744422008258560": {"followers": "647", "datetime": "2018-07-02 11:21:19", "author": "@SakinatTijani", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "966011859840192512": {"followers": "217", "datetime": "2018-02-20 18:09:09", "author": "@dominikroblek", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165695665554567174": {"followers": "88,923", "datetime": "2019-08-25 18:41:18", "author": "@OriolVinyalsML", "content_summary": "A good example of our field moving (too) fast: the all attention layer was actually released in the original transformer paper from 2017, with similar findings. Code: https://t.co/DdiM0PMziS Paper: https://t.co/sejTvvAMh6 (only in v3)"}, "1018988272515608576": {"followers": "170", "datetime": "2018-07-16 22:38:30", "author": "@tremblerz", "content_summary": "@srchvrs @filippie509 They brought positional encoding firstly in this https://t.co/m7BUFcmAVz I think"}, "874443460752809984": {"followers": "1,969", "datetime": "2017-06-13 01:49:02", "author": "@kivantium", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "961005345995976704": {"followers": "1,046", "datetime": "2018-02-06 22:35:03", "author": "@pronamc", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "1166167961486405638": {"followers": "633", "datetime": "2019-08-27 01:58:03", "author": "@OgbanUgot", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874593059501113345": {"followers": "395", "datetime": "2017-06-13 11:43:29", "author": "@jeffmgould", "content_summary": "Feels like a game changer... https://t.co/XuoA2XMXad"}, "874610818494717952": {"followers": "7,670", "datetime": "2017-06-13 12:54:03", "author": "@KyleCranmer", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874451913193922560": {"followers": "12,611", "datetime": "2017-06-13 02:22:37", "author": "@colinraffel", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166034882436485125": {"followers": "24", "datetime": "2019-08-26 17:09:14", "author": "@jrabary", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165791493962919942": {"followers": "1,584", "datetime": "2019-08-26 01:02:06", "author": "@aneesha", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874834071901306880": {"followers": "95", "datetime": "2017-06-14 03:41:11", "author": "@jdot", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "921316794199142400": {"followers": "8,190", "datetime": "2017-10-20 10:06:55", "author": "@arichduvet", "content_summary": "RT @__RAMIN_: @suzatweet yeah! I think he probably must have been talking about these: https://t.co/el8G59uHiy https://t.co/iHvJwLOY5X"}, "874685191356379137": {"followers": "340", "datetime": "2017-06-13 17:49:35", "author": "@bhaavan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874677003722665985": {"followers": "16", "datetime": "2017-06-13 17:17:03", "author": "@p_kot1", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874681764060381184": {"followers": "182", "datetime": "2017-06-13 17:35:58", "author": "@bookwormengr", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874686633844420608": {"followers": "1,514", "datetime": "2017-06-13 17:55:19", "author": "@_DaveSullivan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874483997962903552": {"followers": "671", "datetime": "2017-06-13 04:30:06", "author": "@erwtokritos", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876805327500587010": {"followers": "227", "datetime": "2017-06-19 14:14:14", "author": "@SichuLu", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874598781580320768": {"followers": "133", "datetime": "2017-06-13 12:06:13", "author": "@Loosing_Touch", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874468464647208960": {"followers": "1,967", "datetime": "2017-06-13 03:28:23", "author": "@nikiparmar09", "content_summary": "Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #transformer"}, "874513738740764676": {"followers": "1,938", "datetime": "2017-06-13 06:28:17", "author": "@EldarSilver", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1116950912814084097": {"followers": "70", "datetime": "2019-04-13 06:27:04", "author": "@sumitsethy", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "876599686026543104": {"followers": "18,250", "datetime": "2017-06-19 00:37:06", "author": "@hillbig", "content_summary": "RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co/0g29WJkdtt"}, "874446496011231232": {"followers": "4,019", "datetime": "2017-06-13 02:01:05", "author": "@ballforest", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966027797335490560": {"followers": "3,416", "datetime": "2018-02-20 19:12:29", "author": "@andrey_kurenkov", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874475951580626946": {"followers": "34", "datetime": "2017-06-13 03:58:08", "author": "@vuly16", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "948837363597287424": {"followers": "1,248", "datetime": "2018-01-04 08:43:50", "author": "@daisuzu", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "1227959522448363521": {"followers": "334", "datetime": "2020-02-13 14:15:39", "author": "@Mickdub29", "content_summary": "@rfriedman22 Here\u2019s the original paper, which lead to people using \u201ctransformers\u201d which is basically the thing that has generated all of the really cool recent results like GPT-2. https://t.co/5yzIcGGGXU"}, "877167049021071361": {"followers": "50", "datetime": "2017-06-20 14:11:36", "author": "@NRC_pythoneer", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874583697282002944": {"followers": "28,418", "datetime": "2017-06-13 11:06:17", "author": "@ogrisel", "content_summary": "Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism https://t.co/f9yJTdveID"}, "874934559627919360": {"followers": "635", "datetime": "2017-06-14 10:20:29", "author": "@rkakamilan", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "874446316629139456": {"followers": "4,222", "datetime": "2017-06-13 02:00:22", "author": "@tuntuku_sy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "950742367795064832": {"followers": "506", "datetime": "2018-01-09 14:53:39", "author": "@Joe_Pater", "content_summary": "Someone wanted refs. on attention (maybe @fmailhot?). From @strubell: \"Attention is All You Need\" https://t.co/b9gFPjCRY6 To learn about attention, though, I wouldn't bother reading that paper, this blog post seems to summarize it well https://t.co/8J7S"}, "877438074048741377": {"followers": "419", "datetime": "2017-06-21 08:08:33", "author": "@nexttechlab", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874444393607864321": {"followers": "473", "datetime": "2017-06-13 01:52:44", "author": "@MShahriariNia", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876856014078963715": {"followers": "184", "datetime": "2017-06-19 17:35:39", "author": "@TomSercu", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "961201472342917120": {"followers": "824", "datetime": "2018-02-07 11:34:23", "author": "@morioka", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "874443760213667842": {"followers": "166", "datetime": "2017-06-13 01:50:13", "author": "@RobertBaldini", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874552990836768768": {"followers": "4,386", "datetime": "2017-06-13 09:04:16", "author": "@_Ryobot", "content_summary": "\u81ea\u5df1\u6ce8\u610f\u3067\u5165\u51fa\u529b\u6587\u3092\u9023\u7d9a\u8868\u73fe\u306b\u7b26\u53f7\u5316\u3057\uff0c\u65e2\u8a33\u306e\u51fa\u529b\u6587\u3067\u30de\u30b9\u30ad\u30f3\u30b0\u3057\u306a\u304c\u3089\u7ffb\u8a33\u3059\u308b\u624b\u6cd5\u3067WMT\u82f1\u72ec/\u82f1\u4ecf\u3067SOTA\uff0e \u81ea\u5df1\u6ce8\u610f\u306e\u5165\u529bQ,K,V\u306f\u524d\u5c64\u306e\u540c\u3058\u51fa\u529b\uff08\u7ffb\u8a33\u5c64\u306fK,V\u304cEnc\u306e\u51fa\u529b\uff09\uff0ed\u3068softmax\u3067\u5185\u7a4d\u306e\u5024\u7206\u767a\u3092\u6291\u6b62\uff0e https://t.co/7GFxybWuZt https://t.co/BVq5pkavVi"}, "874957788514058240": {"followers": "532", "datetime": "2017-06-14 11:52:47", "author": "@rnella01", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874654589634596864": {"followers": "257", "datetime": "2017-06-13 15:47:59", "author": "@chajeongwon", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874504087487733761": {"followers": "28,529", "datetime": "2017-06-13 05:49:56", "author": "@thinkmariya", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966316023535173632": {"followers": "152", "datetime": "2018-02-21 14:17:47", "author": "@alirg1", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874629976405950465": {"followers": "292", "datetime": "2017-06-13 14:10:10", "author": "@soaxelbrooke", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874744885278015489": {"followers": "2,290", "datetime": "2017-06-13 21:46:47", "author": "@gregcmartin", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875003553760374785": {"followers": "355", "datetime": "2017-06-14 14:54:38", "author": "@hadiakhojasteh", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874688794179420160": {"followers": "64", "datetime": "2017-06-13 18:03:54", "author": "@supriya_pandhre", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1200084274432495619": {"followers": "72", "datetime": "2019-11-28 16:09:22", "author": "@TamilSelvan_T_S", "content_summary": "\"Attention Is All You Need\" - The best performing models also connect the encoder and decoder through an attention mechanism #MachineLearning #NeuralNetworks #ArtificialIntelliegnce #DeepLearning #ML #model #Coding #DataScience https://t.co/8uImI3RPkr"}, "874472023249965057": {"followers": "686", "datetime": "2017-06-13 03:42:31", "author": "@thetenthart", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165888117112475653": {"followers": "22,788", "datetime": "2019-08-26 07:26:02", "author": "@DotCSV", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876035887121002496": {"followers": "306", "datetime": "2017-06-17 11:16:46", "author": "@derpapst", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874521079837892608": {"followers": "4,947", "datetime": "2017-06-13 06:57:27", "author": "@micahstubbs", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874984097814392832": {"followers": "102", "datetime": "2017-06-14 13:37:19", "author": "@AlfonsoOrtega11", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876864879965093888": {"followers": "477", "datetime": "2017-06-19 18:10:53", "author": "@HaqueIshfaq", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874905836413472768": {"followers": "70", "datetime": "2017-06-14 08:26:21", "author": "@ParthMaji", "content_summary": "RT @graphific: Attention Is All You Need from @GoogleBrain. No RNNs or CNNs, only Attention = fast training & SOTA on WMT'14 https://t.co/Y\u2026"}, "876337488062697472": {"followers": "198", "datetime": "2017-06-18 07:15:13", "author": "@attiamohammedal", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "966216387860549632": {"followers": "3", "datetime": "2018-02-21 07:41:52", "author": "@benchao_fan", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874560178020024320": {"followers": "1,058", "datetime": "2017-06-13 09:32:49", "author": "@GillesMoyse", "content_summary": "Great paper describing a purely attention based model for NMT https://t.co/3lkgHrUqpU"}, "874804840890744832": {"followers": "432", "datetime": "2017-06-14 01:45:01", "author": "@bhargavbardipur", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165695976880902144": {"followers": "1,657", "datetime": "2019-08-25 18:42:33", "author": "@AfelioP", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1249192135540314113": {"followers": "250", "datetime": "2020-04-12 04:26:28", "author": "@YimbyEarth", "content_summary": "RT @tamara_bozovic: Idea: using #MachineLearning to analyse people's perceptions of their streets post #lockdown from tweets. Challenge: u\u2026"}, "879393424742375426": {"followers": "0", "datetime": "2017-06-26 17:38:25", "author": "@sk_1010k", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/KRq3JsN1ck"}, "874751036115865600": {"followers": "825", "datetime": "2017-06-13 22:11:13", "author": "@fly51fly", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "938584252538589184": {"followers": "3,500", "datetime": "2017-12-07 01:41:38", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "874567568346021888": {"followers": "629", "datetime": "2017-06-13 10:02:11", "author": "@oshtim", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874652349880352768": {"followers": "15,224", "datetime": "2017-06-13 15:39:05", "author": "@lmthang", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "878493104260603904": {"followers": "1,527", "datetime": "2017-06-24 06:00:52", "author": "@Pedra999", "content_summary": "All you need is Attention : Training #tensorflow - > https://t.co/fllCNmcQe5"}, "1117362914241990662": {"followers": "28", "datetime": "2019-04-14 09:44:13", "author": "@hey_kishore", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1117415552950112256": {"followers": "25", "datetime": "2019-04-14 13:13:23", "author": "@ASreesaila", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "948901253446119424": {"followers": "902", "datetime": "2018-01-04 12:57:43", "author": "@wk77", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874506424520331265": {"followers": "1,113", "datetime": "2017-06-13 05:59:13", "author": "@achristensen56", "content_summary": "\"... transduction model relying entirely on self-attention to compute continuous representations of it's input and output\" [no RNN, no CNN] https://t.co/D0vGZqxnJ1"}, "996748189947613184": {"followers": "677", "datetime": "2018-05-16 13:44:21", "author": "@kmpetersson", "content_summary": "RT @albertcardona: \"a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence an\u2026"}, "874523527986036738": {"followers": "898", "datetime": "2017-06-13 07:07:11", "author": "@nlothian", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874603080779796480": {"followers": "15", "datetime": "2017-06-13 12:23:18", "author": "@mahiratmis", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1050667941216149505": {"followers": "340", "datetime": "2018-10-12 08:42:12", "author": "@Cedias_", "content_summary": "BERT https://t.co/pt2fjPLD1i is one more reason to have a look at Transformers (https://t.co/za7t7McxAm). Apparently Attention is really all you need !"}, "875458300619497472": {"followers": "7,210", "datetime": "2017-06-15 21:01:38", "author": "@fastml_extra", "content_summary": "Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC1xAutLzJ"}, "874715592523214849": {"followers": "445", "datetime": "2017-06-13 19:50:23", "author": "@pbanavara", "content_summary": "I don't understand how, but this is epic ! https://t.co/9PJzyC7mLQ"}, "1166066699017154560": {"followers": "89", "datetime": "2019-08-26 19:15:40", "author": "@i_shikhar98", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874475234568663040": {"followers": "64", "datetime": "2017-06-13 03:55:17", "author": "@Zhiliyo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874684164729184256": {"followers": "517", "datetime": "2017-06-13 17:45:30", "author": "@engsoares_gyn", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874589388931551232": {"followers": "586", "datetime": "2017-06-13 11:28:54", "author": "@chaoticneural", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "892820865364176903": {"followers": "6", "datetime": "2017-08-02 18:54:16", "author": "@dev_juice", "content_summary": "RT @jekbradbury: @haldaume3 residual layers (b/c https://t.co/68EzsEnOs3), self-attention (b/c https://t.co/R9WHZokiRI) and define-by-run (\u2026"}, "949104478686363648": {"followers": "602", "datetime": "2018-01-05 02:25:15", "author": "@Swall0wTech", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "876185249440518144": {"followers": "186", "datetime": "2017-06-17 21:10:16", "author": "@danielecocozza", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "874664767423619072": {"followers": "492", "datetime": "2017-06-13 16:28:25", "author": "@rajhans_samdani", "content_summary": "RT @deliprao: The new Transformer Network looks deceptively simple for producing SOTA in MT. https://t.co/7otNrERtSH https://t.co/mkGqPin2\u2026"}, "876003763005378560": {"followers": "8,656", "datetime": "2017-06-17 09:09:07", "author": "@Translation", "content_summary": "Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "874502972385918976": {"followers": "292", "datetime": "2017-06-13 05:45:30", "author": "@BFelbo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165906469809602566": {"followers": "12", "datetime": "2019-08-26 08:38:58", "author": "@PercivalDev", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874439444761325570": {"followers": "2,283", "datetime": "2017-06-13 01:33:04", "author": "@AdaptToReality", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876643304024276992": {"followers": "1,847", "datetime": "2017-06-19 03:30:25", "author": "@katsuhitosudoh", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "878852026003668992": {"followers": "304", "datetime": "2017-06-25 05:47:05", "author": "@preddy__", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "877018552649306112": {"followers": "124", "datetime": "2017-06-20 04:21:31", "author": "@cbaziotis", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "880938814788829184": {"followers": "12,547", "datetime": "2017-06-30 23:59:15", "author": "@ml_review", "content_summary": "\"Attention Is All You Need\" paper from Google. Achieves a single-model state-of-the-art BLEU score of 41.0. https://t.co/7XorGfLQyz https://t.co/lW1X8zr9VD"}, "879212779667836929": {"followers": "77", "datetime": "2017-06-26 05:40:36", "author": "@lianggang", "content_summary": "New #NeuralNetwork architecture, Transformer, proposed by #Google. Attention is all it needs. https://t.co/hQxZAgpcwY"}, "876969927416086528": {"followers": "26", "datetime": "2017-06-20 01:08:18", "author": "@rajmadhan", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "877489879315349504": {"followers": "131", "datetime": "2017-06-21 11:34:24", "author": "@olivlogol", "content_summary": "RT @kanair: I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mH\u2026"}, "1117245953012944897": {"followers": "35", "datetime": "2019-04-14 01:59:27", "author": "@humbnlp", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "905811165443268610": {"followers": "244", "datetime": "2017-09-07 15:13:05", "author": "@s_ryosky", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "1250861127724138497": {"followers": "80", "datetime": "2020-04-16 18:58:27", "author": "@keanp8001", "content_summary": "Original paper 'Attention Is All You Need' by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin #NLP #Transformers #AI https://t.co/YFkIODjtVh"}, "877029199835312128": {"followers": "783", "datetime": "2017-06-20 05:03:50", "author": "@muktabh", "content_summary": "RT @arxiv_cs_cl: Attention Is All You Need. (arXiv:1706.03762v2 [cs.CL] UPDATED) https://t.co/8ovOiC3bdC #NLProc"}, "876796761003745281": {"followers": "159,742", "datetime": "2017-06-19 13:40:12", "author": "@Montreal_IA", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "880832593775783938": {"followers": "296", "datetime": "2017-06-30 16:57:10", "author": "@balicea1", "content_summary": "Learning through attention; or, \"Attention is All You Need\": https://t.co/ISJhLhT8tg"}, "874839537909977089": {"followers": "2", "datetime": "2017-06-14 04:02:54", "author": "@JeffreyYe1988", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1176364009114652672": {"followers": "150", "datetime": "2019-09-24 05:13:30", "author": "@AjiteshShukla7", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874506060844773376": {"followers": "2,176", "datetime": "2017-06-13 05:57:47", "author": "@wednesdaymuse", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874697767410556932": {"followers": "76", "datetime": "2017-06-13 18:39:33", "author": "@X_rayAI", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874877625788968960": {"followers": "7,059", "datetime": "2017-06-14 06:34:15", "author": "@naz_erkan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "966040432907079680": {"followers": "163", "datetime": "2018-02-20 20:02:42", "author": "@DMfun", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874601851508936704": {"followers": "23", "datetime": "2017-06-13 12:18:25", "author": "@vicozoh", "content_summary": "RT @iamtrask: woah!! https://t.co/goRc2aH65U"}, "1245457421469958145": {"followers": "668", "datetime": "2020-04-01 21:06:03", "author": "@metadiogenes", "content_summary": "@insurrealist Attention is all you need https://t.co/UQ0UDjVxGw"}, "1189848205359534082": {"followers": "7,914", "datetime": "2019-10-31 10:14:53", "author": "@NicholaStott", "content_summary": "The T in BERT=Transformer. This paper was the original work that proposed an attention based transformer model, thereby changing the game. Interesting read: https://t.co/ZSr2fq1Dpc #seo #ai #BERT #MachineLearning"}, "1171231932421722115": {"followers": "718", "datetime": "2019-09-10 01:20:27", "author": "@_Sharraf", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026"}, "899555652715945984": {"followers": "1,331", "datetime": "2017-08-21 08:55:55", "author": "@im132nd", "content_summary": "\"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.co/iqtPbpOYCp"}, "987060021245685762": {"followers": "979", "datetime": "2018-04-19 20:07:02", "author": "@MachineRockstar", "content_summary": "Ein Paper zum Thema Autoencoder im Bereich #NLP? Lest \"Attention Is All You Need\" von Ashish Vaswani et al. Viel Spa\u00df! #MachineLearning #textprocessing #ml https://t.co/VaBttwZhgx https://t.co/bgouEo9vuM"}, "1083241703870480384": {"followers": "94", "datetime": "2019-01-10 05:58:42", "author": "@Julius_Frost", "content_summary": "I'm ashamed I didn't find the paper (https://t.co/pM9Y1bxzAs) until now since it's pretty influential"}, "877538130407981062": {"followers": "419", "datetime": "2017-06-21 14:46:08", "author": "@Ahkailash1", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874824236103348225": {"followers": "61", "datetime": "2017-06-14 03:02:05", "author": "@AshwinKGanesan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166025105203433472": {"followers": "28", "datetime": "2019-08-26 16:30:23", "author": "@360macky", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876618930781011972": {"followers": "258", "datetime": "2017-06-19 01:53:34", "author": "@AshiData", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874747219269779456": {"followers": "729", "datetime": "2017-06-13 21:56:03", "author": "@aagahi", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "877168515056377857": {"followers": "646", "datetime": "2017-06-20 14:17:25", "author": "@ujwlkarn", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1116957099848634368": {"followers": "219", "datetime": "2019-04-13 06:51:39", "author": "@__phanhoang__", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874590532479078400": {"followers": "135,428", "datetime": "2017-06-13 11:33:26", "author": "@hiconcep", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "998408043904856066": {"followers": "438", "datetime": "2018-05-21 03:40:01", "author": "@ChaseTheTruth", "content_summary": "@balajis This and annotated transformer are the most exciting NLP developments I've seen in years. See: Attention Is All You Need https://t.co/YR04YUfGw3 @GoogleAI"}, "949231678353940480": {"followers": "114", "datetime": "2018-01-05 10:50:42", "author": "@mtothen04", "content_summary": "RT @hillbig: To collect non-local information in feature map, self-attention is promising because it can learn a flexible connection and ca\u2026"}, "1231896984568680449": {"followers": "222", "datetime": "2020-02-24 11:01:43", "author": "@PapersTrending", "content_summary": "[2/10] \ud83d\udcc8 - Attention Is All You Need - 349 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "874436049879146498": {"followers": "25,741", "datetime": "2017-06-13 01:19:35", "author": "@Miles_Brundage", "content_summary": "arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: https://t.co/KMscVIhqQo"}, "874546527078760448": {"followers": "18", "datetime": "2017-06-13 08:38:35", "author": "@vanduc103", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874774400737972229": {"followers": "818", "datetime": "2017-06-13 23:44:04", "author": "@ErmiaBivatan", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874901813862027264": {"followers": "57", "datetime": "2017-06-14 08:10:21", "author": "@jose_e_pons", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874450785849004033": {"followers": "8,829", "datetime": "2017-06-13 02:18:08", "author": "@Ted_Underwood", "content_summary": "RT @jasonbaldridge: A really interesting new model from Google! Not just MT: check out the parsing results & attention viz in appendix. htt\u2026"}, "1185890271080390656": {"followers": "215", "datetime": "2019-10-20 12:07:28", "author": "@PolandCherieM", "content_summary": "..So much of CS/CE has a deeper meaning with my work in AI/DL. Its actually pretty miraculous. Found two papers recently that are truly transformative (literally). (1) Vaswani et al., \u201cAttention is All You Need\u201d about RNNs, LSTM, GRU, & the Transformer"}, "874887915041685505": {"followers": "0", "datetime": "2017-06-14 07:15:08", "author": "@DeepLearningNew", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "938583436155064320": {"followers": "4,200", "datetime": "2017-12-07 01:38:23", "author": "@arxiv_cs_cl", "content_summary": "https://t.co/8ovOiC3bdC Attention Is All You Need. (arXiv:1706.03762v5 [cs.CL] UPDATED) #NLProc"}, "948819611104903169": {"followers": "824", "datetime": "2018-01-04 07:33:18", "author": "@morioka", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874472532518109184": {"followers": "2,530", "datetime": "2017-06-13 03:44:33", "author": "@_329_", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1218595779209617408": {"followers": "1,112", "datetime": "2020-01-18 18:07:28", "author": "@Zanet_96", "content_summary": "\ud835\udfd0\ud835\udfce\ud835\udfd0\ud835\udfce \ud835\udc22\ud835\udc27 \ud835\udc29\ud835\udc1a\ud835\udc29\ud835\udc1e\ud835\udc2b\ud835\udc2c Paper #2: Attention is all you need (Vaswani , ... - 2017) \ud835\udc30\ud835\udc21\ud835\udc1a\ud835\udc2d \ud835\udc1d\ud835\udc28\ud835\udc1e\ud835\udc2c \ud835\udc22\ud835\udc2d \ud835\udc2c\ud835\udc1a\ud835\udc32? In this paper, it gets presented a revolutionary structure to handle time-series data: the Transformer. The Trans\u2026https://t.co/U0Hjuq0A0D https://t.co/nNZYkheQ3C"}, "961199724446474240": {"followers": "487", "datetime": "2018-02-07 11:27:27", "author": "@Quasi_quant2010", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "875914233308999681": {"followers": "1,646", "datetime": "2017-06-17 03:13:21", "author": "@dipanjand", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "1091042558841683979": {"followers": "7,292", "datetime": "2019-01-31 18:36:31", "author": "@MLHispano", "content_summary": "Ayer se eligi\u00f3 el nuevo \ud83d\udcf0paper para #lectura-de-papers Attention Is All You Need Presenta una nueva arquitectura, el Transformer, basada \u00fanicamente en attention mechanism\ud83d\udc40. https://t.co/4QR2nDWqmC https://t.co/sjhpsxhd9J"}, "875401244038365188": {"followers": "306", "datetime": "2017-06-15 17:14:55", "author": "@Qihong_Lu", "content_summary": "RT @gigasquid: Looking forward to the code examples being released for \"Attention is All You Need\". See you RNN/ LSTM. https://t.co/LUuZ6oZ\u2026"}, "1165700223269842945": {"followers": "396", "datetime": "2019-08-25 18:59:25", "author": "@Reido2012", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874578224956813317": {"followers": "920", "datetime": "2017-06-13 10:44:32", "author": "@KloudStrife", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1249181801630838790": {"followers": "624", "datetime": "2020-04-12 03:45:24", "author": "@api_daily", "content_summary": "RT @tamara_bozovic: Idea: using #MachineLearning to analyse people's perceptions of their streets post #lockdown from tweets. Challenge: u\u2026"}, "876689134441648129": {"followers": "590", "datetime": "2017-06-19 06:32:32", "author": "@codekee", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876626965809086464": {"followers": "191", "datetime": "2017-06-19 02:25:30", "author": "@satoshi2373", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "948854692880171008": {"followers": "557", "datetime": "2018-01-04 09:52:42", "author": "@ocaokgbu", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874655584582156288": {"followers": "2,927", "datetime": "2017-06-13 15:51:56", "author": "@evolvingstuff", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1079398769165246465": {"followers": "149", "datetime": "2018-12-30 15:28:15", "author": "@parker_brydon", "content_summary": "Attention Is All You Need paper On Transformers An essential part of most SOTA NLP nets today https://t.co/bRj10H2jAm"}, "874588618865524736": {"followers": "180", "datetime": "2017-06-13 11:25:50", "author": "@lovemuffim114", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "964485262536466432": {"followers": "86", "datetime": "2018-02-16 13:03:00", "author": "@spring_stream", "content_summary": "After implementing Transformer (https://t.co/WQOiUOWfLd) twice myself, and now using Tensor2Tensor's implementation, it's becoming my go-to architecture. It's not giving SOTA-results in every problem, but it's superbly interpretable and it \"feels right\", m"}, "876960419965009921": {"followers": "781", "datetime": "2017-06-20 00:30:31", "author": "@HrSaghir", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874650059723464705": {"followers": "135", "datetime": "2017-06-13 15:29:59", "author": "@Churipapiau", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "892544320833818624": {"followers": "593", "datetime": "2017-08-02 00:35:23", "author": "@soumen_eclectic", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "885121015055085568": {"followers": "13", "datetime": "2017-07-12 12:57:49", "author": "@xyc234", "content_summary": "@PyTorch implementation of Attention is all you need https://t.co/9HylJYUMx1 https://t.co/2Z4OFDUXL2"}, "907217584310964224": {"followers": "1,058", "datetime": "2017-09-11 12:21:42", "author": "@BigsnarfDude", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "874688146578980865": {"followers": "407", "datetime": "2017-06-13 18:01:19", "author": "@_yaowu_", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1114563103360835584": {"followers": "640", "datetime": "2019-04-06 16:18:46", "author": "@copasta_", "content_summary": "\u8aad\u307f\u8aad\u307f/\u91d1\u66dc\u30d0\u30a4\u30c8\u3067\u8aad\u3081\u306a\u304b\u3063\u305f\u306e\u3067"}, "876845381664899074": {"followers": "58", "datetime": "2017-06-19 16:53:24", "author": "@aptr322", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876652264592596993": {"followers": "1,268", "datetime": "2017-06-19 04:06:01", "author": "@dlowd", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "993522912111071233": {"followers": "1,348", "datetime": "2018-05-07 16:08:15", "author": "@udmrzn", "content_summary": "RT @Deep_In_Depth: arXiv - Attention Is All You Need https://t.co/9ixw38rf7T"}, "877470374753120256": {"followers": "23,756", "datetime": "2017-06-21 10:16:54", "author": "@brideOfFacetasm", "content_summary": "RT @dataandme: So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #mach\u2026"}, "874681171099099136": {"followers": "98", "datetime": "2017-06-13 17:33:36", "author": "@TomerLevinboim", "content_summary": "NMT - {RNN or CNN} + {layered self attention models} = Attention Is All You Need @GoogleBrain; https://t.co/GfeXQjw7Qt"}, "874607703200415745": {"followers": "428", "datetime": "2017-06-13 12:41:40", "author": "@yashk2810", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874595378276044800": {"followers": "150", "datetime": "2017-06-13 11:52:42", "author": "@rodgzilla", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "876603103826419712": {"followers": "1,784", "datetime": "2017-06-19 00:50:41", "author": "@mainyaa", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874504432158822400": {"followers": "105", "datetime": "2017-06-13 05:51:18", "author": "@mathieufortier", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874598198018428929": {"followers": "614", "datetime": "2017-06-13 12:03:54", "author": "@sasank51", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874857236924944384": {"followers": "49", "datetime": "2017-06-14 05:13:14", "author": "@MandoPedraza", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1089993287040229376": {"followers": "170", "datetime": "2019-01-28 21:07:05", "author": "@dr_levan", "content_summary": "RT @seb_ruder: @EliasWalyBa @bayethiernodiop @ylecun @JeffDean @fchollet @francesc @dickoah @jeremyphoward @crowdglory @Moustapha_6C @Siley\u2026"}, "1165900968455786496": {"followers": "24", "datetime": "2019-08-26 08:17:06", "author": "@yurakuratov", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874755914431172608": {"followers": "49", "datetime": "2017-06-13 22:30:36", "author": "@zgcarvalho", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1117009077773701122": {"followers": "16,548", "datetime": "2019-04-13 10:18:11", "author": "@GaelVaroquaux", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874952081056071681": {"followers": "561", "datetime": "2017-06-14 11:30:06", "author": "@menace_", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876897777762017280": {"followers": "34", "datetime": "2017-06-19 20:21:36", "author": "@clbam8", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "925685270590246912": {"followers": "10,994", "datetime": "2017-11-01 11:25:41", "author": "@thomaskipf", "content_summary": "Mixture model CNNs https://t.co/63Q7GpYZM7. Similar to multi-head self-attention https://t.co/yYUeXefUSR, but for graphs 2/5"}, "874437832558862336": {"followers": "587", "datetime": "2017-06-13 01:26:40", "author": "@ethancaballero", "content_summary": "RT @arxiv_cs_cl: Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/8ovOiC3bdC #NLProc"}, "876721753749430272": {"followers": "506", "datetime": "2017-06-19 08:42:09", "author": "@dvilasuero", "content_summary": "\ud83d\ude01 https://t.co/K50HMU8vNW"}, "874589957716795392": {"followers": "317", "datetime": "2017-06-13 11:31:09", "author": "@MathWei", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1233573671387779072": {"followers": "1,815", "datetime": "2020-02-29 02:04:16", "author": "@DimejiMudele", "content_summary": "\"Attention is all you need\" is probably the most important scientific publication in Deep learning since 2017. Here you go. Enjoy it! https://t.co/sstJLR0vzu"}, "876694546431836160": {"followers": "17,545", "datetime": "2017-06-19 06:54:02", "author": "@memotv", "content_summary": "@_vade @kcimc @quasimondo @genekogan @pkmital V interesting. Just last week same authors said \"attention is all you need\"! :) https://t.co/7sGDzP9tE2"}, "891791599264350208": {"followers": "4,327", "datetime": "2017-07-30 22:44:20", "author": "@jekbradbury", "content_summary": "@haldaume3 residual layers (b/c https://t.co/68EzsEnOs3), self-attention (b/c https://t.co/R9WHZokiRI) and define-by-run (b/c a 5x productivity boost)"}, "928236073016164353": {"followers": "1,964", "datetime": "2017-11-08 12:21:40", "author": "@mickey24", "content_summary": "Attention Is All You Need\u8aad\u3080\u3002 https://t.co/tUS6CkMtqW"}, "877171157081903104": {"followers": "41", "datetime": "2017-06-20 14:27:55", "author": "@aiton5", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874632158295412737": {"followers": "25,311", "datetime": "2017-06-13 14:18:51", "author": "@deeplearning4j", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876744513976401924": {"followers": "2,874", "datetime": "2017-06-19 10:12:35", "author": "@feyeleanor", "content_summary": "RT @memotv: @_vade @kcimc @quasimondo @genekogan @pkmital V interesting. Just last week same authors said \"attention is all you need\"! :)\u2026"}, "1013950719735582720": {"followers": "4,457", "datetime": "2018-07-03 01:01:04", "author": "@davidcrawshaw", "content_summary": "RT @moultano: @davidcrawshaw https://t.co/Nv7vKV3o0M This is the direction everything is going for short (sentence length) text."}, "876767351492026368": {"followers": "25,741", "datetime": "2017-06-19 11:43:20", "author": "@Miles_Brundage", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "876911644764282882": {"followers": "399", "datetime": "2017-06-19 21:16:42", "author": "@kamilsindi", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874962637955579904": {"followers": "657", "datetime": "2017-06-14 12:12:03", "author": "@komakusaryama", "content_summary": "RT @icoxfog417: RNN/CNN\u3092\u4f7f\u308f\u305a\u7ffb\u8a33\u306eSOTA\u3092\u9054\u6210\u3057\u305f\u8a71\u3002Attention\u3092\u57fa\u790e\u3068\u3057\u305f\u4f1d\u642c\u304c\u809d\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u5358\u8a9e/\u4f4d\u7f6e\u306elookup\u304b\u3089\u5165\u529b\u3092\u4f5c\u6210\u3001Encoder\u306f\u5165\u529b\uff0b\u524d\u56de\u51fa\u529b\u304b\u3089A\u3092\u4f5c\u6210\u3057\u305d\u306e\u5f8c\u4f4d\u7f6e\u3054\u3068\u306b\u4f1d\u642c\u3001Decoder\u306fEncoder\u51fa\u529b\uff0b\u524d\u2026"}, "874511297144016897": {"followers": "859", "datetime": "2017-06-13 06:18:35", "author": "@cosminnegruseri", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874701736757387264": {"followers": "183", "datetime": "2017-06-13 18:55:19", "author": "@dkastaniotis", "content_summary": "RT @cto_movidius: Busy week in Deep Networks: attention networks https://t.co/gI4a2k7n61, SELU https://t.co/utp0GPYTB6 & CortexNet https://\u2026"}, "981915031729311745": {"followers": "855", "datetime": "2018-04-05 15:22:41", "author": "@FMarradi", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/VOXHCgu79I"}, "876756508314349568": {"followers": "711", "datetime": "2017-06-19 11:00:15", "author": "@paidicreed", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "875172340166012928": {"followers": "257", "datetime": "2017-06-15 02:05:20", "author": "@SigP226", "content_summary": "#machinetranslation #nlp [1706.03762] Attention Is All You Need https://t.co/beqJzLgd8Z"}, "880318296330977280": {"followers": "205", "datetime": "2017-06-29 06:53:32", "author": "@jongukim", "content_summary": "\"Attention is All You Need\" \ub17c\ubb38 \ubcf4\ub294 \uc911 https://t.co/9C5OhVzwlm \ubb38\uc81c\ub9c8\ub2e4 \ub124\ud2b8\uc6cc\ud06c\ub97c \ub2e4\ub974\uac8c \uc138\ud305\ud558\uace0 \ud29c\ub2dd\ud558\ub294\ub370, \uadf8\ub7ec\uc9c0 \ub9d0\uace0 \uc5ec\ub7ec \ubb38\uc81c \ub2e4 \uc798\ud478\ub294 \ub124\ud2b8\uc6cc\ud06c \ud558\ub098 \ub9cc\ub4e4\uc5b4\ubcf4\uc790\ub294 \uc544\uc774\ub514\uc5b4."}, "1165893610497355776": {"followers": "127", "datetime": "2019-08-26 07:47:52", "author": "@TonnyESP", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874483411687063553": {"followers": "1,275", "datetime": "2017-06-13 04:27:47", "author": "@debreuil", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874921651674263552": {"followers": "10,902", "datetime": "2017-06-14 09:29:11", "author": "@balajiln", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874694818089156608": {"followers": "44", "datetime": "2017-06-13 18:27:50", "author": "@innovaedge", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874456908593111040": {"followers": "524", "datetime": "2017-06-13 02:42:28", "author": "@Namnamseo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874971700299005952": {"followers": "346", "datetime": "2017-06-14 12:48:04", "author": "@PariAIML", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875475009795366912": {"followers": "825", "datetime": "2017-06-15 22:08:02", "author": "@fly51fly", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "874585900692492288": {"followers": "52,990", "datetime": "2017-06-13 11:15:02", "author": "@iamtrask", "content_summary": "in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://t.co/auYCtqxQ2q"}, "881870213175803904": {"followers": "3,500", "datetime": "2017-07-03 13:40:17", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "874551226662367232": {"followers": "366", "datetime": "2017-06-13 08:57:15", "author": "@surafelml", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1168269983752675328": {"followers": "337", "datetime": "2019-09-01 21:10:44", "author": "@erickherring", "content_summary": "@MelMitchell1 Not an NLP person, per se, but the paper that introduced the model is https://t.co/pt4meJFPIJ - maybe one of @ashVaswani @nikiparmar09 @kyosu or @JonesLlion will chime in"}, "874476938055864321": {"followers": "15,143", "datetime": "2017-06-13 04:02:03", "author": "@alevergara78", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "874588418860351488": {"followers": "78,662", "datetime": "2017-06-13 11:25:02", "author": "@soumithchintala", "content_summary": "Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: https://t.co/90IfPJz2Mv https://t.co/3KZlNHoQbw"}, "874570468929540096": {"followers": "934", "datetime": "2017-06-13 10:13:43", "author": "@pavelkordik", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1028732413768130560": {"followers": "322", "datetime": "2018-08-12 19:58:15", "author": "@sandsubramanian", "content_summary": "RT @tkasasagi: Attention Is All You Need https://t.co/Vuo9FpWexG"}, "874543747526041601": {"followers": "211", "datetime": "2017-06-13 08:27:32", "author": "@vsrnth", "content_summary": "RT @jigarkdoshi: No more RNN or CNN only attention. SOTA on machine translation. The game got spicy! https://t.co/wib3PdV36E"}, "875001139422527489": {"followers": "177,517", "datetime": "2017-06-14 14:45:03", "author": "@Montreal_AI", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165859530338516992": {"followers": "188", "datetime": "2019-08-26 05:32:27", "author": "@ressonliao", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874446118087606274": {"followers": "2,671", "datetime": "2017-06-13 01:59:35", "author": "@ayirpelle", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "888248617449828353": {"followers": "44", "datetime": "2017-07-21 04:05:47", "author": "@SamuelJosephK", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "966321268122255362": {"followers": "615", "datetime": "2018-02-21 14:38:38", "author": "@KouroshMeshgi", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874492932979478528": {"followers": "7,922", "datetime": "2017-06-13 05:05:37", "author": "@jasonbaldridge", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1165700513012146176": {"followers": "2,122", "datetime": "2019-08-25 19:00:34", "author": "@fullNam35087976", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "972250375263842304": {"followers": "525", "datetime": "2018-03-09 23:18:47", "author": "@_tallison", "content_summary": "RT @lintool: You know that game when you only talk using movie or song titles? I wonder if you can do so using only arXiv paper titles. I'l\u2026"}, "874521910415155200": {"followers": "937", "datetime": "2017-06-13 07:00:45", "author": "@iiacobac", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874876933145845761": {"followers": "99", "datetime": "2017-06-14 06:31:29", "author": "@zhanweiz", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874771367614218240": {"followers": "1,580", "datetime": "2017-06-13 23:32:01", "author": "@GlennPeterD", "content_summary": "RT @jasonbaldridge: A really interesting new model from Google! Not just MT: check out the parsing results & attention viz in appendix. htt\u2026"}, "876620278880550916": {"followers": "11,293", "datetime": "2017-06-19 01:58:55", "author": "@gneubig", "content_summary": "Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I Same authors\ud83d\ude00"}, "877596349331423232": {"followers": "974", "datetime": "2017-06-21 18:37:29", "author": "@CerenkovLight", "content_summary": "[1706.03762] Attention Is All You Need (Over my head of course -but What about Love?) \ud83e\udd16\ud83c\udfb6 All You Need Is Love \u2764\ufe0f https://t.co/cozmVqYNY9"}, "876931543935008770": {"followers": "96", "datetime": "2017-06-19 22:35:47", "author": "@uku_peter", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1218331294658191362": {"followers": "10,173", "datetime": "2020-01-18 00:36:30", "author": "@twimlai", "content_summary": "RT @comcomodel: Music & AI plus A Geometric Perspective on RL \u26a0\ufe0fAttention Is All You Need https://t.co/EHmKtIbbKf \ud83d\udce3Unreasonable Effectivene\u2026"}, "874882211035656193": {"followers": "761", "datetime": "2017-06-14 06:52:28", "author": "@yieldthought", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876936347222773765": {"followers": "409", "datetime": "2017-06-19 22:54:52", "author": "@AmirSaffari", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874782055170588672": {"followers": "683", "datetime": "2017-06-14 00:14:29", "author": "@marujiruo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876649606330933248": {"followers": "947", "datetime": "2017-06-19 03:55:28", "author": "@sebschu", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1165752341792010240": {"followers": "219", "datetime": "2019-08-25 22:26:31", "author": "@pabloppp", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874442212527947776": {"followers": "2,826", "datetime": "2017-06-13 01:44:04", "author": "@VenkatNagaswamy", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874475657949990912": {"followers": "99", "datetime": "2017-06-13 03:56:58", "author": "@treasured_write", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876682599464226817": {"followers": "1,365", "datetime": "2017-06-19 06:06:34", "author": "@akinori_ito", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874688193907625985": {"followers": "209", "datetime": "2017-06-13 18:01:31", "author": "@TheAshenLight", "content_summary": "Sequence transduction with attention and positional encodings. Interesting results in machine translation. #NLProc #Transformer https://t.co/vvg6JJNVbE"}, "874593194759028736": {"followers": "107", "datetime": "2017-06-13 11:44:01", "author": "@sdemyanov", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874504438269882368": {"followers": "86", "datetime": "2017-06-13 05:51:20", "author": "@JADORE801120", "content_summary": "Worth reading! The RNN encoder-decoder framework is facing so many challenges recently!!\ud83d\ude0e https://t.co/MmgF4GoKvj"}, "1165712114062909446": {"followers": "534", "datetime": "2019-08-25 19:46:40", "author": "@inemshan", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1013746237147885568": {"followers": "25", "datetime": "2018-07-02 11:28:32", "author": "@oou_wids2018", "content_summary": "RT @DataScienceNIG: ATTENTION is all you need with a TRANSFORMER! An efficient way to replace recurrent networks as a method of modeling de\u2026"}, "874845270839840769": {"followers": "299", "datetime": "2017-06-14 04:25:41", "author": "@AlOrozco53", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874449229497761793": {"followers": "3,258", "datetime": "2017-06-13 02:11:57", "author": "@yellowshippo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "878839944189362176": {"followers": "1,037", "datetime": "2017-06-25 04:59:05", "author": "@motolin5", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874995094696284160": {"followers": "258", "datetime": "2017-06-14 14:21:01", "author": "@AshiData", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874642881247444992": {"followers": "6,390", "datetime": "2017-06-13 15:01:27", "author": "@mtyka", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1064837663243595777": {"followers": "843", "datetime": "2018-11-20 11:07:37", "author": "@IxaTaldea", "content_summary": "RT @xabi_soto: 5 Bestalde, hizkuntzaren prozesamenduan azkenaldian modan dagoen teknika sare neuronaletan oinarritzen da, zehazki Transfor\u2026"}, "967943503396261888": {"followers": "9", "datetime": "2018-02-26 02:04:49", "author": "@WeinroT_xc", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876127381097938944": {"followers": "113", "datetime": "2017-06-17 17:20:19", "author": "@gevorg_s", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "919127535329218560": {"followers": "770", "datetime": "2017-10-14 09:07:35", "author": "@nakano_muramoto", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874670293679132672": {"followers": "286", "datetime": "2017-06-13 16:50:23", "author": "@petervenable", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874510044422840320": {"followers": "2,285", "datetime": "2017-06-13 06:13:36", "author": "@bsaeta", "content_summary": "Attention may be all you need! https://t.co/5MHesku1zx"}, "1117144561858555905": {"followers": "493", "datetime": "2019-04-13 19:16:33", "author": "@RSprugnoli", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1083882123021377536": {"followers": "48", "datetime": "2019-01-12 00:23:30", "author": "@Joeliu2016", "content_summary": "RT @AUEBNLPGroup: Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0\u2026"}, "1165724872426872834": {"followers": "7", "datetime": "2019-08-25 20:37:22", "author": "@LYTiQ_ai", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874657654739656704": {"followers": "251", "datetime": "2017-06-13 16:00:09", "author": "@Tanaygahlot", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "1166098131127324677": {"followers": "77", "datetime": "2019-08-26 21:20:34", "author": "@netw0rkf10w", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "991685979223216128": {"followers": "207", "datetime": "2018-05-02 14:28:56", "author": "@TigerScorpior", "content_summary": "what we need now? https://t.co/uMH48Cm5Ir"}, "966072910455693313": {"followers": "541", "datetime": "2018-02-20 22:11:45", "author": "@AndriiTsok", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "948822382679961601": {"followers": "63", "datetime": "2018-01-04 07:44:18", "author": "@fjnyan__", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874658616543805440": {"followers": "261", "datetime": "2017-06-13 16:03:59", "author": "@GuptaRajat033", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165811794482532357": {"followers": "1,025", "datetime": "2019-08-26 02:22:46", "author": "@desertnaut", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "894619683588014080": {"followers": "2,170", "datetime": "2017-08-07 18:02:08", "author": "@iskander", "content_summary": "Attention Is All You Need: https://t.co/YLd8hwsmGk"}, "875447431521079296": {"followers": "52", "datetime": "2017-06-15 20:18:27", "author": "@hoangcuong0605", "content_summary": "RT @iamaidang: Our paper \"All You Need Is Attention\". SOTA on WMT EN-DE using only feed forward and attention layers. https://t.co/QmXdfgYo\u2026"}, "876784171611336704": {"followers": "245", "datetime": "2017-06-19 12:50:11", "author": "@__jm", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1165702205581778951": {"followers": "441", "datetime": "2019-08-25 19:07:18", "author": "@Elira74031275", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "877148829644185601": {"followers": "44,329", "datetime": "2017-06-20 12:59:12", "author": "@dataandme", "content_summary": "@_brohrer_ hat tip to you, sir! I was trying so hard to remember who it was who pointed me to that paper, & when I saw your avi just now, \ud83d\ude2f! https://t.co/W6hrJx5F2N"}, "874736295444131842": {"followers": "62", "datetime": "2017-06-13 21:12:39", "author": "@VasanSankar", "content_summary": "Do we have another #AI #deeplearning breakthrough with this work? https://t.co/UicaR5JqhU"}, "966011016944340992": {"followers": "4", "datetime": "2018-02-20 18:05:48", "author": "@ZilongZhong", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874651211907055618": {"followers": "49", "datetime": "2017-06-13 15:34:33", "author": "@RenaudBougues", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876262820312207367": {"followers": "58", "datetime": "2017-06-18 02:18:31", "author": "@NeurAutomata", "content_summary": "Attention Is All You Need https://t.co/QuKTyvZQej"}, "1116982429091282944": {"followers": "78", "datetime": "2019-04-13 08:32:18", "author": "@spinthma", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874578483820855296": {"followers": "385", "datetime": "2017-06-13 10:45:34", "author": "@emilmont", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874582207251329025": {"followers": "3,020", "datetime": "2017-06-13 11:00:21", "author": "@NataliaDiazRodr", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876797330032279552": {"followers": "1,614", "datetime": "2017-06-19 13:42:28", "author": "@NatalieRens", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1165778959209885696": {"followers": "65", "datetime": "2019-08-26 00:12:17", "author": "@kli_nlpr", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "877139267100823552": {"followers": "44,329", "datetime": "2017-06-20 12:21:12", "author": "@dataandme", "content_summary": "So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #machinelearning https://t.co/7LOZ0gcOc4"}, "966247146004664320": {"followers": "301", "datetime": "2018-02-21 09:44:06", "author": "@relopezbriega", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874633112453566464": {"followers": "196", "datetime": "2017-06-13 14:22:38", "author": "@dvatterott", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "877981550280888320": {"followers": "47", "datetime": "2017-06-22 20:08:08", "author": "@sat1769", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966214877420380161": {"followers": "494", "datetime": "2018-02-21 07:35:52", "author": "@RemiCadene", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876978129306243072": {"followers": "207", "datetime": "2017-06-20 01:40:54", "author": "@wangly0229", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874439053717929984": {"followers": "7,670", "datetime": "2017-06-13 01:31:31", "author": "@KyleCranmer", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876977401175064576": {"followers": "428", "datetime": "2017-06-20 01:38:00", "author": "@yashk2810", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "891793450802098176": {"followers": "99,939", "datetime": "2017-07-30 22:51:42", "author": "@jeremyphoward", "content_summary": "RT @jekbradbury: @haldaume3 residual layers (b/c https://t.co/68EzsEnOs3), self-attention (b/c https://t.co/R9WHZokiRI) and define-by-run (\u2026"}, "875630101446107136": {"followers": "395", "datetime": "2017-06-16 08:24:19", "author": "@jeffmgould", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "948921425632489474": {"followers": "218", "datetime": "2018-01-04 14:17:52", "author": "@AssistedEvolve", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "874494802196385794": {"followers": "146", "datetime": "2017-06-13 05:13:02", "author": "@xLaszlo", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1111008111488393216": {"followers": "2,031", "datetime": "2019-03-27 20:52:29", "author": "@srchvrs", "content_summary": "@ziwphd https://t.co/mJcOxCKuHG"}, "874686781345681409": {"followers": "6,717", "datetime": "2017-06-13 17:55:54", "author": "@mkrobot", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876014024319619072": {"followers": "78", "datetime": "2017-06-17 09:49:53", "author": "@dougmcgo1", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "874590445543854080": {"followers": "145", "datetime": "2017-06-13 11:33:05", "author": "@esvhd", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1164445863315296256": {"followers": "1,186", "datetime": "2019-08-22 07:55:02", "author": "@CarlRioux", "content_summary": "[D] Positional Encoding in Transformer: Hey all, I was reading up the transformer paper https://t.co/x1rwCpJJDN. This architecture uses positional encoding which the attention layers ignore. I don't understand two things - * Why use Sin & Cos as\u2026 https"}, "1095308407987261442": {"followers": "662", "datetime": "2019-02-12 13:07:28", "author": "@agatan_", "content_summary": "@HigeponJa Attention Is All You Need https://t.co/eUZ17XVu8L \u3067\u63d0\u6848\u3055\u308c\u305f Transformer \u4ee5\u964d\u306f self-attention \u304c\u4e3b\u6d41\u306a\u5370\u8c61\u3067\u3059\u3002\u5148\u65e5\u304a\u304a\u304d\u306a\u8a71\u984c\u306b\u306a\u3063\u305f BERT \u3082 Transformer \u7cfb\u3067\u3059\u3002"}, "875906522248650755": {"followers": "106", "datetime": "2017-06-17 02:42:43", "author": "@simplystupid1", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "948826407576023040": {"followers": "793", "datetime": "2018-01-04 08:00:18", "author": "@__tmats__", "content_summary": "RT @hillbig: \u753b\u50cf\u4e2d\u306e\u5e83\u7bc4\u56f2\u306e\u60c5\u5831\u3092\u96c6\u7d04\u3059\u308b\u306e\u306b\u56fa\u5b9a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u308f\u305a\u306b\u81ea\u5206\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306bAttention\u3092\u304b\u3051\u308bSelf-Attention\u304c\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u4e26\u5217\u5316\u53ef\u80fd\u3067\u6709\u671b\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u305d\u308c\u3092\u4e00\u822c\u5316\u3057\u305fnon-local NN\u3082\u767b\u5834\u3057\u3066\u3044\u308b \u3002https://\u2026"}, "907246404992724992": {"followers": "258", "datetime": "2017-09-11 14:16:13", "author": "@arezae", "content_summary": "RT @Urk0: I thought all we needed was attention :O https://t.co/K73CanoFNV #emnlp2017 https://t.co/IaMnZqjPd1"}, "875152023670403075": {"followers": "2,786", "datetime": "2017-06-15 00:44:36", "author": "@yogthos", "content_summary": "RT @gigasquid: Looking forward to the code examples being released for \"Attention is All You Need\". See you RNN/ LSTM. https://t.co/LUuZ6oZ\u2026"}, "874711707246931968": {"followers": "1,996", "datetime": "2017-06-13 19:34:57", "author": "@arrizalamin", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876624086033551360": {"followers": "13", "datetime": "2017-06-19 02:14:03", "author": "@Lawrencelj", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874703438952755201": {"followers": "160,790", "datetime": "2017-06-13 19:02:05", "author": "@Quebec_AI", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874614953935855618": {"followers": "2,806", "datetime": "2017-06-13 13:10:29", "author": "@nrose", "content_summary": "Nothing has advanced as quickly as this. The field is speeding up WAY faster than anyone predicted. https://t.co/A1lK25wwc0"}, "874762178016018436": {"followers": "104", "datetime": "2017-06-13 22:55:30", "author": "@alfo_512", "content_summary": "RT @iamtrask: in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://\u2026"}, "1166221067452669952": {"followers": "442", "datetime": "2019-08-27 05:29:04", "author": "@AdrianB82", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876903758420205568": {"followers": "81", "datetime": "2017-06-19 20:45:22", "author": "@anotherbugmasta", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874461928529199104": {"followers": "275", "datetime": "2017-06-13 03:02:25", "author": "@kadarakos", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874761648057106432": {"followers": "501", "datetime": "2017-06-13 22:53:23", "author": "@fuzzysphere", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874532107846119426": {"followers": "4,804", "datetime": "2017-06-13 07:41:17", "author": "@cto_movidius", "content_summary": "Busy week in Deep Networks: attention networks https://t.co/gI4a2k7n61, SELU https://t.co/utp0GPYTB6 & CortexNet https://t.co/f9lJVESYkS"}, "874484073133166597": {"followers": "861", "datetime": "2017-06-13 04:30:24", "author": "@ryf_feed", "content_summary": "Attention Is All You Need (Neural Networks) https://t.co/Gw9Y8ROLRO"}, "875969731668344836": {"followers": "76,883", "datetime": "2017-06-17 06:53:53", "author": "@NandoDF", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "1117025871897010176": {"followers": "1,025", "datetime": "2019-04-13 11:24:55", "author": "@desertnaut", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874714327206506497": {"followers": "746", "datetime": "2017-06-13 19:45:21", "author": "@SKRHardwick", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874447774292430848": {"followers": "130", "datetime": "2017-06-13 02:06:10", "author": "@jkronand", "content_summary": "RT @jigarkdoshi: No more RNN or CNN only attention. SOTA on machine translation. The game got spicy! https://t.co/wib3PdV36E"}, "876905855588777985": {"followers": "125", "datetime": "2017-06-19 20:53:42", "author": "@katsuhito_sudoh", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "927525008787750913": {"followers": "1,964", "datetime": "2017-11-06 13:16:09", "author": "@mickey24", "content_summary": "\u8ad6\u6587 \"Attention Is All You Need\": Attention\u30d9\u30fc\u30b9\u306e\u5358\u7d14\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u300cTransformer\u300d\u3067\u7ffb\u8a33\u30bf\u30b9\u30af\u306e\u7cbe\u5ea6\u5411\u4e0a\u30fb\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3092\u5b9f\u73fe\u3002RNN\u3084CNN\u306f\u4f7f\u308f\u306a\u3044\u3002\u8457\u8005\u306fGoogler https://t.co/tUS6CkMtqW"}, "875042796608118784": {"followers": "721", "datetime": "2017-06-14 17:30:34", "author": "@mhrsafa", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/oQ3B4aUd0a #ai #bigdata #datascience #machinelearning #deeplearning"}, "874460157199110144": {"followers": "1,006", "datetime": "2017-06-13 02:55:22", "author": "@alvations", "content_summary": "#neuralempty #emptyneuron ... https://t.co/ktovF1RFRf"}, "1165959883491090434": {"followers": "399", "datetime": "2019-08-26 12:11:13", "author": "@perr0r", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1165771562508681216": {"followers": "169", "datetime": "2019-08-25 23:42:54", "author": "@motor_ai", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874657150005596163": {"followers": "71", "datetime": "2017-06-13 15:58:09", "author": "@MatthieuPerrot", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876656765626175488": {"followers": "607", "datetime": "2017-06-19 04:23:55", "author": "@M4t1ss", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874690445439188992": {"followers": "2,147", "datetime": "2017-06-13 18:10:27", "author": "@sarahcat21", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "874982460685156352": {"followers": "81", "datetime": "2017-06-14 13:30:49", "author": "@schnell_hase", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875139834150572032": {"followers": "7,190", "datetime": "2017-06-14 23:56:10", "author": "@gigasquid", "content_summary": "Looking forward to the code examples being released for \"Attention is All You Need\". See you RNN/ LSTM. https://t.co/LUuZ6oZDNn"}, "874836992420401153": {"followers": "1", "datetime": "2017-06-14 03:52:47", "author": "@deep_blacksky", "content_summary": "@y_o_o_p state-of-the-art https://t.co/eOe7Usp7pd"}, "876775050174763008": {"followers": "177,517", "datetime": "2017-06-19 12:13:56", "author": "@Montreal_AI", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "874703977694277632": {"followers": "221", "datetime": "2017-06-13 19:04:14", "author": "@NedKhawar", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1036827424401514497": {"followers": "367", "datetime": "2018-09-04 04:04:55", "author": "@NorskHaus", "content_summary": "@hikikomorphism @anti_nihilist @allexx54 They are now passe b/c they are inherently sequential and do not parallize well. \"Attention\" is now the rage, which turns out to be a variation of TDNN (late 80's). https://t.co/HAw18RZVaR"}, "876775805543755776": {"followers": "861", "datetime": "2017-06-19 12:16:56", "author": "@ryf_feed", "content_summary": "Attention Is All You Need https://t.co/Gw9Y8ROLRO"}, "875145477251092480": {"followers": "7,190", "datetime": "2017-06-15 00:18:35", "author": "@gigasquid", "content_summary": "@fredbenenson Have you seen this? - it **just** came out https://t.co/LUuZ6oZDNn"}, "874579129257078784": {"followers": "645", "datetime": "2017-06-13 10:48:07", "author": "@ionandrou", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117124313977458688": {"followers": "95", "datetime": "2019-04-13 17:56:06", "author": "@IanHensel", "content_summary": "Reading through \"Attention is all you need\" https://t.co/ShUMkg5wvJ and found this amazing post by @jalammar https://t.co/IbCRFK7XEW Thanks Jay!"}, "1086069798885154816": {"followers": "229", "datetime": "2019-01-18 01:16:32", "author": "@WarwickMLClub", "content_summary": "welcome back to machine learning club. on 21 jan, 4pm to 5:30pm. henry will discuss the paper ATTENTION IS ALL YOU NEED, see https://t.co/oi21A3ctjH"}, "874587705396559872": {"followers": "79", "datetime": "2017-06-13 11:22:12", "author": "@Martix_cz", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "881687005108752384": {"followers": "783", "datetime": "2017-07-03 01:32:17", "author": "@muktabh", "content_summary": "RT @arxiv_cs_cl: Attention Is All You Need. (arXiv:1706.03762v4 [cs.CL] UPDATED) https://t.co/8ovOiC3bdC #NLProc"}, "874506686261846016": {"followers": "76,883", "datetime": "2017-06-13 06:00:16", "author": "@NandoDF", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1165729793444372480": {"followers": "145", "datetime": "2019-08-25 20:56:55", "author": "@esvhd", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "966195217257975808": {"followers": "395", "datetime": "2018-02-21 06:17:45", "author": "@robotbugs", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1165887100945870848": {"followers": "586", "datetime": "2019-08-26 07:22:00", "author": "@apachaves", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874533434693230592": {"followers": "24,449", "datetime": "2017-06-13 07:46:33", "author": "@WiringTheBrain", "content_summary": "RT @cto_movidius: Busy week in Deep Networks: attention networks https://t.co/gI4a2k7n61, SELU https://t.co/utp0GPYTB6 & CortexNet https://\u2026"}, "874821951289790464": {"followers": "29", "datetime": "2017-06-14 02:53:01", "author": "@TILPython", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877627446564184066": {"followers": "3,500", "datetime": "2017-06-21 20:41:03", "author": "@arxiv_cscl", "content_summary": "Attention Is All You Need https://t.co/hNwHudJdqI"}, "874634337366319106": {"followers": "179", "datetime": "2017-06-13 14:27:30", "author": "@GabiBluetit", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "903493213964591104": {"followers": "241", "datetime": "2017-09-01 05:42:22", "author": "@ndjido", "content_summary": "RT @bigdata: Transformer from @googleresearch outperforms both RNN & CNN models on English\u21e2German & English\u21e2French translation https://t.c\u2026"}, "876742415582769152": {"followers": "824", "datetime": "2017-06-19 10:04:15", "author": "@morioka", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "876643469862633472": {"followers": "234", "datetime": "2017-06-19 03:31:05", "author": "@deepnarainsingh", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876618383764013056": {"followers": "9,487", "datetime": "2017-06-19 01:51:24", "author": "@nishio", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "876691931027259393": {"followers": "30", "datetime": "2017-06-19 06:43:39", "author": "@__sukanta__", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874588925553201152": {"followers": "1,593", "datetime": "2017-06-13 11:27:03", "author": "@seaandsailor", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874679270324305921": {"followers": "21", "datetime": "2017-06-13 17:26:03", "author": "@Skonumuri", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "974801468887838720": {"followers": "359", "datetime": "2018-03-17 00:15:55", "author": "@guillaume_che", "content_summary": "Some Friday night fun, coding a little kata - geometric series of sine waves: https://t.co/43aVauo0EY Could be used as a positional embedding for an RNN-less Attention Mechanism: https://t.co/FddJYAJyWW #FridayNight #Coding #Kata #CodeKata #TimeSeries #RNN"}, "874503933112180736": {"followers": "66", "datetime": "2017-06-13 05:49:19", "author": "@off99555", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "876365932355923969": {"followers": "149", "datetime": "2017-06-18 09:08:15", "author": "@hvyamazaki", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "1017623517339111425": {"followers": "1,838", "datetime": "2018-07-13 04:15:27", "author": "@dsweet", "content_summary": "@semil @albertwenger @leepnet One of the more interesting ML papers of 2017 was even titled \u201cAttention is all you need\u201d: https://t.co/UF7nZNLYX8 (though that is slightly different and pretty dense :))"}, "874837985228992513": {"followers": "859", "datetime": "2017-06-14 03:56:44", "author": "@cosminnegruseri", "content_summary": "@fchollet How does it compare to https://t.co/Ncf87cGYlo"}, "874591408535089153": {"followers": "1,088", "datetime": "2017-06-13 11:36:55", "author": "@gokstudio", "content_summary": "You just need attention https://t.co/rY1KL0xOjL"}, "876624866484649985": {"followers": "210", "datetime": "2017-06-19 02:17:09", "author": "@MihaelFeldman", "content_summary": "@MapD @ContinuumIO @h2oai also worth noting new type of computation: https://t.co/RkqpxKX8vk"}, "874642535695568896": {"followers": "20", "datetime": "2017-06-13 15:00:05", "author": "@Cambrio_HQ", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874688865839312898": {"followers": "60", "datetime": "2017-06-13 18:04:11", "author": "@TedInRealLife", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874592346503094272": {"followers": "1,595", "datetime": "2017-06-13 11:40:39", "author": "@nyker_goto", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117034010478141440": {"followers": "128", "datetime": "2019-04-13 11:57:16", "author": "@ekshakhs", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874506439921774592": {"followers": "25,193", "datetime": "2017-06-13 05:59:17", "author": "@Reza_Zadeh", "content_summary": "RT @Reza_Zadeh: Sequence tasks often demand RNNs. Google paper goes against that \"wisdom\", gets leading results on English-To-German https:\u2026"}, "1165919573176213505": {"followers": "742", "datetime": "2019-08-26 09:31:02", "author": "@xpasky", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1191158293240205312": {"followers": "2,157", "datetime": "2019-11-04 01:00:42", "author": "@JonTheGeek", "content_summary": "@hankgreen @iSmashFizzle Attention is all you need! https://t.co/ORiskDc3nS"}, "874908419341381633": {"followers": "235", "datetime": "2017-06-14 08:36:36", "author": "@Arghzoo", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1117201947671650304": {"followers": "109", "datetime": "2019-04-13 23:04:35", "author": "@joseluisalcala", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874621409389051904": {"followers": "15,645", "datetime": "2017-06-13 13:36:08", "author": "@pmedina", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "875359640267223040": {"followers": "1,457", "datetime": "2017-06-15 14:29:36", "author": "@MagicHerb", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876657388261998593": {"followers": "53", "datetime": "2017-06-19 04:26:23", "author": "@MktO740123", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1084732568795258880": {"followers": "2,464", "datetime": "2019-01-14 08:42:52", "author": "@wiomax_cn", "content_summary": "RT @Deep_In_Depth: Attention Is All You Need https://t.co/9ixw38rf7T #DeepLearning #MachineLearning #AI #DataScience #NeuralNetworks #CNN #\u2026"}, "888491230585167873": {"followers": "1,527", "datetime": "2017-07-21 20:09:51", "author": "@Pedra999", "content_summary": "RT @Pedra999: All you need is Attention : Training #tensorflow - > https://t.co/fllCNmcQe5"}, "875231801781997575": {"followers": "359", "datetime": "2017-06-15 06:01:37", "author": "@guillaume_che", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876909129176043520": {"followers": "90,659", "datetime": "2017-06-19 21:06:43", "author": "@stanfordnlp", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1187830555712380929": {"followers": "531", "datetime": "2019-10-25 20:37:27", "author": "@jessthebp", "content_summary": "and because it's important to cite your sources, TO LEARN MORE: https://t.co/NwYpgkDqx9 https://t.co/r18NlJAzPS https://t.co/zKAcWeUX44"}, "875545091640672256": {"followers": "161", "datetime": "2017-06-16 02:46:31", "author": "@changhwan_yi", "content_summary": "RT @golbin: RNN\uacfc CNN \uc5c6\uc774 \uc5b4\ud150\uc158 \uba54\uce74\ub2c8\uc998\ub9cc\uc73c\ub85c Seq2Seq\ub97c \uad6c\ud604\ud588\ub294\ub370, \uc18d\ub3c4\ub3c4 \uac81\ub098 \ube60\ub974\uace0 \uacb0\uacfc\ub3c4 \uc88b\uc558\ub2e4\uace0. \uc73c\uc74c.. @_@;; https://t.co/Uuy5uJ3PZR"}, "874501394857852928": {"followers": "599", "datetime": "2017-06-13 05:39:14", "author": "@francvs", "content_summary": "RT @deliprao: The new Transformer Network looks deceptively simple for producing SOTA in MT. https://t.co/7otNrERtSH https://t.co/mkGqPin2\u2026"}, "876603000659116032": {"followers": "5,903", "datetime": "2017-06-19 00:50:16", "author": "@enakai00", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "1165783744038391808": {"followers": "453", "datetime": "2019-08-26 00:31:18", "author": "@msaffarm", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1194991032305422338": {"followers": "2,235", "datetime": "2019-11-14 14:50:38", "author": "@chatonsky", "content_summary": "Attention Is All You Need https://t.co/l3purvcIxk"}, "874735976139993088": {"followers": "136", "datetime": "2017-06-13 21:11:23", "author": "@ppujari", "content_summary": "Deep Neural Net is moving fast. No RNN, No CNN only Attention Is All You Need. https://t.co/XvLSCAaH7f"}, "925278510800883712": {"followers": "15,143", "datetime": "2017-10-31 08:29:22", "author": "@alevergara78", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "1117084832138506241": {"followers": "1,329", "datetime": "2019-04-13 15:19:13", "author": "@rcalsaverini", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "876605594144096258": {"followers": "286", "datetime": "2017-06-19 01:00:34", "author": "@t_tkd3a", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874471742680383489": {"followers": "8", "datetime": "2017-06-13 03:41:25", "author": "@marshmelloK", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875130822159769600": {"followers": "913", "datetime": "2017-06-14 23:20:21", "author": "@iw_tatsu", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874523253817184256": {"followers": "1,348", "datetime": "2017-06-13 07:06:06", "author": "@Lidinwise", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165731704725786624": {"followers": "6,438", "datetime": "2019-08-25 21:04:31", "author": "@ArtirKel", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874778605938757632": {"followers": "10", "datetime": "2017-06-14 00:00:46", "author": "@WalidAransa", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "893235642909380612": {"followers": "322", "datetime": "2017-08-03 22:22:27", "author": "@letranger14", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165859413200183296": {"followers": "162", "datetime": "2019-08-26 05:31:59", "author": "@YoavR7", "content_summary": "\u05d1\u05e7\u05e6\u05e8\u05d4: \u05e4\u05d9\u05d9\u05e1\u05d1\u05d5\u05e7 \u05db\u05ea\u05d1\u05d5 \u05de\u05d0\u05de\u05e8 \u05e2\u05dc \"\u05e7\u05d5\u05e0\u05e1\u05e4\u05d8 \u05d7\u05d3\u05e9\u05e0\u05d9\" \u05e9\u05de\u05e4\u05e9\u05d8 \u05d0\u05e8\u05db\u05d9\u05d8\u05e7\u05d8\u05d5\u05e8\u05d4 \u05e7\u05d9\u05d9\u05de\u05ea, \u05e8\u05e7 \u05e9\u05d4\u05e7\u05d5\u05e0\u05e1\u05e4\u05d8 \u05d4\u05d6\u05d4 \u05de\u05d5\u05d6\u05db\u05e8 \u05d1\u05d2\u05e8\u05e1\u05d0 \u05de\u05d0\u05d5\u05d7\u05e8\u05ea \u05e9\u05dc \u05d4\u05de\u05d0\u05de\u05e8 \u05d4\u05de\u05e7\u05d5\u05e8\u05d9, \u05e9\u05e0\u05db\u05ea\u05d1\u05d4 \u05e2\"\u05d9 \u05d4\u05d7\u05d5\u05e7\u05e8\u05d9\u05dd \u05dc\u05e4\u05e0\u05d9 \u05d9\u05d5\u05ea\u05e8 \u05de\u05e9\u05e0\u05ea\u05d9\u05d9\u05dd\ud83e\udd26\ud83c\udffd\u200d\u2642\ufe0f - \u05dc\u05e6\u05e2\u05e8\u05d9, \u05d6\u05d4 \u05dc\u05d0 \u05de\u05e4\u05ea\u05d9\u05e2 \u05d9\u05d5\u05ea\u05e8 \u05de\u05d9\u05d3\u05d9. \u05d6\u05d4 \u05de\u05d3\u05db\u05d0 \u05dc\u05e4\u05e2\u05de\u05d9\u05dd \u05e2\u05d3 \u05db\u05de\u05d4 \u05d7\u05d5\u05e7\u05e8\u05d9\u05dd \u05dc\u05d0 \u05de\u05db\u05d9\u05e8\u05d9\u05dd \u05de\u05d7\u05e7\u05e8\u05d9\u05dd \u05e7\u05e8\u05d5\u05d1\u05d9\u05dd, \u05dc\u05e4\u05e2\u05de\u05d9\u05dd \u05d0\u05e4"}, "1131330384556257280": {"followers": "5", "datetime": "2019-05-22 22:45:57", "author": "@AapkaAlterEgo", "content_summary": "@MePurplelicious Attention is all you need https://t.co/2WsHV3ThsR"}, "874779065009319940": {"followers": "8", "datetime": "2017-06-14 00:02:36", "author": "@mh_ha_soar", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "874615914112499712": {"followers": "161", "datetime": "2017-06-13 13:14:18", "author": "@farhanhubble", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "875365940505976832": {"followers": "14", "datetime": "2017-06-15 14:54:38", "author": "@elderofkrikkit", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1214949804725297152": {"followers": "725", "datetime": "2020-01-08 16:39:40", "author": "@kazmuzik", "content_summary": "Positional Encoding\u3067\u691c\u7d22\u3059\u308b\u3068\u3001\u30d6\u30ed\u30b0\u8a18\u4e8b https://t.co/uVRT9a0VvE \u304c\u3001Attention Is All You Need https://t.co/3UxV2ZeyS7 \u306e\u3001sinusoidal\u95a2\u6570\u3092\u4f7f\u3063\u305fPositional Encoding\u3092\u89e3\u8aac\u3057\u3066\u3044\u305f\u3002"}, "875727301886976001": {"followers": "2,462", "datetime": "2017-06-16 14:50:33", "author": "@todrobbins", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874626315814445056": {"followers": "812", "datetime": "2017-06-13 13:55:38", "author": "@michaelmuelly", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874825782278987778": {"followers": "4,701", "datetime": "2017-06-14 03:08:14", "author": "@sebpaquet", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874550317840576512": {"followers": "191", "datetime": "2017-06-13 08:53:38", "author": "@MulberryBeacon", "content_summary": "RT @weballergy: The Transformer network architecture, based on attention mechanisms. #deeplearning https://t.co/GOhXoQ1QDG"}, "1014164110437646336": {"followers": "5,538", "datetime": "2018-07-03 15:09:00", "author": "@cackerman1", "content_summary": "The Illustrated Transformer https://t.co/xXxHhy0lhS https://t.co/oumkpVC09y https://t.co/Zp2EHQcM3f"}, "874446004837371905": {"followers": "1,025", "datetime": "2017-06-13 01:59:08", "author": "@desertnaut", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1165992948200132610": {"followers": "356", "datetime": "2019-08-26 14:22:36", "author": "@anjiefang", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875630051366060032": {"followers": "950", "datetime": "2017-06-16 08:24:07", "author": "@omar13x", "content_summary": ":( https://t.co/4fqLXvLbnm"}, "1171692536781389824": {"followers": "199", "datetime": "2019-09-11 07:50:44", "author": "@schulzAgLop", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876887096908238848": {"followers": "4,231", "datetime": "2017-06-19 19:39:10", "author": "@gmonce", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874819078518693892": {"followers": "34", "datetime": "2017-06-14 02:41:36", "author": "@ShubhamShukla70", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874710303992291329": {"followers": "447", "datetime": "2017-06-13 19:29:22", "author": "@ankit_sachan", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874628669783760901": {"followers": "247", "datetime": "2017-06-13 14:04:59", "author": "@whereisravi", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874649829376311296": {"followers": "29", "datetime": "2017-06-13 15:29:04", "author": "@chenlailin", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "1157034574783963143": {"followers": "194", "datetime": "2019-08-01 21:05:13", "author": "@liufengbrain", "content_summary": "Attention is all you need. https://t.co/5U7F45MNLk"}, "875576911400910850": {"followers": "14", "datetime": "2017-06-16 04:52:57", "author": "@NZakharov", "content_summary": "Attention is all you need? https://t.co/4NWZVsp4Cm"}, "1202835037369982977": {"followers": "598", "datetime": "2019-12-06 06:19:55", "author": "@mothermiria", "content_summary": "@massgame https://t.co/ETayYjacM7 \uc77d\uace0 \ub2e4\uc74c\uc8fc\uae4c\uc9c0 \ub9ac\ubdf0\ud574\uc624\uc138\uc694"}, "876066157241004033": {"followers": "2,501", "datetime": "2017-06-17 13:17:03", "author": "@ESiravegna", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "874901871722364928": {"followers": "291", "datetime": "2017-06-14 08:10:35", "author": "@lpag_ai", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874585788004143104": {"followers": "164", "datetime": "2017-06-13 11:14:35", "author": "@ZachBessinger", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166191910123388928": {"followers": "33", "datetime": "2019-08-27 03:33:12", "author": "@arnavkj95", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "919069026613043202": {"followers": "12,765", "datetime": "2017-10-14 05:15:06", "author": "@jaguring1", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "1165709746760015873": {"followers": "2,331", "datetime": "2019-08-25 19:37:16", "author": "@g_dydx", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874525496410648576": {"followers": "1,377", "datetime": "2017-06-13 07:15:00", "author": "@kaineko", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117894909502066689": {"followers": "15", "datetime": "2019-04-15 20:58:10", "author": "@BADRINATHJAYAK1", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874606208610705408": {"followers": "682", "datetime": "2017-06-13 12:35:44", "author": "@eloipuertas", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166027245359513601": {"followers": "669", "datetime": "2019-08-26 16:38:53", "author": "@arunkumar_bvr", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874711427554189313": {"followers": "171", "datetime": "2017-06-13 19:33:50", "author": "@SerialDev", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "875005281473875968": {"followers": "861", "datetime": "2017-06-14 15:01:30", "author": "@ryf_feed", "content_summary": "Deep Neural Networks \u2013 Attention is all you need (No recurrence or convolutions) https://t.co/Gw9Y8ROLRO"}, "875007808441204737": {"followers": "2,387", "datetime": "2017-06-14 15:11:33", "author": "@kamalikac", "content_summary": "RT @iamtrask: in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://\u2026"}, "1256231364707393537": {"followers": "262", "datetime": "2020-05-01 14:37:51", "author": "@renolarana", "content_summary": "Attention is all you need . Todas mis publicaciones en redes sociales se producen por mi necesidad de atenci\u00f3n\ud83d\ude09 \ud83e\uddd0El art\u00edculo original, escrito por reconocidos investigadores de Google y la Universidad de Toronto lo pueden leer ac\u00e1: https://t.co/SICR6b41xO"}, "874678830732001280": {"followers": "227", "datetime": "2017-06-13 17:24:18", "author": "@hhsecond", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877063402618847232": {"followers": "266", "datetime": "2017-06-20 07:19:44", "author": "@NafiseSadat", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "875255449792245760": {"followers": "75", "datetime": "2017-06-15 07:35:35", "author": "@adityachivu", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874594046588715011": {"followers": "227", "datetime": "2017-06-13 11:47:24", "author": "@SichuLu", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1116981074238722048": {"followers": "898", "datetime": "2019-04-13 08:26:55", "author": "@kylewadegrove", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874960149101895680": {"followers": "4,755", "datetime": "2017-06-14 12:02:10", "author": "@tianhuil", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "1188506063693320192": {"followers": "28", "datetime": "2019-10-27 17:21:41", "author": "@estebarb", "content_summary": "When attention is all you need, but attention is all you don't get... https://t.co/tq0CVEftZl"}, "884659132195901440": {"followers": "34", "datetime": "2017-07-11 06:22:27", "author": "@kostyainua", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "874659357425713152": {"followers": "52", "datetime": "2017-06-13 16:06:55", "author": "@raghakot", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1016562365683281923": {"followers": "384", "datetime": "2018-07-10 05:58:49", "author": "@roopalgarg", "content_summary": "useful read: Attention Is All You Need https://t.co/r5gOTpL3uE Modelling text completely on Attention with no #RNN or #CNN needed. Increases parallelism and thus faster train time. Day 3&4 #100DaysOfMLCode #DeepLearning #NeuralNetworks #attention"}, "966073609490976770": {"followers": "758", "datetime": "2018-02-20 22:14:31", "author": "@amy8492", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "1151034300898217984": {"followers": "1,734", "datetime": "2019-07-16 07:42:17", "author": "@ivivek87", "content_summary": "Interesting feed on my iPhone chrome page on #NLP based code generator using Transformer. Tool to improve my #Python skills, I need to reach a level where I want to be comfortable w/ it like #rstats . Long way hence a hack! \ud83d\ude2c https://t.co/HfDNYAnkMC http"}, "874532978084466690": {"followers": "321", "datetime": "2017-06-13 07:44:44", "author": "@charlottebunne", "content_summary": "RT @goodfellow_ian: https://t.co/TMieyzfKYx"}, "874788937813950464": {"followers": "1,710", "datetime": "2017-06-14 00:41:50", "author": "@jankautz", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "913349724949831680": {"followers": "1,209", "datetime": "2017-09-28 10:28:38", "author": "@marimiya_clc", "content_summary": "\u3046\u307e\u3044\u30bf\u30a4\u30c8\u30eb\u3060\u306a\u301c Attention is all you need https://t.co/tMkT3ZF44P"}, "876606411819540481": {"followers": "254", "datetime": "2017-06-19 01:03:49", "author": "@mokemokechicken", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "1166014654205743104": {"followers": "10", "datetime": "2019-08-26 15:48:51", "author": "@_dabasajay", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874591986602442753": {"followers": "738", "datetime": "2017-06-13 11:39:13", "author": "@faizanj", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874446158940311552": {"followers": "2,603", "datetime": "2017-06-13 01:59:45", "author": "@AdamMarblestone", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "948829417790468096": {"followers": "633", "datetime": "2018-01-04 08:12:16", "author": "@hamraniii", "content_summary": "RT @hillbig: To collect non-local information in feature map, self-attention is promising because it can learn a flexible connection and ca\u2026"}, "874622668141547520": {"followers": "2,387", "datetime": "2017-06-13 13:41:08", "author": "@kamalikac", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874751537859448832": {"followers": "285", "datetime": "2017-06-13 22:13:13", "author": "@thomasquintana", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874466319806062592": {"followers": "783", "datetime": "2017-06-13 03:19:52", "author": "@muktabh", "content_summary": "RT @arxiv_cs_cl: Attention Is All You Need. (arXiv:1706.03762v1 [cs.CL]) https://t.co/8ovOiC3bdC #NLProc"}, "875563863311171584": {"followers": "783", "datetime": "2017-06-16 04:01:06", "author": "@muktabh", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "874505446110904320": {"followers": "27,447", "datetime": "2017-06-13 05:55:20", "author": "@zaoyang", "content_summary": "Machine translation without using RNN. Only using attention model. No convolutions and no recurrence. Very novel. https://t.co/w4TGgX1lVf"}, "874663662958411776": {"followers": "58", "datetime": "2017-06-13 16:24:02", "author": "@shuvendu1roy", "content_summary": "RT @iamtrask: in addition to being an intuition I absolutely love, they found a use for sine and cosine in neural nets! brilliant! https://\u2026"}, "874860226217947136": {"followers": "322", "datetime": "2017-06-14 05:25:06", "author": "@letranger14", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "899591412374581248": {"followers": "1,667", "datetime": "2017-08-21 11:18:00", "author": "@Scaled_Wurm", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "1116970731546402817": {"followers": "5,606", "datetime": "2019-04-13 07:45:49", "author": "@__MLT__", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "961186580990849026": {"followers": "11,476", "datetime": "2018-02-07 10:35:13", "author": "@icoxfog417", "content_summary": "RT @culurciello: Question for NLP experts: Attention is all you need https://t.co/F5noinOIXU if you have seen the paper you see it is 100-1\u2026"}, "874933123460415488": {"followers": "99", "datetime": "2017-06-14 10:14:46", "author": "@treasured_write", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874621966073778176": {"followers": "93", "datetime": "2017-06-13 13:38:21", "author": "@tesujiro", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874814584657330176": {"followers": "324", "datetime": "2017-06-14 02:23:44", "author": "@rahul_a_r", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874680258515136512": {"followers": "1,941", "datetime": "2017-06-13 17:29:59", "author": "@vcjha", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966016501844205568": {"followers": "361", "datetime": "2018-02-20 18:27:36", "author": "@jadhavamitb", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "874861424039923713": {"followers": "14", "datetime": "2017-06-14 05:29:52", "author": "@TsuguoMogami", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1166053678974820352": {"followers": "807", "datetime": "2019-08-26 18:23:55", "author": "@Maugs99", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875382596187717634": {"followers": "443", "datetime": "2017-06-15 16:00:49", "author": "@ivenzor", "content_summary": "RT @jsdelfino: Lots of alternatives to RNNs for machine translation coming up recently... Interesting! https://t.co/uqVUN22rBQ"}, "1231172162825998336": {"followers": "222", "datetime": "2020-02-22 11:01:32", "author": "@PapersTrending", "content_summary": "[2/10] \ud83d\udcc8 - Attention Is All You Need - 256 \u2b50 - \ud83d\udcc4 https://t.co/iwhQq6qdDT - \ud83d\udd17 https://t.co/ieuFAWR8J4"}, "874570832261115905": {"followers": "7,162", "datetime": "2017-06-13 10:15:09", "author": "@Deep_Hub", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1098987211054231553": {"followers": "182", "datetime": "2019-02-22 16:45:43", "author": "@doninred", "content_summary": "RT @willkurt: Finally worked through \"Attention is all you need\". My recommended steps: - Skim the paper: https://t.co/iCNGDeJ62q - Step th\u2026"}, "874731944050819074": {"followers": "485", "datetime": "2017-06-13 20:55:21", "author": "@FabienCampagne", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874664402573705216": {"followers": "156", "datetime": "2017-06-13 16:26:58", "author": "@garygarywang", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874678939645595650": {"followers": "28", "datetime": "2017-06-13 17:24:44", "author": "@Mouatez", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874877980639670274": {"followers": "106", "datetime": "2017-06-14 06:35:39", "author": "@_jruales", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "899775073061527552": {"followers": "75", "datetime": "2017-08-21 23:27:49", "author": "@RyutaroYamauchi", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "1194397895019155456": {"followers": "221", "datetime": "2019-11-12 23:33:43", "author": "@hudsonmendes", "content_summary": "Vaswani et al. \"Attention Is All You Need\u201d, 2017 - Research Paper: https://t.co/WmR6Rr9FQs #NLP #BERT #Attention #Transformer https://t.co/zkadksE9IY"}, "874451488822710278": {"followers": "1,945", "datetime": "2017-06-13 02:20:56", "author": "@zaxtax", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "966087356397715458": {"followers": "1,346", "datetime": "2018-02-20 23:09:09", "author": "@DSakya", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "876917052857909248": {"followers": "3,314", "datetime": "2017-06-19 21:38:12", "author": "@AndySugs", "content_summary": "RT:machinelearnbot: RT gneubig: Yesterday \"attention is all you need\" https://t.co/4yA4xqRyK7 Today \"you need a bunch of other stuff\" \u2026"}, "1165701151976820738": {"followers": "45", "datetime": "2019-08-25 19:03:06", "author": "@artuskg", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "875187004895412224": {"followers": "615", "datetime": "2017-06-15 03:03:36", "author": "@KouroshMeshgi", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875129518075387904": {"followers": "23", "datetime": "2017-06-14 23:15:10", "author": "@osoleole", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876766618923261952": {"followers": "259", "datetime": "2017-06-19 11:40:26", "author": "@brdskggs", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1194396760711540741": {"followers": "221", "datetime": "2019-11-12 23:29:13", "author": "@hudsonmendes", "content_summary": "Vaswani et al. \"Attention Is All You Need\u201d, 2017 - Research Paper: https://t.co/WmR6Rr9FQs \ud83d\udd2c https://t.co/HjdAdJKa5e"}, "874608974527594497": {"followers": "473", "datetime": "2017-06-13 12:46:43", "author": "@Udibr", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874684171020636160": {"followers": "268", "datetime": "2017-06-13 17:45:31", "author": "@moisesvw", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874498391757262848": {"followers": "1,089", "datetime": "2017-06-13 05:27:18", "author": "@Tweetteresearch", "content_summary": "RT @dipanjand: Check this out: https://t.co/N6hZyklj6G SOTA on several metrics using extremely simple models."}, "1118199020621402112": {"followers": "919", "datetime": "2019-04-16 17:06:36", "author": "@charleshumble", "content_summary": ".@joelgrus is referencing the Attention Is All You Need paper from Ashish Vaswani et al at #QConai . \"This is very close to the imagenet moment for NLP.\" https://t.co/N5W4C9ONbh"}, "877140884843896832": {"followers": "4,944", "datetime": "2017-06-20 12:27:38", "author": "@shakamunyi", "content_summary": "RT @dataandme: So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #mach\u2026"}, "966768105174188032": {"followers": "3", "datetime": "2018-02-22 20:14:12", "author": "@sokunmin", "content_summary": "RT @karpathy: seeing self-attention (a kind of global \"message passing\" operation) popping up in a number of places recently, following \"at\u2026"}, "889959765392199680": {"followers": "42", "datetime": "2017-07-25 21:25:17", "author": "@improvedhouse", "content_summary": "Clarification on terminology in the \"Attention is all you need\" paper: https://t.co/vRPlbaGSA1 https://t.co/dXDBM4XJDp"}, "883912535140945928": {"followers": "38,161", "datetime": "2017-07-09 04:55:45", "author": "@mclynd", "content_summary": "RT @v_vashishta: #Google #DeepLearning - Attention Is All You Need https://t.co/Bjxj2dYXxw #MachineLearning"}, "874871585982885889": {"followers": "217", "datetime": "2017-06-14 06:10:15", "author": "@jrosskopf", "content_summary": "RT @graphific: Attention Is All You Need from @GoogleBrain. No RNNs or CNNs, only Attention = fast training & SOTA on WMT'14 https://t.co/Y\u2026"}, "874639612508946432": {"followers": "588", "datetime": "2017-06-13 14:48:28", "author": "@sk121", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1117028941947183104": {"followers": "163,852", "datetime": "2019-04-13 11:37:07", "author": "@ceobillionaire", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "899628733945884672": {"followers": "3,344", "datetime": "2017-08-21 13:46:19", "author": "@tai2an", "content_summary": "RT @im132nd: \"Attention Is All You Need\" https://t.co/Iq2s5F3sZJ \u3067\u306f Adam \u306e\u5b66\u7fd2\u7387\u3092 4000 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u307e\u3067\u3069\u3093\u3069\u3093\u4e0a\u3052\u3066\u305d\u306e\u5f8c\u3067\u5f90\u3005\u306b\u4e0b\u3052\u3066\u308b\u3068\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u2026\u2026\u30ef\u30ed\u30bf\u2026\u2026 https://t.\u2026"}, "921196787356008450": {"followers": "80", "datetime": "2017-10-20 02:10:03", "author": "@__RAMIN_", "content_summary": "@suzatweet yeah! I think he probably must have been talking about these: https://t.co/el8G59uHiy https://t.co/iHvJwLOY5X"}, "874582930072498176": {"followers": "399", "datetime": "2017-06-13 11:03:14", "author": "@kamilsindi", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874596971368951808": {"followers": "1,667", "datetime": "2017-06-13 11:59:01", "author": "@Scaled_Wurm", "content_summary": "RT @_Ryobot: \u81ea\u5df1\u6ce8\u610f\u3067\u5165\u51fa\u529b\u6587\u3092\u9023\u7d9a\u8868\u73fe\u306b\u7b26\u53f7\u5316\u3057\uff0c\u65e2\u8a33\u306e\u51fa\u529b\u6587\u3067\u30de\u30b9\u30ad\u30f3\u30b0\u3057\u306a\u304c\u3089\u7ffb\u8a33\u3059\u308b\u624b\u6cd5\u3067WMT\u82f1\u72ec/\u82f1\u4ecf\u3067SOTA\uff0e \u81ea\u5df1\u6ce8\u610f\u306e\u5165\u529bQ,K,V\u306f\u524d\u5c64\u306e\u540c\u3058\u51fa\u529b\uff08\u7ffb\u8a33\u5c64\u306fK,V\u304cEnc\u306e\u51fa\u529b\uff09\uff0ed\u3068softmax\u3067\u5185\u7a4d\u306e\u5024\u7206\u767a\u3092\u6291\u6b62\uff0e https://t.\u2026"}, "874519397188153345": {"followers": "128", "datetime": "2017-06-13 06:50:46", "author": "@xose_ramon", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874441758456893440": {"followers": "3,544", "datetime": "2017-06-13 01:42:16", "author": "@gutelius", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874672683098791937": {"followers": "46", "datetime": "2017-06-13 16:59:52", "author": "@luchtjon", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1082901954211270657": {"followers": "505", "datetime": "2019-01-09 07:28:39", "author": "@AUEBNLPGroup", "content_summary": "Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0u, https://t.co/AcNq92BHj9. Code: https://t.co/7U4Syr57aE, https://t.co/GsqjZZqu04. Central AUEB buildings, room A36. All welcome."}, "874803215333601280": {"followers": "9", "datetime": "2017-06-14 01:38:34", "author": "@WeinroT_xc", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1167348965072932866": {"followers": "4,812", "datetime": "2019-08-30 08:10:56", "author": "@IntuitMachine", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874498900924801024": {"followers": "73", "datetime": "2017-06-13 05:29:20", "author": "@DataAnalyticsGH", "content_summary": "@goodfellow_ian : https://t.co/93FBLZVoNW (via Twitter https://t.co/7WBG4FfUgg) https://t.co/NOJn4OjVdl"}, "877040122008567809": {"followers": "799", "datetime": "2017-06-20 05:47:14", "author": "@jramcast", "content_summary": "RT @DeepLearningNow: New #deeplearning paper https://t.co/qwZnM1IVje https://t.co/zQUSKLhk5D"}, "874442656671248388": {"followers": "874", "datetime": "2017-06-13 01:45:50", "author": "@robertsdionne", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874763789467615236": {"followers": "754", "datetime": "2017-06-13 23:01:54", "author": "@suneelmarthi", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "878731853158645761": {"followers": "11", "datetime": "2017-06-24 21:49:34", "author": "@nosovicki", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "877185087871340545": {"followers": "103", "datetime": "2017-06-20 15:23:16", "author": "@spacemanslog", "content_summary": "[1706.03762] Attention Is All You Need https://t.co/bKQkDkveLz https://t.co/2KuLyZdLBd"}, "877190740874547201": {"followers": "473", "datetime": "2017-06-20 15:45:44", "author": "@graeme_burnett", "content_summary": "Attention Is All You Need - network architecture for machine translations: high accuracy, less training, more speed https://t.co/v3y4EWaMmA"}, "877117262343413760": {"followers": "971", "datetime": "2017-06-20 10:53:46", "author": "@adnanmasood", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "877284937921122306": {"followers": "30,496", "datetime": "2017-06-20 22:00:03", "author": "@v_vashishta", "content_summary": "#Google #DeepLearning - Attention Is All You Need https://t.co/Bjxj2dYXxw #MachineLearning"}, "1165834616583966721": {"followers": "180", "datetime": "2019-08-26 03:53:27", "author": "@shadow_dawn", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876887341159370752": {"followers": "62", "datetime": "2017-06-19 19:40:08", "author": "@BikashgG", "content_summary": ":) https://t.co/xO2J3RMQGE"}, "874518140557737984": {"followers": "374", "datetime": "2017-06-13 06:45:47", "author": "@Jintai100", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874756770127851520": {"followers": "1,039", "datetime": "2017-06-13 22:34:00", "author": "@nonsensews", "content_summary": "RT @Miles_Brundage: arXiv papers, June 13 - \"Attention Is All You Need,\" Vaswani/Shazeer/Parmar/Uzkoreit/Jones/Gomez/Kaiser/Polosukhin: htt\u2026"}, "874441079298195456": {"followers": "1,655", "datetime": "2017-06-13 01:39:34", "author": "@olanleed", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1219650486346964993": {"followers": "4,323", "datetime": "2020-01-21 15:58:30", "author": "@CancerConnector", "content_summary": "RT @tae_hwang: @CancerConnector @ianholmes Once you read all the relevant #DL papers, read this! \"Attention is all you need\" https://t.co/V\u2026"}, "1100021426713780225": {"followers": "212", "datetime": "2019-02-25 13:15:20", "author": "@manuel_lmartin", "content_summary": "RT @willkurt: Finally worked through \"Attention is all you need\". My recommended steps: - Skim the paper: https://t.co/iCNGDeJ62q - Step th\u2026"}, "874683294935273472": {"followers": "7,137", "datetime": "2017-06-13 17:42:02", "author": "@anshul", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165821955217379329": {"followers": "2,671", "datetime": "2019-08-26 03:03:08", "author": "@ayirpelle", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874940999969243137": {"followers": "309", "datetime": "2017-06-14 10:46:04", "author": "@Daniel_J_Im", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1165752190318727168": {"followers": "995", "datetime": "2019-08-25 22:25:55", "author": "@yablak", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1116940049025617920": {"followers": "101", "datetime": "2019-04-13 05:43:54", "author": "@micmelesse", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "950386020234743808": {"followers": "231", "datetime": "2018-01-08 15:17:39", "author": "@Maaarcocr", "content_summary": "Today I have started my New Year resolution of reading more research. Trying to have a morning paper every weekday. Today I've read \"Attention Is All You Need\" (https://t.co/yWxZNpLZ2a). Interesting results in machine translations w/o the use of any kind"}, "876915697342861312": {"followers": "2,448", "datetime": "2017-06-19 21:32:49", "author": "@abhishektiwari", "content_summary": "A new NN architecture, the Transformer, based solely on attention mechanisms, without recurrence and convolutions https://t.co/1DlUHGOeGI https://t.co/bFApD5EmI3"}, "1112456735661481985": {"followers": "10", "datetime": "2019-03-31 20:48:48", "author": "@clockwk7", "content_summary": "@Abdullah_MA_b @Taras_script Your just sour because I called out your post for what it is. References: https://t.co/ko5LiUG7N2"}, "1166601609893359616": {"followers": "931", "datetime": "2019-08-28 06:41:12", "author": "@Carotene", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876600320511561728": {"followers": "352", "datetime": "2017-06-19 00:39:37", "author": "@141923", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874562035580907520": {"followers": "402", "datetime": "2017-06-13 09:40:12", "author": "@ber24", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874628363205230592": {"followers": "57", "datetime": "2017-06-13 14:03:46", "author": "@rahdirsrahonam", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1095324385836056576": {"followers": "7,503", "datetime": "2019-02-12 14:10:58", "author": "@HigeponJa", "content_summary": "RT @agatan_: @HigeponJa Attention Is All You Need https://t.co/eUZ17XVu8L \u3067\u63d0\u6848\u3055\u308c\u305f Transformer \u4ee5\u964d\u306f self-attention \u304c\u4e3b\u6d41\u306a\u5370\u8c61\u3067\u3059\u3002\u5148\u65e5\u304a\u304a\u304d\u306a\u8a71\u984c\u306b\u306a\u3063\u305f BERT\u2026"}, "874719688089120768": {"followers": "1,039", "datetime": "2017-06-13 20:06:39", "author": "@nonsensews", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874666350144139265": {"followers": "29", "datetime": "2017-06-13 16:34:43", "author": "@chenlailin", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1060764814069456896": {"followers": "104", "datetime": "2018-11-09 05:23:34", "author": "@negatifinoglu", "content_summary": "@michael_nielsen There is this transformer hype, starting from: https://t.co/d7YCzlnSVE"}, "876751560109412354": {"followers": "1,081", "datetime": "2017-06-19 10:40:35", "author": "@roeeaharoni", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "876832768520364032": {"followers": "1,882", "datetime": "2017-06-19 16:03:17", "author": "@AtulAcharya", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1089984670664601600": {"followers": "44,898", "datetime": "2019-01-28 20:32:51", "author": "@seb_ruder", "content_summary": "@EliasWalyBa @bayethiernodiop @ylecun @JeffDean @fchollet @francesc @dickoah @jeremyphoward @crowdglory @Moustapha_6C @SileyeBaba @seysoosey Whenever something has a periodic quality. For instance, the Transformer uses a sinusoid for encoding positional in"}, "1260280294529044480": {"followers": "206", "datetime": "2020-05-12 18:46:51", "author": "@Jiny2001", "content_summary": "@JTPA Let's write a paper, \"Muscle Is All You Need\"! \ud83e\udd1e >> https://t.co/U4bxTb62oP"}, "875036145096298502": {"followers": "1,649", "datetime": "2017-06-14 17:04:09", "author": "@alexk_z", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "885662938932981760": {"followers": "902", "datetime": "2017-07-14 00:51:14", "author": "@wk77", "content_summary": "RT @hillbig: RNN\u304c\u9010\u6b21\u7684\u306a\u8a08\u7b97\u3067\u5b66\u7fd2\u304c\u9045\u3044\u305f\u3081\uff0cCNN\u306a\u3069\u4e26\u5217\u5316\u53ef\u80fd\u306a\u6a5f\u69cb\u3078\u306e\u7f6e\u304d\u63db\u3048\u304c\u9032\u3080\u4e2d\u3001\u81ea\u5206\u81ea\u8eab\u306e\u7cfb\u5217\u3078\u306e\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3044\uff0c\u5165\u529b\u5168\u4f53\u3092\u53d7\u5bb9\u91ce\u3068\u3059\u308b\u8a08\u7b97\u3092\u4e26\u5217\u304b\u3064\u5b9a\u6570\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u73fe\u3059\u308bTransformer\u3092\u63d0\u6848\u3002\u6a5f\u68b0\u7ffb\u8a33\u306eSoTA https://t.co\u2026"}, "874528149647654912": {"followers": "207", "datetime": "2017-06-13 07:25:33", "author": "@TigerScorpior", "content_summary": "Attention Is All You Need https://t.co/nyfLzTqNCm seems a `BIG` name again.\ud83d\ude02"}, "1166036494890332160": {"followers": "471", "datetime": "2019-08-26 17:15:38", "author": "@daytb_twy", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "1116945776049934338": {"followers": "2,671", "datetime": "2019-04-13 06:06:39", "author": "@ayirpelle", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874783141340172288": {"followers": "4", "datetime": "2017-06-14 00:18:48", "author": "@lonnyxiang", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "996744224233197569": {"followers": "2,641", "datetime": "2018-05-16 13:28:36", "author": "@albertcardona", "content_summary": "\"a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. [...] superior in quality while being more parallelizable and requiring significantly less time to train.\" Vasw"}, "875156635773059074": {"followers": "2,062", "datetime": "2017-06-15 01:02:56", "author": "@omasanori", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1117148592014127104": {"followers": "4,771", "datetime": "2019-04-13 19:32:34", "author": "@vRobM", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "1116966664791314433": {"followers": "141", "datetime": "2019-04-13 07:29:39", "author": "@waydegilliam", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874689328089137152": {"followers": "2,619", "datetime": "2017-06-13 18:06:01", "author": "@MCLeopard", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876962157866430466": {"followers": "4,200", "datetime": "2017-06-20 00:37:26", "author": "@arxiv_cs_cl", "content_summary": "Attention Is All You Need. (arXiv:1706.03762v2 [cs.CL] UPDATED) https://t.co/8ovOiC3bdC #NLProc"}, "874848028800290817": {"followers": "3,314", "datetime": "2017-06-14 04:36:38", "author": "@AndySugs", "content_summary": "RT: (ilblackdragon)Paper \"Attention Is All You Need\" is now out - https://t.co/GrjCnDPNeE #MachineLearning #NLU #machinetranslation #dee\u2026"}, "876855443988946944": {"followers": "267", "datetime": "2017-06-19 17:33:23", "author": "@Rpatel_15", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876875763332468741": {"followers": "2,628", "datetime": "2017-06-19 18:54:08", "author": "@Juxi", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "1189848671082467329": {"followers": "9,456", "datetime": "2019-10-31 10:16:44", "author": "@machine_ml", "content_summary": "RT @NicholaStott: The T in BERT=Transformer. This paper was the original work that proposed an attention based transformer model, thereby c\u2026"}, "874501246987665408": {"followers": "721", "datetime": "2017-06-13 05:38:39", "author": "@cmarschner", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874733073417490432": {"followers": "125", "datetime": "2017-06-13 20:59:51", "author": "@srv_m", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "874726566416306179": {"followers": "329", "datetime": "2017-06-13 20:33:59", "author": "@aplokhotnyuk", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1062480734651383808": {"followers": "349", "datetime": "2018-11-13 23:02:01", "author": "@tymwol", "content_summary": "I finally had time to carefully read the \"Attention Is All You Need\" paper, since tomorrow I'm presenting it at ML seminar at my work. Many simple, yet clever ideas! \ud83e\udd2f\ud83d\udc4f Check: (1) https://t.co/Mo9BT03BWV (2) https://t.co/KG1L72sBIR (3) https://t.co/EB9MTpB"}, "877152442282102784": {"followers": "643", "datetime": "2017-06-20 13:13:33", "author": "@brentauble", "content_summary": "RT @dataandme: So, this Transformer network architecture seems kinda awesome... \ud83e\udd16 \"Attention Is All You Need\" https://t.co/2z9tKSh5ut #mach\u2026"}, "874748037242929152": {"followers": "45", "datetime": "2017-06-13 21:59:18", "author": "@Vikasm18", "content_summary": "RT @ogrisel: Attention Is All You Need: cheaper state of the art NMT without conv or lstm: pos embedding + feedforward + attn mechanism htt\u2026"}, "877198773822394368": {"followers": "313", "datetime": "2017-06-20 16:17:39", "author": "@jrbtaylor", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1165730208084713473": {"followers": "576", "datetime": "2019-08-25 20:58:34", "author": "@MatthewMcAteer0", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874808824011542528": {"followers": "1,528", "datetime": "2017-06-14 02:00:51", "author": "@ilblackdragon", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "874706224419295232": {"followers": "63", "datetime": "2017-06-13 19:13:09", "author": "@NeelShah_Indian", "content_summary": "State of art results! https://t.co/rSCyxfum3y"}, "1165739788328157184": {"followers": "397", "datetime": "2019-08-25 21:36:38", "author": "@AfoUnofficial", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "876917336632176640": {"followers": "414", "datetime": "2017-06-19 21:39:20", "author": "@uav4africa", "content_summary": "RT gneubig: Yesterday \"attention is all you need\" https://t.co/XWbNRDwhdq Today \"you need a bunch of other stuff\" https://t.co/XTNlSEQGIZ\u2026"}, "1082902121014534144": {"followers": "645", "datetime": "2019-01-09 07:29:19", "author": "@ionandrou", "content_summary": "RT @AUEBNLPGroup: Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0\u2026"}, "875132441651838978": {"followers": "3,579", "datetime": "2017-06-14 23:26:47", "author": "@kotatsu_mi", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "876769343245348864": {"followers": "413", "datetime": "2017-06-19 11:51:15", "author": "@ThomasBoquet", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1117346788544004096": {"followers": "279", "datetime": "2019-04-14 08:40:08", "author": "@theolivenbaum", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "874976397797838854": {"followers": "33,955", "datetime": "2017-06-14 13:06:44", "author": "@aiprgirl", "content_summary": "RT @nikiparmar09: Our paper \"Attention Is All You Need\" gets SOTA on WMT!!! https://t.co/hwdOQX7yfO Faster to train, better results #trans\u2026"}, "1260572547147497472": {"followers": "38", "datetime": "2020-05-13 14:08:10", "author": "@KotaroMiyasaka", "content_summary": "My disability is ADHD. It is a lack of attention. The importance of attention is now being actively experimented within the artificial intelligence community. \"Attention Is All You Need\" Have you heard this word? https://t.co/VtmTjraJMb"}, "874632889337597953": {"followers": "241", "datetime": "2017-06-13 14:21:45", "author": "@_torontoai", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "1084713783367487488": {"followers": "8,952", "datetime": "2019-01-14 07:28:13", "author": "@Deep_In_Depth", "content_summary": "Attention Is All You Need https://t.co/9ixw38rf7T #DeepLearning #MachineLearning #AI #DataScience #NeuralNetworks #CNN #Reinforcement #Learning #DeepRL #GPU #TensorFlow #Keras #Caffe #Pytorch #Python #AutonomousCar #Quant"}, "875489550864220160": {"followers": "4,631", "datetime": "2017-06-15 23:05:49", "author": "@beam2d", "content_summary": "RT @fastml_extra: Attention Is All You Need: https://t.co/1s6H0BZC66 Unofficial Chainer implementation (work in progress): https://t.co/HC\u2026"}, "877454429632569345": {"followers": "340", "datetime": "2017-06-21 09:13:32", "author": "@Ajay_Talati", "content_summary": "RT @kanair: I need to figure out if Google has solved consciousness in this paper. This has a lot to do with consciousness. https://t.co/mH\u2026"}, "874794706802167808": {"followers": "107", "datetime": "2017-06-14 01:04:45", "author": "@ddahlmeier", "content_summary": "Gotta put this on the reading list for this week https://t.co/fHO43fJxcc"}, "875267118635393024": {"followers": "235", "datetime": "2017-06-15 08:21:57", "author": "@jokedaems", "content_summary": "RT @gumarten: @jokedaems https://t.co/Ht0KZwQWGv"}, "874536303815262208": {"followers": "143", "datetime": "2017-06-13 07:57:57", "author": "@mKaloer", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "874679632792100864": {"followers": "65", "datetime": "2017-06-13 17:27:29", "author": "@russell_lliu", "content_summary": "RT @hardmaru: Attention Is All You Need, from @GoogleBrain. No RNNs, No CNNs, Just Attention. Very fast training; SOTA on WMT'14. https://t\u2026"}, "1093539000718106624": {"followers": "88", "datetime": "2019-02-07 15:56:29", "author": "@aarranzlopez", "content_summary": "#CBigData"}, "876968418498445314": {"followers": "166", "datetime": "2017-06-20 01:02:18", "author": "@almclean_tw", "content_summary": "RT @_brohrer_: Occasionally there is a radically new idea in ML. This is one. https://t.co/sTpr07yA5X"}, "876832269134028801": {"followers": "261", "datetime": "2017-06-19 16:01:18", "author": "@GuptaRajat033", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1166820148206100480": {"followers": "298", "datetime": "2019-08-28 21:09:36", "author": "@existeundelta", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}, "874669122902265856": {"followers": "1,378", "datetime": "2017-06-13 16:45:44", "author": "@salvo62barbato", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "875183773217808384": {"followers": "1,823", "datetime": "2017-06-15 02:50:46", "author": "@trinity_site", "content_summary": "\u8a71\u984c\u306a\u306e\u3067\u3061\u3087\u3063\u3068\u773a\u3081\u308b \u00bb [1706.03762] Attention Is All You Need https://t.co/PxVyhWg7oB"}, "874600104518328320": {"followers": "2,583", "datetime": "2017-06-13 12:11:28", "author": "@tito", "content_summary": "RT @soumithchintala: Translation paper from Google throws out ConvNets, RNN. Just attention + positional encoding = state of the art: http\u2026"}, "876004744317239296": {"followers": "4,021", "datetime": "2017-06-17 09:13:01", "author": "@CollaborativeTM", "content_summary": "RT @Translation: Another step forward in deep learning applied to language translation: https://t.co/SdmcPw1wDi thanks to @GoogleBrain"}, "876059152807866368": {"followers": "363", "datetime": "2017-06-17 12:49:13", "author": "@rodoume", "content_summary": "RT @boredyannlecun: Some hipsters are claiming \"Attention is All You Need\". In reality, just need convolution, coffee & Charlie Parker http\u2026"}, "1174876375112716290": {"followers": "449", "datetime": "2019-09-20 02:42:10", "author": "@HappySlice", "content_summary": "@ID_AA_Carmack Attention combined with internal state feedback... machine learning meditation? \ud83e\udd14 https://t.co/55LSqklFum"}, "1255878816573960193": {"followers": "74", "datetime": "2020-04-30 15:16:57", "author": "@scott_labounty", "content_summary": "For my new postion at @acorns the team is having me read https://t.co/o6TQ8jK9GU. Serves me right for wanting to work with really smart people."}, "1117197516875808770": {"followers": "186", "datetime": "2019-04-13 22:46:59", "author": "@surangasms01", "content_summary": "RT @aureliengeron: In the Transformer architecture (https://t.co/lhRKL4BNPG), a Positional Embedding (PE) is added to each word embedding.\u2026"}, "918967573399687168": {"followers": "609", "datetime": "2017-10-13 22:31:57", "author": "@diegocantor", "content_summary": "Attention is all you need @jkreindler @markchaikelson https://t.co/DiAGQTjWPE https://t.co/WXmklBb934"}, "876367196301271040": {"followers": "586", "datetime": "2017-06-18 09:13:16", "author": "@Prompsit", "content_summary": "Very relevant training time reduction. Would have loved to see results on WMT 2017 newstest rather than 2014 for a fair comparison. https://t.co/JZqypqzHxW"}, "877169933943152640": {"followers": "4,136", "datetime": "2017-06-20 14:23:03", "author": "@hanaken_n", "content_summary": "\u901a\u5e38\u30011\u3064\u306e\u30e2\u30c7\u30eb\u306f\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u3057\u304b\u3067\u304d\u306a\u3044(\u753b\u50cf\u5206\u985e\u3059\u308b\u30e2\u30c7\u30eb\u3068\u6a5f\u68b0\u7ffb\u8a33\u3059\u308b\u30e2\u30c7\u30eb\u306f\u5225\u3082\u306e)\u3002\u753b\u50cf\u5206\u985e\u3001\u8aac\u660e\u6587\u751f\u6210\u3001\u6a5f\u68b0\u7ffb\u8a33\u3001\u97f3\u58f0\u8a8d\u8b58etc\u3092\u5171\u901a\u306e\u30e2\u30c7\u30eb\u3067\u3084\u308b\u3068\u3044\u3046\u7814\u7a76 / One Model To Learn Them All https://t.co/rV0AYv0gE6"}, "874655698407247872": {"followers": "395", "datetime": "2017-06-13 15:52:23", "author": "@zuxfoucault", "content_summary": "RT @ilblackdragon: Paper \"Attention Is All You Need\" is now out - https://t.co/UVRwe2Zkf1 #MachineLearning #NLU #machinetranslation #deeple\u2026"}, "874709877201002497": {"followers": "562", "datetime": "2017-06-13 19:27:40", "author": "@wcornelissen", "content_summary": "RT @arxiv_cscl: Attention Is All You Need https://t.co/hNwHudJdqI"}, "876639284958085120": {"followers": "379", "datetime": "2017-06-19 03:14:27", "author": "@naejimu", "content_summary": "RT @gneubig: Yesterday \"attention is all you need\" https://t.co/IWvo8gyd65 Today \"you need a bunch of other stuff\" https://t.co/Ef6hq1Rq0I\u2026"}, "1166035683212840961": {"followers": "714", "datetime": "2019-08-26 17:12:25", "author": "@xuetal", "content_summary": "RT @OriolVinyalsML: A good example of our field moving (too) fast: the all attention layer was actually released in the original transforme\u2026"}}}