{"citation_id": "51106100", "queriedAt": "2020-05-09 12:42:42", "completed": "0", "twitter": {"1195747591109120002": {"followers": "5,428", "content_summary": "Where does the reward come from? What do we optimize? \u2014More #FoodForThougths at #main2019 w/ T.Lillicrap on model free reinforcement learning (https://t.co/Se3WasHPwu) and @blaiseaguera on chemotaxis, evolution, and addressing reward/motivations messiness", "author": "@introspection", "datetime": "2019-11-16 16:56:56"}, "1195808632925040641": {"followers": "497", "content_summary": "RT @introspection: Where does the reward come from? What do we optimize? \u2014More #FoodForThougths at #main2019 w/ T.Lillicrap on model free r\u2026", "author": "@Kaysonfakhar", "datetime": "2019-11-16 20:59:29"}, "1201687772668383232": {"followers": "206", "content_summary": "Recent RL papers with learned models: (1) Learning to plan in latent spaces: https://t.co/PevI2EU9rh (2) SimPLE, learned simulator: https://t.co/Ks10MHLgcp (3) MuZero, tree-based search on learned model: https://t.co/qJspX0QtT2", "author": "@erik_nijkamp", "datetime": "2019-12-03 02:21:05"}, "1185175297546575872": {"followers": "1,211", "content_summary": "RT @shion_honda: Deep Planning Network [Hafner+, 2019] \u6f5c\u5728\u7a7a\u9593\u3067\u306e\u30d7\u30e9\u30f3\u30cb\u30f3\u30b0\u3001\u6c7a\u5b9a\u7684/\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u518d\u5e30\u7684\u306a\u72b6\u614b\u7a7a\u9593\u30e2\u30c7\u30eb\u3001\u8907\u6570\u30b9\u30c6\u30c3\u30d7\u5148\u306e\u4e88\u6e2c\u304b\u3089\u306a\u308b\u30e2\u30c7\u30eb\u30d9\u30fc\u30b9\u306ePlaNet\u3092\u63d0\u6848\u300250\u500d\u306e\u30b5\u30f3\u30d7\u30eb\u52b9\u7387\u3068\u2026", "author": "@ekusoyt", "datetime": "2019-10-18 12:46:25"}}, "tab": "twitter"}