{"citation_id": "4799444", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 16:59:17", "twitter": {"669790739476275200": {"followers": "677", "datetime": "2015-11-26 08:12:11", "author": "@dovgalec", "content_summary": "@fchollet Fast and Accurate Deep Network Learning by Exponential Linear Units ELUs: https://t.co/uPM2s5RYk7 Reddit: https://t.co/h1nlKC35GZ"}, "1236358577566052352": {"followers": "99,782", "datetime": "2020-03-07 18:30:29", "author": "@jeremyphoward", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "723272435315798016": {"followers": "2,589", "datetime": "2016-04-21 22:09:20", "author": "@snikolov", "content_summary": "RT @alexjc: I like ELUs because they look elegant! Generative models could look better w/ C1 continuity? https://t.co/fC2Jv6qO9p https://t.\u2026"}, "723176826063990784": {"followers": "4,479", "datetime": "2016-04-21 15:49:25", "author": "@rodolfor", "content_summary": "RT @libbykinsey: Another exponential linear unit paper: https://t.co/6s5Oa7DR2M \"speeds up learning in DNNs and leads to higher classificat\u2026"}, "1160342037465620480": {"followers": "710", "datetime": "2019-08-11 00:07:54", "author": "@carlolepelaars", "content_summary": "@lavanyaai To be frank: 1. Use small batch sizes (https://t.co/elWmXeyiKS) 2. ReLU's are ancient. Use ELU or GELU as activations. Leaky ReLU's if the inference time has to be fast.(https://t.co/Ftkq8tPJKG) (https://t.co/l0iW5Y6aLv) 3. EfficientNet is aw"}, "1236312652848271363": {"followers": "6,273", "datetime": "2020-03-07 15:28:00", "author": "@radekosmulski", "content_summary": "To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReLus https://t.co/zrgYVQtXvs And here is how these monitoring capabilities are provided out of the box in fastai v2 \ud83d\ude0d https://t.co/PTAU7J"}, "669584373239447558": {"followers": "19,256", "datetime": "2015-11-25 18:32:09", "author": "@homeAIinfo", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "1051665297294446592": {"followers": "1,118", "datetime": "2018-10-15 02:45:20", "author": "@olshansky", "content_summary": "@quantombone A friend sent me https://t.co/xT2rw1LxRd. At a high level, is there ever any reason not use ELU of ReLU?"}, "669978736003973120": {"followers": "595", "datetime": "2015-11-26 20:39:12", "author": "@AndrewBaidu", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669912331472736256": {"followers": "42,874", "datetime": "2015-11-26 16:15:20", "author": "@DeepLearningHub", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "1236600404734906369": {"followers": "21", "datetime": "2020-03-08 10:31:26", "author": "@nandan_cse", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "698266780758777856": {"followers": "1,922", "datetime": "2016-02-12 22:05:48", "author": "@akira_you", "content_summary": "@akira_you URL\u5f35\u308a\u9593\u9055\u3048\u3066\u3044\u308b\u3000https://t.co/v1aduDP64y\u3000\u304c\u6b63\u89e3\uff1c"}, "833228411451514880": {"followers": "829", "datetime": "2017-02-19 08:15:08", "author": "@natbusa", "content_summary": "ELU is the new RELU #ml #deeplearning https://t.co/Eyiy9WaZtm"}, "805499407831445504": {"followers": "370", "datetime": "2016-12-04 19:49:58", "author": "@desipoika", "content_summary": "@memming came across recent empirical work on training + generalization benefits of lin. exp. link: https://t.co/7aeopb27Pn"}, "669721905222385664": {"followers": "244", "datetime": "2015-11-26 03:38:39", "author": "@s_ryosky", "content_summary": "RT @hillbig: \u6d3b\u6027\u5316\u95a2\u6570ReLU\u306f\uff0c\u6d3b\u6027\u5024\u306e\u5e73\u5747\u304c0\u3067\u306f\u306a\u304fbias\u304c\u304b\u304b\u308b\u554f\u984c\u304c\u3042\u3063\u305f\uff08sigmoid\u3088\u308atanh\u304c\u597d\u307e\u308c\u308b\uff09\uff0eReLU\u306e\u50be\u304d\u3092\u8ca0\u5074\u306b\u3082\u4f38\u3070\u3057\uff0c\u5e73\u5747\u304c0\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u3057\u305f\uff0eCIFAR100\u3067SoTA\uff0e\u3082\u3063\u3068\u7c21\u5358\u306b\u3067\u304d\u305d\u3046 https://t.co\u2026"}, "669603683982827521": {"followers": "156", "datetime": "2015-11-25 19:48:53", "author": "@iscaSAC", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669599910136795136": {"followers": "4,944", "datetime": "2015-11-25 19:33:53", "author": "@shakamunyi", "content_summary": "RT @aidotech: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/hOEI72lvLx #deeplearning #machinelear\u2026"}, "677316575947390976": {"followers": "5,851", "datetime": "2015-12-17 02:37:10", "author": "@hamadakoichi", "content_summary": "\u201c[1511.07289] Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\u201d https://t.co/TksXhdL5cw"}, "669483565420269568": {"followers": "599", "datetime": "2015-11-25 11:51:35", "author": "@BloodyAstaroth", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669584763527831554": {"followers": "5,737", "datetime": "2015-11-25 18:33:42", "author": "@EricdeMarylebon", "content_summary": "RT @aidotech: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/hOEI72lvLx #deeplearning #machinelear\u2026"}, "682234126162903040": {"followers": "5,213", "datetime": "2015-12-30 16:17:45", "author": "@AlisonBLowndes", "content_summary": "From the Bavarian genii: \"like batch normalization #ELUs push the mean => 0\" with way less computation #deeplearning https://t.co/vUhHN1cfvU"}, "669946550592782336": {"followers": "1,171", "datetime": "2015-11-26 18:31:19", "author": "@kibo35", "content_summary": "RT @s_ryosky: \u5148\u65e5\uff0carXiv\u306b\u30a2\u30c3\u30d7\u3055\u308c\u305f\u3070\u304b\u308a\u306eExponential Linear Units (ELUs)\u304c\u65e9\u901fPR\u3055\u308c\u3066\u308b\uff0e\u4ed5\u4e8b\u304c\u65e9\u3044\u3088\uff0e https://t.co/1kQW4tJtuh https://t.co/0P1Z6NxFKR"}, "691073716709638144": {"followers": "487", "datetime": "2016-01-24 01:43:08", "author": "@giffmana", "content_summary": "\"Units that have a non-zero mean activation act as bias for the next layer.\" https://t.co/HogE9FIbWy"}, "1236358779953790976": {"followers": "150", "datetime": "2020-03-07 18:31:18", "author": "@RISHABH50631460", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "669913055602544642": {"followers": "2,341", "datetime": "2015-11-26 16:18:13", "author": "@yshhrknmr", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669896716062093312": {"followers": "244", "datetime": "2015-11-26 15:13:17", "author": "@s_ryosky", "content_summary": "\u5148\u65e5\uff0carXiv\u306b\u30a2\u30c3\u30d7\u3055\u308c\u305f\u3070\u304b\u308a\u306eExponential Linear Units (ELUs)\u304c\u65e9\u901fPR\u3055\u308c\u3066\u308b\uff0e\u4ed5\u4e8b\u304c\u65e9\u3044\u3088\uff0e https://t.co/1kQW4tJtuh https://t.co/0P1Z6NxFKR"}, "1236398506690084864": {"followers": "14", "datetime": "2020-03-07 21:09:09", "author": "@e7mul", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "670734418290544640": {"followers": "1,293", "datetime": "2015-11-28 22:42:01", "author": "@mikaelhuss", "content_summary": "RT @adelong: Exponential Linear Units (ELU) proposed as alternative to ReLU, from Hochreiter's group https://t.co/kDCLOvoIUi https://t.co/4\u2026"}, "669621068475310080": {"followers": "4,023", "datetime": "2015-11-25 20:57:58", "author": "@localstories1", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "670970921629458432": {"followers": "166", "datetime": "2015-11-29 14:21:48", "author": "@charlottodinkle", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669940792530591744": {"followers": "118", "datetime": "2015-11-26 18:08:26", "author": "@FrobeniusNorm", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669703463614308352": {"followers": "290", "datetime": "2015-11-26 02:25:22", "author": "@ms_ghotratweet", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "669569524954476544": {"followers": "6", "datetime": "2015-11-25 17:33:09", "author": "@dev_juice", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669796982983147520": {"followers": "595", "datetime": "2015-11-26 08:36:59", "author": "@AndrewBaidu", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "669745031423639553": {"followers": "530", "datetime": "2015-11-26 05:10:33", "author": "@DL_Ally", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "1236359809982771200": {"followers": "240", "datetime": "2020-03-07 18:35:23", "author": "@blauigris", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "1160347235290361857": {"followers": "92", "datetime": "2019-08-11 00:28:33", "author": "@chauhraj", "content_summary": "RT @carlolepelaars: @lavanyaai To be frank: 1. Use small batch sizes (https://t.co/elWmXeyiKS) 2. ReLU's are ancient. Use ELU or GELU as\u2026"}, "723436599048957953": {"followers": "113", "datetime": "2016-04-22 09:01:40", "author": "@Miraut", "content_summary": "RT @alexjc: I like ELUs because they look elegant! Generative models could look better w/ C1 continuity? https://t.co/fC2Jv6qO9p https://t.\u2026"}, "669786346244939776": {"followers": "28,418", "datetime": "2015-11-26 07:54:43", "author": "@ogrisel", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "1236461593694789632": {"followers": "96", "datetime": "2020-03-08 01:19:50", "author": "@treasured_write", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "669678273329172480": {"followers": "18,214", "datetime": "2015-11-26 00:45:17", "author": "@hillbig", "content_summary": "\u6d3b\u6027\u5316\u95a2\u6570ReLU\u306f\uff0c\u6d3b\u6027\u5024\u306e\u5e73\u5747\u304c0\u3067\u306f\u306a\u304fbias\u304c\u304b\u304b\u308b\u554f\u984c\u304c\u3042\u3063\u305f\uff08sigmoid\u3088\u308atanh\u304c\u597d\u307e\u308c\u308b\uff09\uff0eReLU\u306e\u50be\u304d\u3092\u8ca0\u5074\u306b\u3082\u4f38\u3070\u3057\uff0c\u5e73\u5747\u304c0\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u3057\u305f\uff0eCIFAR100\u3067SoTA\uff0e\u3082\u3063\u3068\u7c21\u5358\u306b\u3067\u304d\u305d\u3046 https://t.co/SAqPmK7TLv"}, "721548759771971586": {"followers": "2,544", "datetime": "2016-04-17 04:00:04", "author": "@sentiwire", "content_summary": "RT @ArxivBot: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/Mdz8x98Hd7 [2015] https://t.co/JyW2NA\u2026"}, "1160342142742478849": {"followers": "6,012", "datetime": "2019-08-11 00:08:19", "author": "@lavanyaai", "content_summary": "RT @carlolepelaars: @lavanyaai To be frank: 1. Use small batch sizes (https://t.co/elWmXeyiKS) 2. ReLU's are ancient. Use ELU or GELU as\u2026"}, "721548754860515329": {"followers": "116", "datetime": "2016-04-17 04:00:03", "author": "@ArxivBot", "content_summary": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/Mdz8x98Hd7 [2015] https://t.co/JyW2NA6b9b"}, "669939539331280896": {"followers": "1,171", "datetime": "2015-11-26 18:03:27", "author": "@kibo35", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "1236360994206580742": {"followers": "7", "datetime": "2020-03-07 18:40:06", "author": "@LakshwinShrees1", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "669968187413278723": {"followers": "71", "datetime": "2015-11-26 19:57:17", "author": "@ChonteMerengue", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669809588997193728": {"followers": "2,092", "datetime": "2015-11-26 09:27:05", "author": "@communicating", "content_summary": "[Very Interesting Paper]: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/jegaQONcjL"}, "669798868238934016": {"followers": "275", "datetime": "2015-11-26 08:44:29", "author": "@vishy_punditry", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669616459526615040": {"followers": "2,423", "datetime": "2015-11-25 20:39:39", "author": "@Chuck_Moeller", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "875311280428417025": {"followers": "1,055", "datetime": "2017-06-15 11:17:26", "author": "@stealthinu", "content_summary": "ReLU/LeakyReLU/ELU/SReLU\u306e\u6bd4\u8f03\u304c\u3042\u308b\u3002\u7d50\u679c\u898b\u308b\u3068SReLU\u304c\u5358\u7d14\u3067ELU\u3068\u6700\u7d42\u7684\u306a\u7d50\u679c\u304c\u3060\u3044\u305f\u3044\u540c\u3058\u306b\u306a\u3063\u3066\u308b\u304b\u3089\u4e00\u756a\u3044\u3044\u3088\u3046\u306a\u611f\u3058\uff1f\u306a\u305c\u3053\u306e\u95a2\u6570\u3060\u3068\u826f\u3044\u7d50\u679c\u51fa\u308b\u306e\u304b\u306f\u308f\u304b\u3089\u306c\u304c\u3002 / \u201c[1511.07\u2026\u201d https://t.co/5mFn5kKExK"}, "670460844841529344": {"followers": "2,018", "datetime": "2015-11-28 04:34:56", "author": "@PMZepto", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669919160470773760": {"followers": "21,118", "datetime": "2015-11-26 16:42:29", "author": "@DataSciNews", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669514346440081409": {"followers": "136", "datetime": "2015-11-25 13:53:53", "author": "@helxsz", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "671068635419582465": {"followers": "625", "datetime": "2015-11-29 20:50:05", "author": "@rayohauno", "content_summary": "RT @adelong: Exponential Linear Units (ELU) proposed as alternative to ReLU, from Hochreiter's group https://t.co/kDCLOvoIUi https://t.co/4\u2026"}, "669728510798225408": {"followers": "2,112", "datetime": "2015-11-26 04:04:54", "author": "@mitmul", "content_summary": "RT @hillbig: \u6d3b\u6027\u5316\u95a2\u6570ReLU\u306f\uff0c\u6d3b\u6027\u5024\u306e\u5e73\u5747\u304c0\u3067\u306f\u306a\u304fbias\u304c\u304b\u304b\u308b\u554f\u984c\u304c\u3042\u3063\u305f\uff08sigmoid\u3088\u308atanh\u304c\u597d\u307e\u308c\u308b\uff09\uff0eReLU\u306e\u50be\u304d\u3092\u8ca0\u5074\u306b\u3082\u4f38\u3070\u3057\uff0c\u5e73\u5747\u304c0\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u3057\u305f\uff0eCIFAR100\u3067SoTA\uff0e\u3082\u3063\u3068\u7c21\u5358\u306b\u3067\u304d\u305d\u3046 https://t.co\u2026"}, "942966711631974400": {"followers": "794", "datetime": "2017-12-19 03:55:58", "author": "@drscotthawley", "content_summary": "@keunwoochoi Noticed your compact_cnn uses ELU & batch norm, yet ELU authors (https://t.co/vVNy03ZA2M) say \"ELUs networks outperform ReLU networks with BN while BN does not improve ELU networks [but they run way faster].\" Have u tried ELU w/o BN?"}, "669583775869886464": {"followers": "1,024", "datetime": "2015-11-25 18:29:47", "author": "@desertnaut", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "670088537941909504": {"followers": "31,172", "datetime": "2015-11-27 03:55:31", "author": "@Smerity", "content_summary": "ELU (exponential linear units) achieve new SotA on CIFAR-100 (in case ReLU, LReLU, PReLU, & RReLU weren't enough :P) https://t.co/9cA4UcsibQ"}, "873979942471516160": {"followers": "83", "datetime": "2017-06-11 19:07:10", "author": "@d0m3z", "content_summary": "To read https://t.co/tvCiUy1t47"}, "1083048854046441474": {"followers": "2", "datetime": "2019-01-09 17:12:23", "author": "@WareZTaO", "content_summary": "https://t.co/7ZrSNlJLnr"}, "669722459973660673": {"followers": "1,265", "datetime": "2015-11-26 03:40:51", "author": "@01_yuu", "content_summary": "RT @hillbig: \u6d3b\u6027\u5316\u95a2\u6570ReLU\u306f\uff0c\u6d3b\u6027\u5024\u306e\u5e73\u5747\u304c0\u3067\u306f\u306a\u304fbias\u304c\u304b\u304b\u308b\u554f\u984c\u304c\u3042\u3063\u305f\uff08sigmoid\u3088\u308atanh\u304c\u597d\u307e\u308c\u308b\uff09\uff0eReLU\u306e\u50be\u304d\u3092\u8ca0\u5074\u306b\u3082\u4f38\u3070\u3057\uff0c\u5e73\u5747\u304c0\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u3057\u305f\uff0eCIFAR100\u3067SoTA\uff0e\u3082\u3063\u3068\u7c21\u5358\u306b\u3067\u304d\u305d\u3046 https://t.co\u2026"}, "1070220001750253568": {"followers": "172", "datetime": "2018-12-05 07:35:06", "author": "@KarvPrime", "content_summary": "@ZachWeiner In example: https://t.co/Tz8ccJ3mo5 I've already corrected the ELU, but the other *eLU functions seem to be wrong (didn't check them thoroughly). In v1 and v2 of https://t.co/R9CWietuny it's still the old function, but it has been updated since"}, "669583503869243393": {"followers": "25,311", "datetime": "2015-11-25 18:28:42", "author": "@deeplearning4j", "content_summary": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machinelearning"}, "669483376810852352": {"followers": "7,210", "datetime": "2015-11-25 11:50:50", "author": "@fastml_extra", "content_summary": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co/8ZOmxJrn4f"}, "724598474574974978": {"followers": "3,446", "datetime": "2016-04-25 13:58:33", "author": "@reiver", "content_summary": "RT @alexjc: I like ELUs because they look elegant! Generative models could look better w/ C1 continuity? https://t.co/fC2Jv6qO9p https://t.\u2026"}, "669496991374811136": {"followers": "473", "datetime": "2015-11-25 12:44:56", "author": "@Udibr", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "670710593108746240": {"followers": "665", "datetime": "2015-11-28 21:07:21", "author": "@adelong", "content_summary": "Exponential Linear Units (ELU) proposed as alternative to ReLU, from Hochreiter's group https://t.co/kDCLOvoIUi https://t.co/4xgwcvwbY5"}, "931091301889482754": {"followers": "381", "datetime": "2017-11-16 09:27:19", "author": "@geekmarcus", "content_summary": "@_bha1 Similar: https://t.co/uAu849RSrz"}, "669787120563838976": {"followers": "378", "datetime": "2015-11-26 07:57:48", "author": "@RemySaissy", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "1236370336490405889": {"followers": "118", "datetime": "2020-03-07 19:17:13", "author": "@matthewopala", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "669765306689191936": {"followers": "4,890", "datetime": "2015-11-26 06:31:07", "author": "@IgorCarron", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "723175039814619136": {"followers": "1,666", "datetime": "2016-04-21 15:42:20", "author": "@libbykinsey", "content_summary": "Another exponential linear unit paper: https://t.co/6s5Oa7DR2M \"speeds up learning in DNNs and leads to higher classification accuracies\""}, "671090525060116480": {"followers": "4,784", "datetime": "2015-11-29 22:17:04", "author": "@AlluviateInc", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "1161475186258063360": {"followers": "97", "datetime": "2019-08-14 03:10:38", "author": "@R_Lemke", "content_summary": "RT @carlolepelaars: @lavanyaai To be frank: 1. Use small batch sizes (https://t.co/elWmXeyiKS) 2. ReLU's are ancient. Use ELU or GELU as\u2026"}, "723140724930150401": {"followers": "25,223", "datetime": "2016-04-21 13:25:58", "author": "@alexjc", "content_summary": "I like ELUs because they look elegant! Generative models could look better w/ C1 continuity? https://t.co/fC2Jv6qO9p https://t.co/7Oa2yIZuVi"}, "833044325219127299": {"followers": "105", "datetime": "2017-02-18 20:03:39", "author": "@alfo_512", "content_summary": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/qZViUfbQVz"}, "1236373547888652295": {"followers": "141", "datetime": "2020-03-07 19:29:59", "author": "@GerardoEGarcia", "content_summary": "RT @radekosmulski: To do the topic of monitoring activations justice, here is a paper introducing ELUs and discussing leaky and shifted ReL\u2026"}, "669684644049264644": {"followers": "530", "datetime": "2015-11-26 01:10:35", "author": "@DL_Ally", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}, "669818559263023104": {"followers": "874", "datetime": "2015-11-26 10:02:43", "author": "@robertsdionne", "content_summary": "RT @deeplearning4j: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) https://t.co/8cAwomZ6pI #deeplearning #machi\u2026"}, "669575983431876608": {"followers": "1,293", "datetime": "2015-11-25 17:58:49", "author": "@mikaelhuss", "content_summary": "RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs): https://t.co/ogld7dX0ef Reddit: https://t.co\u2026"}}}