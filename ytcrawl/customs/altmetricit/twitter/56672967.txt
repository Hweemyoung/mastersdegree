{"tab": "twitter", "completed": "1", "twitter": {"1103878648455606273": {"author": "@stats385", "followers": "970", "datetime": "2019-03-08 04:42:33", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103904516464766976": {"author": "@serrjoa", "followers": "1,036", "datetime": "2019-03-08 06:25:20", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104290678391988224": {"author": "@ayirpelle", "followers": "2,674", "datetime": "2019-03-09 07:59:49", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1103990298697293824": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-03-08 12:06:13", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104444579493920769": {"author": "@dr_levan", "followers": "169", "datetime": "2019-03-09 18:11:21", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104152873233920000": {"author": "@iamknighton", "followers": "427", "datetime": "2019-03-08 22:52:13", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104155780150771712": {"author": "@jeremyphoward", "followers": "100,219", "datetime": "2019-03-08 23:03:46", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104107691205685255": {"author": "@Zahid_Akhtar", "followers": "97", "datetime": "2019-03-08 19:52:41", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103929893711077376": {"author": "@biggiobattista", "followers": "1,260", "datetime": "2019-03-08 08:06:11", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104225114214752257": {"author": "@theabhimanyu", "followers": "772", "datetime": "2019-03-09 03:39:17", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103844689428439040": {"author": "@treasured_write", "followers": "102", "datetime": "2019-03-08 02:27:37", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105423041511616512": {"author": "@morioka", "followers": "823", "datetime": "2019-03-12 10:59:25", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1104601665515057152": {"author": "@eve_yk", "followers": "485", "datetime": "2019-03-10 04:35:34", "content_summary": "RT @asam9891: Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions \u8d85\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u5909\u63db\u3059\u308bbest\u2026"}, "1104076247909523456": {"author": "@josepemanzano", "followers": "564", "datetime": "2019-03-08 17:47:44", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103922025867821056": {"author": "@NandoDF", "followers": "77,292", "datetime": "2019-03-08 07:34:55", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105492955945525248": {"author": "@AElasoni", "followers": "8", "datetime": "2019-03-12 15:37:14", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1104167025692295170": {"author": "@IgorCarron", "followers": "4,889", "datetime": "2019-03-08 23:48:28", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103936512972845062": {"author": "@pacocp9", "followers": "354", "datetime": "2019-03-08 08:32:29", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103849917884321792": {"author": "@BrundageBot", "followers": "3,897", "datetime": "2019-03-08 02:48:23", "content_summary": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions. Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse https://t.co/bOyg1N0CUU"}, "1104123178761150464": {"author": "@DSaience", "followers": "99", "datetime": "2019-03-08 20:54:14", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103862166455640064": {"author": "@Hansatyam", "followers": "81", "datetime": "2019-03-08 03:37:03", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103934731731623939": {"author": "@hu_daa", "followers": "201", "datetime": "2019-03-08 08:25:24", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104039412969627649": {"author": "@jcchinhui", "followers": "256", "datetime": "2019-03-08 15:21:22", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105993836793257984": {"author": "@arxiv_pop", "followers": "711", "datetime": "2019-03-14 00:47:33", "content_summary": "2019/03/07 \u6295\u7a3f 4\u4f4d LG(Machine Learning) Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions https://t.co/COsL5RQxSr 8 Tweets 14 Retweets 68 Favorites"}, "1103977096655896576": {"author": "@blauigris", "followers": "245", "datetime": "2019-03-08 11:13:45", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104179080461762560": {"author": "@SergioSSYR", "followers": "85", "datetime": "2019-03-09 00:36:22", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104312707124260870": {"author": "@malicannoyan", "followers": "226", "datetime": "2019-03-09 09:27:21", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103980579647700994": {"author": "@viktor_m81", "followers": "118", "datetime": "2019-03-08 11:27:35", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104231117744308224": {"author": "@ayirpelle", "followers": "2,674", "datetime": "2019-03-09 04:03:08", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104460670244925441": {"author": "@dahlemd", "followers": "197", "datetime": "2019-03-09 19:15:18", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103961961686073346": {"author": "@arxiv_cs_LG", "followers": "318", "datetime": "2019-03-08 10:13:36", "content_summary": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions. Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse https://t.co/8fpZrcfngo"}, "1104234147768397824": {"author": "@AjayDubey1111", "followers": "133", "datetime": "2019-03-09 04:15:11", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103967105848754178": {"author": "@bttyeo", "followers": "5,910", "datetime": "2019-03-08 10:34:03", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104092557921923073": {"author": "@nisarkhanatwork", "followers": "4", "datetime": "2019-03-08 18:52:33", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104241860590034945": {"author": "@SharanVishnu7", "followers": "56", "datetime": "2019-03-09 04:45:50", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104224829991866368": {"author": "@sannykimchi", "followers": "954", "datetime": "2019-03-09 03:38:09", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103914888282296326": {"author": "@volkuleshov", "followers": "3,772", "datetime": "2019-03-08 07:06:33", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104177453017255936": {"author": "@CherguiSafouane", "followers": "31", "datetime": "2019-03-09 00:29:54", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103966380083372032": {"author": "@pavelkordik", "followers": "941", "datetime": "2019-03-08 10:31:10", "content_summary": "Worth to read, clever way of gradient learning hyperparameters together with parameters during #MachineLearning"}, "1104206934012391424": {"author": "@kishoreugal", "followers": "66", "datetime": "2019-03-09 02:27:02", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103887971227295745": {"author": "@heghbalz", "followers": "1,306", "datetime": "2019-03-08 05:19:36", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105993946226929665": {"author": "@SantchiWeb", "followers": "8,262", "datetime": "2019-03-14 00:47:59", "content_summary": "RT @arxiv_pop: 2019/03/07 \u6295\u7a3f 4\u4f4d LG(Machine Learning) Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Re\u2026"}, "1104106108241604609": {"author": "@russell_lliu", "followers": "65", "datetime": "2019-03-08 19:46:24", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104159693138743296": {"author": "@cyberSM7", "followers": "163", "datetime": "2019-03-08 23:19:19", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1103966659868573696": {"author": "@theChrisChua", "followers": "2,136", "datetime": "2019-03-08 10:32:17", "content_summary": "RT @pavelkordik: Worth to read, clever way of gradient learning hyperparameters together with parameters during #MachineLearning https://t.\u2026"}, "1104390072160407552": {"author": "@AtalRaghav", "followers": "85", "datetime": "2019-03-09 14:34:46", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104358655179264000": {"author": "@alsombra7", "followers": "111", "datetime": "2019-03-09 12:29:56", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104673321839292416": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-03-10 09:20:18", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103836698109231105": {"author": "@RogerGrosse", "followers": "5,469", "datetime": "2019-03-08 01:55:51", "content_summary": "Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during training. This is the work of Matt MacKay, Paul Vicol, and @jonLorraine9, to appear at ICLR 2019. https://t.co/5svBdWVjhu"}, "1103935663567560704": {"author": "@ak1010", "followers": "1,086", "datetime": "2019-03-08 08:29:06", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105822912433405953": {"author": "@stratahw", "followers": "144", "datetime": "2019-03-13 13:28:22", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1105323887074566146": {"author": "@khmlpy", "followers": "199", "datetime": "2019-03-12 04:25:25", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1104277751094161408": {"author": "@t_dhyani", "followers": "41", "datetime": "2019-03-09 07:08:27", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103983987431624705": {"author": "@Sunghyo_Chung", "followers": "21", "datetime": "2019-03-08 11:41:08", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104502391238549507": {"author": "@IgorCarron", "followers": "4,889", "datetime": "2019-03-09 22:01:05", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1104258905150877702": {"author": "@jekbradbury", "followers": "4,341", "datetime": "2019-03-09 05:53:33", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105190182855294976": {"author": "@AndreyDulub", "followers": "44", "datetime": "2019-03-11 19:34:07", "content_summary": "Self-Tuning Networks (STNs) update their own hyperparameters online during training https://t.co/BL6OGxqd6r"}, "1104458118556135425": {"author": "@ayirpelle", "followers": "2,674", "datetime": "2019-03-09 19:05:09", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1104321130955976704": {"author": "@erwtokritos", "followers": "671", "datetime": "2019-03-09 10:00:49", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104357696277544965": {"author": "@jm_alexia", "followers": "5,813", "datetime": "2019-03-09 12:26:07", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104314013721260032": {"author": "@apmoore94", "followers": "159", "datetime": "2019-03-09 09:32:32", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104327423410221058": {"author": "@lievAnastazia", "followers": "636", "datetime": "2019-03-09 10:25:49", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103889619731472384": {"author": "@Luke_Metz", "followers": "4,620", "datetime": "2019-03-08 05:26:09", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103893289760022528": {"author": "@Tsingggg", "followers": "22", "datetime": "2019-03-08 05:40:44", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105374575485804544": {"author": "@ElectronNest", "followers": "227", "datetime": "2019-03-12 07:46:50", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1103928331580301312": {"author": "@heyitsmui", "followers": "12", "datetime": "2019-03-08 07:59:58", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103883983194976256": {"author": "@Tanaygahlot", "followers": "251", "datetime": "2019-03-08 05:03:45", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103850082259095553": {"author": "@hamed_mo7amed", "followers": "352", "datetime": "2019-03-08 02:49:02", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103908738304360448": {"author": "@jeandut14000", "followers": "44", "datetime": "2019-03-08 06:42:07", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103915754045366273": {"author": "@octonion", "followers": "16,831", "datetime": "2019-03-08 07:10:00", "content_summary": "But can it tune a fish."}, "1104358737785942016": {"author": "@kuz44ma69", "followers": "36", "datetime": "2019-03-09 12:30:15", "content_summary": "RT @asam9891: Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions \u8d85\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u5909\u63db\u3059\u308bbest\u2026"}, "1104016843151413249": {"author": "@jbloom22", "followers": "1,272", "datetime": "2019-03-08 13:51:41", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104230688885170177": {"author": "@hereticreader", "followers": "195", "datetime": "2019-03-09 04:01:26", "content_summary": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions https://t.co/8V1twQEOVt"}, "1106192586799177728": {"author": "@twnming", "followers": "1", "datetime": "2019-03-14 13:57:19", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104074749603389441": {"author": "@hakanardo", "followers": "52", "datetime": "2019-03-08 17:41:47", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105293350532395008": {"author": "@icoxfog417", "followers": "11,540", "datetime": "2019-03-12 02:24:04", "content_summary": "\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u30b9\u30b1\u30fc\u30eb\u3055\u308c\u308b\u91cd\u307f/bias\u3092\u52a0\u3048\u5b66\u7fd2\u3092\u884c\u3046\u3002"}, "1104301154664660993": {"author": "@EnikoBiro", "followers": "925", "datetime": "2019-03-09 08:41:26", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103838329135656961": {"author": "@quaidmorris", "followers": "3,301", "datetime": "2019-03-08 02:02:20", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104256360160485377": {"author": "@fengyuncrawl", "followers": "105", "datetime": "2019-03-09 05:43:27", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104386334804897793": {"author": "@cepera_ang", "followers": "317", "datetime": "2019-03-09 14:19:55", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1103915474818031621": {"author": "@miquelmarti92", "followers": "242", "datetime": "2019-03-08 07:08:53", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103870877811130369": {"author": "@arxivml", "followers": "781", "datetime": "2019-03-08 04:11:40", "content_summary": "\"Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions\", Matthew M\u2026 https://t.co/EYfJbuMGZa"}, "1104673917216555015": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-03-10 09:22:40", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104489504671391744": {"author": "@atinm7", "followers": "69", "datetime": "2019-03-09 21:09:52", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103848937189007361": {"author": "@diegovogeid", "followers": "69", "datetime": "2019-03-08 02:44:29", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104349410924744704": {"author": "@asam9891", "followers": "564", "datetime": "2019-03-09 11:53:12", "content_summary": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions \u8d85\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u5909\u63db\u3059\u308bbest response \u95a2\u6570\u3092\u8a2d\u8a08\u3059\u308b\u3053\u3068\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u4e2d\u306b\u52d5\u7684\u306b\u8d85\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8abf\u6574\u3059\u308b\u8a71\u3002\u3080\u305a\u3080\u305a\u3002ICLR2019 https://t.co/yII1A7ZVnb"}, "1104091711947575297": {"author": "@brandondamos", "followers": "7,588", "datetime": "2019-03-08 18:49:11", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104032635062378496": {"author": "@jsupratman13", "followers": "58", "datetime": "2019-03-08 14:54:26", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1110868219168874496": {"author": "@ElectronNest", "followers": "227", "datetime": "2019-03-27 11:36:37", "content_summary": "\"Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions\" https://t.co/JuM7eAhSnc"}, "1104309575145177088": {"author": "@Aljabr0101", "followers": "24", "datetime": "2019-03-09 09:14:54", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104422784313090048": {"author": "@A_K_Nain", "followers": "3,328", "datetime": "2019-03-09 16:44:45", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1104050899784253440": {"author": "@unsorsodicorda", "followers": "740", "datetime": "2019-03-08 16:07:01", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104082100842647554": {"author": "@xuechen1994", "followers": "47", "datetime": "2019-03-08 18:11:00", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104263284969086977": {"author": "@johnnyprothero", "followers": "184", "datetime": "2019-03-09 06:10:58", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103928642931908609": {"author": "@kuz44ma69", "followers": "36", "datetime": "2019-03-08 08:01:13", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104244045985927170": {"author": "@ArchieIndian", "followers": "723", "datetime": "2019-03-09 04:54:31", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1105507084219510785": {"author": "@prototechno", "followers": "4,825", "datetime": "2019-03-12 16:33:22", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1104199420181168128": {"author": "@JoaoVictor_AC", "followers": "742", "datetime": "2019-03-09 01:57:11", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104340875193114625": {"author": "@lokeshsonii", "followers": "30", "datetime": "2019-03-09 11:19:16", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1105313039379705856": {"author": "@jaguring1", "followers": "12,872", "datetime": "2019-03-12 03:42:18", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1104551485562601478": {"author": "@_lpag", "followers": "291", "datetime": "2019-03-10 01:16:10", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104159936202842113": {"author": "@shreyaspadhy", "followers": "52", "datetime": "2019-03-08 23:20:17", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103863986540314625": {"author": "@DeepakB94018371", "followers": "47", "datetime": "2019-03-08 03:44:17", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104047876781498369": {"author": "@maggie_albrecht", "followers": "1,901", "datetime": "2019-03-08 15:55:00", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103898596011732992": {"author": "@anshulkundaje", "followers": "7,194", "datetime": "2019-03-08 06:01:49", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104218448836489216": {"author": "@ArchieIndian", "followers": "723", "datetime": "2019-03-09 03:12:48", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103857742769348609": {"author": "@cghosh_", "followers": "278", "datetime": "2019-03-08 03:19:29", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104173804660580352": {"author": "@alxndrkalinin", "followers": "3,416", "datetime": "2019-03-09 00:15:24", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104380554143821824": {"author": "@curtwehrley", "followers": "271", "datetime": "2019-03-09 13:56:57", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104176456286916609": {"author": "@thapraveensingh", "followers": "21", "datetime": "2019-03-09 00:25:56", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104309339257491456": {"author": "@alfo_512", "followers": "105", "datetime": "2019-03-09 09:13:58", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104663498104041472": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-03-10 08:41:16", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104322009637507074": {"author": "@_dmh", "followers": "1,122", "datetime": "2019-03-09 10:04:19", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1103995509557981184": {"author": "@morioka", "followers": "823", "datetime": "2019-03-08 12:26:55", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104225774498955265": {"author": "@MattiaRigotti", "followers": "508", "datetime": "2019-03-09 03:41:54", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103882049406951424": {"author": "@ceobillionaire", "followers": "163,626", "datetime": "2019-03-08 04:56:04", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104250957439754240": {"author": "@royam0820", "followers": "230", "datetime": "2019-03-09 05:21:58", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104380084998279169": {"author": "@jchassoul", "followers": "156", "datetime": "2019-03-09 13:55:05", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104266781043044354": {"author": "@deliprao", "followers": "12,997", "datetime": "2019-03-09 06:24:51", "content_summary": "Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN"}, "1103904313309454336": {"author": "@TerencePlizga", "followers": "258", "datetime": "2019-03-08 06:24:32", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104200614202863617": {"author": "@sigitpurnomo", "followers": "2,663", "datetime": "2019-03-09 02:01:56", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104338610592120833": {"author": "@gcetinkaya", "followers": "1,082", "datetime": "2019-03-09 11:10:17", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104665721991192576": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-03-10 08:50:06", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104384428506300416": {"author": "@dbparedes", "followers": "167", "datetime": "2019-03-09 14:12:20", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104435470627471361": {"author": "@jacquesludik", "followers": "779", "datetime": "2019-03-09 17:35:10", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104433245045903360": {"author": "@mavropalias", "followers": "157", "datetime": "2019-03-09 17:26:19", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105598577584173056": {"author": "@zumitan1", "followers": "108", "datetime": "2019-03-12 22:36:56", "content_summary": "DNN\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u305f\u3081\u306bSelf-Tuning Network (STN)\u3092\u63d0\u5531\u3002best-response architecture\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\u3092\u884c\u3044\u3001\u305d\u306e\u7d50\u679c\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308b\u3001\u3068\u3044\u3046\u624b\u9806\u3092\u7e70\u308a\u8fd4\u3059\u3002appendix G\u306bPytorch\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3042\u308a\u3002 https://t.co/KA8kMKKHRm"}, "1140617997905608704": {"author": "@vinaydebrou", "followers": "174", "datetime": "2019-06-17 13:51:37", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104264533307092992": {"author": "@cournape", "followers": "1,417", "datetime": "2019-03-09 06:15:55", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103920710097883143": {"author": "@mahesh21aug", "followers": "27", "datetime": "2019-03-08 07:29:41", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104673750904000514": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-03-10 09:22:00", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104229617861242880": {"author": "@kotti_sasikanth", "followers": "270", "datetime": "2019-03-09 03:57:11", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1103901302147362817": {"author": "@san6ee9", "followers": "83", "datetime": "2019-03-08 06:12:34", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103959052701204480": {"author": "@ComputerPapers", "followers": "613", "datetime": "2019-03-08 10:02:03", "content_summary": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions. https://t.co/md0apg7EWO"}, "1162127086774996993": {"author": "@CarlRioux", "followers": "1,186", "datetime": "2019-08-15 22:21:03", "content_summary": "[D] Self Tuning Networks: I Read this paper a while ago and got super excited about it, but didn't see it get implemented or talked about as i expected that it would, any ideas why? Link to the paper: https://t.co/YKioR4EuAy submitted by /u/El__Professor\u2026"}, "1105394675035172864": {"author": "@rose_miura", "followers": "290", "datetime": "2019-03-12 09:06:42", "content_summary": "RT @icoxfog417: \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u81ea\u52d5\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u7814\u7a76\u3002\u5b9f\u73fe\u306b\u306f\u3082\u3061\u308d\u3093\u5b66\u7fd2\u30bb\u30c3\u30c8\u306b\u9069\u5408\u6e08\u307f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\u3060\u304c(\u305d\u306e\u4e0a\u3067validation\u3092\u4f7f\u3044\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081)\u3001\u305d\u306e\u51fa\u529b\u3092\u8fd1\u4f3c\u3059\u308b\u3002\u624b\u6cd5\u3068\u3057\u3066\u306f\u3001\u901a\u5e38\u306e\u91cd\u307f/bias\u306b\u52a0\u3048\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u2026"}, "1104168710300823552": {"author": "@Surreabral", "followers": "55", "datetime": "2019-03-08 23:55:09", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104479125811552256": {"author": "@andrey_kurenkov", "followers": "3,416", "datetime": "2019-03-09 20:28:38", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1104158007846330368": {"author": "@letranger14", "followers": "328", "datetime": "2019-03-08 23:12:37", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104275653979975680": {"author": "@Hansatyam", "followers": "81", "datetime": "2019-03-09 07:00:07", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104081068016185345": {"author": "@ChrisPoptic", "followers": "185", "datetime": "2019-03-08 18:06:54", "content_summary": "Self-Tuning Networks. This is excellent. Well done, Roger. Will be reading your paper this weekend"}, "1103871514649145347": {"author": "@Miles_Brundage", "followers": "25,873", "datetime": "2019-03-08 04:14:12", "content_summary": "RT @BrundageBot: Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions. Matthew MacKay, Pa\u2026"}, "1104300776896253952": {"author": "@altamborrino", "followers": "544", "datetime": "2019-03-09 08:39:56", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1104207055412387840": {"author": "@citnaj", "followers": "12,040", "datetime": "2019-03-09 02:27:31", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104028532936605696": {"author": "@miyayou", "followers": "18,356", "datetime": "2019-03-08 14:38:08", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104170990135844865": {"author": "@p_kot1", "followers": "16", "datetime": "2019-03-09 00:04:13", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104237988383469569": {"author": "@ErmiaBivatan", "followers": "818", "datetime": "2019-03-09 04:30:26", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104176517452427264": {"author": "@thapraveensingh", "followers": "21", "datetime": "2019-03-09 00:26:11", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1105560942983737344": {"author": "@jonLorraine9", "followers": "122", "datetime": "2019-03-12 20:07:23", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105318934159323136": {"author": "@2k0ri", "followers": "972", "datetime": "2019-03-12 04:05:44", "content_summary": "RT @asam9891: Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions \u8d85\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u5909\u63db\u3059\u308bbest\u2026"}, "1104205106319368192": {"author": "@ASreesaila", "followers": "25", "datetime": "2019-03-09 02:19:47", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104106051178160128": {"author": "@F_Vaggi", "followers": "1,202", "datetime": "2019-03-08 19:46:10", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104313949942624257": {"author": "@blauigris", "followers": "245", "datetime": "2019-03-09 09:32:17", "content_summary": "RT @BrundageBot: Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions. Matthew MacKay, Pa\u2026"}, "1104372338567663616": {"author": "@RichmanRonald", "followers": "895", "datetime": "2019-03-09 13:24:18", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1105242690965389313": {"author": "@iamatachyon", "followers": "442", "datetime": "2019-03-11 23:02:46", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103892784971317248": {"author": "@mertyuksekgonul", "followers": "396", "datetime": "2019-03-08 05:38:43", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104989818025586688": {"author": "@luoyuchu", "followers": "21", "datetime": "2019-03-11 06:17:56", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104180516847263756": {"author": "@William33712308", "followers": "33", "datetime": "2019-03-09 00:42:04", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104457409714679809": {"author": "@desertnaut", "followers": "1,032", "datetime": "2019-03-09 19:02:20", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103846129043677185": {"author": "@deep_rl", "followers": "859", "datetime": "2019-03-08 02:33:20", "content_summary": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions - Matthew MacKay https://t.co/Rda5TDIyPh"}, "1104194842152689664": {"author": "@jeremyphoward", "followers": "100,219", "datetime": "2019-03-09 01:38:59", "content_summary": "This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a"}, "1104442246559084545": {"author": "@jchassoul", "followers": "156", "datetime": "2019-03-09 18:02:05", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1104157934596976640": {"author": "@kotti_sasikanth", "followers": "270", "datetime": "2019-03-08 23:12:20", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104075179183943681": {"author": "@Even_Oldridge", "followers": "1,164", "datetime": "2019-03-08 17:43:30", "content_summary": "Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fixed lr. Won't be long before all hyperparameters are scheduled would be my guess."}, "1104548740998692864": {"author": "@SquirrelYellow", "followers": "175", "datetime": "2019-03-10 01:05:16", "content_summary": "RT @asam9891: Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions \u8d85\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u5909\u63db\u3059\u308bbest\u2026"}, "1104508371741241344": {"author": "@akashjainx", "followers": "265", "datetime": "2019-03-09 22:24:51", "content_summary": "RT @jeremyphoward: This looks great awesome - try it out and tell me how it works for you! \ud83d\ude0a https://t.co/9OtzKcCTyS"}, "1104010997415727106": {"author": "@westis96", "followers": "298", "datetime": "2019-03-08 13:28:27", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1104268896977240064": {"author": "@artuskg", "followers": "45", "datetime": "2019-03-09 06:33:16", "content_summary": "RT @Even_Oldridge: Dropout schedules produce the best result, beating out fixed values, just like learning rate schedules beat the best fix\u2026"}, "1119483696191381504": {"author": "@lizoratech", "followers": "48", "datetime": "2019-04-20 06:11:26", "content_summary": "RT @deliprao: Dropout schedule! That and other nice things in this #ICLR2019 accept \"Self-tuning Networks\" https://t.co/SdP8QYmoCN https://\u2026"}, "1104294524233400325": {"author": "@ruben_rrf", "followers": "34", "datetime": "2019-03-09 08:15:06", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}, "1103849768126541824": {"author": "@SingularMattrix", "followers": "5,183", "datetime": "2019-03-08 02:47:47", "content_summary": "RT @RogerGrosse: Excited to release our paper on Self-Tuning Networks, a way of adapting regularization hyperparameters online during train\u2026"}}, "citation_id": "56672967", "queriedAt": "2020-06-04 00:07:50"}