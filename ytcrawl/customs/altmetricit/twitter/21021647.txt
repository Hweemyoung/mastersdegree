{"citation_id": "21021647", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 14:47:00", "twitter": {"995641646845902848": {"followers": "629", "datetime": "2018-05-13 12:27:21", "author": "@hiroyukim", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "995338475950911488": {"followers": "731", "datetime": "2018-05-12 16:22:39", "author": "@moe_moe_moegi", "content_summary": "\u50d5\u306e\u4e00\u756a\u597d\u304d\u306a\u7814\u7a76 https://t.co/X2ASmOPXlo"}, "886748672460967936": {"followers": "783", "datetime": "2017-07-17 00:45:33", "author": "@muktabh", "content_summary": "RT @StatMLPapers: Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] UPDATED) https://t.co/r2xP5XJ29d"}, "943657599475638273": {"followers": "25,578", "datetime": "2017-12-21 01:41:18", "author": "@Miles_Brundage", "content_summary": "OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learning from human preferences,\" by Christiano et al. at OpenAI and DeepMind: https://t.co/Kje9ua0r6x Clearly explained, important problem, and"}, "874677137029996544": {"followers": "122", "datetime": "2017-06-13 17:17:34", "author": "@cottonwoodfluff", "content_summary": "https://t.co/7gl9xElTiH OpenAI's paper on training an AI on human feedback. Infer human reward function, then RL to maximize that."}, "922985661812166656": {"followers": "61", "datetime": "2017-10-25 00:38:24", "author": "@SoEngineering", "content_summary": "#NewPaper: #arXiv cs.AI https://t.co/plM54aWgmd Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] CROSS LI\u2026"}, "877648941143146496": {"followers": "376", "datetime": "2017-06-21 22:06:28", "author": "@Mike1601", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "923216263337439232": {"followers": "145", "datetime": "2017-10-25 15:54:44", "author": "@esvhd", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "875791280881774594": {"followers": "159,742", "datetime": "2017-06-16 19:04:47", "author": "@Montreal_IA", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "923574757768830981": {"followers": "103", "datetime": "2017-10-26 15:39:16", "author": "@agibiansky", "content_summary": "OpenAI: https://t.co/cu5ryXeqy4! Instead of fixing a reward, have reward be the output of a DNN trained based on human evals of trajectories"}, "923252508541071361": {"followers": "2,927", "datetime": "2017-10-25 18:18:45", "author": "@evolvingstuff", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "877771242576543746": {"followers": "1,037", "datetime": "2017-06-22 06:12:27", "author": "@motolin5", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "938552020482523137": {"followers": "20,564", "datetime": "2017-12-06 23:33:33", "author": "@gwern", "content_summary": "An idea I had sometime ago: use Paul Christiano's 'learning from preferences' RL agent (https://t.co/p4RD2UdsTp) to learn to generate globally-coherent & meaningful music better than char-RNN: https://t.co/zTAZVoA2Ce"}, "995294057269051392": {"followers": "2,673", "datetime": "2018-05-12 13:26:09", "author": "@mosko_mule", "content_summary": "Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u95a2\u6570\u3092\u4eba\u624b\u3067\u8a2d\u8a08\u3059\u308b\u3053\u3068\u306a\u3057\u306b\u8907\u96d1\u306a\u632f\u308b\u821e\u3044\u3092\u9ad8\u901f\u306b\u5b66\u7fd2\u3059\u308b\u3002 https://t.co/Bs5qauHlvO https://t.co/LK8tJbR6CM"}, "877949064586731520": {"followers": "70,782", "datetime": "2017-06-22 17:59:03", "author": "@jblefevre60", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "923200610639347713": {"followers": "257", "datetime": "2017-10-25 14:52:32", "author": "@asif_rehan", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "877681768874291200": {"followers": "0", "datetime": "2017-06-22 00:16:54", "author": "@maglione_shawna", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "887437100286652416": {"followers": "38,161", "datetime": "2017-07-18 22:21:07", "author": "@mclynd", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "995299561433255937": {"followers": "12,765", "datetime": "2018-05-12 13:48:01", "author": "@jaguring1", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "874754898889277440": {"followers": "189", "datetime": "2017-06-13 22:26:34", "author": "@shahrukh_athar", "content_summary": "RT @Miles_Brundage: \"Deep reinforcement learning from human preferences,\" Christiano et al.: https://t.co/Kje9ua0r6x (the aforementioned Op\u2026"}, "882659483201044480": {"followers": "31,023", "datetime": "2017-07-05 17:56:34", "author": "@sectest9", "content_summary": "RT @DigammaAI: Deep #ReinforcementLearning from Human Preferences | Read here: https://t.co/VbWKH4Pr5T #machinelearning #ml #ai https://t.c\u2026"}, "878034587024113664": {"followers": "2,379", "datetime": "2017-06-22 23:38:53", "author": "@TerryUm_ML", "content_summary": "\"Deep reinforcement learning from human preferences\": Human in the RL loop without access to the reward function https://t.co/Iei7wtSFWv https://t.co/GG5P2AqQZT"}, "875728631804416002": {"followers": "841", "datetime": "2017-06-16 14:55:50", "author": "@makmanalp", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "877681248683958272": {"followers": "0", "datetime": "2017-06-22 00:14:50", "author": "@jamirose16721", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "945632684075831296": {"followers": "853", "datetime": "2017-12-26 12:29:35", "author": "@blackbox_ml", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "886747088696037378": {"followers": "627", "datetime": "2017-07-17 00:39:15", "author": "@helioRocha_", "content_summary": "#arXiv #stat_ML \"Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] UPDATED)\" https://t.co/tnmHVRnTGV"}, "874627949391118340": {"followers": "2,642", "datetime": "2017-06-13 14:02:07", "author": "@Redo", "content_summary": "[1706.03741] Deep reinforcement learning from human preferences https://t.co/y2jRI4pLXr"}, "874771287087828992": {"followers": "307", "datetime": "2017-06-13 23:31:41", "author": "@tschwartzbiz", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "877648928488935424": {"followers": "1,986", "datetime": "2017-06-21 22:06:25", "author": "@nandito94", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "874467742245543937": {"followers": "5,454", "datetime": "2017-06-13 03:25:31", "author": "@StatsPapers", "content_summary": "Deep reinforcement learning from human preferences. https://t.co/kcLsg9IWZq"}, "995576711910641665": {"followers": "303", "datetime": "2018-05-13 08:09:19", "author": "@213R", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "894616361347031040": {"followers": "1,938", "datetime": "2017-08-07 17:48:56", "author": "@EldarSilver", "content_summary": "Paper interesante sobre c\u00f3mo entrenar funciones de reward de Deep Reinforcement Learning con preferencias de humanos https://t.co/ntFcTRSeMg"}, "877909817423036416": {"followers": "576", "datetime": "2017-06-22 15:23:05", "author": "@stringpoem_", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "881604847073722372": {"followers": "4,312", "datetime": "2017-07-02 20:05:49", "author": "@a_hun1972", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "874432445579382784": {"followers": "635", "datetime": "2017-06-13 01:05:15", "author": "@rkakamilan", "content_summary": "RT @StatMLPapers: Deep reinforcement learning from human preferences. (arXiv:1706.03741v1 [stat.ML]) https://t.co/r2xP5XJ29d"}, "878356648632496130": {"followers": "433", "datetime": "2017-06-23 20:58:38", "author": "@bouzamass", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877684264036380673": {"followers": "0", "datetime": "2017-06-22 00:26:49", "author": "@salgado_elodia", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877825182668386305": {"followers": "15,645", "datetime": "2017-06-22 09:46:47", "author": "@pmedina", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "923447822527037440": {"followers": "1,759", "datetime": "2017-10-26 07:14:52", "author": "@akdm_bot", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "875261858630049792": {"followers": "3,187", "datetime": "2017-06-15 08:01:03", "author": "@Robots_and_AIs", "content_summary": "RT @v_vashishta Deep Reinforcement Learning from Human Preferences https://t.co/oF3Y2IWmyu #DeepLearning #machinele\u2026 https://t.co/yaebx8wXuW"}, "876109754484965377": {"followers": "204", "datetime": "2017-06-17 16:10:17", "author": "@ShezMasters", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "875250019959492609": {"followers": "15,143", "datetime": "2017-06-15 07:14:00", "author": "@alevergara78", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "943720529814470656": {"followers": "3,558", "datetime": "2017-12-21 05:51:22", "author": "@mapc", "content_summary": "RT @Miles_Brundage: On the other hand, @BrundageBot's favorite paper of the year is \"Multi-Label Zero-Shot Learning with Structured Knowled\u2026"}, "943827574769307648": {"followers": "912", "datetime": "2017-12-21 12:56:43", "author": "@ChikaObuah", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "903840328255279105": {"followers": "2,026", "datetime": "2017-09-02 04:41:41", "author": "@geoffreyirving", "content_summary": "@krees Statistical searches for best versions may be the future of https://t.co/kaIXWqruBH. The trick is separating random from systematic errors."}, "882659347070943232": {"followers": "8,726", "datetime": "2017-07-05 17:56:02", "author": "@DigammaAI", "content_summary": "Deep #ReinforcementLearning from Human Preferences | Read here: https://t.co/VbWKH4Pr5T #machinelearning #ml #ai https://t.co/4xrJb7VdvM"}, "884438694182211584": {"followers": "323", "datetime": "2017-07-10 15:46:31", "author": "@VivaNOLA", "content_summary": "Google/Musk have new approach to safer #AI: Good Parenting. Human training rather than blindly rewarding goal hits. https://t.co/MKs06a4JwK https://t.co/8jQM8JeBz9"}, "877680626966593539": {"followers": "0", "datetime": "2017-06-22 00:12:22", "author": "@leroux_virgil", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "923380097263865856": {"followers": "173", "datetime": "2017-10-26 02:45:45", "author": "@PacktDatahub", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "879392760305917952": {"followers": "0", "datetime": "2017-06-26 17:35:47", "author": "@sk_1010k", "content_summary": "https://t.co/1BRMt8TqHn"}, "877648544735350785": {"followers": "15,143", "datetime": "2017-06-21 22:04:53", "author": "@alevergara78", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "943664694492934144": {"followers": "1,058", "datetime": "2017-12-21 02:09:30", "author": "@BigsnarfDude", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "874748223855902720": {"followers": "30,496", "datetime": "2017-06-13 22:00:03", "author": "@v_vashishta", "content_summary": "Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "995386890487398400": {"followers": "613", "datetime": "2018-05-12 19:35:02", "author": "@lithium_03", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "943738346706059264": {"followers": "65", "datetime": "2017-12-21 07:02:10", "author": "@russell_lliu", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "877705051023605760": {"followers": "6,717", "datetime": "2017-06-22 01:49:25", "author": "@chidambara09", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "882660232911925249": {"followers": "756", "datetime": "2017-07-05 17:59:33", "author": "@angel_of_truths", "content_summary": "RT @DigammaAI: Deep #ReinforcementLearning from Human Preferences | Read here: https://t.co/VbWKH4Pr5T #machinelearning #ml #ai https://t.c\u2026"}, "877674117964726272": {"followers": "939", "datetime": "2017-06-21 23:46:30", "author": "@techai21001", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "943792727598120960": {"followers": "10", "datetime": "2017-12-21 10:38:15", "author": "@yarphs", "content_summary": "RT @Miles_Brundage: On the other hand, @BrundageBot's favorite paper of the year is \"Multi-Label Zero-Shot Learning with Structured Knowled\u2026"}, "877683895269044225": {"followers": "0", "datetime": "2017-06-22 00:25:21", "author": "@GudrunBatter", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "875729763255693314": {"followers": "25,578", "datetime": "2017-06-16 15:00:20", "author": "@Miles_Brundage", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "877684438813036545": {"followers": "0", "datetime": "2017-06-22 00:27:31", "author": "@PorscheRobot", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "874772728334757888": {"followers": "8,190", "datetime": "2017-06-13 23:37:25", "author": "@arichduvet", "content_summary": "RT @Miles_Brundage: \"Deep reinforcement learning from human preferences,\" Christiano et al.: https://t.co/Kje9ua0r6x (the aforementioned Op\u2026"}, "981200572786331648": {"followers": "76", "datetime": "2018-04-03 16:03:41", "author": "@MAndrecki", "content_summary": "5/ Examples of Goodhart's curse in AI: Deep reinforcement learning from human preferences https://t.co/PRpEGwS77w page 9: \"poor performance of offline reward predictor training\" When proxy (predicted reward) is not updated online, it fails to teach the de"}, "1096602858936590336": {"followers": "20,564", "datetime": "2019-02-16 02:51:10", "author": "@gwern", "content_summary": "@_Ryobot @lerner_adams Sequence-generation with GANs is still a big open problem AFAIK. Might not be necessary since self-supervised prediction works so well, as GPT-2 strikingly reminded us yesterday. (I also have a theory that prediction loss + RL finetu"}, "923198981919911943": {"followers": "1,943", "datetime": "2017-10-25 14:46:04", "author": "@solutions", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "877791963944300546": {"followers": "91,166", "datetime": "2017-06-22 07:34:47", "author": "@C_Randieri", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "902969519051870208": {"followers": "8,726", "datetime": "2017-08-30 19:01:24", "author": "@DigammaAI", "content_summary": "Deep #ReinforcementLearning from Human Preferences | Read here: https://t.co/2uN8wqW2HK #machinelearning #ml #ai https://t.co/pulj2Sefyq"}, "923360377135656961": {"followers": "295", "datetime": "2017-10-26 01:27:23", "author": "@balicea1", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "877789175306637312": {"followers": "3,498", "datetime": "2017-06-22 07:23:42", "author": "@EnrikeLopez", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877687900527316997": {"followers": "4", "datetime": "2017-06-22 00:41:16", "author": "@yates_kimberlie", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877684237687705600": {"followers": "0", "datetime": "2017-06-22 00:26:43", "author": "@Master_Borntrae", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877843422941290497": {"followers": "281", "datetime": "2017-06-22 10:59:16", "author": "@ProgramadorBug", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "874646953677672448": {"followers": "1,830", "datetime": "2017-06-13 15:17:38", "author": "@s_24_", "content_summary": "OpenAI \u3068 DeepMind \u306e\u5171\u8457\u3060 Deep reinforcement learning from human preferences https://t.co/5QCqQiXHPq"}, "878163403784802304": {"followers": "414", "datetime": "2017-06-23 08:10:45", "author": "@iAmNotAI", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877674105742426113": {"followers": "799", "datetime": "2017-06-21 23:46:27", "author": "@jramcast", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877825067064975360": {"followers": "53,303", "datetime": "2017-06-22 09:46:19", "author": "@pierrepinna", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877680175193960448": {"followers": "0", "datetime": "2017-06-22 00:10:34", "author": "@Isabelnam2259Zo", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877681836905840640": {"followers": "0", "datetime": "2017-06-22 00:17:11", "author": "@deloisefelice12", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "875805501275197440": {"followers": "103", "datetime": "2017-06-16 20:01:17", "author": "@jongen87", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "877881864035475457": {"followers": "2,069", "datetime": "2017-06-22 13:32:01", "author": "@hernangraffe", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "878228703754338305": {"followers": "1,226", "datetime": "2017-06-23 12:30:14", "author": "@JLackey2", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877654920702312448": {"followers": "19,379", "datetime": "2017-06-21 22:30:13", "author": "@StrategyFintech", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "897262010265739268": {"followers": "160", "datetime": "2017-08-15 01:01:48", "author": "@mdiallo", "content_summary": "Excellent read, if not sleeping ! Deep reinforcement learning from human preferences https://t.co/d7Dzua8b3p"}, "874671419216371712": {"followers": "1,528", "datetime": "2017-06-13 16:54:51", "author": "@ilblackdragon", "content_summary": "New #DeepMind & #OpenAI paper on Learning Reward from Human Judgments. https://t.co/i3NoXjsi07 #deeplearning https://t.co/eWRRxuq8zv https://t.co/mky9u5Eu4Q"}, "923106615565475840": {"followers": "4,255", "datetime": "2017-10-25 08:39:02", "author": "@weballergy", "content_summary": "'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #ReinforcementLearning #AI https://t.co/ImBNllGZ9F"}, "877683637344501760": {"followers": "15,143", "datetime": "2017-06-22 00:24:20", "author": "@alevergara78", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "874749471384633345": {"followers": "305", "datetime": "2017-06-13 22:05:00", "author": "@ProblemsGultar", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "922986605329121280": {"followers": "1,388", "datetime": "2017-10-25 00:42:09", "author": "@gastronomy", "content_summary": "[arXiv] Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] CROSS LISTED) --> For s\u2026 https://t.co/ltGp9muai0"}, "888716290046787588": {"followers": "16", "datetime": "2017-07-22 11:04:09", "author": "@terorama4", "content_summary": "[1706.03741] Deep reinforcement learning from human preferences https://t.co/KnhyeLHPla"}, "923298238488133632": {"followers": "20,838", "datetime": "2017-10-25 21:20:28", "author": "@Ashot_", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "875023341752532992": {"followers": "4,017", "datetime": "2017-06-14 16:13:16", "author": "@ballforest", "content_summary": "RT @s_24_: OpenAI \u3068 DeepMind \u306e\u5171\u8457\u3060 Deep reinforcement learning from human preferences https://t.co/5QCqQiXHPq"}, "875462478221631488": {"followers": "1,244", "datetime": "2017-06-15 21:18:14", "author": "@shaohua0116", "content_summary": "@OpenAI Training RL agents with a human-in-the-loop: learn from human preference inferred from pairwise comparisons. https://t.co/XYTp3MMgkN https://t.co/bn9Kjf5cio"}, "923400337620422657": {"followers": "2,922", "datetime": "2017-10-26 04:06:11", "author": "@statalgo", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "874675945839505409": {"followers": "5,407", "datetime": "2017-06-13 17:12:50", "author": "@TechNowOrNever", "content_summary": "RT @ilblackdragon: New #DeepMind & #OpenAI paper on Learning Reward from Human Judgments. https://t.co/i3NoXjsi07 #deeplearning https://t.c\u2026"}, "878388880080334848": {"followers": "266", "datetime": "2017-06-23 23:06:43", "author": "@brandonkindred", "content_summary": "Great research from some folks @DeepMindAI and @OpenAI on integrating human feedback with #ReinforcementLearning https://t.co/dwdyNhMqcN"}, "874738460275466241": {"followers": "115", "datetime": "2017-06-13 21:21:15", "author": "@tibarazmi", "content_summary": "In this paper (https://t.co/khuSVNBtPV) authors presented a learning algorithm that uses small amounts of human feed\u2026https://t.co/Tz0bjhGeKO"}, "995382643410714624": {"followers": "691", "datetime": "2018-05-12 19:18:10", "author": "@SythonUK", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "883533660762259458": {"followers": "442", "datetime": "2017-07-08 03:50:14", "author": "@eveherna", "content_summary": "[1706.03741] Deep reinforcement learning from human preferences https://t.co/bELmrA30qx"}, "874754251528032256": {"followers": "25,578", "datetime": "2017-06-13 22:24:00", "author": "@Miles_Brundage", "content_summary": "\"Deep reinforcement learning from human preferences,\" Christiano et al.: https://t.co/Kje9ua0r6x (the aforementioned OpenAI/DeepMind paper)"}, "897206327486726145": {"followers": "1,969", "datetime": "2017-08-14 21:20:32", "author": "@sherjilozair", "content_summary": "@notmisha @mark_riedl @yoavgo Could be the technique from the \"learning from human preferences\" paper: https://t.co/xpDuGbKZtE"}, "998162553241923606": {"followers": "149", "datetime": "2018-05-20 11:24:32", "author": "@togoturns", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "943661725957664768": {"followers": "625", "datetime": "2017-12-21 01:57:42", "author": "@generuso", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "877682962845794304": {"followers": "0", "datetime": "2017-06-22 00:21:39", "author": "@luba_aitken", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "875099297552805888": {"followers": "215", "datetime": "2017-06-14 21:15:05", "author": "@rsinghbiz", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "943886787264557056": {"followers": "1,215", "datetime": "2017-12-21 16:52:01", "author": "@mietek", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "922778424380477440": {"followers": "9,401", "datetime": "2017-10-24 10:54:55", "author": "@CSERCambridge", "content_summary": "RT @danieldewey: Deep RL from Human Preferences (Christiano, Leike, Brown, Martic, Legg, Amodei) https://t.co/lQhOEbsC0b #AISafetyPapers"}, "875379687496626176": {"followers": "309", "datetime": "2017-06-15 15:49:15", "author": "@cstonebiz", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "883900243607146496": {"followers": "416", "datetime": "2017-07-09 04:06:54", "author": "@humptyhumps", "content_summary": "Training better AI while creating next-gen jobs too? https://t.co/i30ruCkwbJ"}, "943763480363991040": {"followers": "456", "datetime": "2017-12-21 08:42:02", "author": "@PerthMLGroup", "content_summary": "RT @Miles_Brundage: On the other hand, @BrundageBot's favorite paper of the year is \"Multi-Label Zero-Shot Learning with Structured Knowled\u2026"}, "1000290016730345472": {"followers": "171", "datetime": "2018-05-26 08:18:19", "author": "@UMAisForever", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "944143461552394242": {"followers": "226", "datetime": "2017-12-22 09:51:57", "author": "@cetusparibus", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "924505823040147458": {"followers": "3,049", "datetime": "2017-10-29 05:18:59", "author": "@EpicRelevance", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "877866966853332993": {"followers": "328", "datetime": "2017-06-22 12:32:49", "author": "@RealBrutalist", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877683566083231745": {"followers": "0", "datetime": "2017-06-22 00:24:03", "author": "@bruns_yee", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "923182638659178496": {"followers": "743", "datetime": "2017-10-25 13:41:07", "author": "@arxiv_cshc", "content_summary": "Deep reinforcement learning from human preferences https://t.co/IDfOyaBw27"}, "875726593901158400": {"followers": "13,349", "datetime": "2017-06-16 14:47:44", "author": "@timhwang", "content_summary": "Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.co/M6wsH6puWk"}, "886726319928729601": {"followers": "38,161", "datetime": "2017-07-16 23:16:43", "author": "@mclynd", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "922610085104443392": {"followers": "966", "datetime": "2017-10-23 23:46:00", "author": "@HaydnBelfield", "content_summary": "RT @danieldewey: Deep RL from Human Preferences (Christiano, Leike, Brown, Martic, Legg, Amodei) https://t.co/lQhOEbsC0b #AISafetyPapers"}, "877674369979428864": {"followers": "6,611", "datetime": "2017-06-21 23:47:30", "author": "@robotics_monkey", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "875024719954575360": {"followers": "2,050", "datetime": "2017-06-14 16:18:45", "author": "@termoshtt", "content_summary": "RT @s_24_: OpenAI \u3068 DeepMind \u306e\u5171\u8457\u3060 Deep reinforcement learning from human preferences https://t.co/5QCqQiXHPq"}, "1211324090973331456": {"followers": "232", "datetime": "2019-12-29 16:32:23", "author": "@JeWe37", "content_summary": "@Grimeandreason there actually is research in machine learning on that too: https://t.co/zMaK5UyT5s the issue i was describing is the issue of answering why we get certain quantifiable results in system, i.e. modeling that system, rather than answering que"}, "877684403023085568": {"followers": "4", "datetime": "2017-06-22 00:27:22", "author": "@crystalwest7441", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "882659447809683463": {"followers": "15,143", "datetime": "2017-07-05 17:56:26", "author": "@alevergara78", "content_summary": "RT @DigammaAI: Deep #ReinforcementLearning from Human Preferences | Read here: https://t.co/VbWKH4Pr5T #machinelearning #ml #ai https://t.c\u2026"}, "877819423729618945": {"followers": "33,589", "datetime": "2017-06-22 09:23:54", "author": "@RosyCoaching", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "874675905733554176": {"followers": "25", "datetime": "2017-06-13 17:12:41", "author": "@arulliere", "content_summary": "1706.03741.pdf (via @Pocket) https://t.co/PYR8CWjXLI https://t.co/PYR8CWjXLI"}, "877866232829034496": {"followers": "960", "datetime": "2017-06-22 12:29:54", "author": "@Yasir287", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "1110247903967223812": {"followers": "20,564", "datetime": "2019-03-25 18:31:42", "author": "@gwern", "content_summary": "Startup idea: learning-from-preferences (https://t.co/p4RD2UdsTp) RL training of text-generating NNs like for poetry. Sell 'T.S. AIiot' or 'SylvAI Plath' as a service. Secret sauce: it's trained on pairwise ratings, provided by broke MFAs hired to rate poe"}, "938580779864608768": {"followers": "183", "datetime": "2017-12-07 01:27:50", "author": "@SelectivePress", "content_summary": "RT @gwern: An idea I had sometime ago: use Paul Christiano's 'learning from preferences' RL agent (https://t.co/p4RD2UdsTp) to learn to gen\u2026"}, "877809911635116032": {"followers": "31,547", "datetime": "2017-06-22 08:46:06", "author": "@GCPanel", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "874717876149768193": {"followers": "5,214", "datetime": "2017-06-13 19:59:27", "author": "@AlisonBLowndes", "content_summary": "Its truly staggering what humans are capable of when they cooperate: teaching #AI to backflip! @DeepMindAI + @OpenAI https://t.co/vaMq1bNlGQ https://t.co/53nobGQ8Kf"}, "877604927526248448": {"followers": "9,401", "datetime": "2017-06-21 19:11:34", "author": "@CSERCambridge", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "923066889756135424": {"followers": "773", "datetime": "2017-10-25 06:01:10", "author": "@arxivml", "content_summary": "\"Deep reinforcement learning from human preferences\", Paul Christiano, Jan Leike, Tom B\uff0e Brown, Miljan Martic, Shan\u2026 https://t.co/W6Cw4hJbJ7"}, "923126076833923073": {"followers": "191", "datetime": "2017-10-25 09:56:22", "author": "@MulberryBeacon", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "877668397319958528": {"followers": "3,986", "datetime": "2017-06-21 23:23:46", "author": "@Johntech1990", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "875432639649292289": {"followers": "215", "datetime": "2017-06-15 19:19:40", "author": "@rsinghbiz", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "887024445919023109": {"followers": "2,642", "datetime": "2017-07-17 19:01:22", "author": "@Redo", "content_summary": "[1706.03741] Deep reinforcement learning from human preferences V3 https://t.co/SlrDMVnx07"}, "922984484693270529": {"followers": "627", "datetime": "2017-10-25 00:33:44", "author": "@helioRocha_", "content_summary": "#arXiv #cs_AI \"Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] CROSS LISTED)\" https://t.co/tnmHVRnTGV"}, "882453448259756034": {"followers": "1,055", "datetime": "2017-07-05 04:17:51", "author": "@blattnerma", "content_summary": "Deep reinforcement learning from human preferences #ML #AI https://t.co/LRpBjUmIoe"}, "875801539440504832": {"followers": "56,127", "datetime": "2017-06-16 19:45:33", "author": "@IntelligenceTV", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "884891886158270465": {"followers": "633", "datetime": "2017-07-11 21:47:20", "author": "@hamraniii", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "874708925932765184": {"followers": "45", "datetime": "2017-06-13 19:23:53", "author": "@davidcittadini", "content_summary": "Deep reinforcement learning from human preferences https://t.co/Umhl4Tskan"}, "874671851980640256": {"followers": "15,645", "datetime": "2017-06-13 16:56:34", "author": "@pmedina", "content_summary": "RT @ilblackdragon: New #DeepMind & #OpenAI paper on Learning Reward from Human Judgments. https://t.co/i3NoXjsi07 #deeplearning https://t.c\u2026"}, "923447821105184769": {"followers": "13,008", "datetime": "2017-10-26 07:14:52", "author": "@SeattleDataGuy", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "878531897110003712": {"followers": "21,848", "datetime": "2017-06-24 08:35:01", "author": "@ELCavalos", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "943719439660396544": {"followers": "25,578", "datetime": "2017-12-21 05:47:02", "author": "@Miles_Brundage", "content_summary": "On the other hand, @BrundageBot's favorite paper of the year is \"Multi-Label Zero-Shot Learning with Structured Knowledge Graphs\" by Lee and Fang et al. Thanks @amaub for checking on this and for making @BrundageBot :) https://t.co/7eKcsX6zCf"}, "877685362461741057": {"followers": "8", "datetime": "2017-06-22 00:31:11", "author": "@Chocolate_Yjojp", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "875815618213294081": {"followers": "160,790", "datetime": "2017-06-16 20:41:29", "author": "@Quebec_AI", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "995363854098972672": {"followers": "825", "datetime": "2018-05-12 18:03:30", "author": "@morioka", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "878342401814908928": {"followers": "8,726", "datetime": "2017-06-23 20:02:02", "author": "@DigammaAI", "content_summary": "Deep reinforcement learning from human preferences | Read here: https://t.co/D5YXFPS6aF #machinelearning #deeplearning #ml https://t.co/c0kEPbe11x"}, "875836976716406785": {"followers": "18,359", "datetime": "2017-06-16 22:06:22", "author": "@rcalo", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "877788292665757696": {"followers": "113,049", "datetime": "2017-06-22 07:20:12", "author": "@ipfconline1", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "923150196984467456": {"followers": "11,390", "datetime": "2017-10-25 11:32:12", "author": "@_brohrer_", "content_summary": "Agents learn complex reward functions by A/B testing trajectories on human raters. Much less human time required than direct labeling. https://t.co/B2aRuMGNa5"}, "877685350256197632": {"followers": "2", "datetime": "2017-06-22 00:31:08", "author": "@McshaneHoa", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "874850347877093376": {"followers": "1,628", "datetime": "2017-06-14 04:45:51", "author": "@danieldewey", "content_summary": "Deep RL from Human Preferences (Christiano, Leike, Brown, Martic, Legg, Amodei) https://t.co/lQhOEbsC0b #AISafetyPapers"}, "886746862438494208": {"followers": "9,689", "datetime": "2017-07-17 00:38:21", "author": "@StatMLPapers", "content_summary": "Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] UPDATED) https://t.co/r2xP5XJ29d"}, "893197120982753281": {"followers": "355", "datetime": "2017-08-03 19:49:23", "author": "@raelifin", "content_summary": "OpenAI just released code that I helped with! Human guided reinforcement learning: https://t.co/LjbkBw0DLh Based on https://t.co/7SUaKXt4KI"}, "874693243870556164": {"followers": "773", "datetime": "2017-06-13 18:21:35", "author": "@arxivml", "content_summary": "\"Deep reinforcement learning from human preferences\", Paul Christiano, Jan Leike, Tom B\uff0e Brown, Miljan Martic, Shan\u2026 https://t.co/W6Cw4hJbJ7"}, "996779212231950338": {"followers": "173,737", "datetime": "2018-05-16 15:47:38", "author": "@goodfellow_ian", "content_summary": "@Usunder Yes, some researchers do that. A few examples: https://t.co/AlD3McjQwe https://t.co/0cfrp11AxO https://t.co/YnatzGSfs9"}, "877999716960579584": {"followers": "2,561", "datetime": "2017-06-22 21:20:19", "author": "@Irpod5", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "923001458177449984": {"followers": "743", "datetime": "2017-10-25 01:41:10", "author": "@arxiv_cshc", "content_summary": "Deep reinforcement learning from human preferences https://t.co/IDfOyajUDx"}, "923159469810507776": {"followers": "8", "datetime": "2017-10-25 12:09:03", "author": "@simonkrannig", "content_summary": "RT @_brohrer_: Agents learn complex reward functions by A/B testing trajectories on human raters. Much less human time required than direct\u2026"}, "874468267137617921": {"followers": "3,374", "datetime": "2017-06-13 03:27:36", "author": "@renato_umeton", "content_summary": "RT @StatsPapers: Deep reinforcement learning from human preferences. https://t.co/kcLsg9IWZq"}, "1073350626489368578": {"followers": "20,564", "datetime": "2018-12-13 22:55:05", "author": "@gwern", "content_summary": "@ctphoenix One theory of mine is that the likelihood loss *is* bad for creating enjoyable, rather than realistic, samples, and one needs humans in the loop to learn from esthetic comparisons as the loss: https://t.co/p4RD2UdsTp"}, "943720750187290625": {"followers": "1,201", "datetime": "2017-12-21 05:52:14", "author": "@machinaut", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "886756908287246337": {"followers": "395", "datetime": "2017-07-17 01:18:16", "author": "@zuxfoucault", "content_summary": "RT @StatMLPapers: Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] UPDATED) https://t.co/r2xP5XJ29d"}, "943763454229229568": {"followers": "436", "datetime": "2017-12-21 08:41:56", "author": "@letitriip", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "877678174729011200": {"followers": "0", "datetime": "2017-06-22 00:02:37", "author": "@EffieCristoforo", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "874437812749205504": {"followers": "858", "datetime": "2017-06-13 01:26:35", "author": "@deep_rl", "content_summary": "Deep reinforcement learning from human preferences - Paul Christiano https://t.co/jsi1pq6EaP"}, "876224489616539648": {"followers": "3,430", "datetime": "2017-06-17 23:46:12", "author": "@kennethanderson", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "995338677055250433": {"followers": "1,726", "datetime": "2018-05-12 16:23:27", "author": "@superbradyon", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "878531969264623616": {"followers": "15,143", "datetime": "2017-06-24 08:35:18", "author": "@alevergara78", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877609600844943362": {"followers": "10,509", "datetime": "2017-06-21 19:30:08", "author": "@mpshanahan", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "877715753499111424": {"followers": "33,279", "datetime": "2017-06-22 02:31:57", "author": "@HITpol", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877763526118572037": {"followers": "90", "datetime": "2017-06-22 05:41:47", "author": "@ITProfessiona13", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "875730594357030912": {"followers": "966", "datetime": "2017-06-16 15:03:38", "author": "@HaydnBelfield", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "1131791192352526337": {"followers": "96", "datetime": "2019-05-24 05:17:02", "author": "@Delfox29", "content_summary": "RT @gwern: An idea I had sometime ago: use Paul Christiano's 'learning from preferences' RL agent (https://t.co/p4RD2UdsTp) to learn to gen\u2026"}, "875443295698268169": {"followers": "8,726", "datetime": "2017-06-15 20:02:01", "author": "@DigammaAI", "content_summary": "Deep #ReinforcementLearning from Human Preferences | Read here: https://t.co/wOoo2K0REI #machinelearning #ml #ai https://t.co/yUa52eSOlU"}, "1113732051541348352": {"followers": "14", "datetime": "2019-04-04 09:16:27", "author": "@e7mul", "content_summary": "RT @gwern: @vrama91 @RanjayKrishna @poolio Yes. If you train a GAN's D based on human comparisons of real/fake images, you essentially have\u2026"}, "1129636293523042304": {"followers": "33,413", "datetime": "2019-05-18 06:34:14", "author": "@bramcohen", "content_summary": "@Mario_Gibney I don't know of anyone running an alternative AI safety research institute which does things better (or any alternative institute for that matter) but here's an example of good research with much more concrete results and measured claims: htt"}, "1110699631393017856": {"followers": "40", "datetime": "2019-03-27 00:26:42", "author": "@iamGerdes", "content_summary": "RT @gwern: Startup idea: learning-from-preferences (https://t.co/p4RD2UdsTp) RL training of text-generating NNs like for poetry. Sell 'T.S.\u2026"}, "1206245356721319940": {"followers": "871", "datetime": "2019-12-15 16:11:18", "author": "@MonosovLab", "content_summary": "Interesting. Will need to dig in over few days vacation"}, "1014898092229976064": {"followers": "17", "datetime": "2018-07-05 15:45:35", "author": "@photojunkyftb", "content_summary": "@ThomasSimonini Loving your new deep RL course! Could you implement human feedback in one of the videos? like this: https://t.co/35NyzbK6mW \u2026?"}, "929327204902694912": {"followers": "167", "datetime": "2017-11-11 12:37:26", "author": "@pkfactor", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "922999715922190338": {"followers": "10", "datetime": "2017-10-25 01:34:15", "author": "@allegro_smart", "content_summary": "Deep reinforcement learning from human preferences \u4eba\u9593\u306e\u597d\u307f\u304b\u3089\u5b66\u3076\u6df1\u3044\u5f37\u5316https://https://t.co/GfIZvDpIcN \u8981\u7d04\uff1a\u6d17\u7df4\u3055\u308c\u305f\u5f37\u5316\u5b66\u7fd2\uff08RL\uff09\u30b7\u30b9\u30c6\u30e0\u304c\u73fe\u5b9f\u306e\u74b0\u5883\u3068\u6709\u52b9\u306b\u5bfe\u8a71\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u8907\u2026"}, "874774384120037379": {"followers": "4,071", "datetime": "2017-06-13 23:44:00", "author": "@bgoncalves", "content_summary": "[1706.03741] Deep reinforcement learning from human preferences https://t.co/bK6pIcIWY8"}, "874773204249837568": {"followers": "3,256", "datetime": "2017-06-13 23:39:19", "author": "@PaaSDev", "content_summary": "RT @Miles_Brundage: \"Deep reinforcement learning from human preferences,\" Christiano et al.: https://t.co/Kje9ua0r6x (the aforementioned Op\u2026"}, "875800402167558145": {"followers": "164,067", "datetime": "2017-06-16 19:41:02", "author": "@ceobillionaire", "content_summary": "RT @timhwang: Favorite paper of the week is, of course, the joint @OpenAI - @DeepMindAI piece on low-cost human oversight in RL https://t.c\u2026"}, "874717985121980417": {"followers": "5,407", "datetime": "2017-06-13 19:59:53", "author": "@TechNowOrNever", "content_summary": "RT @AlisonBLowndes: Its truly staggering what humans are capable of when they cooperate: teaching #AI to backflip! @DeepMindAI + @OpenAI ht\u2026"}, "886749282128338944": {"followers": "275", "datetime": "2017-07-17 00:47:58", "author": "@kadarakos", "content_summary": "RT @StatMLPapers: Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] UPDATED) https://t.co/r2xP5XJ29d"}, "877647912582266881": {"followers": "127,858", "datetime": "2017-06-21 22:02:22", "author": "@DeepLearn007", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "943758685137862657": {"followers": "217", "datetime": "2017-12-21 08:22:59", "author": "@AssistedEvolve", "content_summary": "RT @Miles_Brundage: On the other hand, @BrundageBot's favorite paper of the year is \"Multi-Label Zero-Shot Learning with Structured Knowled\u2026"}, "891839224969924608": {"followers": "867", "datetime": "2017-07-31 01:53:35", "author": "@Spaulify", "content_summary": "Folks from Open AI and DeepMind/Google have written this paper: https://t.co/jG8ZHosYT0 I wonder... https://t.co/ZYlWq2400l"}, "923195403474006016": {"followers": "1,105", "datetime": "2017-10-25 14:31:50", "author": "@permutans", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "875024001566859264": {"followers": "150", "datetime": "2017-06-14 16:15:53", "author": "@y_yammt", "content_summary": "RT @s_24_: OpenAI \u3068 DeepMind \u306e\u5171\u8457\u3060 Deep reinforcement learning from human preferences https://t.co/5QCqQiXHPq"}, "931236013564923904": {"followers": "381", "datetime": "2017-11-16 19:02:21", "author": "@thomasjelonek", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "964517319866310656": {"followers": "855", "datetime": "2018-02-16 15:10:23", "author": "@FMarradi", "content_summary": "[1706.03741] Deep reinforcement learning from human preferences https://t.co/ZQKWY6JLLI"}, "877791965408104451": {"followers": "16,553", "datetime": "2017-06-22 07:34:47", "author": "@itknowingness", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "877727447273267200": {"followers": "1,527", "datetime": "2017-06-22 03:18:25", "author": "@Pedra999", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "995577230209118208": {"followers": "686", "datetime": "2018-05-13 08:11:23", "author": "@j54854", "content_summary": "RT @mosko_mule: Deep reinforcement learning from human preferences https://t.co/JZYQKx2HFr \u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3044\u3066\u884c\u52d5\u7cfb\u5217\u96c6\u5408\u304b\u3089\u50c5\u304b\u306a\u30da\u30a2\u3092\u9078\u629e\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u826f\u3044\u65b9\u3092\u4eba\u9593\u304c\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u5831\u916c\u2026"}, "923156590286516224": {"followers": "108", "datetime": "2017-10-25 11:57:37", "author": "@templarpt", "content_summary": "RT @_brohrer_: Agents learn complex reward functions by A/B testing trajectories on human raters. Much less human time required than direct\u2026"}, "922986355851972608": {"followers": "743", "datetime": "2017-10-25 00:41:10", "author": "@arxiv_cshc", "content_summary": "Deep reinforcement learning from human preferences https://t.co/IDfOyaBw27"}, "875246501730992128": {"followers": "30,496", "datetime": "2017-06-15 07:00:01", "author": "@v_vashishta", "content_summary": "Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "877687832780865536": {"followers": "0", "datetime": "2017-06-22 00:41:00", "author": "@OnlyWendie", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "943722358392569856": {"followers": "3,126", "datetime": "2017-12-21 05:58:38", "author": "@MrMeritology", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "877808291224576000": {"followers": "2,303", "datetime": "2017-06-22 08:39:40", "author": "@iuinfograd", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877881670258511872": {"followers": "53", "datetime": "2017-06-22 13:31:15", "author": "@vb_tri", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "943759283644121088": {"followers": "4,327", "datetime": "2017-12-21 08:25:22", "author": "@jekbradbury", "content_summary": "RT @Miles_Brundage: OK, the results from my highly arbitrary and biased decision process are in. The winner is: \"Deep reinforcement learnin\u2026"}, "877789247922552832": {"followers": "2,241", "datetime": "2017-06-22 07:23:59", "author": "@TAKEBACKOCH", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877637262879604737": {"followers": "56,433", "datetime": "2017-06-21 21:20:03", "author": "@ahier", "content_summary": "Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sBuAEt7fD7 https://t.co/zVGwiV34kc"}, "923345238495604737": {"followers": "221", "datetime": "2017-10-26 00:27:14", "author": "@positivearrow", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "877657220665663492": {"followers": "2,846", "datetime": "2017-06-21 22:39:22", "author": "@Fd_aoumari", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "877046132806963200": {"followers": "1,428", "datetime": "2017-06-20 06:11:07", "author": "@jreuben1", "content_summary": "Deep reinforcement learning from human preferences https://t.co/atxfqbvWB8"}, "938562424080252929": {"followers": "4,946", "datetime": "2017-12-07 00:14:54", "author": "@micahstubbs", "content_summary": "RT @gwern: An idea I had sometime ago: use Paul Christiano's 'learning from preferences' RL agent (https://t.co/p4RD2UdsTp) to learn to gen\u2026"}, "922985146340569093": {"followers": "662", "datetime": "2017-10-25 00:36:21", "author": "@dbworld_", "content_summary": "https://t.co/1Zyn6uYDIA Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] CROSS LISTED) #ai"}, "874608370279469056": {"followers": "299", "datetime": "2017-06-13 12:44:19", "author": "@madrugad0", "content_summary": "human-in-the-loop for RL tasks: human compares trajectories from time to time https://t.co/KXQtm39LDs"}, "1117921162749784065": {"followers": "217", "datetime": "2019-04-15 22:42:29", "author": "@gopalpsarma", "content_summary": "1/ Somewhat tangentially, this makes me think about the amount of time and human interaction that could be required for ensuring value alignment in sophisticated AI systems using techniques such as https://t.co/mLXUEhdjDm"}, "874650614105559041": {"followers": "12,020", "datetime": "2017-06-13 15:32:11", "author": "@Sam_L_Shead", "content_summary": "Here is the latest DeepMind/OpenAI paper https://t.co/OWlZhIM5VR"}, "1069868227629469697": {"followers": "19", "datetime": "2018-12-04 08:17:17", "author": "@dayouyang", "content_summary": "https://t.co/n2pUt3QlUQ Interesting to such a new way: deep reinforcement learning from (non-expert) human preferences can effectively solve complex RL tasks without access to the reward function, but by \"teaching\" the feedback on less than one percent of\u2026"}, "874426005427027973": {"followers": "627", "datetime": "2017-06-13 00:39:40", "author": "@helioRocha_", "content_summary": "#arXiv #stat_ML \"Deep reinforcement learning from human preferences. (arXiv:1706.03741v1 [stat.ML])\" https://t.co/tnmHVRnTGV"}, "877656288347291649": {"followers": "56,433", "datetime": "2017-06-21 22:35:39", "author": "@ahier", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "923124318174932992": {"followers": "440", "datetime": "2017-10-25 09:49:22", "author": "@FlorinGogianu", "content_summary": "RT @weballergy: 'Deep Reinforcement Learning from Human Preferences' by DeepMind and OpenAI https://t.co/vfubouRTGH #DeepLearning #Reinforc\u2026"}, "883558041152610304": {"followers": "8,190", "datetime": "2017-07-08 05:27:07", "author": "@arichduvet", "content_summary": "RT @eveherna: [1706.03741] Deep reinforcement learning from human preferences https://t.co/bELmrA30qx"}, "877674103724834816": {"followers": "33,279", "datetime": "2017-06-21 23:46:27", "author": "@HITpol", "content_summary": "@ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai https://t.co/nQlvfLnX2Y https://t.co/nRMi1jkz3j"}, "877928588674080768": {"followers": "1,393", "datetime": "2017-06-22 16:37:41", "author": "@ramnivghekar", "content_summary": "RT @HITpol: @ahier @DeepLearn007 @DeepMindAI @OpenAI mt @ahier Deep #reinforcementlearning from #human preferences #machinelearning #ai ht\u2026"}, "875020892056690688": {"followers": "635", "datetime": "2017-06-14 16:03:32", "author": "@rkakamilan", "content_summary": "RT @s_24_: OpenAI \u3068 DeepMind \u306e\u5171\u8457\u3060 Deep reinforcement learning from human preferences https://t.co/5QCqQiXHPq"}, "1113609449380429825": {"followers": "20,564", "datetime": "2019-04-04 01:09:17", "author": "@gwern", "content_summary": "@vrama91 @RanjayKrishna @poolio Yes. If you train a GAN's D based on human comparisons of real/fake images, you essentially have @paulfchristiano 's https://t.co/p4RD2UdsTp . (GANs and actor-critic DRL are almost isomorphic.)"}, "877825827827621888": {"followers": "1,941", "datetime": "2017-06-22 09:49:21", "author": "@vcjha", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "875023483922661377": {"followers": "309", "datetime": "2017-06-14 16:13:50", "author": "@cstonebiz", "content_summary": "RT @v_vashishta: Deep Reinforcement Learning from Human Preferences https://t.co/DJUKPZfNX7 #DeepLearning #machinelearning"}, "877860891634524160": {"followers": "181", "datetime": "2017-06-22 12:08:41", "author": "@lpadukana", "content_summary": "RT @ahier: Deep #reinforcementlearning from #human preferences #machinelearning @DeepLearn007 #ai by @DeepMindAI + @OpenAI https://t.co/sB\u2026"}, "886960249562775552": {"followers": "481", "datetime": "2017-07-17 14:46:17", "author": "@debarko", "content_summary": "RT @helioRocha_: #arXiv #stat_ML \"Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] UPDATED)\" https://t.co/\u2026"}, "1206242595866923009": {"followers": "129", "datetime": "2019-12-15 16:00:20", "author": "@AjarAilerons", "content_summary": "Reward-function free reinforcement learning: replacing the reward function by human-in-the-loop supervision. No pre-existing dataset required since uncertainty-maximising examples are created on-the-fly by the agent. Paper: https://t.co/38ouDWTmBm Code: h"}, "1238104890938884102": {"followers": "648", "datetime": "2020-03-12 14:09:43", "author": "@jvmncs", "content_summary": "@theshawwn that's awesome -- an open-source wireframe for preference learning/active learning with generality in mind would be a huge win, not least for https://t.co/aoUqtlPUjA or https://t.co/hoWD7dul6M"}}}