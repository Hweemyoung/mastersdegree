{"tab": "twitter", "completed": "1", "twitter": {"1209659173576859652": {"author": "@unixpickle", "followers": "1,353", "datetime": "2019-12-25 02:16:36", "content_summary": "There was a slightly earlier paper arriving at the same conclusion: https://t.co/dbc70apY0H"}, "1103274405365334016": {"author": "@__jm", "followers": "245", "datetime": "2019-03-06 12:41:30", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1102786003880751104": {"author": "@ElectronNest", "followers": "230", "datetime": "2019-03-05 04:20:46", "content_summary": "RT @icoxfog417: DNN\u306e\u91cd\u307f\u3092\u758e\u306b\u3059\u308b\u624b\u6cd5\u306b\u3064\u3044\u3066\u3001\u3069\u306e\u624b\u6cd5\u304c\u6709\u52b9\u304b\u3092\u5927\u898f\u6a21\u306a\u30e2\u30c7\u30eb(Transformer/ResNet)\u3067\u691c\u8a3c\u3057\u305f\u7814\u7a76(\u758e=\u91cd\u307f\u304c0\u306b\u8fd1\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u591a\u3044\u3068\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u308b)\u3002\u7d50\u679c\u3068\u3057\u3066\u306f\u3001\u5358\u7d14\u306b\u91cd\u307f\u306eMagnitude\u3067\u679d\u5208\u308a\u3059\u308b\u624b\u6cd5\u304c\u2026"}, "1102760355485356034": {"author": "@StephenPiment", "followers": "6,161", "datetime": "2019-03-05 02:38:51", "content_summary": "The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1101885555875373056": {"author": "@oh_that_hat", "followers": "762", "datetime": "2019-03-02 16:42:43", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1140131442367721472": {"author": "@recurseparadox", "followers": "443", "datetime": "2019-06-16 05:38:13", "content_summary": "@jackclarkSF Closest thing I know to this is network pruning and the sparsity-accuracy tradeoff: https://t.co/NuGs6ZkKK1"}, "1100577428974563330": {"author": "@StatMLPapers", "followers": "9,763", "datetime": "2019-02-27 02:04:41", "content_summary": "The State of Sparsity in Deep Neural Networks. (arXiv:1902.09574v1 [cs.LG]) https://t.co/StRSWaUBXh"}, "1101635532843569154": {"author": "@kevoniano", "followers": "407", "datetime": "2019-03-02 00:09:13", "content_summary": "RT @sarahookr: Oops, I shared a very mysterious link to nowhere. https://t.co/tgX7x0mFLV is perhaps slightly more useful for those interest\u2026"}, "1103052793982550016": {"author": "@kawauso_kun", "followers": "436", "datetime": "2019-03-05 22:00:54", "content_summary": "RT @icoxfog417: DNN\u306e\u91cd\u307f\u3092\u758e\u306b\u3059\u308b\u624b\u6cd5\u306b\u3064\u3044\u3066\u3001\u3069\u306e\u624b\u6cd5\u304c\u6709\u52b9\u304b\u3092\u5927\u898f\u6a21\u306a\u30e2\u30c7\u30eb(Transformer/ResNet)\u3067\u691c\u8a3c\u3057\u305f\u7814\u7a76(\u758e=\u91cd\u307f\u304c0\u306b\u8fd1\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u591a\u3044\u3068\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u308b)\u3002\u7d50\u679c\u3068\u3057\u3066\u306f\u3001\u5358\u7d14\u306b\u91cd\u307f\u306eMagnitude\u3067\u679d\u5208\u308a\u3059\u308b\u624b\u6cd5\u304c\u2026"}, "1102870416958373888": {"author": "@oksuzilkay", "followers": "216", "datetime": "2019-03-05 09:56:12", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1101623642733989888": {"author": "@linkoffate", "followers": "398", "datetime": "2019-03-01 23:21:58", "content_summary": "RT @sarahookr: Oops, I shared a very mysterious link to nowhere. https://t.co/tgX7x0mFLV is perhaps slightly more useful for those interest\u2026"}, "1100767415770611712": {"author": "@dksdc", "followers": "68", "datetime": "2019-02-27 14:39:37", "content_summary": "RT @arxiv_org: The State of Sparsity in Deep Neural Networks. https://t.co/U9xMAUmRdo https://t.co/cbWPukzPHP"}, "1100927240173481984": {"author": "@arxiv_in_review", "followers": "1,345", "datetime": "2019-02-28 01:14:42", "content_summary": "#ICML2019 The State of Sparsity in Deep Neural Networks. (arXiv:1902.09574v1 [cs\\.LG]) https://t.co/jYBbVrjdpM"}, "1107952360477917189": {"author": "@Andrea_ilsergio", "followers": "1,149", "datetime": "2019-03-19 10:30:02", "content_summary": "The State of Sparsity in Deep Neural Networks Evaluating 3 state-of-the-art techniques for inducing sparsity in #NeuralNetworks on 2 large-scale #deeplearning tasks Paper by Trevor Gale, Erich Elsen, Sara Hooker ArXiv Cornel University https://t.co/qWOdvpD"}, "1100624762517082113": {"author": "@Miles_Brundage", "followers": "25,873", "datetime": "2019-02-27 05:12:46", "content_summary": "RT @BrundageBot: The State of Sparsity in Deep Neural Networks. Trevor Gale, Erich Elsen, and Sara Hooker https://t.co/WOOjLCAxJ9"}, "1108431043563085825": {"author": "@NyatzAnger", "followers": "360", "datetime": "2019-03-20 18:12:09", "content_summary": "RT @vykthur: Well written and informative paper from @sarahookr & team on approaches for model compression (mainly sparsity). Good read fo\u2026"}, "1101339550192939009": {"author": "@JS_ElecEngineer", "followers": "151", "datetime": "2019-03-01 04:33:05", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1104174962087206914": {"author": "@CyberStoic", "followers": "19", "datetime": "2019-03-09 00:20:00", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1103224697263665152": {"author": "@indy9000", "followers": "363", "datetime": "2019-03-06 09:23:59", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1100766871219953665": {"author": "@arxiv_org", "followers": "12,804", "datetime": "2019-02-27 14:37:28", "content_summary": "The State of Sparsity in Deep Neural Networks. https://t.co/U9xMAUmRdo https://t.co/cbWPukzPHP"}, "1100624171258630144": {"author": "@iandanforth", "followers": "1,951", "datetime": "2019-02-27 05:10:25", "content_summary": "RT @BrundageBot: The State of Sparsity in Deep Neural Networks. Trevor Gale, Erich Elsen, and Sara Hooker https://t.co/WOOjLCAxJ9"}, "1107979685303062528": {"author": "@IP_ad_man", "followers": "423", "datetime": "2019-03-19 12:18:36", "content_summary": "RT @Andrea_ilsergio: The State of Sparsity in Deep Neural Networks Evaluating 3 state-of-the-art techniques for inducing sparsity in #Neura\u2026"}, "1101216050849427456": {"author": "@skornblith", "followers": "1,446", "datetime": "2019-02-28 20:22:20", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1100694084841324546": {"author": "@udmrzn", "followers": "1,347", "datetime": "2019-02-27 09:48:14", "content_summary": "RT @StatMLPapers: The State of Sparsity in Deep Neural Networks. (arXiv:1902.09574v1 [cs.LG]) https://t.co/StRSWaUBXh"}, "1100612619700887552": {"author": "@tomaxent", "followers": "4", "datetime": "2019-02-27 04:24:31", "content_summary": "RT @StatsPapers: The State of Sparsity in Deep Neural Networks. https://t.co/7KEH2ZERXF"}, "1103280087259070467": {"author": "@unsorsodicorda", "followers": "737", "datetime": "2019-03-06 13:04:05", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1108433277197733888": {"author": "@PyLadiesGhana", "followers": "803", "datetime": "2019-03-20 18:21:01", "content_summary": "RT @vykthur: Well written and informative paper from @sarahookr & team on approaches for model compression (mainly sparsity). Good read fo\u2026"}, "1103220472802627585": {"author": "@hardmaru", "followers": "83,343", "datetime": "2019-03-06 09:07:12", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1108431217928622080": {"author": "@sarahookr", "followers": "10,251", "datetime": "2019-03-20 18:12:50", "content_summary": "RT @vykthur: Well written and informative paper from @sarahookr & team on approaches for model compression (mainly sparsity). Good read fo\u2026"}, "1101096223321272320": {"author": "@sarahookr", "followers": "10,251", "datetime": "2019-02-28 12:26:11", "content_summary": "My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks? Is pruning merely architecture search, can we train sparse structure from scratch to same perf as pruned + finetuned? We find answer"}, "1100747072326647809": {"author": "@cghosh_", "followers": "281", "datetime": "2019-02-27 13:18:47", "content_summary": "RT @senya_ashuha: The State of Sparsity in Deep Neural Networks https://t.co/nnAai2ml7k is a nice comparison of sparsification methods for\u2026"}, "1101565667168403456": {"author": "@sarahookr", "followers": "10,251", "datetime": "2019-03-01 19:31:35", "content_summary": "Oops, I shared a very mysterious link to nowhere. https://t.co/tgX7x0mFLV is perhaps slightly more useful for those interested in model pruning and curious about whether \u201clottery ticket\u201d substructures exist for large scale, multi-domain tasks. :)"}, "1120236009520652291": {"author": "@sarahookr", "followers": "10,251", "datetime": "2019-04-22 08:00:52", "content_summary": "@pgeuder There are a few different ways to achieve model compression. Here are two slides from a recent research talk I gave describing certain approaches. Recently, I've been thinking about pruning https://t.co/9jRrO7BSbw :) Hope that is helpful! https://"}, "1103155512277757953": {"author": "@RndWalk", "followers": "127", "datetime": "2019-03-06 04:49:04", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1103652584521326592": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-03-07 13:44:15", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1101111346266021889": {"author": "@RichmanRonald", "followers": "916", "datetime": "2019-02-28 13:26:17", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1108428752940228608": {"author": "@vykthur", "followers": "1,776", "datetime": "2019-03-20 18:03:02", "content_summary": "Well written and informative paper from @sarahookr & team on approaches for model compression (mainly sparsity). Good read for anyone interested in the broader area of model compression/quantization. https://t.co/WqKTSJG8cQ https://t.co/FVqbCNWXgf"}, "1103073255823273985": {"author": "@thatsdone", "followers": "787", "datetime": "2019-03-05 23:22:12", "content_summary": "RT @icoxfog417: DNN\u306e\u91cd\u307f\u3092\u758e\u306b\u3059\u308b\u624b\u6cd5\u306b\u3064\u3044\u3066\u3001\u3069\u306e\u624b\u6cd5\u304c\u6709\u52b9\u304b\u3092\u5927\u898f\u6a21\u306a\u30e2\u30c7\u30eb(Transformer/ResNet)\u3067\u691c\u8a3c\u3057\u305f\u7814\u7a76(\u758e=\u91cd\u307f\u304c0\u306b\u8fd1\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u591a\u3044\u3068\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u308b)\u3002\u7d50\u679c\u3068\u3057\u3066\u306f\u3001\u5358\u7d14\u306b\u91cd\u307f\u306eMagnitude\u3067\u679d\u5208\u308a\u3059\u308b\u624b\u6cd5\u304c\u2026"}, "1101552218048811008": {"author": "@bartoldson", "followers": "12", "datetime": "2019-03-01 18:38:09", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1103261277613408256": {"author": "@TusharJain_007", "followers": "124", "datetime": "2019-03-06 11:49:20", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1100590629321617410": {"author": "@deep_rl", "followers": "868", "datetime": "2019-02-27 02:57:08", "content_summary": "The State of Sparsity in Deep Neural Networks - Trevor Gale https://t.co/IOHGg3PpD4"}, "1100780607175417861": {"author": "@SagarSharma4244", "followers": "186", "datetime": "2019-02-27 15:32:02", "content_summary": "RT @arxiv_org: The State of Sparsity in Deep Neural Networks. https://t.co/U9xMAUmRdo https://t.co/cbWPukzPHP"}, "1101302642276532224": {"author": "@udmrzn", "followers": "1,347", "datetime": "2019-03-01 02:06:25", "content_summary": "RT @arxiv_in_review: #ICML2019 The State of Sparsity in Deep Neural Networks. (arXiv:1902.09574v1 [cs\\.LG]) https://t.co/jYBbVrjdpM"}, "1103141345554620416": {"author": "@cghosh_", "followers": "281", "datetime": "2019-03-06 03:52:46", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1100776533365526528": {"author": "@eeevgen", "followers": "84", "datetime": "2019-02-27 15:15:51", "content_summary": "RT @senya_ashuha: The State of Sparsity in Deep Neural Networks https://t.co/nnAai2ml7k is a nice comparison of sparsification methods for\u2026"}, "1101329020572954624": {"author": "@KouroshMeshgi", "followers": "621", "datetime": "2019-03-01 03:51:14", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1103399570405552139": {"author": "@data_datum", "followers": "1,782", "datetime": "2019-03-06 20:58:52", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1102732133854932993": {"author": "@icoxfog417", "followers": "11,575", "datetime": "2019-03-05 00:46:43", "content_summary": "DNN\u306e\u91cd\u307f\u3092\u758e\u306b\u3059\u308b\u624b\u6cd5\u306b\u3064\u3044\u3066\u3001\u3069\u306e\u624b\u6cd5\u304c\u6709\u52b9\u304b\u3092\u5927\u898f\u6a21\u306a\u30e2\u30c7\u30eb(Transformer/ResNet)\u3067\u691c\u8a3c\u3057\u305f\u7814\u7a76(\u758e=\u91cd\u307f\u304c0\u306b\u8fd1\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u591a\u3044\u3068\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u308b)\u3002\u7d50\u679c\u3068\u3057\u3066\u306f\u3001\u5358\u7d14\u306b\u91cd\u307f\u306eMagnitude\u3067\u679d\u5208\u308a\u3059\u308b\u624b\u6cd5\u304c\u826f\u597d\u3067\u3042\u308a\u3001\u679d\u5208\u308a\u3067\u5f97\u3089\u308c\u308b\u69cb\u9020\u306f\u7d20\u306e\u72b6\u614b\u304b\u3089\u306f\u5f97\u96e3\u3044\u3068\u306e\u3053\u3068\u3002"}, "1103566172841279490": {"author": "@RadekBartyzal", "followers": "160", "datetime": "2019-03-07 08:00:53", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1100587537196670976": {"author": "@StatsPapers", "followers": "5,454", "datetime": "2019-02-27 02:44:51", "content_summary": "The State of Sparsity in Deep Neural Networks. https://t.co/7KEH2ZERXF"}, "1101317697025925120": {"author": "@JeremiahHarmsen", "followers": "1,090", "datetime": "2019-03-01 03:06:15", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1101100139182284800": {"author": "@sarahookr", "followers": "10,251", "datetime": "2019-02-28 12:41:45", "content_summary": "And yes, full story with a trove of empirical results here https://t.co/wp7siEaNeo."}, "1103446563689385984": {"author": "@AssistedEvolve", "followers": "216", "datetime": "2019-03-07 00:05:36", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1101518474931855360": {"author": "@pacocp9", "followers": "354", "datetime": "2019-03-01 16:24:04", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1100737089551978496": {"author": "@senya_ashuha", "followers": "612", "datetime": "2019-02-27 12:39:07", "content_summary": "The State of Sparsity in Deep Neural Networks https://t.co/nnAai2ml7k is a nice comparison of sparsification methods for large scale problems! A more simple non-Bayesian sparsification performs better! https://t.co/3IqxBvmrgF"}, "1100592206321274880": {"author": "@BrundageBot", "followers": "3,913", "datetime": "2019-02-27 03:03:24", "content_summary": "The State of Sparsity in Deep Neural Networks. Trevor Gale, Erich Elsen, and Sara Hooker https://t.co/WOOjLCAxJ9"}, "1102823406398001152": {"author": "@PRONOjits", "followers": "2,245", "datetime": "2019-03-05 06:49:24", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1108105205281890304": {"author": "@SirJamesDelon", "followers": "231", "datetime": "2019-03-19 20:37:23", "content_summary": "RT @Andrea_ilsergio: The State of Sparsity in Deep Neural Networks Evaluating 3 state-of-the-art techniques for inducing sparsity in #Neura\u2026"}, "1207972664221978625": {"author": "@rkchif", "followers": "105", "datetime": "2019-12-20 10:35:00", "content_summary": "\u00bb The State of Sparsity in Deep Neural Networks https://t.co/ZzvdXo1BuZ Appendix\u76ee\u5f53\u3066"}, "1103285912258015232": {"author": "@ElectronNest", "followers": "230", "datetime": "2019-03-06 13:27:14", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1103187999431557120": {"author": "@wilderlopes", "followers": "60", "datetime": "2019-03-06 06:58:09", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}, "1102762582048333826": {"author": "@jaialkdanel", "followers": "1,770", "datetime": "2019-03-05 02:47:42", "content_summary": "RT @icoxfog417: DNN\u306e\u91cd\u307f\u3092\u758e\u306b\u3059\u308b\u624b\u6cd5\u306b\u3064\u3044\u3066\u3001\u3069\u306e\u624b\u6cd5\u304c\u6709\u52b9\u304b\u3092\u5927\u898f\u6a21\u306a\u30e2\u30c7\u30eb(Transformer/ResNet)\u3067\u691c\u8a3c\u3057\u305f\u7814\u7a76(\u758e=\u91cd\u307f\u304c0\u306b\u8fd1\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u591a\u3044\u3068\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u308b)\u3002\u7d50\u679c\u3068\u3057\u3066\u306f\u3001\u5358\u7d14\u306b\u91cd\u307f\u306eMagnitude\u3067\u679d\u5208\u308a\u3059\u308b\u624b\u6cd5\u304c\u2026"}, "1101560065528659970": {"author": "@sarahookr", "followers": "10,251", "datetime": "2019-03-01 19:09:20", "content_summary": "@bartoldson Thank you Brian! + for spotting my very mysterious link to nowhere. This should would better - https://t.co/tgX7x0mFLV :)"}, "1101212365826093056": {"author": "@iraphas13", "followers": "1,449", "datetime": "2019-02-28 20:07:42", "content_summary": "RT @sarahookr: My recent work w Trevor Gale, @erich_elsen. Do pruning methods perform consistently across large-scale, multi-domain tasks?\u2026"}, "1102375040669016064": {"author": "@arxiv_pop", "followers": "715", "datetime": "2019-03-04 01:07:45", "content_summary": "2019/02/25 \u6295\u7a3f 2\u4f4d LG(Machine Learning) The State of Sparsity in Deep Neural Networks https://t.co/xVDMLCVrdx 10 Tweets 12 Retweets 75 Favorites"}, "1100607589296021506": {"author": "@yapp1e", "followers": "49", "datetime": "2019-02-27 04:04:32", "content_summary": "The State of Sparsity in Deep Neural Networks. (arXiv:1902.09574v1 [cs.LG]) https://t.co/JyhqNiJxaa We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer train"}, "1101281922737680389": {"author": "@bgoncalves", "followers": "4,096", "datetime": "2019-03-01 00:44:05", "content_summary": "The State of Sparsity in Deep Neural Networks https://t.co/SA3Vq6r1Nz"}, "1102964714324189184": {"author": "@KSKSKSKS2", "followers": "216", "datetime": "2019-03-05 16:10:54", "content_summary": "RT @icoxfog417: DNN\u306e\u91cd\u307f\u3092\u758e\u306b\u3059\u308b\u624b\u6cd5\u306b\u3064\u3044\u3066\u3001\u3069\u306e\u624b\u6cd5\u304c\u6709\u52b9\u304b\u3092\u5927\u898f\u6a21\u306a\u30e2\u30c7\u30eb(Transformer/ResNet)\u3067\u691c\u8a3c\u3057\u305f\u7814\u7a76(\u758e=\u91cd\u307f\u304c0\u306b\u8fd1\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u591a\u3044\u3068\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u308b)\u3002\u7d50\u679c\u3068\u3057\u3066\u306f\u3001\u5358\u7d14\u306b\u91cd\u307f\u306eMagnitude\u3067\u679d\u5208\u308a\u3059\u308b\u624b\u6cd5\u304c\u2026"}, "1101622799486070785": {"author": "@kchonyc", "followers": "24,295", "datetime": "2019-03-01 23:18:37", "content_summary": "RT @sarahookr: Oops, I shared a very mysterious link to nowhere. https://t.co/tgX7x0mFLV is perhaps slightly more useful for those interest\u2026"}, "1103023126428991488": {"author": "@unsorsodicorda", "followers": "737", "datetime": "2019-03-05 20:03:01", "content_summary": "RT @StephenPiment: The State of Sparsity in Deep Neural Networks https://t.co/TjhhyIrYJU"}}, "citation_id": "56100467", "queriedAt": "2020-06-04 00:28:26"}