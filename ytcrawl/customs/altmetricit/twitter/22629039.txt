{"citation_id": "22629039", "queriedAt": "2020-05-09 14:20:51", "completed": "0", "twitter": {"1069976359483539458": {"followers": "137", "content_summary": "- Disentangled VAE's (DeepMind 2016): https://t.co/NEkQ3Kr85Y - Applying disentangled VAE's to RL: DARLA (DeepMind 2017): https://t.co/D2ox9MQ5Wk - Original VAE paper (2013): https://t.co/Ik3KMn5uoI", "author": "@DatazDude", "datetime": "2018-12-04 15:26:57"}, "1060246620489445377": {"followers": "137", "content_summary": "DAE https://t.co/Rpc8KVctBH", "author": "@DatazDude", "datetime": "2018-11-07 19:04:27"}, "1232540279606890497": {"followers": "36", "content_summary": "@hardmaru @iclr_conf @honglaklee DARLA https://t.co/FfDqbKLJVF is a good example \ud83d\ude00 They first learn good representations from beta-VAE (+DAE) and then train a policy on latent spaces (similar to world model). Basically, I think combining representation le", "author": "@kimin_le2", "datetime": "2020-02-26 05:37:56"}, "1233019669004595200": {"followers": "259", "content_summary": "RT @kimin_le2: @hardmaru @iclr_conf @honglaklee DARLA https://t.co/FfDqbKLJVF is a good example \ud83d\ude00 They first learn good representations fro\u2026", "author": "@jcchinhui", "datetime": "2020-02-27 13:22:52"}}, "tab": "twitter"}