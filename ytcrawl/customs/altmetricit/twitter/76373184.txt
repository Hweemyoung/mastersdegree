{"citation_id": "76373184", "queriedAt": "2020-05-09 12:39:52", "completed": "0", "twitter": {"1254948722900979719": {"followers": "3,464", "content_summary": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages https://t.co/V76ZtoxjcG", "author": "@arxiv_cscl", "datetime": "2020-04-28 01:41:06"}, "1255145097236082688": {"followers": "3,464", "content_summary": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages https://t.co/V76ZtoOU4e", "author": "@arxiv_cscl", "datetime": "2020-04-28 14:41:25"}, "1255130034706305029": {"followers": "3,464", "content_summary": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages https://t.co/V76ZtoxjcG", "author": "@arxiv_cscl", "datetime": "2020-04-28 13:41:34"}, "1248132213306273793": {"followers": "1,297", "content_summary": "#ICML2020 CodeBERT: A Pre-Trained Model for Programming and Natural Languages. (arXiv:2002.08155v2 [cs\\.CL] UPDATED) https://t.co/c6q2dfEVhs", "author": "@arxiv_in_review", "datetime": "2020-04-09 06:14:43"}}, "tab": "twitter"}