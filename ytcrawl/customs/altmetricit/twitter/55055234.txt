{"twitter": {"1093449935108730880": {"content_summary": "Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Fa... https://t.co/j1Z6kosEbM https://t.co/sTmjZXaFQX", "datetime": "2019-02-07 10:02:34", "followers": "12,789", "author": "@arxiv_org"}, "1096298743899529217": {"content_summary": "RT @arxiv_in_review: #ICML2019 Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization. (arXiv\u2026", "datetime": "2019-02-15 06:42:43", "followers": "4,893", "author": "@IgorCarron"}, "1093341833957134336": {"content_summary": "Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization. Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman https://t.co/Rup25wFkaf", "datetime": "2019-02-07 02:53:01", "followers": "3,903", "author": "@BrundageBot"}, "1093414538647650307": {"content_summary": "\"Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization\", Eldad Melle\u2026 https://t.co/zmiRPHQowL", "datetime": "2019-02-07 07:41:55", "followers": "783", "author": "@arxivml"}, "1093330409188966400": {"content_summary": "Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization. (arXiv:1902.01917v1 [cs.LG]) https://t.co/vowbccL106 Quantization of neural networks has become common practice, driven by the need for efficient impleme", "datetime": "2019-02-07 02:07:37", "followers": "50", "author": "@yapp1e"}, "1093325676604735489": {"content_summary": "Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization. (arXiv:1902.01917v1 [cs.LG]) https://t.co/CBVktKOI0L", "datetime": "2019-02-07 01:48:48", "followers": "9,739", "author": "@StatMLPapers"}, "1125665611432984576": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-07 07:36:10", "followers": "21", "author": "@HadarZeitlin"}, "1094167321105567744": {"content_summary": "RT @arxiv_org: Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Fa... https://t.co/j1Z6kosEbM https:/\u2026", "datetime": "2019-02-09 09:33:12", "followers": "11,977", "author": "@Rosenchild"}, "1125665580168679424": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-07 07:36:02", "followers": "172", "author": "@Hailo_ai"}, "1093473028653101056": {"content_summary": "RT @Hailo_Tech: Valuable research paper by Hailo's Machine Learning team published today on arXiv on quantization of neural networks. The H\u2026", "datetime": "2019-02-07 11:34:20", "followers": "7", "author": "@shkolnik_yo"}, "1139008072372948992": {"content_summary": "RT @Hailo_ai: Kudos to our very own Eldad Meller for giving a fascinating lecture today at #ICML conference, covering our new approach to r\u2026", "datetime": "2019-06-13 03:14:20", "followers": "21", "author": "@HadarZeitlin"}, "1096133711446401029": {"content_summary": "RT @Hailo_Tech: Valuable research paper by Hailo's Machine Learning team published today on arXiv on quantization of neural networks. The H\u2026", "datetime": "2019-02-14 19:46:56", "followers": "54", "author": "@sinatv52"}, "1138860844606001153": {"content_summary": "Kudos to our very own Eldad Meller for giving a fascinating lecture today at #ICML conference, covering our new approach to reducing #DNN quantization error - \"inversely proportional factorization\". Read full article here: https://t.co/peUVoORuVN #icml2", "datetime": "2019-06-12 17:29:19", "followers": "172", "author": "@Hailo_ai"}, "1093495152214138880": {"content_summary": "RT @arxiv_org: Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Fa... https://t.co/j1Z6kosEbM https:/\u2026", "datetime": "2019-02-07 13:02:15", "followers": "307", "author": "@subhobrata1"}, "1093450416707186690": {"content_summary": "RT @arxiv_org: Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Fa... https://t.co/j1Z6kosEbM https:/\u2026", "datetime": "2019-02-07 10:04:29", "followers": "1,770", "author": "@jaialkdanel"}, "1096032408996962304": {"content_summary": "RT @Hailo_Tech: Valuable research paper by Hailo's Machine Learning team published today on arXiv on quantization of neural networks. The H\u2026", "datetime": "2019-02-14 13:04:24", "followers": "1,295", "author": "@EmbeddedVision"}, "1125707246820839425": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-07 10:21:36", "followers": "0", "author": "@galongin"}, "1096291693249351680": {"content_summary": "#ICML2019 Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization. (arXiv:1902.01917v1 [cs\\.LG]) https://t.co/gmMWbDQHQv", "datetime": "2019-02-15 06:14:42", "followers": "1,317", "author": "@arxiv_in_review"}, "1241327092970729472": {"content_summary": "@ID_AA_Carmack @ilyasut If reading about neural network quantization interests you, you can take a look at \"Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization\" https://t.co/RkRrpGwpoj submitted by @mark_gr", "datetime": "2020-03-21 11:33:36", "followers": "7", "author": "@shkolnik_yo"}, "1125790771435565058": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-07 15:53:30", "followers": "91", "author": "@bdhammel"}, "1093462649013264384": {"content_summary": "#DeepLearning | Recovering #NeuralNetworks Quantization Error Through Weight Factorization driven by the need for efficient implementations of Deep #NeuralNetworks on #Embedded Devices. \ud83d\udda5\ufe0fhttps://t.co/KgiZf8PGCo @HubBucket @HubDataScience @HubAnalytics1", "datetime": "2019-02-07 10:53:05", "followers": "5,313", "author": "@HubBucket"}, "1125485721546944512": {"content_summary": "\"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel scaling of the previous layer", "datetime": "2019-05-06 19:41:21", "followers": "19,214", "author": "@petewarden"}, "1125549855198236672": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-06 23:56:11", "followers": "219", "author": "@AssistedEvolve"}, "1093597287325028353": {"content_summary": "RT @HubBucket: #DeepLearning | Recovering #NeuralNetworks Quantization Error Through Weight Factorization driven by the need for efficient\u2026", "datetime": "2019-02-07 19:48:06", "followers": "5,313", "author": "@HubBucket"}, "1126114016865148929": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-08 13:17:58", "followers": "456", "author": "@PerthMLGroup"}, "1125626568049668097": {"content_summary": "RT @petewarden: \"Same, Same but Different\" - https://t.co/4vtJ8XzBAU - Interesting approach to tweaking quantization error with per-channel\u2026", "datetime": "2019-05-07 05:01:01", "followers": "45", "author": "@juvesss"}, "1138867022924828673": {"content_summary": "RT @Hailo_ai: Kudos to our very own Eldad Meller for giving a fascinating lecture today at #ICML conference, covering our new approach to r\u2026", "datetime": "2019-06-12 17:53:52", "followers": "0", "author": "@galongin"}, "1093387081693376512": {"content_summary": "RT @BrundageBot: Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization. Eldad Meller, Alexan\u2026", "datetime": "2019-02-07 05:52:49", "followers": "102", "author": "@treasured_write"}, "1093487361202274304": {"content_summary": "RT @Hailo_Tech: Valuable research paper by Hailo's Machine Learning team published today on arXiv on quantization of neural networks. The H\u2026", "datetime": "2019-02-07 12:31:17", "followers": "0", "author": "@galongin"}, "1093565514901794816": {"content_summary": "RT @arxiv_org: Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Fa... https://t.co/j1Z6kosEbM https:/\u2026", "datetime": "2019-02-07 17:41:50", "followers": "66", "author": "@shubh_300595"}, "1093472903666950144": {"content_summary": "RT @arxiv_org: Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Fa... https://t.co/j1Z6kosEbM https:/\u2026", "datetime": "2019-02-07 11:33:50", "followers": "67", "author": "@dksdc"}, "1093471035842854912": {"content_summary": "Valuable research paper by Hailo's Machine Learning team published today on arXiv on quantization of neural networks. The Hailo software tool chain already implements this method allowing our partners to benefit from reduced quantization error https://t.co", "datetime": "2019-02-07 11:26:25", "followers": "172", "author": "@Hailo_Tech"}, "1093339433351503872": {"content_summary": "Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization - Eldad Meller https://t.co/ZNDo1Ky0Eu", "datetime": "2019-02-07 02:43:28", "followers": "862", "author": "@deep_rl"}, "1093501687778721794": {"content_summary": "RT @HubBucket: #DeepLearning | Recovering #NeuralNetworks Quantization Error Through Weight Factorization driven by the need for efficient\u2026", "datetime": "2019-02-07 13:28:13", "followers": "11,977", "author": "@Rosenchild"}}, "queriedAt": "2020-06-03 22:39:24", "tab": "twitter", "completed": "1", "citation_id": "55055234"}