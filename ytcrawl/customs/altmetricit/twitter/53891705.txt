{"twitter": {"1226543909389529088": {"author": "@dpkingma", "datetime": "2020-02-09 16:30:30", "content_summary": "@hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing complete (see e.g. https://t.co/eE5UdzGNJj). Optimization is, of course, finding a program that explains out-of-sample (validation", "followers": "16,069"}, "1227843320455532545": {"author": "@hakanardo", "datetime": "2020-02-13 06:33:54", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "52"}, "1226671928913600513": {"author": "@JohannesNaegele", "datetime": "2020-02-10 00:59:12", "content_summary": "Crazy stuff.", "followers": "141"}, "1133391697969385472": {"author": "@perez", "datetime": "2019-05-28 15:16:52", "content_summary": "Interesting! This is, somehow, in-line with our Theoretical results presented at ICLR2019 where we show that the Transformer with a single attention head is Turing complete https://t.co/EtD21d7U9r", "followers": "6,359"}, "1226663299711811585": {"author": "@VenkyMsd", "datetime": "2020-02-10 00:24:55", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "41"}, "1226812968668561408": {"author": "@iugoaoj", "datetime": "2020-02-10 10:19:39", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "366"}, "1201705707994828800": {"author": "@perez", "datetime": "2019-12-03 03:32:22", "content_summary": "@GaryMarcus @evansd66 @ylecun A technical observation: by an extension of Siegelmann & Sontag's, seq2seq RNNs are Turing complete without needing an explicit tape. Moreover, auto-regressive self-attention nets are also Turing complete, even without usi", "followers": "6,359"}, "1209864810411761672": {"author": "@perez", "datetime": "2019-12-25 15:53:43", "content_summary": "@sir_deenicus @ChombaBupe @tdietterich @GaryMarcus One theoretical point (not claiming an advantage): Transformers can directly simulate a Turing machine operating over input symbols. The typical LSTM (RNN) TM simulation is by encoding the tape computation", "followers": "6,359"}, "1084639690173476864": {"author": "@BrundageBot", "datetime": "2019-01-14 02:33:48", "content_summary": "On the Turing Completeness of Modern Neural Network Architectures. Jorge P\u00e9rez, Javier Marinkovi\u0107, and Pablo Barcel\u00f3 https://t.co/a4zary7wwb", "followers": "3,882"}, "1226702749691269122": {"author": "@perez", "datetime": "2020-02-10 03:01:41", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "6,359"}, "1226659350212571136": {"author": "@SparkTeletype", "datetime": "2020-02-10 00:09:13", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "27"}, "1226561038654439424": {"author": "@grjenkin", "datetime": "2020-02-09 17:38:34", "content_summary": "On the Turing Completeness of Modern Neural Network Architectures https://t.co/43jmXILhHn via @rightrelevance thanks @dpkingma", "followers": "7,905"}, "1226656829981069312": {"author": "@njwfish", "datetime": "2020-02-09 23:59:13", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "283"}, "1226654718702870531": {"author": "@dave_co_dev", "datetime": "2020-02-09 23:50:49", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "63"}, "1201835651827740672": {"author": "@diegovogeid", "datetime": "2019-12-03 12:08:43", "content_summary": "RT @perez: @GaryMarcus @evansd66 @ylecun A technical observation: by an extension of Siegelmann & Sontag's, seq2seq RNNs are Turing complet\u2026", "followers": "66"}, "1226937883136790528": {"author": "@future_of_AI", "datetime": "2020-02-10 18:36:01", "content_summary": "On the Turing Completeness of Modern Neural Network Architectures https://t.co/UPtqFeNck2 #ai #machinelearning #artificialintelligence via @hardmaru", "followers": "2,285"}, "1226680305748070403": {"author": "@ArnabKantiKar", "datetime": "2020-02-10 01:32:30", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "38"}, "1084664507769806848": {"author": "@arxivml", "datetime": "2019-01-14 04:12:25", "content_summary": "\"On the Turing Completeness of Modern Neural Network Architectures\", Jorge P\u00e9rez, Javier Marinkovi\u0107, Pablo Barcel\u00f3 https://t.co/MfUYafvetr", "followers": "773"}, "1084628276616810497": {"author": "@StatMLPapers", "datetime": "2019-01-14 01:48:27", "content_summary": "On the Turing Completeness of Modern Neural Network Architectures. (arXiv:1901.03429v1 [cs.LG]) https://t.co/2vim3RJBq5", "followers": "9,689"}, "1084939698253058050": {"author": "@quantum_tunnel", "datetime": "2019-01-14 22:25:55", "content_summary": "On the #Turing Completeness of Modern Neural Network Architectures. (arXiv:1901.03429v1 [cs.LG]) https://t.co/IH6BYfV89s", "followers": "2,098"}, "1226657866204971008": {"author": "@robanhk", "datetime": "2020-02-10 00:03:20", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "1,371"}, "1226653602506166272": {"author": "@hardmaru", "datetime": "2020-02-09 23:46:23", "content_summary": "RT @dpkingma: @hardmaru @slashML What I find fascinating is that many of the artificial neural network architectures that we use are Turing\u2026", "followers": "81,490"}}, "queriedAt": "2020-05-21 19:25:49", "completed": "1", "citation_id": "53891705", "tab": "twitter"}