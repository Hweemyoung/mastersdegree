{"tab": "twitter", "completed": "1", "twitter": {"1141700301092790272": {"author": "@dcpage3", "followers": "2,420", "datetime": "2019-06-20 13:32:18", "content_summary": "1/ Early signs of trouble. We learn that deep ReLU nets with He-init, but no batch norm, basically ignore their inputs! (Check out https://t.co/F6KqavG5LG by Luther, @SebastianSeung for background.) Easy to miss if you pool across channels: https://t.co/"}, "1095877190737117184": {"author": "@BrundageBot", "followers": "3,891", "datetime": "2019-02-14 02:47:37", "content_summary": "Variance-Preserving Initialization Schemes Improve Deep Network Training: But Which Variance is Preserved?. Kyle Luther and H. Sebastian Seung https://t.co/fKSFaHx087"}, "1095878482951847936": {"author": "@deep_rl", "followers": "858", "datetime": "2019-02-14 02:52:45", "content_summary": "Variance-Preserving Initialization Schemes Improve Deep Network Training: But Which Variance is Preserved? - Kyle Luther https://t.co/IhkDnnlNT8"}, "1097665749928370176": {"author": "@arxiv_in_review", "followers": "1,312", "datetime": "2019-02-19 01:14:43", "content_summary": "#ICML2019 Variance-Preserving Initialization Schemes Improve Deep Network Training: But Which Variance is Preserved?. (arXiv:1902.04942v1 [cs\\.LG]) https://t.co/b68nMAUihu"}, "1095878074024054784": {"author": "@arxiv_cs_LG", "followers": "318", "datetime": "2019-02-14 02:51:07", "content_summary": "Variance-Preserving Initialization Schemes Improve Deep Network Training: But Which Variance is Preserved?. Kyle Luther and H. Sebastian Seung https://t.co/pmf61B25Hz"}, "1095913498201202688": {"author": "@arxivml", "followers": "780", "datetime": "2019-02-14 05:11:53", "content_summary": "\"Variance-Preserving Initialization Schemes Improve Deep Network Training: But Which Variance is Preserved?\", Kyle \u2026 https://t.co/43RFVnH7om"}, "1170949455098646528": {"author": "@dcpage3", "followers": "2,420", "datetime": "2019-09-09 06:37:59", "content_summary": "@TheGregYang @kobai19 @yoavgo @srush_nlp @philipmlong Thanks @TheGregYang. My post explains that leading evecs of the Hessian relate to changes in distn projected out by BN. The intro on const init comes from https://t.co/bjwfOwAOEv and https://t.co/F6Kqav"}, "1141763222283800576": {"author": "@ayirpelle", "followers": "2,671", "datetime": "2019-06-20 17:42:19", "content_summary": "RT @dcpage3: 1/ Early signs of trouble. We learn that deep ReLU nets with He-init, but no batch norm, basically ignore their inputs! (Chec\u2026"}, "1142690964454068225": {"author": "@mikegr55", "followers": "9", "datetime": "2019-06-23 07:08:50", "content_summary": "RT @dcpage3: 1/ Early signs of trouble. We learn that deep ReLU nets with He-init, but no batch norm, basically ignore their inputs! (Chec\u2026"}, "1171068586531655680": {"author": "@dcpage3", "followers": "2,420", "datetime": "2019-09-09 14:31:23", "content_summary": "@TheGregYang This result is from https://t.co/bjwfOwAOEv https://t.co/6NlbK4Tw8l - beautifully explained in the latter. The argument is elementary and makes it clear why BN helps."}, "1251943545994309634": {"author": "@R4_Unit", "followers": "1,159", "datetime": "2020-04-19 18:39:36", "content_summary": "@theshawwn This is far and away my favorite current explanation: basically batch norm prevents the function from easily becoming independent of the input https://t.co/cg2UGY9jjR"}}, "citation_id": "55432824", "queriedAt": "2020-06-03 23:22:40"}