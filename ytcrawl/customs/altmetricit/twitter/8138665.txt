{"citation_id": "8138665", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 16:56:40", "twitter": {"735042568216416256": {"followers": "98", "datetime": "2016-05-24 09:39:39", "author": "@treasured_write", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "830689230267437056": {"followers": "1,171", "datetime": "2017-02-12 08:05:20", "author": "@kibo35", "content_summary": "RT @CVpaperChalleng: Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/\u2026"}, "758413836093235200": {"followers": "664", "datetime": "2016-07-27 21:28:43", "author": "@crude2refined", "content_summary": "RT @_onionesque: Wide Residual Networks https://t.co/mv8r6VJkoE"}, "990734978806894595": {"followers": "8,900", "datetime": "2018-04-29 23:30:00", "author": "@Deep_In_Depth", "content_summary": "arXiv - Wide Residual Networks https://t.co/IfEqJjYvA3"}, "735180920995581952": {"followers": "413", "datetime": "2016-05-24 18:49:25", "author": "@ThomasBoquet", "content_summary": "RT @alexjc: Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/\u2026"}, "875483333580410880": {"followers": "4,090", "datetime": "2017-06-15 22:41:07", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNt3Rx"}, "735057104008929280": {"followers": "672", "datetime": "2016-05-24 10:37:24", "author": "@pwarier", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734998414853738496": {"followers": "131", "datetime": "2016-05-24 06:44:12", "author": "@HogwartsScience", "content_summary": "Harry Potter And The Wide Residual Networks https://t.co/uU0ynmdPmq #science"}, "803473257303248896": {"followers": "98", "datetime": "2016-11-29 05:38:47", "author": "@DeepLearningNow", "content_summary": "New #deeplearning paper https://t.co/bIANJ485fp https://t.co/m9CpoJAFPM"}, "735122371795898368": {"followers": "78", "datetime": "2016-05-24 14:56:46", "author": "@jordi_delatorre", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "875151265730355200": {"followers": "4,090", "datetime": "2017-06-15 00:41:35", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNKFg7"}, "960000884972314624": {"followers": "597", "datetime": "2018-02-04 04:03:41", "author": "@hyper_pigeon", "content_summary": "Wide Residual Network \u3061\u3087\u3063\u3068\u6614\u306e\u8ad6\u6587\u3060\u3051\u3069\u3001\u30c7\u30a3\u30fc\u30d7\u306a\u30e9\u30fc\u30cb\u30f3\u30b0\u306f\u305d\u3093\u306a\u306b\u30c7\u30a3\u30fc\u30d7\u3058\u3083\u306a\u304f\u3066\u3082\u3059\u3054\u304f\u3088\u304b\u3063\u305f\u3068\u3044\u3046\u304a\u8a71 https://t.co/9R9hlIVU01 #\u63a8\u3057\u8ad6\u6587"}, "734940272400826369": {"followers": "1,841", "datetime": "2016-05-24 02:53:10", "author": "@anirudhkoul", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735339464961232897": {"followers": "25,311", "datetime": "2016-05-25 05:19:25", "author": "@deeplearning4j", "content_summary": "Wide Residual Networks https://t.co/nrKs45IgoK #deeplearning #widelearning"}, "734929925329850368": {"followers": "862", "datetime": "2016-05-24 02:12:03", "author": "@deep_rl", "content_summary": "Wide Residual Networks - Sergey Zagoruyko https://t.co/l9yCgZdLHr"}, "972334010524291073": {"followers": "4,403", "datetime": "2018-03-10 04:51:07", "author": "@mrkn", "content_summary": "@hatappi Wide ResNet \u306e\u8ad6\u6587\u306f\u3053\u308c https://t.co/yg29avF6D2"}, "735081969772429312": {"followers": "340", "datetime": "2016-05-24 12:16:13", "author": "@bhaavan", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735003528217956352": {"followers": "4,891", "datetime": "2016-05-24 07:04:31", "author": "@IgorCarron", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735152762896289792": {"followers": "69", "datetime": "2016-05-24 16:57:31", "author": "@gsakr16", "content_summary": "https://t.co/j2IDUBoa09 wide res nets much better than deep res nets!! New state of the art results on cifar 10 and cifar 100!!!!"}, "837897658920398848": {"followers": "3,805", "datetime": "2017-03-04 05:29:03", "author": "@CVpaperChalleng", "content_summary": "Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/wtNsANXzEf"}, "735334686394863616": {"followers": "787", "datetime": "2016-05-25 05:00:25", "author": "@arXivStats", "content_summary": "Wide Residual Networks https://t.co/MioS7xMVu8 #arxiv"}, "803639809495289856": {"followers": "4,090", "datetime": "2016-11-29 16:40:36", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNt3Rx"}, "830679954778005504": {"followers": "3,805", "datetime": "2017-02-12 07:28:29", "author": "@CVpaperChalleng", "content_summary": "Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/wtNsANXzEf"}, "735304489935110146": {"followers": "787", "datetime": "2016-05-25 03:00:26", "author": "@arXivStats", "content_summary": "Wide Residual Networks https://t.co/MioS7xvk5y #arxiv"}, "734996947602407424": {"followers": "74", "datetime": "2016-05-24 06:38:22", "author": "@DangTLam", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735041656995483648": {"followers": "301", "datetime": "2016-05-24 09:36:02", "author": "@gangeshwark", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "879332403562651650": {"followers": "88", "datetime": "2017-06-26 13:35:56", "author": "@hugeiezzy", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "758422649059979265": {"followers": "2,927", "datetime": "2016-07-27 22:03:45", "author": "@evolvingstuff", "content_summary": "RT @_onionesque: Wide Residual Networks https://t.co/mv8r6VJkoE"}, "821635815281463297": {"followers": "98", "datetime": "2017-01-18 08:30:18", "author": "@DeepLearningNow", "content_summary": "New #deeplearning paper https://t.co/bIANJ485fp https://t.co/PizOolJC8N"}, "875165466141970432": {"followers": "784", "datetime": "2017-06-15 01:38:01", "author": "@muktabh", "content_summary": "RT @arxiv_cs_CV: Wide Residual Networks. https://t.co/guqSuVzWJM"}, "735049264783462400": {"followers": "102", "datetime": "2016-05-24 10:06:15", "author": "@serene99", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735040194584412160": {"followers": "825", "datetime": "2016-05-24 09:30:13", "author": "@fly51fly", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734951284877320194": {"followers": "73", "datetime": "2016-05-24 03:36:55", "author": "@abhaygupta_94", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735025749732777985": {"followers": "628", "datetime": "2016-05-24 08:32:49", "author": "@NieletSa", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "803565217342885889": {"followers": "131", "datetime": "2016-11-29 11:44:11", "author": "@HogwartsScience", "content_summary": "Harry Potter And The Wide Residual Networks https://t.co/cNhcT6ZVee #science"}, "752412193883168768": {"followers": "3,805", "datetime": "2016-07-11 08:00:20", "author": "@CVpaperChalleng", "content_summary": "\u201cWide Residual Networks\u201d, in arXiv pre-print 1605.07146, 2016.\uff1aWide ResNet\u306e\u63d0\u6848 https://t.co/SfVbCwacAb https://t.co/pNeE4PgqWs"}, "734924400756035584": {"followers": "8,538", "datetime": "2016-05-24 01:50:06", "author": "@rbhar90", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734939340199985153": {"followers": "282", "datetime": "2016-05-24 02:49:27", "author": "@Babble_Away", "content_summary": "Wide Residual Networks: Computer Science > Computer Vision and Pattern Recognition Abstract:... https://t.co/Mg9LoWSWb5 #machinelearning"}, "980654696338567170": {"followers": "1,379", "datetime": "2018-04-02 03:54:33", "author": "@shinohara0919", "content_summary": "RT @dosei_sanga: ?\u300cchannel\u304c\u5897\u3048\u308b\u6642\u3060\u3051\u9055\u3046\u632f\u308b\u821e\u3044\u3057\u3066\u3066\u8349\u300d https://t.co/c4cPmdMVPt ?\u300c\u3068\u3044\u3046\u304bchannel\u5897\u3084\u3057\u305f\u3060\u3051\u3067\u6539\u5584\u3057\u3066\u8349\u300d https://t.co/iy0ax3ffEa ?\u300c\u6094\u3057\u304b\u3063\u305f\u30891000\u5c64over\u2026"}, "736692002323062784": {"followers": "1,286", "datetime": "2016-05-28 22:53:55", "author": "@heghbalz", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735086112096550912": {"followers": "230", "datetime": "2016-05-24 12:32:41", "author": "@KimThatWas", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735217921035776001": {"followers": "1,109", "datetime": "2016-05-24 21:16:26", "author": "@BrendanShilling", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735802282445398016": {"followers": "59", "datetime": "2016-05-26 11:58:29", "author": "@liqiangniu", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "879638240248180736": {"followers": "340", "datetime": "2017-06-27 09:51:14", "author": "@bhaavan", "content_summary": "Go wide, not deep. https://t.co/Dvr3olEURg"}, "816214053068554241": {"followers": "4,006", "datetime": "2017-01-03 09:26:09", "author": "@ballforest", "content_summary": "RT @hardmaru: @n_hidekey @ballforest I found combining BN/LN with dropout helped RNNs a great deal. For CNNs, https://t.co/1q9XfA3JZt showe\u2026"}, "735128919435661312": {"followers": "4,947", "datetime": "2016-05-24 15:22:47", "author": "@micahstubbs", "content_summary": "RT @alexjc: Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/\u2026"}, "735351429876047872": {"followers": "231", "datetime": "2016-05-25 06:06:57", "author": "@CesareMontresor", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734973647069315072": {"followers": "49", "datetime": "2016-05-24 05:05:47", "author": "@davidkh", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "848134214352379905": {"followers": "3,805", "datetime": "2017-04-01 11:25:28", "author": "@CVpaperChalleng", "content_summary": "Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/wtNsANXzEf"}, "734991028499841024": {"followers": "2,922", "datetime": "2016-05-24 06:14:51", "author": "@modeless", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735022278648307712": {"followers": "156", "datetime": "2016-05-24 08:19:01", "author": "@PatniAseem", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "821532670295425024": {"followers": "4,090", "datetime": "2017-01-18 01:40:26", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNKFg7"}, "735106586603986949": {"followers": "962", "datetime": "2016-05-24 13:54:02", "author": "@jnhwkim", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "980459372403834881": {"followers": "2,041", "datetime": "2018-04-01 14:58:25", "author": "@dosei_sanga", "content_summary": "?\u300cchannel\u304c\u5897\u3048\u308b\u6642\u3060\u3051\u9055\u3046\u632f\u308b\u821e\u3044\u3057\u3066\u3066\u8349\u300d https://t.co/c4cPmdMVPt ?\u300c\u3068\u3044\u3046\u304bchannel\u5897\u3084\u3057\u305f\u3060\u3051\u3067\u6539\u5584\u3057\u3066\u8349\u300d https://t.co/iy0ax3ffEa ?\u300c\u6094\u3057\u304b\u3063\u305f\u30891000\u5c64over\u306b\u306a\u3063\u3066\u307f\u308d\u3088\u300d https://t.co/d5Xh5eexd5 \u300e\u30b0\u30ef\u30a2\u30a2\u30a2\u30a2\u2026\u6d88\u3048\u308b\u2026\u79c1\u304c\u3001\u6700\u3082\u7406\u77e5\u7684\u306aCNN\u304c\u6d88\u3048\u3066\u3044\u304f\u2026\u300f"}, "735032745563414528": {"followers": "7,162", "datetime": "2016-05-24 09:00:37", "author": "@Deep_Hub", "content_summary": "Wide ResNets: Improves acc. & speed (more parallel computations for GPU) https://t.co/W3ptBnfa5o #deeplearning #ML https://t.co/to5KeaiC4z"}, "879451619213152257": {"followers": "5,437", "datetime": "2017-06-26 21:29:40", "author": "@Daniel_Lerch", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "821713914526363649": {"followers": "4,090", "datetime": "2017-01-18 13:40:38", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNKFg7"}, "875347365577859074": {"followers": "4,090", "datetime": "2017-06-15 13:40:49", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNKFg7"}, "735108910760218624": {"followers": "54", "datetime": "2016-05-24 14:03:16", "author": "@mattkrzus", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "842653663948554240": {"followers": "3,805", "datetime": "2017-03-17 08:27:43", "author": "@CVpaperChalleng", "content_summary": "Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/wtNsANXzEf"}, "735510192402616320": {"followers": "52", "datetime": "2016-05-25 16:37:49", "author": "@hakanardo", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "803413361190961152": {"followers": "4,090", "datetime": "2016-11-29 01:40:46", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNKFg7"}, "735052417394442240": {"followers": "17", "datetime": "2016-05-24 10:18:47", "author": "@robhritz", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "879324621190959104": {"followers": "1,417", "datetime": "2017-06-26 13:05:01", "author": "@seanmylaw", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "735180907909287939": {"followers": "770", "datetime": "2016-05-24 18:49:22", "author": "@aCraigPfeifer", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735068745777975296": {"followers": "7,210", "datetime": "2016-05-24 11:23:40", "author": "@fastml_extra", "content_summary": "Wide Residual Networks: https://t.co/POg7kHemiG Torch code: https://t.co/kziAoaj0WJ Reddit: https://t.co/TlcxiKYGSa"}, "833684726183387137": {"followers": "3,805", "datetime": "2017-02-20 14:28:22", "author": "@CVpaperChalleng", "content_summary": "Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/wtNsANXzEf"}, "735469149669986304": {"followers": "27", "datetime": "2016-05-25 13:54:44", "author": "@mllrkln", "content_summary": "My implementation of Wide Residual Networks from (https://t.co/T1lM4mz8a0) in Lasagne. https://t.co/Pn7j3KcHbb #deeplearning #fatlearning"}, "735169029883600898": {"followers": "119", "datetime": "2016-05-24 18:02:10", "author": "@toto_toilet", "content_summary": "RT @arxiv_cs_CV: Wide Residual Networks. https://t.co/ZJsvCvIMmd"}, "758397523769778176": {"followers": "3,410", "datetime": "2016-07-27 20:23:54", "author": "@_onionesque", "content_summary": "Wide Residual Networks https://t.co/mv8r6VJkoE"}, "735925293785976832": {"followers": "2,551", "datetime": "2016-05-26 20:07:17", "author": "@brianpiercy", "content_summary": "Wide Residual Networks: equal to / better than very long Residual Networks | ArXiV https://t.co/HjiaDULq6P #DeepLearning #MachineLearing"}, "1169908829292597248": {"followers": "63", "datetime": "2019-09-06 09:42:55", "author": "@Ronmemo1", "content_summary": "ABST100\u30e1\u30e2\uff1a7 WideResNet\uff082016\uff09\u2192https://t.co/W3fymjXTfK\u3000\uff0cwide residual networks\uff08WRN\uff09\u306f\u6d45\u304f\u5e83\u3044ResNet"}, "734951272407699459": {"followers": "2,112", "datetime": "2016-05-24 03:36:52", "author": "@mitmul", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "1060655340251951110": {"followers": "51", "datetime": "2018-11-08 22:08:33", "author": "@antbrl", "content_summary": "Wide Resnet: https://t.co/wQAtSqq2nz Dropout (between convolutions vs on shortcut link in ResNet), no bottleneck block, wider means more channels (factor k up to 10), shallower (less convolutions) #OneMLPaperADay #MachineLearning"}, "735305298240380928": {"followers": "2,451", "datetime": "2016-05-25 03:03:39", "author": "@dan_s_becker", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735170808591601665": {"followers": "1,221", "datetime": "2016-05-24 18:09:14", "author": "@VinFL", "content_summary": "RT @alexjc: Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/\u2026"}, "816213748008423424": {"followers": "81,374", "datetime": "2017-01-03 09:24:56", "author": "@hardmaru", "content_summary": "@n_hidekey @ballforest I found combining BN/LN with dropout helped RNNs a great deal. For CNNs, https://t.co/1q9XfA3JZt showed it can help."}, "735106420685733889": {"followers": "962", "datetime": "2016-05-24 13:53:22", "author": "@jnhwkim", "content_summary": "RT @Deep_Hub: Wide ResNets: Improves acc. & speed (more parallel computations for GPU) https://t.co/W3ptBnfa5o #deeplearning #ML https://t.\u2026"}, "818958218793652225": {"followers": "1,058", "datetime": "2017-01-10 23:10:29", "author": "@BigsnarfDude", "content_summary": "[1605.07146] Wide Residual Networks https://t.co/yfUM40xPCA"}, "830684380150378496": {"followers": "3,007", "datetime": "2017-02-12 07:46:04", "author": "@syoyo", "content_summary": "RT @CVpaperChalleng: Wide Residual Networks (BMVC2016)\uff1aResNet\u306b\u3064\u3044\u3066\u3001\u7e26\u65b9\u5411\u3067\u306f\u306a\u304f\u3001\u30c1\u30e3\u30cd\u30eb\u3092\u5897\u3084\u3057\u3066Wide\u306b\u3057\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u308b\u3053\u3068\u304c\u5224\u660e\u3002\u4e26\u5217\u5316\u306e\u9ad8\u52b9\u7387\u5316\u306b\u3088\u308a\u9ad8\u901f\u306a\u51e6\u7406\u3082\u5b9f\u73fe\u3002 https://t.co/\u2026"}, "735102144873934849": {"followers": "147", "datetime": "2016-05-24 13:36:23", "author": "@EnriqueSMarquez", "content_summary": "Wider Residual Networks https://t.co/NWywjrDRot VS Residual networks are exponentially large ensembles https://t.co/pX5YRtn7no"}, "735012253355900928": {"followers": "4,751", "datetime": "2016-05-24 07:39:11", "author": "@Slychief", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "879462367540764672": {"followers": "825", "datetime": "2017-06-26 22:12:22", "author": "@fly51fly", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "735529859187757056": {"followers": "1,820", "datetime": "2016-05-25 17:55:58", "author": "@SheerIuck", "content_summary": "Enggak perlu deeper, wider juga bisa...... \"Wide Residual Networks\" https://t.co/blAWrHMjmu [pdf]"}, "875211525341892608": {"followers": "4,090", "datetime": "2017-06-15 04:41:02", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNt3Rx"}, "734958337754693635": {"followers": "650", "datetime": "2016-05-24 04:04:57", "author": "@timwee", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735166901395939328": {"followers": "575", "datetime": "2016-05-24 17:53:42", "author": "@hniemeye", "content_summary": "RT @alexjc: Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/\u2026"}, "734924166974083072": {"followers": "78,662", "datetime": "2016-05-24 01:49:10", "author": "@soumithchintala", "content_summary": "Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... https://t.co/1wXwlOt6zN"}, "804414233232424964": {"followers": "1,160", "datetime": "2016-12-01 19:57:53", "author": "@_ambodi", "content_summary": "Wide Residual Networks (2016) are showing great results now. and I think they look promising: https://t.co/M2P7O423Wp"}, "735132019076665344": {"followers": "2,927", "datetime": "2016-05-24 15:35:06", "author": "@evolvingstuff", "content_summary": "RT @alexjc: Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/\u2026"}, "734936439557750785": {"followers": "7,580", "datetime": "2016-05-24 02:37:56", "author": "@poolio", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734978878603567105": {"followers": "155", "datetime": "2016-05-24 05:26:34", "author": "@keylinker", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "879306319131545601": {"followers": "7,210", "datetime": "2017-06-26 11:52:17", "author": "@fastml_extra", "content_summary": "Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "735050940269617153": {"followers": "198", "datetime": "2016-05-24 10:12:55", "author": "@dahlemd", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734962686698491904": {"followers": "4,744", "datetime": "2016-05-24 04:22:14", "author": "@ArnoCandel", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "806217431656136704": {"followers": "4,683", "datetime": "2016-12-06 19:23:09", "author": "@bradneuberg", "content_summary": "RT @_ambodi: Wide Residual Networks (2016) are showing great results now. and I think they look promising: https://t.co/M2P7O423Wp"}, "735000323266379776": {"followers": "37,494", "datetime": "2016-05-24 06:51:47", "author": "@sedielem", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735126844782071809": {"followers": "235", "datetime": "2016-05-24 15:14:32", "author": "@samanthafmccabe", "content_summary": "RT @alexjc: Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/\u2026"}, "922669359666905088": {"followers": "10,129", "datetime": "2017-10-24 03:41:32", "author": "@sylvan5", "content_summary": "RT @hardmaru: @sylvan5 Hi, I think one can get > 92% test accuracy with 20 resnet layers w/ Relu. 85% is too low for meaningful comparison.\u2026"}, "735125403761844224": {"followers": "25,260", "datetime": "2016-05-24 15:08:48", "author": "@alexjc", "content_summary": "Preprints from NIPS show bias towards wider NNs rather than deep: https://t.co/8pnlI7xNrb https://t.co/8IkT5emUru https://t.co/jmYr4G8Qrz"}, "735142566300155904": {"followers": "1,670", "datetime": "2016-05-24 16:17:00", "author": "@StuartReid1929", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "752420180664262660": {"followers": "3,633", "datetime": "2016-07-11 08:32:05", "author": "@HirokatuKataoka", "content_summary": "RT @CVpaperChalleng: \u201cWide Residual Networks\u201d, in arXiv pre-print 1605.07146, 2016.\uff1aWide ResNet\u306e\u63d0\u6848 https://t.co/SfVbCwacAb https://t.co/pNe\u2026"}, "734951143122472960": {"followers": "173", "datetime": "2016-05-24 03:36:21", "author": "@masvsn", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735079088709357569": {"followers": "226", "datetime": "2016-05-24 12:04:46", "author": "@chuchaba", "content_summary": "Wide and (relatively) shallow residual networks outperform the standard: https://t.co/JvLy3Dxnz1"}, "735170383335198720": {"followers": "74", "datetime": "2016-05-24 18:07:32", "author": "@DangTLam", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/POg7kHemiG Torch code: https://t.co/kziAoaj0WJ Reddit: https://t.co/TlcxiKYGSa"}, "735188959618334721": {"followers": "28", "datetime": "2016-05-24 19:21:21", "author": "@jinman_zhao", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735070326401466368": {"followers": "215", "datetime": "2016-05-24 11:29:57", "author": "@_cmry", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/POg7kHemiG Torch code: https://t.co/kziAoaj0WJ Reddit: https://t.co/TlcxiKYGSa"}, "735029420344369152": {"followers": "314", "datetime": "2016-05-24 08:47:24", "author": "@LuCrFr", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735077742178373632": {"followers": "1,023", "datetime": "2016-05-24 11:59:25", "author": "@desertnaut", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/POg7kHemiG Torch code: https://t.co/kziAoaj0WJ Reddit: https://t.co/TlcxiKYGSa"}, "921629524722200576": {"followers": "81,374", "datetime": "2017-10-21 06:49:36", "author": "@hardmaru", "content_summary": "@sylvan5 Hi, I think one can get > 92% test accuracy with 20 resnet layers w/ Relu. 85% is too low for meaningful comparison. https://t.co/1q9XfA3JZt"}, "734995498927587328": {"followers": "145", "datetime": "2016-05-24 06:32:37", "author": "@esvhd", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "1036601489584574464": {"followers": "1,186", "datetime": "2018-09-03 13:07:08", "author": "@colspan", "content_summary": "\u201c[1605.07146] Wide Residual Networks\u201d https://t.co/krXoCfsKew"}, "821790380387540992": {"followers": "131", "datetime": "2017-01-18 18:44:29", "author": "@HogwartsScience", "content_summary": "Harry Potter And The Wide Residual Networks https://t.co/n6D4ZX1MyO #harrypotter"}, "879501277100613632": {"followers": "29", "datetime": "2017-06-27 00:46:59", "author": "@chenlailin", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "879335955404017664": {"followers": "784", "datetime": "2017-06-26 13:50:03", "author": "@muktabh", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "734928099834466304": {"followers": "683", "datetime": "2016-05-24 02:04:47", "author": "@analyticsaurabh", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "1019172023975665664": {"followers": "18", "datetime": "2018-07-17 10:48:40", "author": "@ShinobuKinjo1", "content_summary": "Wide Residual Networks https://t.co/gwYNpDg07K"}, "879476751595970561": {"followers": "1,348", "datetime": "2017-06-26 23:09:32", "author": "@udmrzn", "content_summary": "RT @fastml_extra: Wide Residual Networks: https://t.co/xZVu0zAlD4 Code: https://t.co/kziAoaj0WJ"}, "735216071939129344": {"followers": "12", "datetime": "2016-05-24 21:09:05", "author": "@felipefariax", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "735346053076144128": {"followers": "1,939", "datetime": "2016-05-25 05:45:35", "author": "@EldarSilver", "content_summary": "Wide Residual Networks https://t.co/N7iqzZU0a4 #DeepLearning"}, "821597500654161923": {"followers": "1,086", "datetime": "2017-01-18 05:58:03", "author": "@ai_papers", "content_summary": "Wide Residual Networks. (arXiv:1605.07146v3 [cs.CV] UPDATED) https://t.co/hiSGVr5fVG"}, "803609674356551680": {"followers": "4,090", "datetime": "2016-11-29 14:40:51", "author": "@arxiv_cscv", "content_summary": "Wide Residual Networks https://t.co/pbT3vNKFg7"}, "735025456873930752": {"followers": "7,590", "datetime": "2016-05-24 08:31:39", "author": "@graphific", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}, "734972927003615232": {"followers": "4,327", "datetime": "2016-05-24 05:02:55", "author": "@jekbradbury", "content_summary": "RT @soumithchintala: Wide Residual Networks: deeper != better. Squeezed out ~80% on CIFAR-100 with 1.8x to 8x faster. Imagenet pending... h\u2026"}}}