{"citation_id": "62425909", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 15:23:22", "twitter": {"1141723658722824192": {"followers": "132", "datetime": "2019-06-20 15:05:07", "author": "@_Wzard", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1143894540480331776": {"followers": "164", "datetime": "2019-06-26 14:51:25", "author": "@ZachBessinger", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141713767752843264": {"followers": "367", "datetime": "2019-06-20 14:25:48", "author": "@hardik1973", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141649985378889728": {"followers": "7", "datetime": "2019-06-20 10:12:22", "author": "@moraitis55", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143647274754551817": {"followers": "10,645", "datetime": "2019-06-25 22:28:52", "author": "@mchorowitz", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1252316730153529344": {"followers": "205", "datetime": "2020-04-20 19:22:30", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141652912797560832": {"followers": "2,568", "datetime": "2019-06-20 10:23:59", "author": "@mathz_aragao", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143454387274047489": {"followers": "62", "datetime": "2019-06-25 09:42:24", "author": "@samhardyhey", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141579874311630848": {"followers": "158", "datetime": "2019-06-20 05:33:46", "author": "@ArpitJ_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141660686197465088": {"followers": "48", "datetime": "2019-06-20 10:54:53", "author": "@chbalajitilak", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141789598722510848": {"followers": "625", "datetime": "2019-06-20 19:27:08", "author": "@nova77t", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141721274562990082": {"followers": "45", "datetime": "2019-06-20 14:55:38", "author": "@andromeda_aaa1", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143386698790785024": {"followers": "93", "datetime": "2019-06-25 05:13:26", "author": "@ammar_awan", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141859190450741252": {"followers": "218", "datetime": "2019-06-21 00:03:40", "author": "@AssistedEvolve", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141707269605855233": {"followers": "29,645", "datetime": "2019-06-20 13:59:59", "author": "@zacharylipton", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142597459077554179": {"followers": "4", "datetime": "2019-06-23 00:57:17", "author": "@alpha11331", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141641270445535232": {"followers": "150", "datetime": "2019-06-20 09:37:44", "author": "@anil_iitkgp", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141730429596315649": {"followers": "1,356", "datetime": "2019-06-20 15:32:01", "author": "@stjaco", "content_summary": "strong demand for multilingual/chinese pretrained models https://t.co/dLhi8S9pBY"}, "1143370624020074496": {"followers": "556", "datetime": "2019-06-25 04:09:34", "author": "@nathankjer", "content_summary": "We're starting to see state-of-the-art models cost hundreds of thousands of dollars to train. This is one way AI is more centralized than it was in the early 2010s, even though it has become more open source"}, "1143847837899169794": {"followers": "1,244", "datetime": "2019-06-26 11:45:50", "author": "@import_godiva", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141628996850860032": {"followers": "78", "datetime": "2019-06-20 08:48:57", "author": "@en_zxteloiv", "content_summary": "RT @arxiv_cscl: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141529406759825408": {"followers": "101", "datetime": "2019-06-20 02:13:13", "author": "@dongmds", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141547463221755904": {"followers": "12,765", "datetime": "2019-06-20 03:24:58", "author": "@jaguring1", "content_summary": "pre-trained\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u3066\u308b\u3002 https://t.co/56dLplYivW \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Q5YEuTNKed"}, "1141773307286556672": {"followers": "33", "datetime": "2019-06-20 18:22:24", "author": "@rahnjonathan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143782297759444993": {"followers": "377", "datetime": "2019-06-26 07:25:24", "author": "@alkari", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144982424549412864": {"followers": "721", "datetime": "2019-06-29 14:54:17", "author": "@david_nort", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141724676043018242": {"followers": "284", "datetime": "2019-06-20 15:09:09", "author": "@devesh_batra", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141695799052496897": {"followers": "367", "datetime": "2019-06-20 13:14:24", "author": "@25__toma", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1145494677774163968": {"followers": "78", "datetime": "2019-07-01 00:49:48", "author": "@watchdog20xx", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1143685315233812480": {"followers": "8,435", "datetime": "2019-06-26 01:00:02", "author": "@ngkabra", "content_summary": "For all those who think machine learning is easy/cheap, please note the costs of training a deep learning network"}, "1143718093883002881": {"followers": "795", "datetime": "2019-06-26 03:10:17", "author": "@yuji9511_compro", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143528630024200192": {"followers": "3,041", "datetime": "2019-06-25 14:37:25", "author": "@vrandezo", "content_summary": "RT @neal_lathia: I wonder whether these huge training costs are offset by these models being open sourced (so that they can be fine-tuned a\u2026"}, "1141584782356185095": {"followers": "3", "datetime": "2019-06-20 05:53:16", "author": "@zyqy1", "content_summary": "RT @pranavrajpurkar: Wow! https://t.co/cbK35XdIjV"}, "1141737045653479424": {"followers": "2", "datetime": "2019-06-20 15:58:18", "author": "@Oireniar", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143909917910360064": {"followers": "741", "datetime": "2019-06-26 15:52:31", "author": "@beckerfuffle", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141706122983661575": {"followers": "73", "datetime": "2019-06-20 13:55:26", "author": "@uc7xe5t", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143673168315465728": {"followers": "457", "datetime": "2019-06-26 00:11:46", "author": "@FedPernici", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141850007924068352": {"followers": "556", "datetime": "2019-06-20 23:27:11", "author": "@phu_pmh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1257166610168786945": {"followers": "82", "datetime": "2020-05-04 04:34:11", "author": "@nottakumasato", "content_summary": "@balajis Care to elaborate on open source NLP tools? If not, I am going to assume you saw some articles written by models but don't understand the limitations/inputs to such a model. Read some papers before you use NLP to hype something. Start with XLNet:"}, "1141699880487796737": {"followers": "2,067", "datetime": "2019-06-20 13:30:37", "author": "@yo_ehara", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141707810503135232": {"followers": "551", "datetime": "2019-06-20 14:02:08", "author": "@AND__SO", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143428615289352192": {"followers": "66", "datetime": "2019-06-25 08:00:00", "author": "@battle8500", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141842787727552514": {"followers": "1,022", "datetime": "2019-06-20 22:58:29", "author": "@kzykmyzw", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1143455693464190977": {"followers": "3", "datetime": "2019-06-25 09:47:36", "author": "@AlexKang1717", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141744586819297281": {"followers": "1,390", "datetime": "2019-06-20 16:28:16", "author": "@miguelmalvarez", "content_summary": "RT @JeffD: Great to see continued progress on pretraining methods that surpass BERT. But will it catch on without a sesame street referenc\u2026"}, "1142476458276745216": {"followers": "9,181", "datetime": "2019-06-22 16:56:28", "author": "@edchi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144017545218940931": {"followers": "59", "datetime": "2019-06-26 23:00:12", "author": "@atabakd", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141513110492086272": {"followers": "3,284", "datetime": "2019-06-20 01:08:28", "author": "@A_K_Nain", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141701706805153792": {"followers": "842", "datetime": "2019-06-20 13:37:53", "author": "@kazanagisora", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141840527551356928": {"followers": "33", "datetime": "2019-06-20 22:49:30", "author": "@William33712308", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141712830229557250": {"followers": "158", "datetime": "2019-06-20 14:22:05", "author": "@DrRaviPatel", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141581501772861440": {"followers": "12,765", "datetime": "2019-06-20 05:40:14", "author": "@jaguring1", "content_summary": "RT @stomohide: \u8ad6\u6587\u51fa\u3066\u307e\u3057\u305f\u3002 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Dl10X4T442"}, "1141624026701488128": {"followers": "150", "datetime": "2019-06-20 08:29:12", "author": "@rodgzilla", "content_summary": "\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" The newest member of the pretrained Transformer network family! The pretraining objective is different and the architecture uses ideas from Transformer XL. New GLUE SotA ! https:/"}, "1141711410486923264": {"followers": "784", "datetime": "2019-06-20 14:16:26", "author": "@SubkrishnaRao", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141734126774562817": {"followers": "365", "datetime": "2019-06-20 15:46:42", "author": "@JeanMarcJAzzi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141830983035039744": {"followers": "86", "datetime": "2019-06-20 22:11:35", "author": "@tiagomluis", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141885673525264384": {"followers": "592", "datetime": "2019-06-21 01:48:54", "author": "@shohomiura", "content_summary": "BERT\u3092\u8d85\u3048\u305f\u30e2\u30c7\u30ebcoming out!"}, "1167376815280836608": {"followers": "222", "datetime": "2019-08-30 10:01:36", "author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 11,428 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1141882268161458176": {"followers": "1,350", "datetime": "2019-06-21 01:35:22", "author": "@kikuchy", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141745873468006400": {"followers": "191", "datetime": "2019-06-20 16:33:23", "author": "@Jun_Morphin", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141728586501505024": {"followers": "2,139", "datetime": "2019-06-20 15:24:41", "author": "@mukuwaffoo", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141600863183675392": {"followers": "100", "datetime": "2019-06-20 06:57:10", "author": "@KarimiRabeeh", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141607101925662720": {"followers": "363", "datetime": "2019-06-20 07:21:57", "author": "@PfeiffJo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141735796870959104": {"followers": "1,993", "datetime": "2019-06-20 15:53:21", "author": "@dvdgrs", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1144443663549792256": {"followers": "234", "datetime": "2019-06-28 03:13:26", "author": "@quantumbtc", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143501704253575169": {"followers": "3,157", "datetime": "2019-06-25 12:50:26", "author": "@TDMoss", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144983424001703936": {"followers": "548", "datetime": "2019-06-29 14:58:15", "author": "@keno_ss", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141694967611400193": {"followers": "1,504", "datetime": "2019-06-20 13:11:06", "author": "@tawatawara", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1144149720358502400": {"followers": "643", "datetime": "2019-06-27 07:45:25", "author": "@tos_kamiya", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141658196655955968": {"followers": "264", "datetime": "2019-06-20 10:44:59", "author": "@hellorahulk", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143469210103762944": {"followers": "1,330", "datetime": "2019-06-25 10:41:18", "author": "@bipr", "content_summary": "\ud83e\udd2e"}, "1141775603374280704": {"followers": "0", "datetime": "2019-06-20 18:31:31", "author": "@Sam09lol", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1176798758614577152": {"followers": "222", "datetime": "2019-09-25 10:01:02", "author": "@PapersTrending", "content_summary": "[3/10] \ud83d\udcc8 - fast-bert - 534 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1145827801775239169": {"followers": "824", "datetime": "2019-07-01 22:53:31", "author": "@morioka", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1161454438093066240": {"followers": "60", "datetime": "2019-08-14 01:48:11", "author": "@devmessias", "content_summary": "@rationalexpec Muita GPU e energia el\u00e9trica. Alguns papers custaram na faixa de 40~200 K USD para realizar o treinamento https://t.co/FFHUHeCYFA ."}, "1142006513277558784": {"followers": "138", "datetime": "2019-06-21 09:49:04", "author": "@gogothorr", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141661939606536193": {"followers": "282", "datetime": "2019-06-20 10:59:52", "author": "@vksbhandary", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143870468904116224": {"followers": "140", "datetime": "2019-06-26 13:15:46", "author": "@DextWard", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141941286682759168": {"followers": "360", "datetime": "2019-06-21 05:29:53", "author": "@yamuuuuuun", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141812749665808386": {"followers": "61", "datetime": "2019-06-20 20:59:08", "author": "@johngonzalezv", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143075213598314496": {"followers": "82", "datetime": "2019-06-24 08:35:42", "author": "@miorgash", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1141734594900877317": {"followers": "2,415", "datetime": "2019-06-20 15:48:34", "author": "@JeffD", "content_summary": "Great to see continued progress on pretraining methods that surpass BERT. But will it catch on without a sesame street reference?"}, "1142206590197481473": {"followers": "14", "datetime": "2019-06-21 23:04:06", "author": "@pbcquoc", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141523261072609280": {"followers": "1", "datetime": "2019-06-20 01:48:48", "author": "@hieuqng26", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141752240971141120": {"followers": "840", "datetime": "2019-06-20 16:58:41", "author": "@maliannejadi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141915806449586176": {"followers": "501", "datetime": "2019-06-21 03:48:38", "author": "@fuzzysphere", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141783443984015360": {"followers": "221", "datetime": "2019-06-20 19:02:41", "author": "@fdasilva59fr", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141911797814845440": {"followers": "257", "datetime": "2019-06-21 03:32:42", "author": "@bulbulpaul", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142781014734864385": {"followers": "110", "datetime": "2019-06-23 13:06:40", "author": "@macdavid313", "content_summary": "RT @arxiv_cscl: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141761463930912768": {"followers": "25", "datetime": "2019-06-20 17:35:20", "author": "@lcl4lnl", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1143642240532779011": {"followers": "594", "datetime": "2019-06-25 22:08:52", "author": "@NeemaShahbazi", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1142084293981687808": {"followers": "71", "datetime": "2019-06-21 14:58:09", "author": "@semiinvariant", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1210226406791081984": {"followers": "45", "datetime": "2019-12-26 15:50:34", "author": "@andromeda_aaa1", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141898458497871872": {"followers": "49", "datetime": "2019-06-21 02:39:42", "author": "@tw_exception", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141563672957018112": {"followers": "27", "datetime": "2019-06-20 04:29:23", "author": "@thoounn", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141704044940972037": {"followers": "43", "datetime": "2019-06-20 13:47:10", "author": "@AbhishekNadgeri", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1143454015470133248": {"followers": "2,537", "datetime": "2019-06-25 09:40:56", "author": "@zarawesome", "content_summary": "it costs two hundred thousand dollars to teach this artificial intelligence for twelve seconds"}, "1141617219924660224": {"followers": "1,880", "datetime": "2019-06-20 08:02:10", "author": "@hirosehideo", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1141887492838178816": {"followers": "494", "datetime": "2019-06-21 01:56:08", "author": "@ponyo_ponyo115", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141579951176638464": {"followers": "8,374", "datetime": "2019-06-20 05:34:04", "author": "@mldcmu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141837292740521984": {"followers": "424", "datetime": "2019-06-20 22:36:39", "author": "@debanjanbhucs", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1143412294912790528": {"followers": "360", "datetime": "2019-06-25 06:55:09", "author": "@NyatzAnger", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141544270282706945": {"followers": "71", "datetime": "2019-06-20 03:12:17", "author": "@semiinvariant", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144978692461850630": {"followers": "725", "datetime": "2019-06-29 14:39:27", "author": "@kazmuzik", "content_summary": "Transformer-XL https://t.co/KQndN7mVDc \u306b\u3088\u3063\u3066\u3001BERT\u3092\u8d85\u3048\u305f\u3068\u3044\u3046\u5642\u306eXLNet https://t.co/zRXG66hZBd \u306ePython\u306e\u5b9f\u88c5 https://t.co/McKw6O1QJ3"}, "1199268868960571394": {"followers": "561", "datetime": "2019-11-26 10:09:14", "author": "@fanks_vision", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142479534056333314": {"followers": "25", "datetime": "2019-06-22 17:08:41", "author": "@everyday_kuma", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1143520367102660609": {"followers": "5", "datetime": "2019-06-25 14:04:35", "author": "@GeminiWWWG", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141918025827799041": {"followers": "160", "datetime": "2019-06-21 03:57:27", "author": "@Foivos_Diak", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1210214418161627139": {"followers": "288", "datetime": "2019-12-26 15:02:56", "author": "@rotto55791838", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141731498262847489": {"followers": "398", "datetime": "2019-06-20 15:36:16", "author": "@armancohan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143035230170673152": {"followers": "233", "datetime": "2019-06-24 05:56:50", "author": "@ayatenkangem", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143448590364229632": {"followers": "237", "datetime": "2019-06-25 09:19:22", "author": "@veydpz_public", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141850573924253697": {"followers": "1,251", "datetime": "2019-06-20 23:29:26", "author": "@nikhilbd", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142707486576775168": {"followers": "25", "datetime": "2019-06-23 08:14:29", "author": "@NegruEduard", "content_summary": "Great news for #NLP enthusiasts. XLnet[1] is said to outperform Bert on certain nlp tasks according to this article [2]. #machinelearning [1] https://t.co/scmaIn6glT [2] https://t.co/ePzrIYIX8r"}, "1156313920497283072": {"followers": "205", "datetime": "2019-07-30 21:21:36", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141730496130498560": {"followers": "12,511", "datetime": "2019-06-20 15:32:17", "author": "@suzatweet", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141718962138300416": {"followers": "1,152", "datetime": "2019-06-20 14:46:27", "author": "@jd_mashiro", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141742146451623936": {"followers": "249", "datetime": "2019-06-20 16:18:34", "author": "@5hirish", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143581850994188288": {"followers": "262", "datetime": "2019-06-25 18:08:54", "author": "@pete_stmarie", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141514516209029120": {"followers": "506", "datetime": "2019-06-20 01:14:03", "author": "@sksq96", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141778219802202112": {"followers": "322", "datetime": "2019-06-20 18:41:55", "author": "@letranger14", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141784189945176065": {"followers": "236", "datetime": "2019-06-20 19:05:38", "author": "@flowing", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141697975292518401": {"followers": "771", "datetime": "2019-06-20 13:23:03", "author": "@aCraigPfeifer", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141589580304539648": {"followers": "114", "datetime": "2019-06-20 06:12:20", "author": "@JoanGibert4", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141652703719960577": {"followers": "167", "datetime": "2019-06-20 10:23:10", "author": "@Shaojie_Jiang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1210432522594717696": {"followers": "10,310", "datetime": "2019-12-27 05:29:36", "author": "@malhamid", "content_summary": "@muath2 @mznmel \u0646\u0645\u0627\u0630\u062c BERT \u0631\u0627\u0626\u0639\u0629 \u062c\u062f\u0627\u064b \u062f< \u0645\u0639\u0627\u0630.\u0647\u0646\u0627\u0643 \u0646\u0645\u0648\u0630\u062c XLNet \u0634\u062f\u0646\u064a \u0643\u062b\u064a\u0631\u0627\u064b \u062a\u0641\u0648\u0642 \u0639\u0644\u0649 \u0646\u0645\u0648\u0630\u062c (BERT) \u0628\u0647\u0627\u0645\u0634 \u0643\u0628\u064a\u0631. \u0648\u064a\u0645\u062b\u0644 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0637\u0631\u064a\u0642\u0629 \u062a\u0639\u0644\u0645 \u0627\u0644\u062a\u0645\u062b\u064a\u0644 \u0627\u0644\u0644\u063a\u0648\u064a \u0628\u0637\u0631\u064a\u0642\u0629 \u0645\u0634\u0627\u0628\u0647\u0629. \u0647\u0630\u0647 \u0648\u0631\u0642\u0629 \u0627\u0644\u0628\u062d\u062b \u0627\u0644\u062a\u064a \u0646\u0634\u0631 \u0641\u064a\u0647\u0627 https://t.co/ggzdujr5EL"}, "1143595584844173317": {"followers": "134", "datetime": "2019-06-25 19:03:29", "author": "@Boazrciasn", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141764590998568961": {"followers": "218", "datetime": "2019-06-20 17:47:46", "author": "@david_macedo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141745408957456384": {"followers": "66", "datetime": "2019-06-20 16:31:32", "author": "@battle8500", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141722380575920130": {"followers": "163,852", "datetime": "2019-06-20 15:00:02", "author": "@ceobillionaire", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1145840006814040064": {"followers": "8", "datetime": "2019-07-01 23:42:00", "author": "@patrik_cajka", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1146139621698367488": {"followers": "26", "datetime": "2019-07-02 19:32:34", "author": "@weifang_tw", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143577252992565248": {"followers": "413", "datetime": "2019-06-25 17:50:38", "author": "@intelligenz_b", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141520241614315521": {"followers": "898", "datetime": "2019-06-20 01:36:48", "author": "@bayethiernodiop", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143702092042788865": {"followers": "658", "datetime": "2019-06-26 02:06:42", "author": "@HiNA_ILLIYA", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1144270409195388929": {"followers": "526", "datetime": "2019-06-27 15:44:59", "author": "@maralmkh", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143785405197758464": {"followers": "1,645", "datetime": "2019-06-26 07:37:45", "author": "@abellogin", "content_summary": "RT @neal_lathia: I wonder whether these huge training costs are offset by these models being open sourced (so that they can be fine-tuned a\u2026"}, "1141568834706403329": {"followers": "273", "datetime": "2019-06-20 04:49:54", "author": "@_g40n_", "content_summary": "RT @golbin: BERT\ubcf4\ub2e4 20\uac1c\uc758 \ud0dc\uc2a4\ud06c\uc5d0\uc11c \ud070 \ud3ed\uc73c\ub85c \ub354 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778 XLNet \ucd9c\uc2dc(?!) \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378\uae4c\uc9c0 \uacf5\uac1c!! https://t.co/0fRVFA0RGM"}, "1141650096896872453": {"followers": "270", "datetime": "2019-06-20 10:12:48", "author": "@kotti_sasikanth", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143665080858480641": {"followers": "14", "datetime": "2019-06-25 23:39:38", "author": "@CalligraphicCMS", "content_summary": "I'm floored: training XLNet to convergence releases around 4.9 metric tons of CO2 into the atmosphere"}, "1145658764906729474": {"followers": "685", "datetime": "2019-07-01 11:41:49", "author": "@satoshihirose", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141720571027132416": {"followers": "202", "datetime": "2019-06-20 14:52:50", "author": "@ChunNanHsu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143395054716952576": {"followers": "66", "datetime": "2019-06-25 05:46:38", "author": "@SkBlaz", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143460638951448577": {"followers": "52", "datetime": "2019-06-25 10:07:15", "author": "@UsFeelWe", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141694789928333313": {"followers": "81", "datetime": "2019-06-20 13:10:24", "author": "@JingchengDu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1218385668092514304": {"followers": "755", "datetime": "2020-01-18 04:12:34", "author": "@mitsuki20Gin", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141769501761048576": {"followers": "78", "datetime": "2019-06-20 18:07:16", "author": "@knvinh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141707328611397634": {"followers": "1,060", "datetime": "2019-06-20 14:00:13", "author": "@loretoparisi", "content_summary": "@rsalakhu That's amazing! Just for reference, the paper: https://t.co/m667OQWts7 and code: https://t.co/sCNHW6K4tr"}, "1152203467680821248": {"followers": "164", "datetime": "2019-07-19 13:08:08", "author": "@davamix", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141791884085534720": {"followers": "24,902", "datetime": "2019-06-20 19:36:13", "author": "@NathanBenaich", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141605692580569088": {"followers": "2,673", "datetime": "2019-06-20 07:16:21", "author": "@mosko_mule", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1150780193755320321": {"followers": "4,124", "datetime": "2019-07-15 14:52:33", "author": "@WIOMAX_PA", "content_summary": "RT @BishnuPChowdhu1: Today\u2019s read- https://t.co/1JrwnFaWJ6 Video explanation - https://t.co/SmaHp8V4G8 Reference - Bert - https://t.co/9\u2026"}, "1141848723116326912": {"followers": "44", "datetime": "2019-06-20 23:22:04", "author": "@howardmeng", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141672586545250304": {"followers": "502", "datetime": "2019-06-20 11:42:10", "author": "@yaboo_oyabu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141667003025018880": {"followers": "1,178", "datetime": "2019-06-20 11:19:59", "author": "@mettle", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141698138597576710": {"followers": "39", "datetime": "2019-06-20 13:23:42", "author": "@twioog", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1143442044926361600": {"followers": "7", "datetime": "2019-06-25 08:53:22", "author": "@arjunsriv", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1200583230950268928": {"followers": "476", "datetime": "2019-11-30 01:12:02", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143389275452583936": {"followers": "117", "datetime": "2019-06-25 05:23:41", "author": "@wingrime", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141727889576120320": {"followers": "521", "datetime": "2019-06-20 15:21:55", "author": "@DenisseQ", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143702933277573120": {"followers": "140", "datetime": "2019-06-26 02:10:02", "author": "@siingUB", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1143690719598526465": {"followers": "1,340", "datetime": "2019-06-26 01:21:30", "author": "@sugi3_34", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1142038266985639937": {"followers": "128", "datetime": "2019-06-21 11:55:15", "author": "@before_pocky", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143482673689702400": {"followers": "27", "datetime": "2019-06-25 11:34:48", "author": "@MedUseful", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141884815794245632": {"followers": "1,522", "datetime": "2019-06-21 01:45:29", "author": "@proton_1602", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1161216837104762880": {"followers": "12,765", "datetime": "2019-08-13 10:04:02", "author": "@jaguring1", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141877570243198976": {"followers": "71", "datetime": "2019-06-21 01:16:42", "author": "@hjguyhan", "content_summary": "RT @golbin: BERT\ubcf4\ub2e4 20\uac1c\uc758 \ud0dc\uc2a4\ud06c\uc5d0\uc11c \ud070 \ud3ed\uc73c\ub85c \ub354 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778 XLNet \ucd9c\uc2dc(?!) \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378\uae4c\uc9c0 \uacf5\uac1c!! https://t.co/0fRVFA0RGM"}, "1141597756202504192": {"followers": "12,765", "datetime": "2019-06-20 06:44:49", "author": "@jaguring1", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1141828989226561536": {"followers": "279", "datetime": "2019-06-20 22:03:39", "author": "@theolivenbaum", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141606111054827521": {"followers": "531", "datetime": "2019-06-20 07:18:01", "author": "@vitojph", "content_summary": "RT @bradenjhancock: Some seriously impressive gains on popular benchmarks, with nice analysis. I'm sure the pretrained model will see a lot\u2026"}, "1143708556090335232": {"followers": "19", "datetime": "2019-06-26 02:32:23", "author": "@rackingroll", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1186110347150708742": {"followers": "476", "datetime": "2019-10-21 02:41:58", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143358500476637185": {"followers": "898", "datetime": "2019-06-25 03:21:23", "author": "@kylewadegrove", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142035387419942918": {"followers": "229", "datetime": "2019-06-21 11:43:49", "author": "@mattmcknight", "content_summary": "Progress still seems rapid."}, "1141588685395132416": {"followers": "517", "datetime": "2019-06-20 06:08:46", "author": "@jasonkessler", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1177161448109367296": {"followers": "222", "datetime": "2019-09-26 10:02:14", "author": "@PapersTrending", "content_summary": "[3/10] \ud83d\udcc8 - fast-bert - 553 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141514081758695424": {"followers": "331", "datetime": "2019-06-20 01:12:20", "author": "@dkumazaw", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145728644335034370": {"followers": "1,152", "datetime": "2019-07-01 16:19:30", "author": "@jd_mashiro", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1142707982440042496": {"followers": "1,833", "datetime": "2019-06-23 08:16:28", "author": "@msarozz", "content_summary": "RT @NegruEduard: Great news for #NLP enthusiasts. XLnet[1] is said to outperform Bert on certain nlp tasks according to this article [2]. #\u2026"}, "1143476870224084992": {"followers": "456", "datetime": "2019-06-25 11:11:45", "author": "@PerthMLGroup", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1208784676690616320": {"followers": "205", "datetime": "2019-12-22 16:21:39", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143951264662691840": {"followers": "36", "datetime": "2019-06-26 18:36:49", "author": "@punnettsaini", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143841449798062080": {"followers": "993", "datetime": "2019-06-26 11:20:27", "author": "@ayaette", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143119546968154113": {"followers": "70", "datetime": "2019-06-24 11:31:52", "author": "@HackTheAI", "content_summary": "\u201cXLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking\u201d https://t.co/O7kl9TxNx6"}, "1141519955512324096": {"followers": "5,606", "datetime": "2019-06-20 01:35:40", "author": "@__MLT__", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141532347206029313": {"followers": "12", "datetime": "2019-06-20 02:24:54", "author": "@myutwo150", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143831343685521408": {"followers": "110", "datetime": "2019-06-26 10:40:18", "author": "@Raquel1934", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141691953559429122": {"followers": "28", "datetime": "2019-06-20 12:59:07", "author": "@AmineTrab", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1159766774176473089": {"followers": "222", "datetime": "2019-08-09 10:02:01", "author": "@PapersTrending", "content_summary": "[8/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 10,523 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1141936659719606272": {"followers": "3,541", "datetime": "2019-06-21 05:11:30", "author": "@tankazunori0914", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143839860072992770": {"followers": "314", "datetime": "2019-06-26 11:14:08", "author": "@leo11_ds", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1143529734355091457": {"followers": "12,040", "datetime": "2019-06-25 14:41:49", "author": "@citnaj", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141946791769907201": {"followers": "674", "datetime": "2019-06-21 05:51:46", "author": "@peacefulcyborg", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1155821262641467392": {"followers": "1,726", "datetime": "2019-07-29 12:43:57", "author": "@Libardopez", "content_summary": "RT @PapersTrending: [6/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 9,948 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1144149258385276928": {"followers": "658", "datetime": "2019-06-27 07:43:35", "author": "@virtualion", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1145721051445207040": {"followers": "50", "datetime": "2019-07-01 15:49:19", "author": "@hoimi1031", "content_summary": "\u3046\u304e\u3083\u3042\u3002"}, "1141533545267003392": {"followers": "145", "datetime": "2019-06-20 02:29:40", "author": "@edirgarcia", "content_summary": "Deep Learning research is going expeditiously fast. It's hard to keep up."}, "1145493096588726272": {"followers": "165", "datetime": "2019-07-01 00:43:31", "author": "@samurairodeo", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141576913158324224": {"followers": "55", "datetime": "2019-06-20 05:22:00", "author": "@GeoPerakis", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141798852363784193": {"followers": "212", "datetime": "2019-06-20 20:03:54", "author": "@michelpf", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141517318603730944": {"followers": "99", "datetime": "2019-06-20 01:25:11", "author": "@treasured_write", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141696266641895424": {"followers": "1,309", "datetime": "2019-06-20 13:16:16", "author": "@akihiro_akichan", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141689305120817154": {"followers": "149", "datetime": "2019-06-20 12:48:36", "author": "@katemargatina", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143756558263836673": {"followers": "757", "datetime": "2019-06-26 05:43:08", "author": "@jtregunna", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142094104266006528": {"followers": "1,585", "datetime": "2019-06-21 15:37:08", "author": "@jakubzavrel", "content_summary": "Amazing new XLNet paper out on arXiv. The rate of progress in Deep Learning NLP is impressive...! (thanks to @AkiraAsimov for the pointer)"}, "1141803763503652870": {"followers": "182", "datetime": "2019-06-20 20:23:25", "author": "@fbalbach", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141768533568905217": {"followers": "43", "datetime": "2019-06-20 18:03:26", "author": "@alleviate_hq", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142515517850152960": {"followers": "2,067", "datetime": "2019-06-22 19:31:41", "author": "@yo_ehara", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142060331411243008": {"followers": "78", "datetime": "2019-06-21 13:22:56", "author": "@watchdog20xx", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1141551815399337985": {"followers": "86", "datetime": "2019-06-20 03:42:16", "author": "@writetoswarna", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143805059366621184": {"followers": "219", "datetime": "2019-06-26 08:55:51", "author": "@Neuw84", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141702210834718721": {"followers": "203", "datetime": "2019-06-20 13:39:53", "author": "@oct_path", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1144118469610397701": {"followers": "32", "datetime": "2019-06-27 05:41:14", "author": "@maxim_leon", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141541097023275008": {"followers": "867", "datetime": "2019-06-20 02:59:40", "author": "@_ivana__anavi_", "content_summary": "XLNet outperforms BERT on 20 tasks: https://t.co/cMLGlQ6v2O"}, "1141983656531116032": {"followers": "231", "datetime": "2019-06-21 08:18:15", "author": "@CristianRobertM", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141564855989743616": {"followers": "168", "datetime": "2019-06-20 04:34:05", "author": "@brunoboutteau", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141690967113654272": {"followers": "1,667", "datetime": "2019-06-20 12:55:12", "author": "@Scaled_Wurm", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1142276691626082305": {"followers": "1,192", "datetime": "2019-06-22 03:42:40", "author": "@windx0303", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1143354637879263232": {"followers": "307", "datetime": "2019-06-25 03:06:02", "author": "@ProbNotABot", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141809888743444481": {"followers": "195", "datetime": "2019-06-20 20:47:45", "author": "@KafleSushant", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141820497006514176": {"followers": "386", "datetime": "2019-06-20 21:29:55", "author": "@itukano", "content_summary": "\u3053\u3046\u3044\u3046\u306e\u9583\u3044\u305f\u3068\u304d\u8133\u6c41\u30e4\u30d0\u305d\u3046\u3002"}, "1141538343303303168": {"followers": "596", "datetime": "2019-06-20 02:48:44", "author": "@khanhxuannguyen", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141621629149167616": {"followers": "202", "datetime": "2019-06-20 08:19:41", "author": "@zein_shahine", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142003505424666624": {"followers": "89", "datetime": "2019-06-21 09:37:07", "author": "@margusb", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141894300038389760": {"followers": "12,765", "datetime": "2019-06-21 02:23:11", "author": "@jaguring1", "content_summary": "RT @shohomiura: BERT\u3092\u8d85\u3048\u305f\u30e2\u30c7\u30ebcoming out! https://t.co/kIAYdheNPU"}, "1143617944456482819": {"followers": "103", "datetime": "2019-06-25 20:32:20", "author": "@michaelc_io", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141777866033643520": {"followers": "1,060", "datetime": "2019-06-20 18:40:31", "author": "@loretoparisi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141602303994036224": {"followers": "100", "datetime": "2019-06-20 07:02:53", "author": "@hyodo_net", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141565166917545984": {"followers": "1,858", "datetime": "2019-06-20 04:35:19", "author": "@ravi_mohan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143584645100867584": {"followers": "2,671", "datetime": "2019-06-25 18:20:00", "author": "@ayirpelle", "content_summary": "RT @atulbutte: Luckily it costs a lot less to train a child how to understand language\u2026 HT @IAmSamFin https://t.co/BZA3fX2A3U"}, "1163876541169852418": {"followers": "42", "datetime": "2019-08-20 18:12:45", "author": "@MishakinSergey", "content_summary": "RT @PapersTrending: [9/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 10,953 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSR\u2026"}, "1141953839333089280": {"followers": "66", "datetime": "2019-06-21 06:19:46", "author": "@shubh_300595", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141888488989761537": {"followers": "332", "datetime": "2019-06-21 02:00:05", "author": "@pabloibieta", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143395287161028609": {"followers": "171", "datetime": "2019-06-25 05:47:34", "author": "@AmirHadifar", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141656931695595520": {"followers": "16", "datetime": "2019-06-20 10:39:58", "author": "@Poornapragnams", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143513341094191104": {"followers": "3,327", "datetime": "2019-06-25 13:36:40", "author": "@neal_lathia", "content_summary": "I wonder whether these huge training costs are offset by these models being open sourced (so that they can be fine-tuned and deployed at nearly no cost for 100s of problems)."}, "1143356389328396288": {"followers": "2,195", "datetime": "2019-06-25 03:13:00", "author": "@baykenney", "content_summary": "access to compute can't be decoupled from responsible development of ml systems."}, "1143439104597737474": {"followers": "18", "datetime": "2019-06-25 08:41:41", "author": "@JuanLDominguez_", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1199143659041476608": {"followers": "205", "datetime": "2019-11-26 01:51:41", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141826978481344517": {"followers": "284", "datetime": "2019-06-20 21:55:40", "author": "@MaxVico", "content_summary": "Este a\u00f1o sin duda el NLP est\u00e1 de moda..."}, "1142708103869272064": {"followers": "9,456", "datetime": "2019-06-23 08:16:57", "author": "@machine_ml", "content_summary": "RT @NegruEduard: Great news for #NLP enthusiasts. XLnet[1] is said to outperform Bert on certain nlp tasks according to this article [2]. #\u2026"}, "1151380449555468288": {"followers": "294", "datetime": "2019-07-17 06:37:45", "author": "@rose_miura", "content_summary": "RT @kosuke_tsujino: XLNet\u306fsentencepiece\u3064\u304b\u3063\u3066\u308b\u307d\u3044 https://t.co/kHwuT3myLO"}, "1143374965896384513": {"followers": "772", "datetime": "2019-06-25 04:26:49", "author": "@mattbagg", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141729402922782720": {"followers": "287", "datetime": "2019-06-20 15:27:56", "author": "@amArunava", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141890731914567681": {"followers": "2,067", "datetime": "2019-06-21 02:09:00", "author": "@yo_ehara", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1191017669639561217": {"followers": "476", "datetime": "2019-11-03 15:41:55", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143543711738912768": {"followers": "1,909", "datetime": "2019-06-25 15:37:21", "author": "@maggie_albrecht", "content_summary": "RT @baykenney: access to compute can't be decoupled from responsible development of ml systems. https://t.co/8pAmAV39Mg"}, "1142074120940617733": {"followers": "520", "datetime": "2019-06-21 14:17:43", "author": "@araitatsuya", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1144140675983613955": {"followers": "35", "datetime": "2019-06-27 07:09:28", "author": "@Geek_alphaomega", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk \u2026 https://t.co/owzX13zbgg #DeepLear\u2026"}, "1141555603917148161": {"followers": "1,411", "datetime": "2019-06-20 03:57:19", "author": "@RexDouglass", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141704354979692544": {"followers": "300", "datetime": "2019-06-20 13:48:24", "author": "@DIVYANSHJHA", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143889612458332160": {"followers": "180", "datetime": "2019-06-26 14:31:50", "author": "@GenesisCloud_", "content_summary": "With us, you train it for less than $10k, with ZERO CO2 footprint thanks to the #RenewableEnergy of our #datacenters in Iceland and Sweden #Cloud #MachineLearning (@eturner303 according to the paper it's 512 TPU chips, which is 128 TPU devices, or $61k"}, "1143459003185422336": {"followers": "222", "datetime": "2019-06-25 10:00:45", "author": "@PapersTrending", "content_summary": "[2/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 2,727 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141608753638203392": {"followers": "1", "datetime": "2019-06-20 07:28:31", "author": "@PaulHuang668", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1248630017484918784": {"followers": "476", "datetime": "2020-04-10 15:12:49", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141932642159538176": {"followers": "853", "datetime": "2019-06-21 04:55:32", "author": "@Ektropos", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1183832217300152320": {"followers": "106", "datetime": "2019-10-14 19:49:29", "author": "@jau1990", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141541192779198464": {"followers": "1,265", "datetime": "2019-06-20 03:00:03", "author": "@blackopscyber1", "content_summary": "RT @420Cyber: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/txeTpImuU8"}, "1144868455721496577": {"followers": "1", "datetime": "2019-06-29 07:21:25", "author": "@ngoctrunght19", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141529132074962945": {"followers": "2,081", "datetime": "2019-06-20 02:12:08", "author": "@diyiy_cmu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141587929787260928": {"followers": "72", "datetime": "2019-06-20 06:05:46", "author": "@datashinobi", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141613731119554561": {"followers": "1,049", "datetime": "2019-06-20 07:48:18", "author": "@debayan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141689173344169984": {"followers": "1,848", "datetime": "2019-06-20 12:48:05", "author": "@coastalcph", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143433506963501056": {"followers": "26", "datetime": "2019-06-25 08:19:26", "author": "@BogdanGliga", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141663477704069120": {"followers": "1,165", "datetime": "2019-06-20 11:05:58", "author": "@YangKevinK", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143805556945227776": {"followers": "342", "datetime": "2019-06-26 08:57:50", "author": "@avineshpvs", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142038877231669251": {"followers": "1,152", "datetime": "2019-06-21 11:57:41", "author": "@jd_mashiro", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1141777080381857793": {"followers": "1,231", "datetime": "2019-06-20 18:37:23", "author": "@CA_Streaming", "content_summary": "Will try this one"}, "1141638398689435649": {"followers": "2", "datetime": "2019-06-20 09:26:19", "author": "@sivia89024", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143640837076787205": {"followers": "1,853", "datetime": "2019-06-25 22:03:18", "author": "@denisparra", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141750164429987840": {"followers": "4,444", "datetime": "2019-06-20 16:50:26", "author": "@evisoft", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1159404344887918593": {"followers": "222", "datetime": "2019-08-08 10:01:51", "author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 10,447 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/7bzXyCHTjk"}, "1141630542384914432": {"followers": "590", "datetime": "2019-06-20 08:55:06", "author": "@codekee", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141536801460539394": {"followers": "80", "datetime": "2019-06-20 02:42:36", "author": "@ricklentz", "content_summary": "Impressive work demonstrating state of the art performance across many reading comprehension tasks: https://t.co/qpmH8QQCeh"}, "1142492404634664960": {"followers": "10", "datetime": "2019-06-22 17:59:50", "author": "@yarphs", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141661623184084992": {"followers": "3,179", "datetime": "2019-06-20 10:58:36", "author": "@tjmlab", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1143867705575034880": {"followers": "667", "datetime": "2019-06-26 13:04:47", "author": "@pchandarr", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1219602684610777094": {"followers": "15", "datetime": "2020-01-21 12:48:33", "author": "@BKK113", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1142542937483386880": {"followers": "635", "datetime": "2019-06-22 21:20:38", "author": "@rkakamilan", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1142088969057738752": {"followers": "120", "datetime": "2019-06-21 15:16:43", "author": "@ashish9277", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141785580780437504": {"followers": "133", "datetime": "2019-06-20 19:11:10", "author": "@imtechmonk", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143475194201165827": {"followers": "27", "datetime": "2019-06-25 11:05:05", "author": "@Indraneeldesh", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143520981496942595": {"followers": "15", "datetime": "2019-06-25 14:07:02", "author": "@uthamkamath", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141730767703355392": {"followers": "318", "datetime": "2019-06-20 15:33:22", "author": "@garamirez", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143724771433046016": {"followers": "696", "datetime": "2019-06-26 03:36:49", "author": "@takatoh1", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141560143810686976": {"followers": "70", "datetime": "2019-06-20 04:15:22", "author": "@sumitsethy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1224651829918568448": {"followers": "476", "datetime": "2020-02-04 11:12:03", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141626386047746049": {"followers": "27", "datetime": "2019-06-20 08:38:35", "author": "@zarmime", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141957785317335040": {"followers": "22", "datetime": "2019-06-21 06:35:27", "author": "@Lampmat", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141619716873687042": {"followers": "1,188", "datetime": "2019-06-20 08:12:05", "author": "@fnielsen", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141768919876886529": {"followers": "326", "datetime": "2019-06-20 18:04:58", "author": "@madavidj", "content_summary": "BERT is so 2018"}, "1143463187670622208": {"followers": "419", "datetime": "2019-06-25 10:17:23", "author": "@AritzBi", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143460922712870912": {"followers": "111", "datetime": "2019-06-25 10:08:23", "author": "@Viehzeug", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141730516544237568": {"followers": "199", "datetime": "2019-06-20 15:32:22", "author": "@mathpluscode", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1143636216073486336": {"followers": "482", "datetime": "2019-06-25 21:44:56", "author": "@CyborgTribe", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143469122384203777": {"followers": "422", "datetime": "2019-06-25 10:40:58", "author": "@jcvasquezc1", "content_summary": "Wowwwww #NLProc #DeepLearning"}, "1141733960424337408": {"followers": "74", "datetime": "2019-06-20 15:46:03", "author": "@AdilZtn", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141901505429037056": {"followers": "76", "datetime": "2019-06-21 02:51:49", "author": "@sivakd", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1219323196203204611": {"followers": "2,475", "datetime": "2020-01-20 18:17:58", "author": "@crypto_magix", "content_summary": "In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain $MAN"}, "1142008539658854401": {"followers": "768", "datetime": "2019-06-21 09:57:08", "author": "@DUXROLL", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141757788911230976": {"followers": "4", "datetime": "2019-06-20 17:20:44", "author": "@gblearning42", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141895839507337216": {"followers": "637", "datetime": "2019-06-21 02:29:18", "author": "@KuraOyo", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141640693816782848": {"followers": "246", "datetime": "2019-06-20 09:35:26", "author": "@mingszuliang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141557805381406721": {"followers": "77", "datetime": "2019-06-20 04:06:04", "author": "@savinay1986", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1143452349261729792": {"followers": "201", "datetime": "2019-06-25 09:34:19", "author": "@absolut_todd", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143949509124218881": {"followers": "109", "datetime": "2019-06-26 18:29:51", "author": "@JamesONeil21", "content_summary": "A prime example https://t.co/mqLnT16j8R"}, "1154798190601953283": {"followers": "1,447", "datetime": "2019-07-26 16:58:38", "author": "@__olamilekan__", "content_summary": "I go back to this tweet everytime I need to remind myself that the SOTA life is not for me."}, "1142911933202481152": {"followers": "427", "datetime": "2019-06-23 21:46:53", "author": "@eigencoffee", "content_summary": "NLP\u306e\u767a\u5c55\u51c4\u307e\u3058\u3044"}, "1142283850317365248": {"followers": "456", "datetime": "2019-06-22 04:11:07", "author": "@PerthMLGroup", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141939977149829120": {"followers": "304", "datetime": "2019-06-21 05:24:41", "author": "@stkt_KU", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145611202451537920": {"followers": "82", "datetime": "2019-07-01 08:32:49", "author": "@miorgash", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141789994526420999": {"followers": "85", "datetime": "2019-06-20 19:28:42", "author": "@FlorianDreher", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141710741428428800": {"followers": "100", "datetime": "2019-06-20 14:13:47", "author": "@mrdanieldsouza", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142202266478243840": {"followers": "129", "datetime": "2019-06-21 22:46:56", "author": "@omitawo", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141546127348514816": {"followers": "237", "datetime": "2019-06-20 03:19:40", "author": "@veydpz_public", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143278991190552576": {"followers": "7", "datetime": "2019-06-24 22:05:27", "author": "@Prince94531", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143010688052318208": {"followers": "143", "datetime": "2019-06-24 04:19:18", "author": "@n_kats_", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1143879294411608066": {"followers": "1,022", "datetime": "2019-06-26 13:50:50", "author": "@nishizawa", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141565101071314944": {"followers": "221", "datetime": "2019-06-20 04:35:04", "author": "@Davide__Cirillo", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1219633728668012545": {"followers": "1,474", "datetime": "2020-01-21 14:51:55", "author": "@steadydee", "content_summary": "At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will explode when all those distributed GPUs are diverted to AI and not just wasteful mining. Coming February 2020 #MATRIXAI $MAN https:/"}, "1210209125663035392": {"followers": "476", "datetime": "2019-12-26 14:41:54", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141811144434298882": {"followers": "33", "datetime": "2019-06-20 20:52:45", "author": "@data_topology", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141858258140958720": {"followers": "3,096", "datetime": "2019-06-20 23:59:58", "author": "@ivan_bezdomny", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1253499587509145601": {"followers": "476", "datetime": "2020-04-24 01:42:45", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1142476620856320001": {"followers": "12,765", "datetime": "2019-06-22 16:57:07", "author": "@jaguring1", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141697308658196482": {"followers": "12,363", "datetime": "2019-06-20 13:20:24", "author": "@sleepinyourhat", "content_summary": "Paper's out: https://t.co/P6yYhxSZB2"}, "1143433912972005376": {"followers": "210", "datetime": "2019-06-25 08:21:03", "author": "@ox9710090", "content_summary": "RT @MikiBear_: \uc544\ub2c8 \ubbf8\uce5c \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b https://t.co/ixHqPog8Yu https://t.co/DSxHpesHSN"}, "1142038868419416064": {"followers": "294", "datetime": "2019-06-21 11:57:38", "author": "@rose_miura", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1141608124937363456": {"followers": "219", "datetime": "2019-06-20 07:26:01", "author": "@pabloppp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143711195515502592": {"followers": "179,069", "datetime": "2019-06-26 02:42:52", "author": "@Montreal_AI", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144600103426150402": {"followers": "3,700", "datetime": "2019-06-28 13:35:04", "author": "@JuergenKob", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk \u2026 https://t.co/owzX13zbgg #DeepLear\u2026"}, "1141721775224397825": {"followers": "164", "datetime": "2019-06-20 14:57:38", "author": "@ZachBessinger", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141968570710024192": {"followers": "854", "datetime": "2019-06-21 07:18:18", "author": "@tall_tree_desu", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141600293689446400": {"followers": "89", "datetime": "2019-06-20 06:54:54", "author": "@mgrankin", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143770189189779456": {"followers": "16,817", "datetime": "2019-06-26 06:37:17", "author": "@octonion", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141708076262858753": {"followers": "8,829", "datetime": "2019-06-20 14:03:11", "author": "@Ted_Underwood", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143718332668903424": {"followers": "11,787", "datetime": "2019-06-26 03:11:14", "author": "@yutakashino", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141636905894727680": {"followers": "2,481", "datetime": "2019-06-20 09:20:23", "author": "@shion_honda", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141834119296344065": {"followers": "169", "datetime": "2019-06-20 22:24:02", "author": "@metacognition12", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142003853245575168": {"followers": "254", "datetime": "2019-06-21 09:38:30", "author": "@hrsma2i", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1142087770816430080": {"followers": "366", "datetime": "2019-06-21 15:11:58", "author": "@ashishawasthi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141698291790487552": {"followers": "6,717", "datetime": "2019-06-20 13:24:19", "author": "@mkrobot", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145492840673234944": {"followers": "765", "datetime": "2019-07-01 00:42:30", "author": "@makudara", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141689857674059776": {"followers": "644", "datetime": "2019-06-20 12:50:48", "author": "@finelined_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141661524412383232": {"followers": "1,769", "datetime": "2019-06-20 10:58:13", "author": "@jaialkdanel", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1145554956671279104": {"followers": "1,667", "datetime": "2019-07-01 04:49:19", "author": "@Scaled_Wurm", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141512102923968517": {"followers": "754", "datetime": "2019-06-20 01:04:28", "author": "@suneelmarthi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143448429001150465": {"followers": "1,998", "datetime": "2019-06-25 09:18:44", "author": "@RecklessCoding", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142191358058192896": {"followers": "12,924", "datetime": "2019-06-21 22:03:35", "author": "@deliprao", "content_summary": "PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You thought Sesame Street names were bad. Okay, now check out this super cool work \ud83d\udc47\ud83c\udffc"}, "1141579700504018944": {"followers": "721", "datetime": "2019-06-20 05:33:04", "author": "@mhrsafa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143693969068453888": {"followers": "3,179", "datetime": "2019-06-26 01:34:25", "author": "@tjmlab", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141886757488615424": {"followers": "2,916", "datetime": "2019-06-21 01:53:12", "author": "@guglilac", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143398537943769088": {"followers": "222", "datetime": "2019-06-25 06:00:29", "author": "@ismailah28", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142483558231003136": {"followers": "110", "datetime": "2019-06-22 17:24:41", "author": "@Raquel1934", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1210222098569822208": {"followers": "144", "datetime": "2019-12-26 15:33:27", "author": "@Jack_hibi", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143424613898231808": {"followers": "198", "datetime": "2019-06-25 07:44:06", "author": "@jvcossu", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141571325061783553": {"followers": "12,765", "datetime": "2019-06-20 04:59:47", "author": "@jaguring1", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141519563529625600": {"followers": "160", "datetime": "2019-06-20 01:34:07", "author": "@tuvuumass", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141521045729492993": {"followers": "297", "datetime": "2019-06-20 01:40:00", "author": "@jmcimula", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143723790368563200": {"followers": "537", "datetime": "2019-06-26 03:32:55", "author": "@TakaoOkano", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143462016407035904": {"followers": "7", "datetime": "2019-06-25 10:12:43", "author": "@sdrpierre", "content_summary": "Wow, didn't expect that... Is the calculation right though?"}, "1143707263728701440": {"followers": "38", "datetime": "2019-06-26 02:27:15", "author": "@AnRakui", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141671192346210305": {"followers": "173", "datetime": "2019-06-20 11:36:38", "author": "@DucQDuong", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141556892713414656": {"followers": "615", "datetime": "2019-06-20 04:02:26", "author": "@KouroshMeshgi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142693272659406848": {"followers": "489", "datetime": "2019-06-23 07:18:01", "author": "@amitness", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142398993743470593": {"followers": "45", "datetime": "2019-06-22 11:48:39", "author": "@artuskg", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141552558298632192": {"followers": "293", "datetime": "2019-06-20 03:45:13", "author": "@arrrnav", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143025958183526400": {"followers": "29", "datetime": "2019-06-24 05:19:59", "author": "@chenlailin", "content_summary": "RT @hillbig: BERT ignores dependency between the masked positions. XLNet uses a random permutation of the factorization and the transformer\u2026"}, "1141670006649810944": {"followers": "3", "datetime": "2019-06-20 11:31:55", "author": "@vanducng", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143519339204755457": {"followers": "942", "datetime": "2019-06-25 14:00:30", "author": "@giodegas", "content_summary": "So, apparently training an #AI to understand written text, that a human can do when teenager, it costs more than training a human for a University degree"}, "1142110669594136577": {"followers": "798", "datetime": "2019-06-21 16:42:57", "author": "@absamy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141775165317156866": {"followers": "77,472", "datetime": "2019-06-20 18:29:47", "author": "@rsalakhu", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141757712822235136": {"followers": "100", "datetime": "2019-06-20 17:20:26", "author": "@RajaswaPatil", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141738317337321473": {"followers": "11", "datetime": "2019-06-20 16:03:21", "author": "@Je_Hofmann", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143605359581519872": {"followers": "1,840", "datetime": "2019-06-25 19:42:19", "author": "@leifos", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141742780785577984": {"followers": "65", "datetime": "2019-06-20 16:21:06", "author": "@PathakArchita", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141558116452130817": {"followers": "1,679", "datetime": "2019-06-20 04:07:18", "author": "@dchaplot", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141605646783197189": {"followers": "103", "datetime": "2019-06-20 07:16:10", "author": "@jongen87", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141534149934587904": {"followers": "109", "datetime": "2019-06-20 02:32:04", "author": "@Charles9n", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141523372662300674": {"followers": "771", "datetime": "2019-06-20 01:49:15", "author": "@aCraigPfeifer", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1143542516920139776": {"followers": "3,793", "datetime": "2019-06-25 15:32:36", "author": "@IAmSamFin", "content_summary": "This is a lot of $$$, and one could raise solid q's about the cost-benefit to society if ML research increasingly focuses on scale rather than more fundamental algo innovation However, if the price alone blows your mind + rustles your jimmies, you should"}, "1141665330021982208": {"followers": "582", "datetime": "2019-06-20 11:13:20", "author": "@_stefan_munich", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141699547204186112": {"followers": "52", "datetime": "2019-06-20 13:29:18", "author": "@midsumwork", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141688374006374408": {"followers": "163,852", "datetime": "2019-06-20 12:44:54", "author": "@ceobillionaire", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141939791887396864": {"followers": "304", "datetime": "2019-06-21 05:23:57", "author": "@stkt_KU", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1218357789459828736": {"followers": "205", "datetime": "2020-01-18 02:21:47", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143759350323798017": {"followers": "674", "datetime": "2019-06-26 05:54:13", "author": "@hidemotoNakada", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141998289837015040": {"followers": "32", "datetime": "2019-06-21 09:16:24", "author": "@numPerfecto28", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141967302805446656": {"followers": "126", "datetime": "2019-06-21 07:13:16", "author": "@metasyn", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141765569848446977": {"followers": "548", "datetime": "2019-06-20 17:51:39", "author": "@didier_schwab", "content_summary": "RT @FR_Chaumartin: XLNet : un apprentissage de mod\u00e8le de langue qui donne des r\u00e9sultats encore meilleurs que BERT#NLP #DeepLearning https:/\u2026"}, "1141708439942500355": {"followers": "381", "datetime": "2019-06-20 14:04:38", "author": "@shyamal_chandra", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141727346925293569": {"followers": "260", "datetime": "2019-06-20 15:19:46", "author": "@hyohyi", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143471586999177217": {"followers": "92", "datetime": "2019-06-25 10:50:45", "author": "@padmalcom", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143494759446695937": {"followers": "79", "datetime": "2019-06-25 12:22:50", "author": "@trashcanmagic", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141824525576867842": {"followers": "172", "datetime": "2019-06-20 21:45:55", "author": "@mrbarbasa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141757523935948800": {"followers": "223", "datetime": "2019-06-20 17:19:41", "author": "@posadajd", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141609978899435520": {"followers": "2,134", "datetime": "2019-06-20 07:33:23", "author": "@lespetitescases", "content_summary": "La course pour l'am\u00e9lioration des algos de traitement du langage naturel se poursuit \u00e0 un rythme effr\u00e9n\u00e9... BERT est sorti il y a 8 mois mais un nouvel algo s'annonce comme meilleur... \ud83d\ude2e\ud83d\ude2e"}, "1143375046251110406": {"followers": "63", "datetime": "2019-06-25 04:27:08", "author": "@AI_DeepDive", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141645051434610689": {"followers": "24", "datetime": "2019-06-20 09:52:45", "author": "@pradiptadeb90", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141584682179551232": {"followers": "2,102", "datetime": "2019-06-20 05:52:52", "author": "@datitran", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141559802524340224": {"followers": "12,765", "datetime": "2019-06-20 04:14:00", "author": "@jaguring1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141942605871079425": {"followers": "1", "datetime": "2019-06-21 05:35:08", "author": "@PaulHuang668", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141700640730693632": {"followers": "6", "datetime": "2019-06-20 13:33:39", "author": "@BhayaniDebu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143358046015541252": {"followers": "47", "datetime": "2019-06-25 03:19:35", "author": "@aumitleon", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142027011650740224": {"followers": "1,670", "datetime": "2019-06-21 11:10:32", "author": "@StuartReid1929", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143533624387653633": {"followers": "1,403", "datetime": "2019-06-25 14:57:16", "author": "@RTFMCelia", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141701523417624577": {"followers": "842", "datetime": "2019-06-20 13:37:09", "author": "@kazanagisora", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1143694716384317440": {"followers": "1,777", "datetime": "2019-06-26 01:37:23", "author": "@BrianTRice", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141738709315805184": {"followers": "17", "datetime": "2019-06-20 16:04:55", "author": "@JoelChen95", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141558714949865472": {"followers": "996", "datetime": "2019-06-20 04:09:41", "author": "@ialuronico", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1141568833334857729": {"followers": "38", "datetime": "2019-06-20 04:49:53", "author": "@experiencor", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141609631904673793": {"followers": "638", "datetime": "2019-06-20 07:32:00", "author": "@ellliottt", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141789323378995200": {"followers": "25", "datetime": "2019-06-20 19:26:02", "author": "@nobillygreen", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141744078192644096": {"followers": "0", "datetime": "2019-06-20 16:26:15", "author": "@_sora2riku_", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1141697588430815232": {"followers": "68", "datetime": "2019-06-20 13:21:31", "author": "@shendinghan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1161417544189636608": {"followers": "4,963", "datetime": "2019-08-13 23:21:35", "author": "@m_nishiba", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141955078850568195": {"followers": "579", "datetime": "2019-06-21 06:24:41", "author": "@TMLeiden", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143404741788987392": {"followers": "131", "datetime": "2019-06-25 06:25:08", "author": "@KrishaMehta2", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141901332627873795": {"followers": "1,542", "datetime": "2019-06-21 02:51:07", "author": "@auratus", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1161189386546892800": {"followers": "2,514", "datetime": "2019-08-13 08:14:58", "author": "@y8o", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143601507054608385": {"followers": "1,120", "datetime": "2019-06-25 19:27:01", "author": "@thinking_code", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/YXADUCaSYd"}, "1141753659916783619": {"followers": "227", "datetime": "2019-06-20 17:04:19", "author": "@J_P_Raymond", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143475554928209920": {"followers": "1,448", "datetime": "2019-06-25 11:06:31", "author": "@jcllobet", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141647017401311232": {"followers": "1,025", "datetime": "2019-06-20 10:00:34", "author": "@desertnaut", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141958247596908544": {"followers": "818", "datetime": "2019-06-21 06:37:17", "author": "@ErmiaBivatan", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1143646854774607872": {"followers": "822", "datetime": "2019-06-25 22:27:12", "author": "@axfelix", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1141824266259828736": {"followers": "23,866", "datetime": "2019-06-20 21:44:53", "author": "@kchonyc", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141934822916407297": {"followers": "23", "datetime": "2019-06-21 05:04:12", "author": "@zhongxiaoshi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141591207929364482": {"followers": "200", "datetime": "2019-06-20 06:18:48", "author": "@mewopean", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141754614011846657": {"followers": "7", "datetime": "2019-06-20 17:08:07", "author": "@romehrah", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141778741120688128": {"followers": "774", "datetime": "2019-06-20 18:43:59", "author": "@dmbanga", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143410292564480001": {"followers": "28", "datetime": "2019-06-25 06:47:11", "author": "@hey_kishore", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141799640532226054": {"followers": "697", "datetime": "2019-06-20 20:07:02", "author": "@Akanksha_Ahuja9", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141676530273157121": {"followers": "824", "datetime": "2019-06-20 11:57:50", "author": "@morioka", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1141831415966789633": {"followers": "161", "datetime": "2019-06-20 22:13:18", "author": "@Sohilnewa", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141647187711012867": {"followers": "222", "datetime": "2019-06-20 10:01:14", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 842 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1143757718991204353": {"followers": "2,627", "datetime": "2019-06-26 05:47:44", "author": "@apstndb", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1143454455028801538": {"followers": "186", "datetime": "2019-06-25 09:42:41", "author": "@surangasms01", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143646796163375104": {"followers": "13,349", "datetime": "2019-06-25 22:26:58", "author": "@timhwang", "content_summary": "the inherent structure of markets in artificial intelligence: oligopoly"}, "1141768040977227776": {"followers": "240", "datetime": "2019-06-20 18:01:28", "author": "@jgmorenof", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1146715155050500096": {"followers": "90", "datetime": "2019-07-04 09:39:32", "author": "@iedmrc", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141710627376947203": {"followers": "53", "datetime": "2019-06-20 14:13:20", "author": "@anasbaaaa", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1245762216734965762": {"followers": "751", "datetime": "2020-04-02 17:17:12", "author": "@carlolepelaars", "content_summary": "Finally got around to reading up on some recent NLP papers. Currently reading: ALBERT: https://t.co/AfVSLTf866 RoBERTa: https://t.co/eGLVZm3qfE XLNet: https://t.co/bypmySZdc2 BERTje (Dutch BERT model): https://t.co/Nh1uLi0pIV Do you have any other NLP"}, "1212917920373837824": {"followers": "4,200", "datetime": "2020-01-03 02:05:41", "author": "@arxiv_cs_cl", "content_summary": "https://t.co/peW3FRNE1b XLNet: Generalized Autoregressive Pretraining for Language Understanding. (arXiv:1906.08237v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc"}, "1141576558441725952": {"followers": "1", "datetime": "2019-06-20 05:20:35", "author": "@PaulHuang668", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141838813972574210": {"followers": "81,553", "datetime": "2019-06-20 22:42:42", "author": "@hardmaru", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1203960417522814979": {"followers": "205", "datetime": "2019-12-09 08:51:46", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141626992208547840": {"followers": "3,500", "datetime": "2019-06-20 08:41:00", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141551982210977792": {"followers": "12", "datetime": "2019-06-20 03:42:56", "author": "@2000Qiu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1158679531097657344": {"followers": "222", "datetime": "2019-08-06 10:01:42", "author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 10,350 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1141657827695456256": {"followers": "2,530", "datetime": "2019-06-20 10:43:31", "author": "@_329_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141953599678767104": {"followers": "311", "datetime": "2019-06-21 06:18:49", "author": "@ssakares", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141953379494584320": {"followers": "824", "datetime": "2019-06-21 06:17:56", "author": "@morioka", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143466746331877376": {"followers": "64", "datetime": "2019-06-25 10:31:31", "author": "@WilliamSteimel", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143362925853315072": {"followers": "1,283", "datetime": "2019-06-25 03:38:58", "author": "@_sbr1", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143578853807464448": {"followers": "1,036", "datetime": "2019-06-25 17:57:00", "author": "@mitevpi", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143761599351554048": {"followers": "48", "datetime": "2019-06-26 06:03:10", "author": "@_afun", "content_summary": "\u8d85\u8d8a BERT \u7684\u4ee3\u50f9\uff1a$245,000 (\uff17\u767e\u591a\u842c\u53f0\u5e63\uff09"}, "1143504282001580034": {"followers": "442", "datetime": "2019-06-25 13:00:40", "author": "@iamatachyon", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141697442452312064": {"followers": "3,942", "datetime": "2019-06-20 13:20:56", "author": "@XandaSchofield", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143721387380502528": {"followers": "358", "datetime": "2019-06-26 03:23:22", "author": "@oc11atta", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141704369173192705": {"followers": "13,798", "datetime": "2019-06-20 13:48:28", "author": "@mark_riedl", "content_summary": "RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then models. And then they will be cast aside when something better comes along in a few months."}, "1143364768872509441": {"followers": "98", "datetime": "2019-06-25 03:46:18", "author": "@shpotes", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141526815455895552": {"followers": "58", "datetime": "2019-06-20 02:02:56", "author": "@nguyenvo09", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141727880004689921": {"followers": "312", "datetime": "2019-06-20 15:21:53", "author": "@EdwardDixon3", "content_summary": "@julien_c @seb_ruder This #NLP thing is getting a bit out of hand - looks like a big delta vs. BERT..."}, "1141585218408734720": {"followers": "25", "datetime": "2019-06-20 05:55:00", "author": "@ASreesaila", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141734917103083520": {"followers": "17", "datetime": "2019-06-20 15:49:51", "author": "@ngdlaura", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141593589518147586": {"followers": "538", "datetime": "2019-06-20 06:28:16", "author": "@stormtroper1721", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141741037821390849": {"followers": "1,336", "datetime": "2019-06-20 16:14:10", "author": "@jigarkdoshi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143408052286164992": {"followers": "103", "datetime": "2019-06-25 06:38:17", "author": "@testphys", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1194304254439120896": {"followers": "205", "datetime": "2019-11-12 17:21:38", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141598245577056256": {"followers": "54", "datetime": "2019-06-20 06:46:46", "author": "@ttnam93", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143558933086396416": {"followers": "481", "datetime": "2019-06-25 16:37:50", "author": "@john_maverick", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143723593940885504": {"followers": "2,387", "datetime": "2019-06-26 03:32:08", "author": "@avalon1982", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143784772860145664": {"followers": "2,681", "datetime": "2019-06-26 07:35:15", "author": "@tacmasi", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141657297099214848": {"followers": "72", "datetime": "2019-06-20 10:41:25", "author": "@kotaaaa1110", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141690386508894208": {"followers": "77,472", "datetime": "2019-06-20 12:52:54", "author": "@rsalakhu", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1162698085618913282": {"followers": "689", "datetime": "2019-08-17 12:10:00", "author": "@BenSingletonNYC", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding #DataScience #BigData https://t.co/v7wdH4j6rT"}, "1141700375642292224": {"followers": "268", "datetime": "2019-06-20 13:32:35", "author": "@moisesvw", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1144183789570134018": {"followers": "222", "datetime": "2019-06-27 10:00:47", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 2,971 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1161238780067471360": {"followers": "1,315", "datetime": "2019-08-13 11:31:14", "author": "@eb00298", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142515725187330053": {"followers": "66", "datetime": "2019-06-22 19:32:30", "author": "@shubh_300595", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141544552605605888": {"followers": "1,429", "datetime": "2019-06-20 03:13:24", "author": "@jreuben1", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/QXuSCVZLSk"}, "1219650794821210114": {"followers": "64", "datetime": "2020-01-21 15:59:44", "author": "@ruep3l11", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1143626246292738048": {"followers": "232", "datetime": "2019-06-25 21:05:19", "author": "@augeas", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141678489789186048": {"followers": "598", "datetime": "2019-06-20 12:05:37", "author": "@chrisaballard", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141666314802438144": {"followers": "7", "datetime": "2019-06-20 11:17:15", "author": "@DeepLearnngMonk", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145057090798534656": {"followers": "687", "datetime": "2019-06-29 19:50:59", "author": "@innovimax", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141592876385652737": {"followers": "292", "datetime": "2019-06-20 06:25:26", "author": "@_sucream", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141780132899590144": {"followers": "102", "datetime": "2019-06-20 18:49:31", "author": "@Jaambvant", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1142386731997491200": {"followers": "509", "datetime": "2019-06-22 10:59:56", "author": "@bhagirathl", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141697780307628032": {"followers": "111", "datetime": "2019-06-20 13:22:17", "author": "@betterhn50", "content_summary": "50 \u2013 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Z1uxF9bVUm"}, "1142018028059500546": {"followers": "82", "datetime": "2019-06-21 10:34:50", "author": "@RyoHWS", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141583670496546816": {"followers": "18", "datetime": "2019-06-20 05:48:51", "author": "@tadityasrinivas", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142490018126159875": {"followers": "25", "datetime": "2019-06-22 17:50:21", "author": "@CezaryMierzejek", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143397090535583744": {"followers": "608", "datetime": "2019-06-25 05:54:44", "author": "@sanjaykamath", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143464548239724544": {"followers": "398", "datetime": "2019-06-25 10:22:47", "author": "@srinathsmn", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142103110631878656": {"followers": "129", "datetime": "2019-06-21 16:12:55", "author": "@Maninthemillor", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141711993264660480": {"followers": "3,651", "datetime": "2019-06-20 14:18:45", "author": "@adjiboussodieng", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143483648529838080": {"followers": "1,447", "datetime": "2019-06-25 11:38:41", "author": "@__olamilekan__", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143530367455911936": {"followers": "13", "datetime": "2019-06-25 14:44:20", "author": "@adityankur", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141627825717252096": {"followers": "182", "datetime": "2019-06-20 08:44:18", "author": "@Dak_ssh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141615435709763584": {"followers": "1,868", "datetime": "2019-06-20 07:55:04", "author": "@Beeeender", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142062284153544705": {"followers": "85", "datetime": "2019-06-21 13:30:41", "author": "@bathia_mkwr", "content_summary": "XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: https://t.co/ksHgxadI6f github (code + pretrained models): https://t.co/pmX0P5ARCb Medium: https://t.co/ClPyoznPnG https://t.co"}, "1141559232820592640": {"followers": "2,029", "datetime": "2019-06-20 04:11:44", "author": "@srchvrs", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142150804171657216": {"followers": "54", "datetime": "2019-06-21 19:22:26", "author": "@jsdelfino", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141689117652029440": {"followers": "449", "datetime": "2019-06-20 12:47:51", "author": "@michalwols", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141754366820401152": {"followers": "11", "datetime": "2019-06-20 17:07:08", "author": "@RETdW3BccikwmHT", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143690475645177857": {"followers": "558", "datetime": "2019-06-26 01:20:32", "author": "@i4mwh4ti4m", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1143867469662052353": {"followers": "824", "datetime": "2019-06-26 13:03:51", "author": "@morioka", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141571756051714049": {"followers": "12,765", "datetime": "2019-06-20 05:01:30", "author": "@jaguring1", "content_summary": "RT @reddit_ml: [R] XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE,... https:/\u2026"}, "1141956812838592519": {"followers": "31", "datetime": "2019-06-21 06:31:35", "author": "@namanjn1998", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141691229408829441": {"followers": "8,374", "datetime": "2019-06-20 12:56:15", "author": "@mldcmu", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141730475318243329": {"followers": "7,324", "datetime": "2019-06-20 15:32:12", "author": "@eman1972", "content_summary": "RT @smochi_pub: \u65e9\u304f\u3082BERT\u8d85\u3048\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u304d\u305f\u2026\uff01 https://t.co/86pHSazItX"}, "1143399561580482560": {"followers": "313", "datetime": "2019-06-25 06:04:33", "author": "@ThomasNiebler", "content_summary": "Just imagine how smaller teams can afford to achieve a new SOTA. Either they have to find approaches that are way more efficient and are thus more cost-efficient or they don't."}, "1141521837471543297": {"followers": "4,812", "datetime": "2019-06-20 01:43:09", "author": "@IntuitMachine", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1143649419671351297": {"followers": "63", "datetime": "2019-06-25 22:37:24", "author": "@_madved", "content_summary": "Luxury #ML :)"}, "1141697219386630144": {"followers": "108", "datetime": "2019-06-20 13:20:03", "author": "@AlShafi077", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141585262444593152": {"followers": "40", "datetime": "2019-06-20 05:55:10", "author": "@kale_divergence", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141892852009431040": {"followers": "573", "datetime": "2019-06-21 02:17:25", "author": "@Jeeva_G", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141924968516608007": {"followers": "12,765", "datetime": "2019-06-21 04:25:03", "author": "@jaguring1", "content_summary": "RT @tankazunori0914: BERT\u8d85\u3048\u306f\u3084\u3044\u306a\u3002\u3002 https://t.co/kQyBj1fAQv"}, "1141563998204129280": {"followers": "727", "datetime": "2019-06-20 04:30:41", "author": "@wenmingye", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141895553460166656": {"followers": "41", "datetime": "2019-06-21 02:28:09", "author": "@ElectroDod", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141787659213447169": {"followers": "145", "datetime": "2019-06-20 19:19:26", "author": "@esvhd", "content_summary": "RT @ChrSzegedy: Wow! https://t.co/09A58FFsKU"}, "1143452351795122176": {"followers": "59", "datetime": "2019-06-25 09:34:19", "author": "@flapdoodle_sand", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141735857260507137": {"followers": "101", "datetime": "2019-06-20 15:53:35", "author": "@MundraShivansh", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1163761676665049088": {"followers": "0", "datetime": "2019-08-20 10:36:20", "author": "@tuanle173", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1158842409394089987": {"followers": "342", "datetime": "2019-08-06 20:48:55", "author": "@1vnzh", "content_summary": "RT @hoveidar: I'll be facilitating an interesting talk on the XLNet paper today. So come and join us if you are in Toronto, or join the liv\u2026"}, "1141957465698971648": {"followers": "593", "datetime": "2019-06-21 06:34:11", "author": "@rasmusbergpalm", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141815070348984323": {"followers": "241", "datetime": "2019-06-20 21:08:21", "author": "@SashoSavkov", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143765031663951872": {"followers": "252", "datetime": "2019-06-26 06:16:48", "author": "@_DLPBGJ80C04Z_", "content_summary": "RT @IAmSamFin: This is a lot of $$$, and one could raise solid q's about the cost-benefit to society if ML research increasingly focuses on\u2026"}, "1143959427306983425": {"followers": "245", "datetime": "2019-06-26 19:09:15", "author": "@__jm", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141519844099182594": {"followers": "115", "datetime": "2019-06-20 01:35:13", "author": "@shohdi2", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141887369962020869": {"followers": "124", "datetime": "2019-06-21 01:55:38", "author": "@Santiag72427700", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143890237740998661": {"followers": "2,478", "datetime": "2019-06-26 14:34:19", "author": "@oyoyoy", "content_summary": "@yoavgo @annargrs @chipro Pardon me for barging in, but it seems like an interesting problem https://t.co/Vs0MzSjuLc"}, "1143509095317757952": {"followers": "245", "datetime": "2019-06-25 13:19:48", "author": "@yazdavar", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1160894741778616322": {"followers": "10", "datetime": "2019-08-12 12:44:09", "author": "@a_kin_ai", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143084753232338944": {"followers": "608", "datetime": "2019-06-24 09:13:37", "author": "@1997synohro", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1142473879312453632": {"followers": "2,671", "datetime": "2019-06-22 16:46:13", "author": "@ayirpelle", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1143429895495790592": {"followers": "138", "datetime": "2019-06-25 08:05:05", "author": "@gogothorr", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141908320082485249": {"followers": "78", "datetime": "2019-06-21 03:18:53", "author": "@watchdog20xx", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143578555344805889": {"followers": "1,118", "datetime": "2019-06-25 17:55:48", "author": "@jsteward2930", "content_summary": "RT @atulbutte: Luckily it costs a lot less to train a child how to understand language\u2026 HT @IAmSamFin https://t.co/BZA3fX2A3U"}, "1141511813709717504": {"followers": "18,260", "datetime": "2019-06-20 01:03:19", "author": "@quocleix", "content_summary": "XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: https://t.co/C1tFMwZvyW github (code + pretrained models): https://t.co/kI4jsVzT1u with Zhilin Yang, @ZihangDai, Yiming Yang, Jaim"}, "1158830197497638913": {"followers": "39", "datetime": "2019-08-06 20:00:23", "author": "@hoveidar", "content_summary": "I'll be facilitating an interesting talk on the XLNet paper today. So come and join us if you are in Toronto, or join the live streaming online #AISC #XLNet @AISC_TO: Paper: https://t.co/7BvAcn1GHo https://t.co/eV7NriIge2"}, "1141819889201627142": {"followers": "25,311", "datetime": "2019-06-20 21:27:30", "author": "@deeplearning4j", "content_summary": "Better than BERT! XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/b0uBVcbv2N #deeplearning"}, "1141751817929408513": {"followers": "123", "datetime": "2019-06-20 16:57:00", "author": "@Yeungy1212", "content_summary": "And BERT was only a few months ago."}, "1142021887091867648": {"followers": "91", "datetime": "2019-06-21 10:50:10", "author": "@8salahuddinkhan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143467226042949633": {"followers": "929", "datetime": "2019-06-25 10:33:25", "author": "@wzuidema", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141541188958380032": {"followers": "17,703", "datetime": "2019-06-20 03:00:02", "author": "@HNTweets", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding: https://t.co/BjAUO3VsuT Comments: https://t.co/ZIJ30Cvar5"}, "1141917531352866817": {"followers": "620", "datetime": "2019-06-21 03:55:29", "author": "@apsdehal", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141676015279890432": {"followers": "89", "datetime": "2019-06-20 11:55:48", "author": "@sachan_devendra", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141945598561538048": {"followers": "1,187", "datetime": "2019-06-21 05:47:01", "author": "@EkAurBottleLa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143459362452561920": {"followers": "1,044", "datetime": "2019-06-25 10:02:11", "author": "@drpuffa", "content_summary": "Yep, cutting-edge #ML costs a lot. It's not going to be too long until we start seeing a lot of new approaches that are computationally less expensive. #hybridAI #AI"}, "1143485420874997766": {"followers": "3,020", "datetime": "2019-06-25 11:45:43", "author": "@h_thoreson", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143607064427737089": {"followers": "203", "datetime": "2019-06-25 19:49:06", "author": "@a_Bill", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1142503191285813249": {"followers": "66", "datetime": "2019-06-22 18:42:42", "author": "@shubh_300595", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1143516688220151809": {"followers": "1,408", "datetime": "2019-06-25 13:49:58", "author": "@mobilepixel", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141847376648216576": {"followers": "1,192", "datetime": "2019-06-20 23:16:43", "author": "@windx0303", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141770729358336000": {"followers": "125", "datetime": "2019-06-20 18:12:09", "author": "@nickdaleburns", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1143856976226598912": {"followers": "163,852", "datetime": "2019-06-26 12:22:09", "author": "@ceobillionaire", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1141938420136271873": {"followers": "72", "datetime": "2019-06-21 05:18:30", "author": "@jonasrbati", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143512659695931393": {"followers": "412", "datetime": "2019-06-25 13:33:58", "author": "@crscardellino", "content_summary": "Ten\u00e9s algo de plata que te sobre? Quiz\u00e1s pod\u00e9s invertir en tratar de ganarle a BERT..."}, "1141580878130438144": {"followers": "147", "datetime": "2019-06-20 05:37:45", "author": "@FR_Chaumartin", "content_summary": "XLNet : un apprentissage de mod\u00e8le de langue qui donne des r\u00e9sultats encore meilleurs que BERT#NLP #DeepLearning"}, "1141600500997079040": {"followers": "4,255", "datetime": "2019-06-20 06:55:44", "author": "@weballergy", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1144538272133259265": {"followers": "511", "datetime": "2019-06-28 09:29:23", "author": "@soobrosa", "content_summary": "RT @flavioclesio: Holy Jesus! https://t.co/efC7LKkNLz"}, "1143461361932013568": {"followers": "248", "datetime": "2019-06-25 10:10:07", "author": "@UsMeUApp", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141879750396276736": {"followers": "71", "datetime": "2019-06-21 01:25:22", "author": "@hjguyhan", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141965066167652352": {"followers": "78", "datetime": "2019-06-21 07:04:23", "author": "@watchdog20xx", "content_summary": "RT @shohomiura: BERT\u3092\u8d85\u3048\u305f\u30e2\u30c7\u30ebcoming out! https://t.co/kIAYdheNPU"}, "1142056170548543490": {"followers": "357", "datetime": "2019-06-21 13:06:24", "author": "@_florianmai", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141994229239046144": {"followers": "1,793", "datetime": "2019-06-21 09:00:16", "author": "@esXFdfOJxiGBFLx", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141649842453798912": {"followers": "772", "datetime": "2019-06-20 10:11:47", "author": "@arxivml", "content_summary": "\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\", Zhilin Yang, Zihang Dai, Yiming Yang, J\u2026 https://t.co/N7YyzizAHU"}, "1141884417868042240": {"followers": "701", "datetime": "2019-06-21 01:43:55", "author": "@Nidhiya_V_Raj", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143592949604896768": {"followers": "2,642", "datetime": "2019-06-25 18:53:00", "author": "@Redo", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/y4b7vkF0xj"}, "1141709635138674688": {"followers": "230", "datetime": "2019-06-20 14:09:23", "author": "@ko_ash", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143374611716956160": {"followers": "1,288", "datetime": "2019-06-25 04:25:24", "author": "@heghbalz", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143078670283657217": {"followers": "19", "datetime": "2019-06-24 08:49:27", "author": "@lixx3527", "content_summary": "https://t.co/lyoXwHQ3GS"}, "1141578762041139201": {"followers": "9", "datetime": "2019-06-20 05:29:21", "author": "@cypark424", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142619815439458304": {"followers": "2,391", "datetime": "2019-06-23 02:26:07", "author": "@alkaf2012", "content_summary": "RT @malhamid: \u0646\u0634\u0631 \u0642\u0628\u0644 \u064a\u0648\u0645\u064a\u0646 \u0648\u0631\u0642\u0629 \u0628\u062d\u062b \u0645\u0645\u064a\u0632\u0629 \u0641\u064a \u0646\u0633\u062e\u062a\u0647\u0627 \u0627\u0644\u0623\u0648\u0644\u064a\u0629 \u0639\u0646 \u0646\u0645\u0648\u0630\u062c #\u062a\u0639\u0644\u0645_\u0627\u0644\u0622\u0644\u0629 \u0627\u0637\u0644\u0642 \u0639\u0644\u064a\u0647 (XLNet) \u064a\u062a\u0641\u0648\u0642 \u0639\u0644\u0649 \u0646\u0645\u0648\u0630\u062c (BERT) \u0628\u0647\u0627\u0645\u0634 \u0643\u0628\u064a\u0631. \u0648\u064a\u0645\u062b\u2026"}, "1239102140980723712": {"followers": "476", "datetime": "2020-03-15 08:12:26", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141745326417747968": {"followers": "1,141", "datetime": "2019-06-20 16:31:13", "author": "@JulioGonzalo1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143947648623403008": {"followers": "1,060", "datetime": "2019-06-26 18:22:27", "author": "@loretoparisi", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141590066516520960": {"followers": "392", "datetime": "2019-06-20 06:14:16", "author": "@AlexGuoHan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141630122237276161": {"followers": "222", "datetime": "2019-06-20 08:53:26", "author": "@sannikpatel", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141732037960900608": {"followers": "53", "datetime": "2019-06-20 15:38:24", "author": "@RandomNumberG8r", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141828908142211073": {"followers": "887", "datetime": "2019-06-20 22:03:20", "author": "@fanichet", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141697063387848704": {"followers": "250", "datetime": "2019-06-20 13:19:26", "author": "@hackernewsrobot", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/xmri02IlRt"}, "1164980917187821568": {"followers": "42", "datetime": "2019-08-23 19:21:09", "author": "@MishakinSergey", "content_summary": "RT @PapersTrending: [10/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 11,113 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JS\u2026"}, "1142839553725739008": {"followers": "38", "datetime": "2019-06-23 16:59:17", "author": "@spallasblog", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142175755876614144": {"followers": "22", "datetime": "2019-06-21 21:01:35", "author": "@ThanhDacTran", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141870861332467714": {"followers": "625", "datetime": "2019-06-21 00:50:02", "author": "@sussenglish", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141678831620616192": {"followers": "122", "datetime": "2019-06-20 12:06:59", "author": "@cryptexcode", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142047731810107392": {"followers": "1,163", "datetime": "2019-06-21 12:32:52", "author": "@iBotamon", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1141811875669250048": {"followers": "5", "datetime": "2019-06-20 20:55:39", "author": "@eofnull", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143676054214795264": {"followers": "1,210", "datetime": "2019-06-26 00:23:14", "author": "@infosecanon", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141795650511790087": {"followers": "17", "datetime": "2019-06-20 19:51:11", "author": "@prathamd451", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141544637586264064": {"followers": "15", "datetime": "2019-06-20 03:13:45", "author": "@The_UBD", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143947573578747904": {"followers": "5,581", "datetime": "2019-06-26 18:22:09", "author": "@Tim_Dettmers", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143417823731101701": {"followers": "1,117", "datetime": "2019-06-25 07:17:07", "author": "@_dmh", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142009462749876225": {"followers": "929", "datetime": "2019-06-21 10:00:48", "author": "@EvpokPadding", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141696321679765505": {"followers": "1,460", "datetime": "2019-06-20 13:16:29", "author": "@towards_AI", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1164563793357074432": {"followers": "191", "datetime": "2019-08-22 15:43:39", "author": "@Jun_Morphin", "content_summary": "RT @gijigae: \u53bb\u5e74\u8a71\u984c\u3068\u306a\u3063\u3066\u305f\u3001BERT\u3002 \u30b0\u30fc\u30b0\u30eb\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u9762\u3067BERT\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u65b0\u3057\u3044\u30e2\u30c7\u30eb\uff08XLNet\uff09\u3092\u767a\u8868\u3002SQuAD2.0\u3067\u306f\uff17\u30dd\u30a4\u30f3\u30c8\u3082\u30a2\u30c3\u30d7\ud83d\ude80\u3002 \u25a0\u4eba\u9593\uff1a86.831 \u25a0XLNet\uff1a86.346 \u25a0BERT\uff1a78.98 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u80fd\u2026"}, "1147289744067350529": {"followers": "173", "datetime": "2019-07-05 23:42:45", "author": "@matthiasblume", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142385191220064256": {"followers": "76", "datetime": "2019-06-22 10:53:48", "author": "@Waydze1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141697509187903489": {"followers": "6,205", "datetime": "2019-06-20 13:21:12", "author": "@newsyc50", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/qHLYcdMVei (https://t.co/bSCXHwRUi8)"}, "1143075158778875905": {"followers": "544", "datetime": "2019-06-24 08:35:29", "author": "@altamborrino", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143868969855705089": {"followers": "179,069", "datetime": "2019-06-26 13:09:49", "author": "@Montreal_AI", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1141545912382050304": {"followers": "428", "datetime": "2019-06-20 03:18:49", "author": "@Jotarun", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142345992743706630": {"followers": "1,268", "datetime": "2019-06-22 08:18:03", "author": "@dlowd", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141578981654896640": {"followers": "15,960", "datetime": "2019-06-20 05:30:13", "author": "@newsyc20", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/l7HRb2wleW (https://t.co/FA8pqasrSr)"}, "1143817525983158272": {"followers": "202", "datetime": "2019-06-26 09:45:23", "author": "@Lisamedrouk", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141707465819598849": {"followers": "49", "datetime": "2019-06-20 14:00:46", "author": "@Mocha_320", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141661002095718400": {"followers": "12,765", "datetime": "2019-06-20 10:56:08", "author": "@jaguring1", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1143993880926412802": {"followers": "112", "datetime": "2019-06-26 21:26:10", "author": "@RoelvanEst", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141539111355867139": {"followers": "7,942", "datetime": "2019-06-20 02:51:47", "author": "@angsuman", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/jmmDYr7HwN"}, "1145586957046747136": {"followers": "111", "datetime": "2019-07-01 06:56:29", "author": "@milanomrc", "content_summary": "Definitely not a cheap model \ud83d\ude42"}, "1141900768036810752": {"followers": "220", "datetime": "2019-06-21 02:48:53", "author": "@zigzagzackey", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1141525289953972225": {"followers": "564", "datetime": "2019-06-20 01:56:52", "author": "@gazay", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141711600560115712": {"followers": "53", "datetime": "2019-06-20 14:17:12", "author": "@MktO740123", "content_summary": "RT @stomohide: \u8ad6\u6587\u51fa\u3066\u307e\u3057\u305f\u3002 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Dl10X4T442"}, "1142005794998800384": {"followers": "251", "datetime": "2019-06-21 09:46:13", "author": "@chaololo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141633262437056513": {"followers": "10", "datetime": "2019-06-20 09:05:54", "author": "@yarphs", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141845605121220609": {"followers": "784", "datetime": "2019-06-20 23:09:41", "author": "@GTARobotics", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142813064380575744": {"followers": "42", "datetime": "2019-06-23 15:14:01", "author": "@m4ki02", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141520036353302528": {"followers": "19,440", "datetime": "2019-06-20 01:35:59", "author": "@Sam_Witteveen", "content_summary": "This looks super impressive!!"}, "1141768426593144833": {"followers": "3,491", "datetime": "2019-06-20 18:03:00", "author": "@angelina", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143725519478779905": {"followers": "3,187", "datetime": "2019-06-26 03:39:47", "author": "@Maxwell_110", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141526774859390978": {"followers": "206", "datetime": "2019-06-20 02:02:46", "author": "@amaru_muru", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143452708738936832": {"followers": "1,677", "datetime": "2019-06-25 09:35:44", "author": "@libbykinsey", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1156142724958969857": {"followers": "222", "datetime": "2019-07-30 10:01:20", "author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 9,999 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JT91Nk - \ud83d\udd17 https://t.co/RdEWpQ3pBp"}, "1141747442490232834": {"followers": "49", "datetime": "2019-06-20 16:39:37", "author": "@ysrknshl", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141563010630090752": {"followers": "277", "datetime": "2019-06-20 04:26:45", "author": "@praateekmahajan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141618099411701760": {"followers": "395", "datetime": "2019-06-20 08:05:39", "author": "@jeffmgould", "content_summary": "Google makes another breakthrough in deep learning models for language understanding. But it took extraordinary compute resources. If you don't have a boatload of Google's own TPU chips and near-infinite RAM, you will need \"32 to 128\" Nvidia GPUs to matc"}, "1216190074775314432": {"followers": "4,592", "datetime": "2020-01-12 02:48:04", "author": "@TheCuriousLuke", "content_summary": "RT @Deep_In_Depth: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/cgMIUDyRmD #DeepLearning #NeuralNe\u2026"}, "1143767622913126400": {"followers": "496", "datetime": "2019-06-26 06:27:06", "author": "@neshkatrapati", "content_summary": "Just why ! ?"}, "1141965354664636416": {"followers": "134", "datetime": "2019-06-21 07:05:31", "author": "@jemtzl", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141712959477047296": {"followers": "5,722", "datetime": "2019-06-20 14:22:36", "author": "@t2pitchy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141912717919309825": {"followers": "153", "datetime": "2019-06-21 03:36:22", "author": "@zakki", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141732350197288960": {"followers": "282", "datetime": "2019-06-20 15:39:39", "author": "@hmita_2011", "content_summary": "RT @smochi_pub: \u65e9\u304f\u3082BERT\u8d85\u3048\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u304d\u305f\u2026\uff01 https://t.co/86pHSazItX"}, "1141818477939965958": {"followers": "98", "datetime": "2019-06-20 21:21:53", "author": "@asiedubrempong", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141523912368562176": {"followers": "176", "datetime": "2019-06-20 01:51:23", "author": "@BertNieves", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1143464788481269761": {"followers": "4,053", "datetime": "2019-06-25 10:23:44", "author": "@Vilinthril", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143733399892316161": {"followers": "143", "datetime": "2019-06-26 04:11:06", "author": "@n_kats_", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1171894156941070336": {"followers": "476", "datetime": "2019-09-11 21:11:54", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143756030897082369": {"followers": "2,463", "datetime": "2019-06-26 05:41:02", "author": "@CocoaGeek", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141894597737615360": {"followers": "45", "datetime": "2019-06-21 02:24:22", "author": "@PM_Wolff", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143403782161817600": {"followers": "742", "datetime": "2019-06-25 06:21:19", "author": "@oren_data", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143354094985326592": {"followers": "3,020", "datetime": "2019-06-25 03:03:53", "author": "@h_thoreson", "content_summary": "I sometimes think computing has become the modern horse. Feudal societies were organized in large part around feeding and training horses. Today we farm electricity to feed computers. #machinelearning"}, "1142049231613546496": {"followers": "216", "datetime": "2019-06-21 12:38:49", "author": "@KSKSKSKS2", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142322936533200901": {"followers": "212", "datetime": "2019-06-22 06:46:26", "author": "@fulowa", "content_summary": "https://t.co/Njuw1v9jPx"}, "1141578339934789633": {"followers": "821", "datetime": "2019-06-20 05:27:40", "author": "@zdepablo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141661469844447232": {"followers": "470", "datetime": "2019-06-20 10:58:00", "author": "@SrirangaTarun", "content_summary": "We now have a model that surpasses BERT in several NLP tasks ! The rate at which this field is moving is absolutely crazy !"}, "1142747090495451137": {"followers": "201", "datetime": "2019-06-23 10:51:52", "author": "@francescospiu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144443258723921920": {"followers": "234", "datetime": "2019-06-28 03:11:50", "author": "@quantumbtc", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1143937408917889024": {"followers": "192", "datetime": "2019-06-26 17:41:46", "author": "@yonaga", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143647134429978625": {"followers": "11", "datetime": "2019-06-25 22:28:19", "author": "@ncsquirll99", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141578603605504000": {"followers": "156", "datetime": "2019-06-20 05:28:43", "author": "@PiotrZelasko", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142007944378118145": {"followers": "824", "datetime": "2019-06-21 09:54:46", "author": "@morioka", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1219692231638077440": {"followers": "48", "datetime": "2020-01-21 18:44:23", "author": "@MarioMusu5", "content_summary": "RT @steadydee: At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will exp\u2026"}, "1143396594143956992": {"followers": "131", "datetime": "2019-06-25 05:52:45", "author": "@bendi007", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141768040574533632": {"followers": "4", "datetime": "2019-06-20 18:01:28", "author": "@QishangCheng", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143710135576977408": {"followers": "271", "datetime": "2019-06-26 02:38:40", "author": "@sakura314159265", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1143009122339577856": {"followers": "133", "datetime": "2019-06-24 04:13:05", "author": "@sakaikuro5", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1142284858913259520": {"followers": "133", "datetime": "2019-06-22 04:15:07", "author": "@salamander_jp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143738924608577537": {"followers": "143", "datetime": "2019-06-26 04:33:03", "author": "@n_kats_", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1145671550609195008": {"followers": "50", "datetime": "2019-07-01 12:32:37", "author": "@sea85419", "content_summary": "\u30cf\u30cf\u30cf"}, "1143080068937322496": {"followers": "81", "datetime": "2019-06-24 08:55:00", "author": "@willy_au", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144253053371109376": {"followers": "177", "datetime": "2019-06-27 14:36:01", "author": "@agodika", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142759773097476101": {"followers": "2,905", "datetime": "2019-06-23 11:42:16", "author": "@osaremochi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141871612767031297": {"followers": "65", "datetime": "2019-06-21 00:53:02", "author": "@_dongkwan_kim", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142645874482282497": {"followers": "242", "datetime": "2019-06-23 04:09:40", "author": "@tkrk_p", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1213289174809358336": {"followers": "3,500", "datetime": "2020-01-04 02:40:55", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1232179177266089984": {"followers": "5,396", "datetime": "2020-02-25 05:43:03", "author": "@hsianghui", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding #nowreading https://t.co/nCZurod1w7"}, "1144092907043000320": {"followers": "665", "datetime": "2019-06-27 03:59:39", "author": "@adelong", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141614726365032449": {"followers": "442", "datetime": "2019-06-20 07:52:15", "author": "@AdrianB82", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141614661307179008": {"followers": "10,655", "datetime": "2019-06-20 07:52:00", "author": "@kaalam_ai", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141668546239447040": {"followers": "152", "datetime": "2019-06-20 11:26:07", "author": "@hsviscarra", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141789817283518470": {"followers": "699", "datetime": "2019-06-20 19:28:00", "author": "@ulliwaltinger", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141851592104091650": {"followers": "4,037", "datetime": "2019-06-20 23:33:28", "author": "@dwhitena", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141691633559228417": {"followers": "12,765", "datetime": "2019-06-20 12:57:51", "author": "@jaguring1", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141696452000919554": {"followers": "733", "datetime": "2019-06-20 13:17:00", "author": "@Pythonner", "content_summary": "RT @eturner303: Remember BERT? Meet XLNet. Very impressive results on Squad 1.1+2.0 QA tasks and many other datasets. Impressive how fas\u2026"}, "1143724808011563008": {"followers": "696", "datetime": "2019-06-26 03:36:58", "author": "@takatoh1", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143834158986342400": {"followers": "747", "datetime": "2019-06-26 10:51:29", "author": "@ericcharton", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143750169760022528": {"followers": "27", "datetime": "2019-06-26 05:17:44", "author": "@dhruvkr1993", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141721867973214208": {"followers": "330", "datetime": "2019-06-20 14:58:00", "author": "@mr_ubik", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141700795915653120": {"followers": "185", "datetime": "2019-06-20 13:34:16", "author": "@CyrusMaher", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141683837694435328": {"followers": "30", "datetime": "2019-06-20 12:26:53", "author": "@gbiwer", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1141694355788316672": {"followers": "7,503", "datetime": "2019-06-20 13:08:40", "author": "@HigeponJa", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141630236011798528": {"followers": "19", "datetime": "2019-06-20 08:53:53", "author": "@lostinio", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141718019594264578": {"followers": "16", "datetime": "2019-06-20 14:42:42", "author": "@pro_about_pro", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141795489429504000": {"followers": "28,418", "datetime": "2019-06-20 19:50:32", "author": "@ogrisel", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1219591358756872192": {"followers": "247", "datetime": "2020-01-21 12:03:33", "author": "@Ben7Faz", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1176436494799691777": {"followers": "222", "datetime": "2019-09-24 10:01:32", "author": "@PapersTrending", "content_summary": "[3/10] \ud83d\udcc8 - fast-bert - 507 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1147807874170667009": {"followers": "222", "datetime": "2019-07-07 10:01:37", "author": "@PapersTrending", "content_summary": "[9/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 3,578 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141592155783254016": {"followers": "30", "datetime": "2019-06-20 06:22:34", "author": "@chenfeiyang2018", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141587518669938693": {"followers": "43", "datetime": "2019-06-20 06:04:08", "author": "@jeandut14000", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141864327319674880": {"followers": "308", "datetime": "2019-06-21 00:24:05", "author": "@NotFounds8080", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141820044826927104": {"followers": "93", "datetime": "2019-06-20 21:28:07", "author": "@0xhexhex", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141942548950159361": {"followers": "415", "datetime": "2019-06-21 05:34:54", "author": "@__nggih", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141539733274877952": {"followers": "2", "datetime": "2019-06-20 02:54:15", "author": "@feng_dapeng", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143893683399536640": {"followers": "3,020", "datetime": "2019-06-26 14:48:01", "author": "@h_thoreson", "content_summary": "@housesciencegop No central planning. Consult economists too https://t.co/yXWdoDnCCx"}, "1141593098520281088": {"followers": "20", "datetime": "2019-06-20 06:26:19", "author": "@Boristream", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143353080077008899": {"followers": "13,798", "datetime": "2019-06-25 02:59:51", "author": "@mark_riedl", "content_summary": "That is 4x the average salary in the US and 9.5x the poverty line."}, "1170371546268852224": {"followers": "205", "datetime": "2019-09-07 16:21:35", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142119323848171521": {"followers": "375", "datetime": "2019-06-21 17:17:21", "author": "@erikadmenezes", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141877473396887552": {"followers": "652", "datetime": "2019-06-21 01:16:19", "author": "@phtully", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142037408705159168": {"followers": "47", "datetime": "2019-06-21 11:51:50", "author": "@akixkat", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141677529327124480": {"followers": "262", "datetime": "2019-06-20 12:01:48", "author": "@katli_01", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141730229876137984": {"followers": "79,158", "datetime": "2019-06-20 15:31:13", "author": "@machinelearnflx", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141834039306928128": {"followers": "172", "datetime": "2019-06-20 22:23:43", "author": "@ggdupont", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141863027492265984": {"followers": "128", "datetime": "2019-06-21 00:18:55", "author": "@ekshakhs", "content_summary": "New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lms with auto encoding lms in a clever way. Very cleanly written paper!"}, "1143896764769918978": {"followers": "2,601", "datetime": "2019-06-26 15:00:15", "author": "@Nklarer", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141560440519974912": {"followers": "12,765", "datetime": "2019-06-20 04:16:32", "author": "@jaguring1", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1142095409705185280": {"followers": "314", "datetime": "2019-06-21 15:42:19", "author": "@savanvisalpara7", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141645106610446336": {"followers": "55", "datetime": "2019-06-20 09:52:58", "author": "@jaisalmer992", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141725073063075840": {"followers": "50", "datetime": "2019-06-20 15:10:44", "author": "@12kay226", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1143718575703646208": {"followers": "769", "datetime": "2019-06-26 03:12:12", "author": "@bigtechprof", "content_summary": "RT @atulbutte: Luckily it costs a lot less to train a child how to understand language\u2026 HT @IAmSamFin https://t.co/BZA3fX2A3U"}, "1146695560746766336": {"followers": "205", "datetime": "2019-07-04 08:21:40", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1166884663073263616": {"followers": "27", "datetime": "2019-08-29 01:25:58", "author": "@andysun1222", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk https://t.co/owzX13zbgg #DeepLearnin\u2026"}, "1143760468949000193": {"followers": "9", "datetime": "2019-06-26 05:58:40", "author": "@EranKamber", "content_summary": "Yet another step in the NLP world. These benchmarks are getting harder to follow \ud83d\ude05"}, "1141546978590900224": {"followers": "360", "datetime": "2019-06-20 03:23:03", "author": "@smendozab", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141718672110583811": {"followers": "327", "datetime": "2019-06-20 14:45:18", "author": "@cdathuraliya", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141939577692622848": {"followers": "21", "datetime": "2019-06-21 05:23:06", "author": "@thapraveensingh", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1141655588348125185": {"followers": "271", "datetime": "2019-06-20 10:34:37", "author": "@overfitted", "content_summary": "\u0642\u0634\u0646\u06af \u06a9\u06cc\u0644\u0648\u06cc\u06cc \u0634\u062f\u0647!! \u0647\u0631 \u062f\u0648 \u0645\u0627\u0647 \u06cc\u06a9\u0628\u0627\u0631 \u06cc\u0647 \u06af\u0631\u0648\u0647 \u0627\u0632 \u06a9\u0645\u067e\u0627\u0646\u06cc\u0647\u0627\u06cc \u062e\u0641\u0646 \u06cc\u0647 \u0645\u062f\u0644 \u0645\u06cc\u062f\u0647 \u06a9\u0647 \u0686\u0646\u062f \u062f\u0647\u0645 \u0628\u0646\u0686\u0645\u0627\u0631\u06a9\u0647\u0627 \u0631\u0648 \u0628\u0647\u0628\u0648\u062f \u0645\u06cc\u062f\u0647! \u0627\u06cc\u0646\u0645 \u0634\u062f \u0631\u06cc\u0633\u0631\u0686 \u0627\u062e\u0647!!!"}, "1141880927976509445": {"followers": "66", "datetime": "2019-06-21 01:30:03", "author": "@tenzinchang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141565590026375169": {"followers": "476", "datetime": "2019-06-20 04:37:00", "author": "@PuncozNepal", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141711584516902913": {"followers": "195", "datetime": "2019-06-20 14:17:08", "author": "@camchenry", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141929975194558464": {"followers": "4,754", "datetime": "2019-06-21 04:44:56", "author": "@rzembo", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141622214384599040": {"followers": "45", "datetime": "2019-06-20 08:22:00", "author": "@DeepMatrixCloud", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142433237286961152": {"followers": "378", "datetime": "2019-06-22 14:04:43", "author": "@RyanAEMetz", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1143584710490238976": {"followers": "489", "datetime": "2019-06-25 18:20:16", "author": "@nicib83", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141679746691977218": {"followers": "154", "datetime": "2019-06-20 12:10:37", "author": "@BharatBKaul", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141513753281777664": {"followers": "166", "datetime": "2019-06-20 01:11:01", "author": "@d_q_nguyen", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145332939380711424": {"followers": "20", "datetime": "2019-06-30 14:07:06", "author": "@MihailSalnikov", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141666991796891648": {"followers": "80", "datetime": "2019-06-20 11:19:56", "author": "@MattShardlow", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141889472042295301": {"followers": "91", "datetime": "2019-06-21 02:04:00", "author": "@dalcimar", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143812757470371840": {"followers": "280", "datetime": "2019-06-26 09:26:27", "author": "@noahcse", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143831410370678784": {"followers": "841", "datetime": "2019-06-26 10:40:34", "author": "@a2uky", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1212911836615589889": {"followers": "3,500", "datetime": "2020-01-03 01:41:31", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141586352762322944": {"followers": "86", "datetime": "2019-06-20 05:59:30", "author": "@mbodhisattwa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141737384003985408": {"followers": "468", "datetime": "2019-06-20 15:59:39", "author": "@falconius", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1213586313422630912": {"followers": "205", "datetime": "2020-01-04 22:21:39", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1190601542463705088": {"followers": "395", "datetime": "2019-11-02 12:08:22", "author": "@schunske", "content_summary": "XLnet\u3082\u96e3\u3057\u3044\u3002\u4f55\u5ea6\u3082\u8aad\u307f\u8fbc\u3080\u306e\u30c1\u30e3\u30ec\u30f3\u30b8\u3057\u3088\u3046\u3002\u5c11\u3057\u305a\u3064\u308f\u304b\u3063\u3066\u304f\u308b\u3082\u306e\u3060\u3057\u3002XLNet: Generalized Autoregressive Pretraining for Language Understanding  https://t.co/qI76dJcBBt"}, "1229506282618376192": {"followers": "476", "datetime": "2020-02-17 20:41:55", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141660216615768064": {"followers": "155", "datetime": "2019-06-20 10:53:01", "author": "@Trtd6Trtd", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142120397912993793": {"followers": "31", "datetime": "2019-06-21 17:21:37", "author": "@patelmaitreya", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143375001149722624": {"followers": "45", "datetime": "2019-06-25 04:26:57", "author": "@lrsobrien", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141597562379677697": {"followers": "10", "datetime": "2019-06-20 06:44:03", "author": "@AngelSa86204807", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1221083313479258113": {"followers": "63", "datetime": "2020-01-25 14:52:03", "author": "@mathiasmaelleb", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/bHlWmPatP5 #Xlnet #GeneralizedAutoregressivePretraining #LanguageUnderstanding"}, "1142121029604691969": {"followers": "510", "datetime": "2019-06-21 17:24:07", "author": "@drjohncliu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143408162382524419": {"followers": "701", "datetime": "2019-06-25 06:38:44", "author": "@Omarito2412", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141520183938506753": {"followers": "1,646", "datetime": "2019-06-20 01:36:34", "author": "@dipanjand", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141883990812569605": {"followers": "70", "datetime": "2019-06-21 01:42:13", "author": "@GabrielTomberl1", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1146549794183139328": {"followers": "4,388", "datetime": "2019-07-03 22:42:27", "author": "@ChrisDiehl", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1141694540878962688": {"followers": "253", "datetime": "2019-06-20 13:09:24", "author": "@LukasMasuch", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1174167690736492545": {"followers": "986", "datetime": "2019-09-18 03:46:07", "author": "@kasaikouhei", "content_summary": "20\u306e\u4f5c\u696d\u3067BERT\u3092\u8d85\u3048\u305f\u3068\u8a00\u3063\u3066\u3044\u308b\uff08\u3055\u3063\u304d\u77e5\u3063\u305f\uff09 https://t.co/RwU8zj38rV"}, "1143733600401031168": {"followers": "12,765", "datetime": "2019-06-26 04:11:54", "author": "@jaguring1", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141779279354576896": {"followers": "7", "datetime": "2019-06-20 18:46:08", "author": "@jnzhsb", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1143748592806400000": {"followers": "81", "datetime": "2019-06-26 05:11:29", "author": "@kencyke", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141725341452398592": {"followers": "29", "datetime": "2019-06-20 15:11:48", "author": "@_ranjodh_singh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143376463699369984": {"followers": "422", "datetime": "2019-06-25 04:32:46", "author": "@maxisawesome538", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141666509606936576": {"followers": "105", "datetime": "2019-06-20 11:18:01", "author": "@sheikmohdimran", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1219579944747048961": {"followers": "14", "datetime": "2020-01-21 11:18:12", "author": "@Crypto_Cotton", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1141913540380377091": {"followers": "65", "datetime": "2019-06-21 03:39:38", "author": "@Mitsunori19", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142029317301571585": {"followers": "1,337", "datetime": "2019-06-21 11:19:41", "author": "@s_0samu", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1211359277845102592": {"followers": "48", "datetime": "2019-12-29 18:52:12", "author": "@GrangierDavid", "content_summary": "@francoisfleuret Transformers works with sets and can be conditioned on any subset of the variables, usually presented as pairs (variable id/position, variable value). XLNet https://t.co/Eh1pnTvNap uses this property to train a language model over all poss"}, "1141523147818065920": {"followers": "15,224", "datetime": "2019-06-20 01:48:21", "author": "@lmthang", "content_summary": "I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @ZihangDai @quocleix https://t.co/PxzNxuHtAf"}, "1143007504663642113": {"followers": "18,250", "datetime": "2019-06-24 04:06:39", "author": "@hillbig", "content_summary": "\u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u30bf\u30b9\u30af\u3067SOTA\u3092\u9054\u6210 https://t.co/TSBzCY75rY"}, "1142825454304399360": {"followers": "4,754", "datetime": "2019-06-23 16:03:15", "author": "@rzembo", "content_summary": "RT @NegruEduard: Great news for #NLP enthusiasts. XLnet[1] is said to outperform Bert on certain nlp tasks according to this article [2]. #\u2026"}, "1145491556561907712": {"followers": "11,476", "datetime": "2019-07-01 00:37:23", "author": "@icoxfog417", "content_summary": "BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(660\u4e07\u307b\u3069)\u3002\u4e00\u767a\u3067\u4e0a\u624b\u304f\u3044\u3063\u305f\u306f\u305a\u306f\u306a\u3044\u3068\u601d\u3046\u306e\u3067\u5b9f\u614b\u306f\u3055\u3089\u306b\u4e0a\u3068\u601d\u308f\u308c\u308b https://t.co/ZpnHUq3GwV"}, "1141873287452155904": {"followers": "34", "datetime": "2019-06-21 00:59:41", "author": "@alberthaiwang", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1228333136054046726": {"followers": "699", "datetime": "2020-02-14 15:00:15", "author": "@guillefix", "content_summary": "has anyone tried a generalized autoregressive model like XLNet (https://t.co/D3Q6qRjjiF) where the generation order is chosen like in wave-function collapse algorithm (by chosing the min-entropy point in the sequence)?"}, "1143430900501438465": {"followers": "1,957", "datetime": "2019-06-25 08:09:05", "author": "@chrshmmmr", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143615800642850822": {"followers": "252", "datetime": "2019-06-25 20:23:48", "author": "@_DLPBGJ80C04Z_", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141542813429129216": {"followers": "130", "datetime": "2019-06-20 03:06:30", "author": "@DattaAnkur", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142000783036166144": {"followers": "26", "datetime": "2019-06-21 09:26:18", "author": "@MurgioMurmani", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141890828966608896": {"followers": "2", "datetime": "2019-06-21 02:09:23", "author": "@2675Nyatszy284", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141880181310676992": {"followers": "339", "datetime": "2019-06-21 01:27:04", "author": "@kmotohas", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143531459187228673": {"followers": "84", "datetime": "2019-06-25 14:48:40", "author": "@dermcnor", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144495396128878593": {"followers": "1,591", "datetime": "2019-06-28 06:39:00", "author": "@santhoshtr", "content_summary": "The Staggering Cost of Training SOTA AI Models https://t.co/s4OvXv86Zn"}, "1141728067347451904": {"followers": "", "datetime": "2019-06-20 15:22:38", "author": "@ramhiser", "content_summary": "that didn't take long!"}, "1141606275668725760": {"followers": "298", "datetime": "2019-06-20 07:18:40", "author": "@jsmrcaga", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142263683407581185": {"followers": "783", "datetime": "2019-06-22 02:50:59", "author": "@K_Ryuichirou", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141884962486046721": {"followers": "293", "datetime": "2019-06-21 01:46:04", "author": "@wallyest", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143403830928760832": {"followers": "1,382", "datetime": "2019-06-25 06:21:31", "author": "@bouzoukipunks", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141585710077501440": {"followers": "229", "datetime": "2019-06-20 05:56:57", "author": "@amitunix", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141546919585406976": {"followers": "131", "datetime": "2019-06-20 03:22:49", "author": "@penguinkang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1155820573081133058": {"followers": "818", "datetime": "2019-07-29 12:41:13", "author": "@deepgradient", "content_summary": "RT @PapersTrending: [6/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 9,948 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1143436240169164800": {"followers": "8,440", "datetime": "2019-06-25 08:30:18", "author": "@rocketman528", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141799651885977600": {"followers": "41", "datetime": "2019-06-20 20:07:05", "author": "@ATK34911370", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1142343888914518018": {"followers": "155", "datetime": "2019-06-22 08:09:41", "author": "@forodeeplearn", "content_summary": "RT @MaxVico: Este a\u00f1o sin duda el NLP est\u00e1 de moda... https://t.co/aKnOMEIbVr"}, "1141720348695695361": {"followers": "946", "datetime": "2019-06-20 14:51:57", "author": "@hackernewsj", "content_summary": "XLNet\uff1a\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306e\u4e00\u822c\u5316\u81ea\u5df1\u56de\u5e30\u4e88\u7fd2 https://t.co/we845IhAW7"}, "1143459067928571904": {"followers": "4,676", "datetime": "2019-06-25 10:01:00", "author": "@PhilippBayer", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1183183008498499585": {"followers": "63", "datetime": "2019-10-13 00:49:46", "author": "@Ronmemo1", "content_summary": "ABST100\u30e1\u30e2\uff1a64 XLNet\uff082019\uff09\u2192https://t.co/gjh6Z9KafG\u3000\uff0cmask\u304cBERT\u306e\u554f\u984c\u70b9\u3067\u3042\u308a\u305d\u308c\u3092\u514b\u670d\uff0c\u7279\u306b\u30dd\u30a4\u30f3\u30c8\u306f\u300cenables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\u300d\u3064\u307e\u308a\u9806\u5e8f\u306e\u5165\u308c\u66ff\u3048\u304b"}, "1143616779807277061": {"followers": "213", "datetime": "2019-06-25 20:27:42", "author": "@matt_vowels", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141641236844990465": {"followers": "1,818", "datetime": "2019-06-20 09:37:36", "author": "@YoshiHotta", "content_summary": "XLNet \u3068\u3044\u3046\u8a00\u8a9e\u30e2\u30c7\u30eb\u304cBERT\u309218\u30bf\u30b9\u30af\u3067\u4e0a\u56de\u3063\u305f\u305d\u3046\u3002\u3057\u304b\u3082\u7d50\u69cb\u5927\u5e45\u306b\u4e0a\u56de\u3063\u3066\u308b\u304b\u3089\u8a66\u3057\u3066\u307f\u305f\u3044\u6c17\u6301\u3061\u306b\u306a\u3063\u3066\u304d\u305f\u3002 https://t.co/3D5cXbDO27 https://t.co/bxjMVsbQ0U"}, "1143438854994583552": {"followers": "883", "datetime": "2019-06-25 08:40:41", "author": "@AltangChagnaa", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1148150207483273216": {"followers": "476", "datetime": "2019-07-08 08:41:55", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1148616773660397568": {"followers": "145", "datetime": "2019-07-09 15:35:53", "author": "@naiiytom", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141533063270281218": {"followers": "65", "datetime": "2019-06-20 02:27:45", "author": "@billy_nlp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142985885484326913": {"followers": "91", "datetime": "2019-06-24 02:40:45", "author": "@Devon_Z_Sun", "content_summary": "XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.co/jNSbMpvScs https://t.co/3cmhOvxhYL"}, "1143558425672126465": {"followers": "328", "datetime": "2019-06-25 16:35:49", "author": "@rudzinskimaciej", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143422404645048320": {"followers": "354", "datetime": "2019-06-25 07:35:19", "author": "@indy9000", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1142268181588803584": {"followers": "6", "datetime": "2019-06-22 03:08:51", "author": "@nahidcse05", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141698074059800577": {"followers": "2,043", "datetime": "2019-06-20 13:23:27", "author": "@imenurok", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1142280744632406017": {"followers": "47", "datetime": "2019-06-22 03:58:46", "author": "@RajenChatterje", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142286102071566336": {"followers": "24", "datetime": "2019-06-22 04:20:04", "author": "@ENaziga", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1155780378675822592": {"followers": "222", "datetime": "2019-07-29 10:01:30", "author": "@PapersTrending", "content_summary": "[6/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 9,948 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1143549802371866625": {"followers": "407", "datetime": "2019-06-25 16:01:33", "author": "@hugojair", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143821387100053519": {"followers": "222", "datetime": "2019-06-26 10:00:44", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 2,874 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141895413294698496": {"followers": "358", "datetime": "2019-06-21 02:27:36", "author": "@christianwbsn", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1147412236425519104": {"followers": "62", "datetime": "2019-07-06 07:49:29", "author": "@guptapv", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1161999719935172610": {"followers": "0", "datetime": "2019-08-15 13:54:56", "author": "@dogydev", "content_summary": "RT @dogydev: Hyperparameter tuning using SHERPA (https://t.co/Kjh6MgMao3) on XLNet (https://t.co/RUTEgIVwzV) https://t.co/LBvv9DgxA8"}, "1141706447337721857": {"followers": "452", "datetime": "2019-06-20 13:56:43", "author": "@bryant1410", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142076940267819008": {"followers": "534", "datetime": "2019-06-21 14:28:55", "author": "@krishnamrith12", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1143717915478200320": {"followers": "10", "datetime": "2019-06-26 03:09:34", "author": "@ranip_cool", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142301106095022080": {"followers": "12,765", "datetime": "2019-06-22 05:19:41", "author": "@jaguring1", "content_summary": "RT @jaguring1: \u53c2\u8003\u6587\u732e3 \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Q5YEuTNKed \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9 + pretraine\u2026"}, "1197274845676879872": {"followers": "254", "datetime": "2019-11-20 22:05:42", "author": "@tkbpyZlBD5smm8G", "content_summary": "RT @jaguring1: \u53c2\u8003\u6587\u732e3 \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Q5YEuTNKed \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9 + pretraine\u2026"}, "1151428468732846080": {"followers": "1,278", "datetime": "2019-07-17 09:48:34", "author": "@jochenleidner", "content_summary": "Yang et al. (2019) XLNet https://t.co/6U7hpwzJGG (Is the end of muppet naming near?)"}, "1142480615477878784": {"followers": "21,804", "datetime": "2019-06-22 17:12:59", "author": "@vaaaaanquish", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1151805872756187136": {"followers": "245", "datetime": "2019-07-18 10:48:14", "author": "@Rovio_Red", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1219583916891348992": {"followers": "141", "datetime": "2020-01-21 11:33:59", "author": "@H0dI3r", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1141790164039208961": {"followers": "65", "datetime": "2019-06-20 19:29:23", "author": "@LaurentJakubina", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141688588582694912": {"followers": "232", "datetime": "2019-06-20 12:45:45", "author": "@fabiofumarola", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143374274075475968": {"followers": "97", "datetime": "2019-06-25 04:24:04", "author": "@code_star", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141764253101244417": {"followers": "34", "datetime": "2019-06-20 17:46:25", "author": "@data_mike_j", "content_summary": "@seb_ruder @jeremyphoward new sota for sentiment and classification (among other things)."}, "1142152509911265280": {"followers": "129", "datetime": "2019-06-21 19:29:13", "author": "@daewonyoon", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141916761773576193": {"followers": "2,861", "datetime": "2019-06-21 03:52:26", "author": "@Tarpon_red2", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142146060963504134": {"followers": "5", "datetime": "2019-06-21 19:03:35", "author": "@rameshkjes", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143577296126656512": {"followers": "25,296", "datetime": "2019-06-25 17:50:48", "author": "@atulbutte", "content_summary": "Luckily it costs a lot less to train a child how to understand language\u2026 HT @IAmSamFin"}, "1143529678470148096": {"followers": "483", "datetime": "2019-06-25 14:41:35", "author": "@akitelok", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143581474677018625": {"followers": "1,031", "datetime": "2019-06-25 18:07:24", "author": "@AsnoDeBuridan", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141694654410190850": {"followers": "4,389", "datetime": "2019-06-20 13:09:51", "author": "@hshimodaira", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141555964698480640": {"followers": "117", "datetime": "2019-06-20 03:58:45", "author": "@hendyfergus", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142096382800482304": {"followers": "132", "datetime": "2019-06-21 15:46:11", "author": "@chaiyi_lin", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1162941099821993984": {"followers": "179", "datetime": "2019-08-18 04:15:39", "author": "@rajneeshagrawal", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144497167853215744": {"followers": "594", "datetime": "2019-06-28 06:46:03", "author": "@flavioclesio", "content_summary": "Holy Jesus!"}, "1146245135329452032": {"followers": "54", "datetime": "2019-07-03 02:31:51", "author": "@callmefons", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141594090984919042": {"followers": "378", "datetime": "2019-06-20 06:30:15", "author": "@brvilar", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143556789272809472": {"followers": "277", "datetime": "2019-06-25 16:29:19", "author": "@johnfoleyiv", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1142290421743144961": {"followers": "1", "datetime": "2019-06-22 04:37:13", "author": "@PoisonNow", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142009239386435584": {"followers": "54", "datetime": "2019-06-21 09:59:54", "author": "@drorhilman", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142039546135232514": {"followers": "432", "datetime": "2019-06-21 12:00:20", "author": "@sofide_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142264015546195969": {"followers": "10,310", "datetime": "2019-06-22 02:52:18", "author": "@malhamid", "content_summary": "\u0646\u0634\u0631 \u0642\u0628\u0644 \u064a\u0648\u0645\u064a\u0646 \u0648\u0631\u0642\u0629 \u0628\u062d\u062b \u0645\u0645\u064a\u0632\u0629 \u0641\u064a \u0646\u0633\u062e\u062a\u0647\u0627 \u0627\u0644\u0623\u0648\u0644\u064a\u0629 \u0639\u0646 \u0646\u0645\u0648\u0630\u062c #\u062a\u0639\u0644\u0645_\u0627\u0644\u0622\u0644\u0629 \u0627\u0637\u0644\u0642 \u0639\u0644\u064a\u0647 (XLNet) \u064a\u062a\u0641\u0648\u0642 \u0639\u0644\u0649 \u0646\u0645\u0648\u0630\u062c (BERT) \u0628\u0647\u0627\u0645\u0634 \u0643\u0628\u064a\u0631. \u0648\u064a\u0645\u062b\u0644 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0637\u0631\u064a\u0642\u0629 \u062a\u0639\u0644\u0645 \u0627\u0644\u062a\u0645\u062b\u064a\u0644 \u0627\u0644\u0644\u063a\u0648\u064a. https://t.co/ggzdujr5EL \u0627\u0644\u0623\u0643\u0648\u0627\u062f \u0627\u0644\u0628\u0631\u0645\u062c\u064a\u0629 \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645\u0629 \u0641\u064a \u0627\u0644\u062a\u062c\u0631\u0628\u0629 \u062a\u0645 \u0627\u062a\u0627\u062d\u062a\u0647\u0627 \u0644\u0644\u062c\u0645\u064a\u0639 \u0639\u0644\u0649 \u0647\u0630\u0627 \u0627\u0644\u0631\u0627\u0628\u0637"}, "1141541841659023365": {"followers": "175", "datetime": "2019-06-20 03:02:38", "author": "@SquirrelYellow", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143391052189114369": {"followers": "8,425", "datetime": "2019-06-25 05:30:44", "author": "@debasishg", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143718473308094466": {"followers": "42", "datetime": "2019-06-26 03:11:47", "author": "@masa_kakaroto", "content_summary": "XLNet\u3092\u4f7f\u3046\u6599\u91d1\u306f24\u4e075000\u30c9\u30eb(2600\u4e07)\u304b\u304b\u308b\u3002"}, "1141662368021340160": {"followers": "63", "datetime": "2019-06-20 11:01:34", "author": "@dpsmarques", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143465196502962176": {"followers": "214", "datetime": "2019-06-25 10:25:22", "author": "@moizsaifee", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141691751326986242": {"followers": "26", "datetime": "2019-06-20 12:58:19", "author": "@oskaus", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143627796704575488": {"followers": "9", "datetime": "2019-06-25 21:11:28", "author": "@jack_dahms", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141869453589614593": {"followers": "135,428", "datetime": "2019-06-21 00:44:27", "author": "@hiconcep", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141697186914136065": {"followers": "649", "datetime": "2019-06-20 13:19:55", "author": "@qavion_", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141523661691731968": {"followers": "2,583", "datetime": "2019-06-20 01:50:24", "author": "@tito", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142099396554829824": {"followers": "13", "datetime": "2019-06-21 15:58:09", "author": "@BesanHalwa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142511909578711042": {"followers": "249", "datetime": "2019-06-22 19:17:20", "author": "@derekchen14", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1143746536343367681": {"followers": "2,530", "datetime": "2019-06-26 05:03:18", "author": "@_329_", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141727655571542016": {"followers": "591", "datetime": "2019-06-20 15:21:00", "author": "@smochi_pub", "content_summary": "\u65e9\u304f\u3082BERT\u8d85\u3048\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u304d\u305f\u2026\uff01"}, "1141557032115306496": {"followers": "316", "datetime": "2019-06-20 04:03:00", "author": "@liuyangumass", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141676677002616834": {"followers": "137", "datetime": "2019-06-20 11:58:25", "author": "@rozvidchek", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141614936528961536": {"followers": "127", "datetime": "2019-06-20 07:53:05", "author": "@MayerThommy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141611774346694656": {"followers": "1,824", "datetime": "2019-06-20 07:40:31", "author": "@N_Perreaux", "content_summary": "RT @lespetitescases: La course pour l'am\u00e9lioration des algos de traitement du langage naturel se poursuit \u00e0 un rythme effr\u00e9n\u00e9... BERT est s\u2026"}, "1143687096235773954": {"followers": "414", "datetime": "2019-06-26 01:07:07", "author": "@MJDeshpande", "content_summary": "RT @ngkabra: For all those who think machine learning is easy/cheap, please note the costs of training a deep learning network https://t.co\u2026"}, "1141535982719664129": {"followers": "151", "datetime": "2019-06-20 02:39:21", "author": "@gettoankit", "content_summary": "RT @pranavrajpurkar: Wow! https://t.co/cbK35XdIjV"}, "1141689702438715392": {"followers": "59", "datetime": "2019-06-20 12:50:11", "author": "@flapdoodle_sand", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141695062671331328": {"followers": "254", "datetime": "2019-06-20 13:11:29", "author": "@gntrm", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143480887482302471": {"followers": "450", "datetime": "2019-06-25 11:27:43", "author": "@jesserobertson", "content_summary": "We have gone from data being the new oil to oil (in a generator) being the new data."}, "1141883656060825600": {"followers": "819", "datetime": "2019-06-21 01:40:53", "author": "@akf", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1160922960896466944": {"followers": "122", "datetime": "2019-08-12 14:36:17", "author": "@hasansaikatt", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141779791508647937": {"followers": "11", "datetime": "2019-06-20 18:48:10", "author": "@YvesM67070676", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143514970262507522": {"followers": "399", "datetime": "2019-06-25 13:43:09", "author": "@dafevara", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141523358200340485": {"followers": "608", "datetime": "2019-06-20 01:49:11", "author": "@tiagotvv", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141657637513256961": {"followers": "24", "datetime": "2019-06-20 10:42:46", "author": "@healthonrails", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1147290159085498370": {"followers": "17,661", "datetime": "2019-07-05 23:44:24", "author": "@jfeldizzle", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143354915248640000": {"followers": "1,192", "datetime": "2019-06-25 03:07:08", "author": "@dfabbri", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141612498350559234": {"followers": "14,836", "datetime": "2019-06-20 07:43:24", "author": "@mathwuyue", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141720649313837056": {"followers": "824", "datetime": "2019-06-20 14:53:09", "author": "@morioka", "content_summary": "RT @jaguring1: pre-trained\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u3066\u308b\u3002 https://t.co/56dLplYivW \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding\u2026"}, "1161999818388070400": {"followers": "0", "datetime": "2019-08-15 13:55:20", "author": "@dogydev", "content_summary": "#NLP #MachineLearning"}, "1141968553957945345": {"followers": "1,022", "datetime": "2019-06-21 07:18:14", "author": "@nishizawa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141909279131045889": {"followers": "264", "datetime": "2019-06-21 03:22:42", "author": "@ka10ryu1", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1143382334022066176": {"followers": "485", "datetime": "2019-06-25 04:56:06", "author": "@_LMiguel", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143375285322227712": {"followers": "655", "datetime": "2019-06-25 04:28:05", "author": "@jerofad", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141641792158351360": {"followers": "344", "datetime": "2019-06-20 09:39:48", "author": "@davlanade", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143774414724653063": {"followers": "139", "datetime": "2019-06-26 06:54:05", "author": "@mmeierer", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141961327461617664": {"followers": "42", "datetime": "2019-06-21 06:49:31", "author": "@m4ki02", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1154042026653962240": {"followers": "30", "datetime": "2019-07-24 14:53:54", "author": "@gbiwer", "content_summary": "RT @YoshiHotta: XLNet \u3068\u3044\u3046\u8a00\u8a9e\u30e2\u30c7\u30eb\u304cBERT\u309218\u30bf\u30b9\u30af\u3067\u4e0a\u56de\u3063\u305f\u305d\u3046\u3002\u3057\u304b\u3082\u7d50\u69cb\u5927\u5e45\u306b\u4e0a\u56de\u3063\u3066\u308b\u304b\u3089\u8a66\u3057\u3066\u307f\u305f\u3044\u6c17\u6301\u3061\u306b\u306a\u3063\u3066\u304d\u305f\u3002 https://t.co/3D5cXbDO27 https://t.co/bxjMVsbQ0U"}, "1141818548135837700": {"followers": "10", "datetime": "2019-06-20 21:22:10", "author": "@Sushant46135900", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143358183345254400": {"followers": "2,671", "datetime": "2019-06-25 03:20:08", "author": "@ayirpelle", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1142050275647209472": {"followers": "34", "datetime": "2019-06-21 12:42:58", "author": "@Mariia97913314", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141711206941700097": {"followers": "66", "datetime": "2019-06-20 14:15:38", "author": "@battle8500", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141539929220165633": {"followers": "344", "datetime": "2019-06-20 02:55:02", "author": "@hacker_news_hir", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding : https://t.co/cXdC85XZMY Comments: https://t.co/oe1TlQr9pF"}, "1143736666105868288": {"followers": "1,073", "datetime": "2019-06-26 04:24:05", "author": "@terashimahiroki", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141547683376586752": {"followers": "768", "datetime": "2019-06-20 03:25:51", "author": "@DUXROLL", "content_summary": "RT @jaguring1: pre-trained\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u3066\u308b\u3002 https://t.co/56dLplYivW \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding\u2026"}, "1143577134859870208": {"followers": "49", "datetime": "2019-06-25 17:50:10", "author": "@piyush2896", "content_summary": "https://t.co/QMA2lHVO5F"}, "1141814299666419713": {"followers": "42", "datetime": "2019-06-20 21:05:17", "author": "@m4ki02", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141605264308785152": {"followers": "1,163", "datetime": "2019-06-20 07:14:39", "author": "@proxem", "content_summary": "RT @FR_Chaumartin: XLNet : un apprentissage de mod\u00e8le de langue qui donne des r\u00e9sultats encore meilleurs que BERT#NLP #DeepLearning https:/\u2026"}, "1141668274783973376": {"followers": "26", "datetime": "2019-06-20 11:25:02", "author": "@shizukanayosei", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143365268082548737": {"followers": "45", "datetime": "2019-06-25 03:48:17", "author": "@kr_gopala", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1156109826180538368": {"followers": "162", "datetime": "2019-07-30 07:50:36", "author": "@kurama554101", "content_summary": "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30de\u30eb\u30c1\u30bf\u30b9\u30af\u306b\u3064\u3044\u3066\u3001Google vs Facebook\u306e\u4e89\u3044\u3092\u898b\u305b\u3066\u3044\u3066\u3001\u9762\u767d\u3044\u3002 https://t.co/m98RGrDf5A https://t.co/htXV3dKRLQ https://t.co/HZlJ8wo6pw"}, "1143390155392159744": {"followers": "1,584", "datetime": "2019-06-25 05:27:10", "author": "@aneesha", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141933012705136640": {"followers": "19", "datetime": "2019-06-21 04:57:00", "author": "@FlyRaeb", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142487486817431553": {"followers": "9", "datetime": "2019-06-22 17:40:17", "author": "@PrakashCicky1", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141809567065673729": {"followers": "442", "datetime": "2019-06-20 20:46:29", "author": "@AdrianB82", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141930368485826565": {"followers": "868", "datetime": "2019-06-21 04:46:30", "author": "@ikuyamada", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143431872204201985": {"followers": "169", "datetime": "2019-06-25 08:12:56", "author": "@data4gud", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141994372965126146": {"followers": "240", "datetime": "2019-06-21 09:00:50", "author": "@kojimizu1028", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142318381749567488": {"followers": "17", "datetime": "2019-06-22 06:28:20", "author": "@pidueck", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141758368287268872": {"followers": "187", "datetime": "2019-06-20 17:23:02", "author": "@gascosta", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141625868839739392": {"followers": "1,050", "datetime": "2019-06-20 08:36:32", "author": "@PSH_Lewis", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141937525650362368": {"followers": "39", "datetime": "2019-06-21 05:14:56", "author": "@Ujjwal_1999", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141743192716353536": {"followers": "281", "datetime": "2019-06-20 16:22:44", "author": "@koba_taka", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141737065949753344": {"followers": "81", "datetime": "2019-06-20 15:58:23", "author": "@mayank_lunayach", "content_summary": "2019\u2019s best NLP work or its too early to say \ud83d\ude00"}, "1143564398583263234": {"followers": "34", "datetime": "2019-06-25 16:59:33", "author": "@clbam8", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141533208644767745": {"followers": "169", "datetime": "2019-06-20 02:28:20", "author": "@data4gud", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143213982565720069": {"followers": "43", "datetime": "2019-06-24 17:47:07", "author": "@jeandut14000", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143801226448052224": {"followers": "3,719", "datetime": "2019-06-26 08:40:37", "author": "@AlbertVilella", "content_summary": "The @GCPcloud #TPUv3 was used for #XLNet which comes very close to human ceiling performance of reading comprehension of the #RACE dataset. An example of #Hardware #Acceleration applied to #ML . Can #TPUv3 be applied to #Genomics ? https://t.co/SpQgbwc1Zb"}, "1141690536576741377": {"followers": "9", "datetime": "2019-06-20 12:53:30", "author": "@ptpuyen1511", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141683545435443200": {"followers": "159", "datetime": "2019-06-20 12:25:43", "author": "@matasramon", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142100196203073538": {"followers": "422", "datetime": "2019-06-21 16:01:20", "author": "@jcvasquezc1", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1242600125978955776": {"followers": "205", "datetime": "2020-03-24 23:52:11", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141530918483611648": {"followers": "163,852", "datetime": "2019-06-20 02:19:14", "author": "@ceobillionaire", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143042103926120449": {"followers": "1,816", "datetime": "2019-06-24 06:24:08", "author": "@MikiBear_", "content_summary": "\uc544\ub2c8 \ubbf8\uce5c \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b https://t.co/ixHqPog8Yu https://t.co/DSxHpesHSN"}, "1141912777667178496": {"followers": "87", "datetime": "2019-06-21 03:36:36", "author": "@if_004", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141774718154002433": {"followers": "354", "datetime": "2019-06-20 18:28:00", "author": "@O_uaziz", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1226115825557540864": {"followers": "67", "datetime": "2020-02-08 12:09:27", "author": "@valeromarta", "content_summary": "RT @steadydee: At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will exp\u2026"}, "1143509236477042689": {"followers": "46", "datetime": "2019-06-25 13:20:22", "author": "@sajjad_abdoli", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141876761363279872": {"followers": "1,461", "datetime": "2019-06-21 01:13:29", "author": "@rfrsarmiento", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141690055028858880": {"followers": "397", "datetime": "2019-06-20 12:51:35", "author": "@AfoUnofficial", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141722300703633413": {"followers": "184", "datetime": "2019-06-20 14:59:43", "author": "@bari_seino", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141607649546526720": {"followers": "22", "datetime": "2019-06-20 07:24:08", "author": "@yimanunaavari", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143723331696205824": {"followers": "644", "datetime": "2019-06-26 03:31:06", "author": "@finelined_", "content_summary": "RT @deliprao: In the long run, withholding large parameter models on public datasets rarely prevents well resourced bad actors from weaponi\u2026"}, "1141647978446426112": {"followers": "1,975", "datetime": "2019-06-20 10:04:23", "author": "@gdm3000", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1145100697391833089": {"followers": "1,009", "datetime": "2019-06-29 22:44:15", "author": "@Colin_GJ", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141603279140528128": {"followers": "9", "datetime": "2019-06-20 07:06:46", "author": "@FroZone_FR", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142567792945721344": {"followers": "1,513", "datetime": "2019-06-22 22:59:24", "author": "@candidusflumen", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1162740897848733702": {"followers": "2,071", "datetime": "2019-08-17 15:00:07", "author": "@EricSchles", "content_summary": "RT @BenSingletonNYC: XLNet: Generalized Autoregressive Pretraining for Language Understanding #DataScience #BigData https://t.co/v7wdH4j6rT"}, "1145548309550886914": {"followers": "515", "datetime": "2019-07-01 04:22:54", "author": "@podhmo", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141841944877907968": {"followers": "19", "datetime": "2019-06-20 22:55:08", "author": "@MassBassLol", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1142706017827819520": {"followers": "18", "datetime": "2019-06-23 08:08:39", "author": "@deepakpokkalla", "content_summary": "Outperforms BERT on 20 tasks! Would have been amazing to try this out for @Jigsaw\u2019s toxicity challenge, have it been released few days earlier \ud83d\ude05. Nevertheless, will explore it. \ud83d\ude04"}, "1143721387036536833": {"followers": "229", "datetime": "2019-06-26 03:23:22", "author": "@amitunix", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141826731008909316": {"followers": "2,861", "datetime": "2019-06-20 21:54:41", "author": "@Tarpon_red2", "content_summary": "RT @smochi_pub: \u65e9\u304f\u3082BERT\u8d85\u3048\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u304d\u305f\u2026\uff01 https://t.co/86pHSazItX"}, "1141722028258418689": {"followers": "80", "datetime": "2019-06-20 14:58:38", "author": "@BobbyAlter", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141750481427030022": {"followers": "623", "datetime": "2019-06-20 16:51:42", "author": "@texttheater", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143535945687322625": {"followers": "10", "datetime": "2019-06-25 15:06:29", "author": "@_Mingyang_Zhang", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141970903213006848": {"followers": "331", "datetime": "2019-06-21 07:27:34", "author": "@philomate", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1146718714705764353": {"followers": "224", "datetime": "2019-07-04 09:53:41", "author": "@ElectronNest", "content_summary": "\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" https://t.co/E3XE4JDfOA"}, "1141560017843294209": {"followers": "299", "datetime": "2019-06-20 04:14:52", "author": "@westis96", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142115184166756352": {"followers": "170", "datetime": "2019-06-21 17:00:54", "author": "@dr_levan", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141886938179354624": {"followers": "65", "datetime": "2019-06-21 01:53:55", "author": "@SonyStates", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1146177327870726145": {"followers": "231", "datetime": "2019-07-02 22:02:24", "author": "@Paraglokhande", "content_summary": "RT @rctatman: Time to pick the next @kaggle reading group paper! Your options: - XLNet: Generalized Autoregressive Pretraining for NLU htt\u2026"}, "1143457742084071424": {"followers": "1,317", "datetime": "2019-06-25 09:55:44", "author": "@zoombapup", "content_summary": "RT @baykenney: access to compute can't be decoupled from responsible development of ml systems. https://t.co/8pAmAV39Mg"}, "1141621594432987136": {"followers": "150", "datetime": "2019-06-20 08:19:33", "author": "@q_retourne", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143457948724711425": {"followers": "18", "datetime": "2019-06-25 09:56:34", "author": "@caymanlee", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1210213811434618881": {"followers": "8,478", "datetime": "2019-12-26 15:00:31", "author": "@ai_news_jp", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1142192009324769281": {"followers": "1,106", "datetime": "2019-06-21 22:06:10", "author": "@permutans", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141611033301151744": {"followers": "90", "datetime": "2019-06-20 07:37:35", "author": "@SinghNaruto", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141577428231315457": {"followers": "30", "datetime": "2019-06-20 05:24:03", "author": "@Tn23008624", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141866417601548288": {"followers": "370", "datetime": "2019-06-21 00:32:23", "author": "@Dadang_Ewp", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141520303807270912": {"followers": "27", "datetime": "2019-06-20 01:37:03", "author": "@KhanhDuyVu1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141669541409427463": {"followers": "494", "datetime": "2019-06-20 11:30:04", "author": "@jbohnslav", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142028278577807360": {"followers": "1,152", "datetime": "2019-06-21 11:15:34", "author": "@jd_mashiro", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141753311013380096": {"followers": "251", "datetime": "2019-06-20 17:02:56", "author": "@Tanaygahlot", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141587194601021445": {"followers": "82", "datetime": "2019-06-20 06:02:51", "author": "@mzeid4real", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141697344985059328": {"followers": "9,925", "datetime": "2019-06-20 13:20:33", "author": "@amunategui", "content_summary": "Thanks Quoc - curious to try this out."}, "1141898161096544256": {"followers": "30,496", "datetime": "2019-06-21 02:38:31", "author": "@v_vashishta", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141680026133262337": {"followers": "175", "datetime": "2019-06-20 12:11:44", "author": "@SquirrelYellow", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1144662627257372672": {"followers": "1,477", "datetime": "2019-06-28 17:43:31", "author": "@shadercat", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1247446969112395778": {"followers": "205", "datetime": "2020-04-07 08:51:48", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141625844365848576": {"followers": "783", "datetime": "2019-06-20 08:36:26", "author": "@muktabh", "content_summary": "RT @arxiv_cs_LG: XLNet: Generalized Autoregressive Pretraining for Language Understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carb\u2026"}, "1141676031314726915": {"followers": "234", "datetime": "2019-06-20 11:55:51", "author": "@JustasDauparas", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141893029789175809": {"followers": "351", "datetime": "2019-06-21 02:18:08", "author": "@Kazk1018", "content_summary": "XLNet\u304c\u8a71\u984c https://t.co/ZrpUUwCDLb"}, "1143685402227724289": {"followers": "1,337", "datetime": "2019-06-26 01:00:23", "author": "@math_phys", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141893388783869952": {"followers": "9", "datetime": "2019-06-21 02:19:33", "author": "@literacynor", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141744293654290433": {"followers": "40", "datetime": "2019-06-20 16:27:06", "author": "@FedericoFloris4", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143634634795900932": {"followers": "1,044", "datetime": "2019-06-25 21:38:39", "author": "@scholarcy", "content_summary": "RT @anat_elhalal: Long live #MIGarage @DigiCatapult\u2019s programme to support startups with access to computation. Att:@DigiCatNI https://t.c\u2026"}, "1142052858981945345": {"followers": "29", "datetime": "2019-06-21 12:53:14", "author": "@arunsethupat", "content_summary": "Bye Bye BERT!"}, "1161194223456022529": {"followers": "7,671", "datetime": "2019-08-13 08:34:11", "author": "@ynupc", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141789863357890560": {"followers": "233", "datetime": "2019-06-20 19:28:11", "author": "@tsp_thomas", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141761631770181632": {"followers": "615", "datetime": "2019-06-20 17:36:00", "author": "@KouroshMeshgi", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1142093646742712325": {"followers": "230", "datetime": "2019-06-21 15:35:19", "author": "@ko_ash", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1141708685405634560": {"followers": "289", "datetime": "2019-06-20 14:05:37", "author": "@JUN_NETWORKS_JP", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141913586522099712": {"followers": "2,352", "datetime": "2019-06-21 03:39:49", "author": "@AndreiOprisan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143884883917451266": {"followers": "24", "datetime": "2019-06-26 14:13:03", "author": "@javiherreranl", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1243820805617668097": {"followers": "476", "datetime": "2020-03-28 08:42:43", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143838829511663616": {"followers": "58", "datetime": "2019-06-26 11:10:03", "author": "@jquattrocchi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142247242587262977": {"followers": "3", "datetime": "2019-06-22 01:45:39", "author": "@junwei_liao", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143707761844183042": {"followers": "325", "datetime": "2019-06-26 02:29:14", "author": "@ichi5c2", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1143507668134322176": {"followers": "731", "datetime": "2019-06-25 13:14:08", "author": "@williamstome", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1145500590702616581": {"followers": "1,060", "datetime": "2019-07-01 01:13:17", "author": "@Wataoka_Koki", "content_summary": "\u52dd\u3066\u308b\u308f\u3051\u306d\u3047...\u3002"}, "1141548350631796737": {"followers": "1", "datetime": "2019-06-20 03:28:30", "author": "@rucjzb", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141964724969435136": {"followers": "37", "datetime": "2019-06-21 07:03:01", "author": "@Rnoobie", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141618535212429312": {"followers": "291", "datetime": "2019-06-20 08:07:23", "author": "@_lpag", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1171089550581518337": {"followers": "105", "datetime": "2019-09-09 15:54:41", "author": "@tetsu_nakamura8", "content_summary": "RT @overleo: BERT\u3092\u8d85\u3048\u305fXLNet\u306e\u7d39\u4ecb - akihiro_f - Medium: \u6982\u8981https://t.co/khdHR0GZWP XLNet\u306f2019/6/19\u306b\u3001\u201dXLNet: Generalized Autoregressive Pretraini\u2026"}, "1157723299557675008": {"followers": "476", "datetime": "2019-08-03 18:41:58", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141638640117985280": {"followers": "29", "datetime": "2019-06-20 09:27:17", "author": "@ouali_yas", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141606903908376582": {"followers": "219", "datetime": "2019-06-20 07:21:10", "author": "@Neuw84", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141583124561899521": {"followers": "1,247", "datetime": "2019-06-20 05:46:41", "author": "@mlmemoirs", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sentiment analysis), while integrating ideas from Transformer-XL: arxiv: https://t.co/TrNjRLbJxT code + pretrained mode"}, "1141532421914914816": {"followers": "186", "datetime": "2019-06-20 02:25:12", "author": "@DrCMcMaster", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141636934684557312": {"followers": "51", "datetime": "2019-06-20 09:20:30", "author": "@el_Robinio", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1181338903711633409": {"followers": "476", "datetime": "2019-10-07 22:41:57", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1142004044174520320": {"followers": "2,456", "datetime": "2019-06-21 09:39:16", "author": "@jinbeizame007", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1144962158897139715": {"followers": "170", "datetime": "2019-06-29 13:33:45", "author": "@soranobunko", "content_summary": "RT @gijigae: \u53bb\u5e74\u8a71\u984c\u3068\u306a\u3063\u3066\u305f\u3001BERT\u3002 \u30b0\u30fc\u30b0\u30eb\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u9762\u3067BERT\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u65b0\u3057\u3044\u30e2\u30c7\u30eb\uff08XLNet\uff09\u3092\u767a\u8868\u3002SQuAD2.0\u3067\u306f\uff17\u30dd\u30a4\u30f3\u30c8\u3082\u30a2\u30c3\u30d7\ud83d\ude80\u3002 \u25a0\u4eba\u9593\uff1a86.831 \u25a0XLNet\uff1a86.346 \u25a0BERT\uff1a78.98 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u80fd\u2026"}, "1141515213692264448": {"followers": "1,976", "datetime": "2019-06-20 01:16:49", "author": "@StrongDuality", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142032187983900673": {"followers": "45", "datetime": "2019-06-21 11:31:06", "author": "@DeepMatrixCloud", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1141619073588944896": {"followers": "62", "datetime": "2019-06-20 08:09:32", "author": "@samhardyhey", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142002480164868097": {"followers": "314", "datetime": "2019-06-21 09:33:03", "author": "@deuslexia", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142153656705462277": {"followers": "71", "datetime": "2019-06-21 19:33:46", "author": "@GGozzoli", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1143518233653317633": {"followers": "601", "datetime": "2019-06-25 13:56:07", "author": "@tevnpowers", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141847589119320064": {"followers": "136", "datetime": "2019-06-20 23:17:34", "author": "@ahammami0", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141762892716548096": {"followers": "438", "datetime": "2019-06-20 17:41:01", "author": "@guildai", "content_summary": "XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking. https://t.co/albhF7d6TI"}, "1141912907157917696": {"followers": "331", "datetime": "2019-06-21 03:37:07", "author": "@atibhi_a", "content_summary": "Whoa \ud83d\ude0d"}, "1143505006416371713": {"followers": "639", "datetime": "2019-06-25 13:03:33", "author": "@williamaparker", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141626447460818945": {"followers": "57", "datetime": "2019-06-20 08:38:50", "author": "@irh86753o9", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141792192198873088": {"followers": "307", "datetime": "2019-06-20 19:37:26", "author": "@michaeld7", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142060537284612097": {"followers": "1,467", "datetime": "2019-06-21 13:23:45", "author": "@MLnick", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141694170387509250": {"followers": "63", "datetime": "2019-06-20 13:07:56", "author": "@GoMinSun", "content_summary": "Latest breakthrough!"}, "1143528484695752704": {"followers": "190", "datetime": "2019-06-25 14:36:51", "author": "@QingyaoAi", "content_summary": "Oh my..."}, "1145011706944458752": {"followers": "2,601", "datetime": "2019-06-29 16:50:38", "author": "@Nklarer", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141815403037044736": {"followers": "82", "datetime": "2019-06-20 21:09:40", "author": "@Saurabh98G", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1237783356969865216": {"followers": "205", "datetime": "2020-03-11 16:52:03", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141882683925991424": {"followers": "591", "datetime": "2019-06-21 01:37:01", "author": "@ridoridho", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141956047990603776": {"followers": "6,426", "datetime": "2019-06-21 06:28:33", "author": "@mauricioaniche", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141962801696690176": {"followers": "158", "datetime": "2019-06-21 06:55:23", "author": "@jlibovicky", "content_summary": "I always thought it would be Cookie Monster, but XLNet is not a bad name at all for a villain killing BERT. #nlpproc"}, "1143680182886289409": {"followers": "426", "datetime": "2019-06-26 00:39:38", "author": "@MK_Mayer", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1213410042050859009": {"followers": "3,500", "datetime": "2020-01-04 10:41:12", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnAr5cBA"}, "1143480718334537733": {"followers": "490", "datetime": "2019-06-25 11:27:02", "author": "@_davemacdonald", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143621876897460224": {"followers": "11,002", "datetime": "2019-06-25 20:47:57", "author": "@data_hpz", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142000475656445952": {"followers": "78", "datetime": "2019-06-21 09:25:05", "author": "@watchdog20xx", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141769059664613376": {"followers": "595", "datetime": "2019-06-20 18:05:31", "author": "@radevd", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143545081766563841": {"followers": "30", "datetime": "2019-06-25 15:42:48", "author": "@strangeip", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143125517425500160": {"followers": "635", "datetime": "2019-06-24 11:55:36", "author": "@rkakamilan", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1143900668152963072": {"followers": "177", "datetime": "2019-06-26 15:15:46", "author": "@GharibiHadi", "content_summary": "1/4 \u0645\u06cc\u0644\u06cc\u0648\u0646 \u062f\u0644\u0627\u0631 \u0628\u0631\u0627\u06cc \u062a\u0631\u06cc\u0646 \u0627\u06cc\u0646 \u0645\u062f\u0644 \u06a9\u0630\u0627\u06cc\u06cc. \u062f\u0631 \u0636\u0645\u0646 \u0646\u06a9\u062a\u0647 \u0627\u06cc\u0646\u0647 \u06a9\u0647 \u0645\u0639\u0644\u0648\u0645 \u0646\u06cc\u0633 \u0686\u0646 \u0645\u06cc\u0644\u06cc\u0648\u0646 \u062f\u0644\u0627\u0631 \u0647\u0632\u06cc\u0646\u0647 \u0634\u062f\u0647 \u062a\u0627 \u0628\u0647 \u0627\u06cc\u0646 \u06a9\u0627\u0646\u0641\u06cc\u06af \u0648 \u067e\u0627\u0631\u0627\u0645\u062a\u0631\u0627 \u0631\u0633\u06cc\u062f\u0646. \u0627\u0632 \u0627\u06cc\u0646\u062c\u0627 \u06cc\u0647 \u0633\u0644\u0627\u0645\u06cc \u0645\u06cc\u06a9\u0646\u0645 \u0628\u0647 \u0647\u0645\u0647 \u0627\u0648\u0646\u0627\u06cc\u06cc \u06a9\u0647 \u0631\u0648\u06cc \u062a\u06a9\u0633\u062a \u062f\u0627\u0631\u0646 \u0631\u06cc\u0633\u0631\u0686 \u0645\u06cc\u06a9\u0646\u0646 :)) \u0627\u06cc\u0634\u0627\u0644\u0627 \u062f\u0627\u0646\u0634\u06af\u0627\u0647 \u0634\u0645\u0627 \u0647\u0645 \u0628\u0647\u062a\u0648\u0646 512 \u062a\u0627 tpu \u0645\u06cc\u062f\u0647"}, "1141626248352808962": {"followers": "75", "datetime": "2019-06-20 08:38:02", "author": "@parag_jain", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142839285348913152": {"followers": "16", "datetime": "2019-06-23 16:58:13", "author": "@SyenPark", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141629617226145793": {"followers": "291", "datetime": "2019-06-20 08:51:25", "author": "@sculpepper", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142791722256228359": {"followers": "56,127", "datetime": "2019-06-23 13:49:13", "author": "@IntelligenceTV", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1213666777319854080": {"followers": "3,500", "datetime": "2020-01-05 03:41:23", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141684532107759617": {"followers": "727", "datetime": "2019-06-20 12:29:38", "author": "@DesireYavro", "content_summary": "\" Empirically, #XLNet outperforms #BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including #questionanswering, #naturallanguageinference, #sentiment analysis, and document ranking\" #NLP #ML #AI https:// https"}, "1141519976056012800": {"followers": "228", "datetime": "2019-06-20 01:35:45", "author": "@niviksha", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1141654368518844418": {"followers": "16", "datetime": "2019-06-20 10:29:47", "author": "@SeneseMatteo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141844357764403200": {"followers": "69", "datetime": "2019-06-20 23:04:43", "author": "@asiddhant1", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141840707839115264": {"followers": "212", "datetime": "2019-06-20 22:50:13", "author": "@nanjakorewa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141527994214699008": {"followers": "366", "datetime": "2019-06-20 02:07:37", "author": "@_arohan_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141603563493318656": {"followers": "645", "datetime": "2019-06-20 07:07:54", "author": "@ionandrou", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142348674745286656": {"followers": "16", "datetime": "2019-06-22 08:28:42", "author": "@umy_chang", "content_summary": "XLNet, our next choice for ML!?"}, "1143720564093259776": {"followers": "11,787", "datetime": "2019-06-26 03:20:06", "author": "@yutakashino", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cCloud TPU v3\u306f\u4e00\u3064\u3042\u305f\u308a$8/h\u3060\u304b\u3089\uff0c2654\u4e07\u5186\u304b\u304b\u308b\u2026\uff0e\u3082\u3046NLP\u306e\u7814\u7a76\u3082\u5727\u5012\u7684\u306a\u8a08\u7b97\u529b\u3067\u6bb4\u308b\u6642\u4ee3\u306b\u306a\u3063\u3066\u307e\u3059\u306d\u2026 https://t.co/K55jMrXKlI"}, "1141888722725756928": {"followers": "340", "datetime": "2019-06-21 02:01:01", "author": "@vineelaero", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141544298837499904": {"followers": "1,821", "datetime": "2019-06-20 03:12:24", "author": "@rosinality", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143574913493688320": {"followers": "6", "datetime": "2019-06-25 17:41:20", "author": "@themasad789", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141590007628668928": {"followers": "130", "datetime": "2019-06-20 06:14:02", "author": "@Mo_Fad3l", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141982050129469440": {"followers": "3", "datetime": "2019-06-21 08:11:52", "author": "@starimpact1983", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141698465258332161": {"followers": "80", "datetime": "2019-06-20 13:25:00", "author": "@Mr_WhiteHawk", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143912498636054530": {"followers": "5", "datetime": "2019-06-26 16:02:47", "author": "@yuhang_dl", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141693052458885121": {"followers": "689", "datetime": "2019-06-20 13:03:29", "author": "@BenSingletonNYC", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143446802340950016": {"followers": "589", "datetime": "2019-06-25 09:12:16", "author": "@ArthurCamara", "content_summary": "Small (and medium, even) research groups just cannot compete on that level. We can only compete on crazy or side ideas."}, "1143652066285498368": {"followers": "154", "datetime": "2019-06-25 22:47:55", "author": "@ms_minervini", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141582486566100992": {"followers": "101", "datetime": "2019-06-20 05:44:09", "author": "@iamgk91", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141960037947940864": {"followers": "45", "datetime": "2019-06-21 06:44:24", "author": "@andromeda_aaa1", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142036606901194756": {"followers": "151", "datetime": "2019-06-21 11:48:39", "author": "@_Boris_Belousov", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1200720727797665792": {"followers": "554", "datetime": "2019-11-30 10:18:24", "author": "@FBWM8888", "content_summary": "RT @jaguring1: Meta-Learning Update Rules for Unsupervised Representation Learning https://t.co/BJsGsZxbeL On the Variance of the Adaptive\u2026"}, "1179869131677454337": {"followers": "205", "datetime": "2019-10-03 21:21:36", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141594129039708163": {"followers": "106", "datetime": "2019-06-20 06:30:24", "author": "@fengyuncrawl", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141734589834178563": {"followers": "297", "datetime": "2019-06-20 15:48:33", "author": "@jmcimula", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1143519208984207365": {"followers": "170", "datetime": "2019-06-25 13:59:59", "author": "@RuochengGuoASU", "content_summary": "Suddenly people start to pay attention to the cost of deep learning experiments. I guess one episode of Grand Tour costs more money and generates more CO2. The key is, they contribute something to our life."}, "1141696108940238849": {"followers": "219", "datetime": "2019-06-20 13:15:38", "author": "@__phanhoang__", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1228334813892968451": {"followers": "215", "datetime": "2020-02-14 15:06:55", "author": "@CCottager", "content_summary": "RT @guillefix: has anyone tried a generalized autoregressive model like XLNet (https://t.co/D3Q6qRjjiF) where the generation order is chose\u2026"}, "1196571351802241034": {"followers": "554", "datetime": "2019-11-18 23:30:16", "author": "@FBWM8888", "content_summary": "RT @jaguring1: pre-trained\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u3066\u308b\u3002 https://t.co/56dLplYivW \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding\u2026"}, "1143123860344193025": {"followers": "181", "datetime": "2019-06-24 11:49:01", "author": "@kennyhelsens", "content_summary": "With XLNet, machines take yet another step forward in understanding human language. \ud83d\udce2 https://t.co/bBM96vvzaO"}, "1142049959375560704": {"followers": "2,671", "datetime": "2019-06-21 12:41:43", "author": "@ayirpelle", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141772274388951040": {"followers": "3,374", "datetime": "2019-06-20 18:18:17", "author": "@renato_umeton", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141574312249516032": {"followers": "5,248", "datetime": "2019-06-20 05:11:40", "author": "@ParisMLgroup", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141929002099068939": {"followers": "203", "datetime": "2019-06-21 04:41:04", "author": "@wayama_ryousuke", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141724274417442817": {"followers": "2,077", "datetime": "2019-06-20 15:07:33", "author": "@ojahnn", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1166019865104117762": {"followers": "1", "datetime": "2019-08-26 16:09:34", "author": "@WeiChengTseng1", "content_summary": "RT @GoMinSun: Latest breakthrough! https://t.co/eNj8yo5M2p"}, "1141670480685850624": {"followers": "558", "datetime": "2019-06-20 11:33:48", "author": "@i4mwh4ti4m", "content_summary": "https://t.co/GDFBN8Ft1P \u306f\uff1f\uff1f\uff1f https://t.co/7MCbjya4c4"}, "1142254593021104129": {"followers": "727", "datetime": "2019-06-22 02:14:51", "author": "@wenmingye", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1167019175480635393": {"followers": "5,314", "datetime": "2019-08-29 10:20:28", "author": "@HubBucket", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk https://t.co/owzX13zbgg #DeepLearnin\u2026"}, "1141603527887937538": {"followers": "1,433", "datetime": "2019-06-20 07:07:45", "author": "@GlobeNewsExp", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding by https://t.co/HMaarfxGgG #GlobeNewsExpress #News"}, "1143433618892775424": {"followers": "1,125", "datetime": "2019-06-25 08:19:53", "author": "@0xSkywalker", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143570585408004096": {"followers": "1,896", "datetime": "2019-06-25 17:24:08", "author": "@timalthoff", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141901892164804608": {"followers": "96", "datetime": "2019-06-21 02:53:21", "author": "@harusugi5", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141552991486369792": {"followers": "65", "datetime": "2019-06-20 03:46:56", "author": "@kli_nlpr", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1143688993210454016": {"followers": "251", "datetime": "2019-06-26 01:14:39", "author": "@Vaishaal", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1165668063754088448": {"followers": "205", "datetime": "2019-08-25 16:51:38", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141730290479587328": {"followers": "76", "datetime": "2019-06-20 15:31:28", "author": "@JhairGallardo", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141710959192530944": {"followers": "249", "datetime": "2019-06-20 14:14:39", "author": "@derekchen14", "content_summary": "A new pre-trained model that outperforms BERT https://t.co/8VWGAsLb1f from @GoogleAI New SOTA of GLUE benchmarks and many others! The madness continues \ud83d\ude01"}, "1141697897588654083": {"followers": "203", "datetime": "2019-06-20 13:22:45", "author": "@wayama_ryousuke", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1142059709505122308": {"followers": "8,398", "datetime": "2019-06-21 13:20:27", "author": "@HelpfulTangent", "content_summary": "@IS_GEOCommunity"}, "1143375041561812993": {"followers": "189", "datetime": "2019-06-25 04:27:07", "author": "@AvneeshSarwate", "content_summary": "RT @baykenney: access to compute can't be decoupled from responsible development of ml systems. https://t.co/8pAmAV39Mg"}, "1143458984260591616": {"followers": "218", "datetime": "2019-06-25 10:00:40", "author": "@AssistedEvolve", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143813428798115844": {"followers": "373", "datetime": "2019-06-26 09:29:07", "author": "@rfh100", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144187701190483969": {"followers": "68", "datetime": "2019-06-27 10:16:20", "author": "@polyatree", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143654928621039617": {"followers": "109", "datetime": "2019-06-25 22:59:17", "author": "@ID9112150632304", "content_summary": "So, for the humanity\u2019s sake, every models should be shared in public."}, "1143769630785150978": {"followers": "1,131", "datetime": "2019-06-26 06:35:04", "author": "@BioDecoded", "content_summary": "Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk \u2026 https://t.co/owzX13zbgg #DeepLearning #NLP https://t.co/4GPE7dkP8R"}, "1141580423090376705": {"followers": "818", "datetime": "2019-06-20 05:35:57", "author": "@ErmiaBivatan", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141708672965304322": {"followers": "10", "datetime": "2019-06-20 14:05:34", "author": "@_Mingyang_Zhang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143670262400835584": {"followers": "824", "datetime": "2019-06-26 00:00:13", "author": "@morioka", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143729255009751041": {"followers": "2,481", "datetime": "2019-06-26 03:54:38", "author": "@gravitino", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141513759254622208": {"followers": "2,675", "datetime": "2019-06-20 01:11:03", "author": "@MohitIyyer", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141834221289463808": {"followers": "309", "datetime": "2019-06-20 22:24:27", "author": "@MichaelMallari", "content_summary": "#DataScience #Analytics https://t.co/PHLSE2CzG8"}, "1184617935278444544": {"followers": "205", "datetime": "2019-10-16 23:51:39", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141667077247266817": {"followers": "15", "datetime": "2019-06-20 11:20:17", "author": "@Andompesta90", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1146622214818500608": {"followers": "29", "datetime": "2019-07-04 03:30:13", "author": "@naru_arena", "content_summary": "XLNet\u304cSOTA\u306b\u306a\u3063\u305f\u3051\u3069\u3001\u5024\u6bb5\u2026\u2026"}, "1152089942341881857": {"followers": "121", "datetime": "2019-07-19 05:37:01", "author": "@blaine_bateman", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/QUtjzwPZQs https://t.co/NEtFN5Capq"}, "1143767900940976128": {"followers": "6", "datetime": "2019-06-26 06:28:12", "author": "@nahidcse05", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141614929406877697": {"followers": "350", "datetime": "2019-06-20 07:53:04", "author": "@vochicong", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142745931655200768": {"followers": "4", "datetime": "2019-06-23 10:47:15", "author": "@ucGe7kAGr8a05M8", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1145492645050871809": {"followers": "635", "datetime": "2019-07-01 00:41:43", "author": "@rkakamilan", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1143390004846030848": {"followers": "8", "datetime": "2019-06-25 05:26:34", "author": "@Haruforstudy1", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1141630534482898944": {"followers": "16", "datetime": "2019-06-20 08:55:04", "author": "@p_kot1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141671834049400832": {"followers": "45", "datetime": "2019-06-20 11:39:11", "author": "@andromeda_aaa1", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1145632196138434560": {"followers": "2", "datetime": "2019-07-01 09:56:15", "author": "@vtemker", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142036531235844096": {"followers": "12,765", "datetime": "2019-06-21 11:48:21", "author": "@jaguring1", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1141840747169120256": {"followers": "206", "datetime": "2019-06-20 22:50:23", "author": "@bg01shan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145849401447014401": {"followers": "1,077", "datetime": "2019-07-02 00:19:20", "author": "@AaronHosford", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141988175436374016": {"followers": "65", "datetime": "2019-06-21 08:36:12", "author": "@Motono_Mokuami", "content_summary": "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067BERT\u3092\u4e0a\u56de\u308b\u6027\u80fd\u3092\u793a\u3059XLNet\u3002"}, "1143443298297831424": {"followers": "104", "datetime": "2019-06-25 08:58:21", "author": "@daumke", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141832449418309633": {"followers": "81", "datetime": "2019-06-20 22:17:24", "author": "@giulianobertoti", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141707027724431366": {"followers": "273", "datetime": "2019-06-20 13:59:01", "author": "@morinosuke__p", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1141611914205835264": {"followers": "493", "datetime": "2019-06-20 07:41:05", "author": "@RSprugnoli", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1162441882011463680": {"followers": "476", "datetime": "2019-08-16 19:11:56", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141910063411392514": {"followers": "546", "datetime": "2019-06-21 03:25:49", "author": "@U_M_V_U_E", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142383025604349954": {"followers": "615", "datetime": "2019-06-22 10:45:12", "author": "@KouroshMeshgi", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141702931227369473": {"followers": "165", "datetime": "2019-06-20 13:42:45", "author": "@samurairodeo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141917389660905472": {"followers": "14,836", "datetime": "2019-06-21 03:54:56", "author": "@mathwuyue", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141775673167667201": {"followers": "742", "datetime": "2019-06-20 18:31:48", "author": "@xpasky", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141883679595089920": {"followers": "11,476", "datetime": "2019-06-21 01:40:59", "author": "@icoxfog417", "content_summary": "BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001Context\u81ea\u4f53\u306f\u901a\u5e38\u306eSelf\u3092\u542b\u3080Attention\u3067\u4f5c\u6210\u3059\u308b\u300220\u30bf\u30b9\u30af\u3067BERT\u3092\u4e0a\u56de\u308b\u6210\u679c"}, "1141578435824934913": {"followers": "4,567", "datetime": "2019-06-20 05:28:03", "author": "@DrALauterbach", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1163753013254721536": {"followers": "222", "datetime": "2019-08-20 10:01:54", "author": "@PapersTrending", "content_summary": "[9/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 10,953 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1143817846130118656": {"followers": "1,102", "datetime": "2019-06-26 09:46:40", "author": "@AltunaAkalin", "content_summary": "RT @AlbertVilella: The @GCPcloud #TPUv3 was used for #XLNet which comes very close to human ceiling performance of reading comprehension of\u2026"}, "1141708818784514055": {"followers": "871", "datetime": "2019-06-20 14:06:08", "author": "@mihail_eric", "content_summary": "Woah BERT got an upgrade! New SOTA on 18 tasks \ud83d\ude27 from @GoogleAI"}, "1141526726238826496": {"followers": "898", "datetime": "2019-06-20 02:02:34", "author": "@kylewadegrove", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141823371954577408": {"followers": "3,500", "datetime": "2019-06-20 21:41:20", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1142019227202703360": {"followers": "19,021", "datetime": "2019-06-21 10:39:36", "author": "@gijigae", "content_summary": "\u53bb\u5e74\u8a71\u984c\u3068\u306a\u3063\u3066\u305f\u3001BERT\u3002 \u30b0\u30fc\u30b0\u30eb\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u9762\u3067BERT\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u65b0\u3057\u3044\u30e2\u30c7\u30eb\uff08XLNet\uff09\u3092\u767a\u8868\u3002SQuAD2.0\u3067\u306f\uff17\u30dd\u30a4\u30f3\u30c8\u3082\u30a2\u30c3\u30d7\ud83d\ude80\u3002 \u25a0\u4eba\u9593\uff1a86.831 \u25a0XLNet\uff1a86.346 \u25a0BERT\uff1a78.98 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u80fd\u529b\u306e\u5411\u4e0a\u306f\u4eca\u5f8c\u6a5f\u68b0\u7ffb\u8a33\u306e\u7cbe\u5ea6\u30a2\u30c3\u30d7\u306b\u3082\u7e4b\u304c\u308b\u306f\u305a\ud83e\udd16\u3002 https://t.co/6nj8H3xcAf"}, "1141599687537020928": {"followers": "74", "datetime": "2019-06-20 06:52:30", "author": "@_nicolasmichel", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141677292084531200": {"followers": "824", "datetime": "2019-06-20 12:00:52", "author": "@morioka", "content_summary": "RT @stomohide: \u8ad6\u6587\u51fa\u3066\u307e\u3057\u305f\u3002 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Dl10X4T442"}, "1162732519768363009": {"followers": "42", "datetime": "2019-08-17 14:26:49", "author": "@MishakinSergey", "content_summary": "RT @PapersTrending: [10/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 10,860 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JS\u2026"}, "1160919271737679873": {"followers": "122", "datetime": "2019-08-12 14:21:37", "author": "@hasansaikatt", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143701629662875648": {"followers": "498", "datetime": "2019-06-26 02:04:52", "author": "@bitking69", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141780378849611776": {"followers": "1,136", "datetime": "2019-06-20 18:50:30", "author": "@mjp39", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141791172882501632": {"followers": "303", "datetime": "2019-06-20 19:33:23", "author": "@covolution", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141614750566178817": {"followers": "11,002", "datetime": "2019-06-20 07:52:21", "author": "@data_hpz", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141881635253772289": {"followers": "135", "datetime": "2019-06-21 01:32:51", "author": "@deebuls", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1258804000100474880": {"followers": "276", "datetime": "2020-05-08 17:00:35", "author": "@holic_ak199827", "content_summary": "\u306f\u30fc\u3001bert\u3082\u8d8a\u3048\u308b\u3093\u3067\u3059\u304b"}, "1143448856178450432": {"followers": "184", "datetime": "2019-06-25 09:20:26", "author": "@valentinp", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141602811064590336": {"followers": "7", "datetime": "2019-06-20 07:04:54", "author": "@GnosisYu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142503251981606913": {"followers": "66", "datetime": "2019-06-22 18:42:56", "author": "@shubh_300595", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143499706636689408": {"followers": "4,812", "datetime": "2019-06-25 12:42:29", "author": "@IntuitMachine", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141704823135526912": {"followers": "239", "datetime": "2019-06-20 13:50:16", "author": "@binalkp91", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141644604565086210": {"followers": "560", "datetime": "2019-06-20 09:50:59", "author": "@yuvalmarton", "content_summary": "RT @zehavoc: I'm a bit disappointed they didn't title their paper \"Bert is Dead: Behold Optimus Prime\" https://t.co/00V8BnW2Qr"}, "1141904943105765376": {"followers": "598", "datetime": "2019-06-21 03:05:28", "author": "@databoydg", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141583831473913856": {"followers": "5", "datetime": "2019-06-20 05:49:29", "author": "@Koundinya33", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1167006802795466752": {"followers": "11,930", "datetime": "2019-08-29 09:31:18", "author": "@Rosenchild", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk https://t.co/owzX13zbgg #DeepLearnin\u2026"}, "1141911761001439232": {"followers": "294", "datetime": "2019-06-21 03:32:34", "author": "@rose_miura", "content_summary": "RT @shohomiura: BERT\u3092\u8d85\u3048\u305f\u30e2\u30c7\u30ebcoming out! https://t.co/kIAYdheNPU"}, "1144016030647410693": {"followers": "933", "datetime": "2019-06-26 22:54:11", "author": "@sertansenturk", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144173985392943104": {"followers": "63", "datetime": "2019-06-27 09:21:50", "author": "@DebjitPaul2", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141570248623005697": {"followers": "7", "datetime": "2019-06-20 04:55:31", "author": "@ruanning", "content_summary": "Awesome, look forward to using it in the products!"}, "1141519294208991232": {"followers": "154", "datetime": "2019-06-20 01:33:02", "author": "@davidkmyang", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1141776052181766144": {"followers": "18", "datetime": "2019-06-20 18:33:18", "author": "@LudoGibert", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141774664420737027": {"followers": "372", "datetime": "2019-06-20 18:27:47", "author": "@jvlmdr", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142449272270544896": {"followers": "164", "datetime": "2019-06-22 15:08:26", "author": "@keviv9", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141747387708399616": {"followers": "620", "datetime": "2019-06-20 16:39:24", "author": "@ykilcher", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141775335609864192": {"followers": "188", "datetime": "2019-06-20 18:30:27", "author": "@blueviggen", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1147650047506558983": {"followers": "696", "datetime": "2019-07-06 23:34:28", "author": "@HumansOfML", "content_summary": "BERT does really well, but it's not a true language model because you can't get a factorization over the whole sequence. So just consider all the permutations (and incorporate every other language model trick in the book too). https://t.co/7KRtzaGjmz"}, "1145682334370287617": {"followers": "70", "datetime": "2019-07-01 13:15:28", "author": "@mlmasasing", "content_summary": "XLNet\u306e\u8ad6\u6587\u3092\u30e1\u30e2\u3057\u3066\u304a\u304f https://t.co/nHgneekTeM"}, "1141600622279446528": {"followers": "608", "datetime": "2019-06-20 06:56:12", "author": "@thtrieu_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141588491324674053": {"followers": "45", "datetime": "2019-06-20 06:08:00", "author": "@yoquankara", "content_summary": "BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google)"}, "1143579685072384000": {"followers": "38", "datetime": "2019-06-25 18:00:18", "author": "@woellhof", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141597312445296641": {"followers": "936", "datetime": "2019-06-20 06:43:03", "author": "@n0mad_0", "content_summary": "\ud83d\ude31"}, "1141708121665990658": {"followers": "52", "datetime": "2019-06-20 14:03:22", "author": "@salil_23", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144980958019489792": {"followers": "48", "datetime": "2019-06-29 14:48:27", "author": "@luciamac_", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1219340436503875584": {"followers": "22", "datetime": "2020-01-20 19:26:28", "author": "@IntoCrypto2", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1142566561108168706": {"followers": "2,910", "datetime": "2019-06-22 22:54:30", "author": "@carsondahlberg", "content_summary": "Google Brain's #XLNet overcomes the limitations of Google's #BERT on 20 #NLP tasks and achieves SOTA in 18 NLP benchmark tasks #datascience #ai #dl https://t.co/EssHpoTtH9"}, "1145817253847293952": {"followers": "2", "datetime": "2019-07-01 22:11:36", "author": "@sunny_mogu", "content_summary": "Looking into it. A more powerful model, especially for long sentences."}, "1142272654105354240": {"followers": "271", "datetime": "2019-06-22 03:26:37", "author": "@K4L1_Linux", "content_summary": "RT @malhamid: \u0646\u0634\u0631 \u0642\u0628\u0644 \u064a\u0648\u0645\u064a\u0646 \u0648\u0631\u0642\u0629 \u0628\u062d\u062b \u0645\u0645\u064a\u0632\u0629 \u0641\u064a \u0646\u0633\u062e\u062a\u0647\u0627 \u0627\u0644\u0623\u0648\u0644\u064a\u0629 \u0639\u0646 \u0646\u0645\u0648\u0630\u062c #\u062a\u0639\u0644\u0645_\u0627\u0644\u0622\u0644\u0629 \u0627\u0637\u0644\u0642 \u0639\u0644\u064a\u0647 (XLNet) \u064a\u062a\u0641\u0648\u0642 \u0639\u0644\u0649 \u0646\u0645\u0648\u0630\u062c (BERT) \u0628\u0647\u0627\u0645\u0634 \u0643\u0628\u064a\u0631. \u0648\u064a\u0645\u062b\u2026"}, "1141694504442855424": {"followers": "115", "datetime": "2019-06-20 13:09:16", "author": "@HumanYaduvanshi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141702808795635713": {"followers": "165", "datetime": "2019-06-20 13:42:16", "author": "@samurairodeo", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143371649380581376": {"followers": "0", "datetime": "2019-06-25 04:13:38", "author": "@Sam09lol", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143638062167220224": {"followers": "32", "datetime": "2019-06-25 21:52:16", "author": "@gazbock", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1210412700133556226": {"followers": "29", "datetime": "2019-12-27 04:10:50", "author": "@satochan3955", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141996774871015429": {"followers": "117", "datetime": "2019-06-21 09:10:23", "author": "@Kage2no4te", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141713648190029824": {"followers": "764", "datetime": "2019-06-20 14:25:20", "author": "@cfiken", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1142567956389535744": {"followers": "534", "datetime": "2019-06-22 23:00:03", "author": "@PiotrKrosniak", "content_summary": "RT @carsondahlberg: Google Brain's #XLNet overcomes the limitations of Google's #BERT on 20 #NLP tasks and achieves SOTA in 18 NLP benchmar\u2026"}, "1143099100897800192": {"followers": "824", "datetime": "2019-06-24 10:10:38", "author": "@morioka", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1142309685770383360": {"followers": "103", "datetime": "2019-06-22 05:53:46", "author": "@Saurabh300795", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143711157959712769": {"followers": "177", "datetime": "2019-06-26 02:42:43", "author": "@leonfrench", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141565192922263552": {"followers": "13", "datetime": "2019-06-20 04:35:25", "author": "@adityankur", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141997792128188417": {"followers": "189", "datetime": "2019-06-21 09:14:25", "author": "@tnarihi", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143571219087802368": {"followers": "1,993", "datetime": "2019-06-25 17:26:39", "author": "@dvdgrs", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144551040257318913": {"followers": "5,314", "datetime": "2019-06-28 10:20:07", "author": "@HubBucket", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk \u2026 https://t.co/owzX13zbgg #DeepLear\u2026"}, "1141748075423313920": {"followers": "301", "datetime": "2019-06-20 16:42:08", "author": "@tobias_sterbak", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142154039079006208": {"followers": "385", "datetime": "2019-06-21 19:35:17", "author": "@shravankumar147", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141602083772293120": {"followers": "142", "datetime": "2019-06-20 07:02:01", "author": "@soroushjv", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141885094413475840": {"followers": "16", "datetime": "2019-06-21 01:46:36", "author": "@JiaJimmy1", "content_summary": "Never stop learning."}, "1141700860696645637": {"followers": "6", "datetime": "2019-06-20 13:34:31", "author": "@SaidFaraby", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143372699877748736": {"followers": "441", "datetime": "2019-06-25 04:17:49", "author": "@Tuhin66978276", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143453392817156096": {"followers": "2,491", "datetime": "2019-06-25 09:38:27", "author": "@maelorin", "content_summary": "RT @baykenney: access to compute can't be decoupled from responsible development of ml systems. https://t.co/8pAmAV39Mg"}, "1141853995549356032": {"followers": "81", "datetime": "2019-06-20 23:43:01", "author": "@kartikperisetla", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141550125698838528": {"followers": "3", "datetime": "2019-06-20 03:35:33", "author": "@bellar3464", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141761748115968005": {"followers": "515", "datetime": "2019-06-20 17:36:28", "author": "@belizgunel", "content_summary": "Includes great discussion, very impressive work!"}, "1148532516866994176": {"followers": "222", "datetime": "2019-07-09 10:01:05", "author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 3,656 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141716981713788928": {"followers": "204", "datetime": "2019-06-20 14:38:35", "author": "@aarcher510", "content_summary": "Even when working at Google in NLP research, I still hear about brand new Google Brain NLP research on Twitter \ud83d\ude05"}, "1141520900875051010": {"followers": "179", "datetime": "2019-06-20 01:39:25", "author": "@ajnovice", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141676879235227648": {"followers": "261", "datetime": "2019-06-20 11:59:13", "author": "@GuptaRajat033", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141605957899825152": {"followers": "31", "datetime": "2019-06-20 07:17:25", "author": "@CherguiSafouane", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143702097499578369": {"followers": "194", "datetime": "2019-06-26 02:06:43", "author": "@raynbowy23", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143368032158461952": {"followers": "97", "datetime": "2019-06-25 03:59:16", "author": "@TricklerHQ", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142032565945229313": {"followers": "183", "datetime": "2019-06-21 11:32:36", "author": "@dipeshtech", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142552876264382464": {"followers": "98", "datetime": "2019-06-22 22:00:07", "author": "@JekiCode", "content_summary": "Xlnet-pytorch: 2019 Google Brain's XLNet Pytorch Implementation https://t.co/a149VVNI8L #Python"}, "1141635523011891200": {"followers": "107", "datetime": "2019-06-20 09:14:53", "author": "@agispof", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141894263065759744": {"followers": "27", "datetime": "2019-06-21 02:23:02", "author": "@mahesh21aug", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143649139810549761": {"followers": "778", "datetime": "2019-06-25 22:36:17", "author": "@RyanFedasiuk", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1141801079841218560": {"followers": "27", "datetime": "2019-06-20 20:12:45", "author": "@chaolincsu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141602145399185408": {"followers": "23,239", "datetime": "2019-06-20 07:02:16", "author": "@newsycbot", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/EMNYfZdWK8 (cmts https://t.co/0JU89BaZov)"}, "1143643021101031424": {"followers": "476", "datetime": "2019-06-25 22:11:58", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141625416630730752": {"followers": "112", "datetime": "2019-06-20 08:34:44", "author": "@MSripadarao", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141700508387663874": {"followers": "59", "datetime": "2019-06-20 13:33:07", "author": "@cooiiiuu", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1146069364036296704": {"followers": "533", "datetime": "2019-07-02 14:53:23", "author": "@thulani_io", "content_summary": "RT @rctatman: Time to pick the next @kaggle reading group paper! Your options: - XLNet: Generalized Autoregressive Pretraining for NLU htt\u2026"}, "1165747344022765568": {"followers": "1,131", "datetime": "2019-08-25 22:06:40", "author": "@BioDecoded", "content_summary": "Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk https://t.co/owzX13zbgg #DeepLearning #NLP https://t.co/g1roeJq9db"}, "1141523679588655104": {"followers": "146", "datetime": "2019-06-20 01:50:28", "author": "@SergeyFeldman", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143563273423470593": {"followers": "119", "datetime": "2019-06-25 16:55:05", "author": "@cpdiku", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141539559559188480": {"followers": "19,440", "datetime": "2019-06-20 02:53:34", "author": "@Sam_Witteveen", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141844818324123648": {"followers": "366", "datetime": "2019-06-20 23:06:33", "author": "@gamaga_ai", "content_summary": "A new king is in town, and it's not a character from Sesame Street. Impressive work achieving SoTA in 18 #NLProc tasks. It's amazing how far a good old large-scale pre-training can get ya. Paper at https://t.co/UydJ6d1qCM https://t.co/Vs5sjDvpaD"}, "1186277619966197761": {"followers": "302", "datetime": "2019-10-21 13:46:39", "author": "@subhobrata1", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1142128806859165697": {"followers": "1,692", "datetime": "2019-06-21 17:55:01", "author": "@YadKonrad", "content_summary": "Woah, for once went off twitter for a couple of days and missed some ML nxt lvl shit, aka SOTA! \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\". - \"XLNet outperforms BERT on 20 tasks\" - paper: (https://t.co/aWbfynKlbW) - source:"}, "1143855077955760128": {"followers": "54", "datetime": "2019-06-26 12:14:37", "author": "@atsuc", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143693627484454914": {"followers": "1,850", "datetime": "2019-06-26 01:33:04", "author": "@seamustuohy", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143403638137581568": {"followers": "1,054", "datetime": "2019-06-25 06:20:45", "author": "@nersonu", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143883730114752515": {"followers": "24", "datetime": "2019-06-26 14:08:28", "author": "@javiherreranl", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143578550403887104": {"followers": "256", "datetime": "2019-06-25 17:55:47", "author": "@PositivityNoH8", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143730258752888833": {"followers": "569", "datetime": "2019-06-26 03:58:37", "author": "@T45356", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141727735443673089": {"followers": "415", "datetime": "2019-06-20 15:21:19", "author": "@unrahu1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143504080599433216": {"followers": "52", "datetime": "2019-06-25 12:59:52", "author": "@p_fecht", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143786759723069440": {"followers": "405", "datetime": "2019-06-26 07:43:08", "author": "@kerstin_bach", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141916450296217600": {"followers": "2,861", "datetime": "2019-06-21 03:51:12", "author": "@Tarpon_red2", "content_summary": "RT @shohomiura: BERT\u3092\u8d85\u3048\u305f\u30e2\u30c7\u30ebcoming out! https://t.co/kIAYdheNPU"}, "1141587824707162112": {"followers": "41", "datetime": "2019-06-20 06:05:21", "author": "@pranaydeeps", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143581399603134467": {"followers": "28,154", "datetime": "2019-06-25 18:07:07", "author": "@AthertonKD", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141849274147205120": {"followers": "1,132", "datetime": "2019-06-20 23:24:16", "author": "@vsatayamas", "content_summary": "RT @n0mad_0: \ud83d\ude31 https://t.co/uRioG1LkqW"}, "1143802192257622016": {"followers": "51", "datetime": "2019-06-26 08:44:28", "author": "@forasteran", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143517954480463872": {"followers": "771", "datetime": "2019-06-25 13:55:00", "author": "@aCraigPfeifer", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141755160588238848": {"followers": "36", "datetime": "2019-06-20 17:10:17", "author": "@Yo_Lucky_Rockz", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141701321424171008": {"followers": "842", "datetime": "2019-06-20 13:36:21", "author": "@kazanagisora", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141723621225897984": {"followers": "57", "datetime": "2019-06-20 15:04:58", "author": "@lmdehaas", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142928375100502016": {"followers": "1,347", "datetime": "2019-06-23 22:52:13", "author": "@tswaterman", "content_summary": "We\u2019re clearly still in the \u201covertaking on the straightaway\u201d phase of deep learning for NLP."}, "1141887307588366337": {"followers": "3,450", "datetime": "2019-06-21 01:55:24", "author": "@piacere_ex", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1142178274132389888": {"followers": "109", "datetime": "2019-06-21 21:11:35", "author": "@JamesONeil21", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141912396321185792": {"followers": "92", "datetime": "2019-06-21 03:35:05", "author": "@andre134679", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141576646434017280": {"followers": "1", "datetime": "2019-06-20 05:20:56", "author": "@PaulHuang668", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141581640000364544": {"followers": "382", "datetime": "2019-06-20 05:40:47", "author": "@beramMusa", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141513605570973696": {"followers": "0", "datetime": "2019-06-20 01:10:26", "author": "@Sam09lol", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141770980169277440": {"followers": "1,896", "datetime": "2019-06-20 18:13:09", "author": "@Allpowersphysio", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1154298971935764481": {"followers": "9", "datetime": "2019-07-25 07:54:55", "author": "@anurag0676", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141727234224377856": {"followers": "143", "datetime": "2019-06-20 15:19:19", "author": "@hermosillo_17", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141883091943714818": {"followers": "72", "datetime": "2019-06-21 01:38:38", "author": "@gabeibagon", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141682801273556992": {"followers": "824", "datetime": "2019-06-20 12:22:45", "author": "@morioka", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143503955588243456": {"followers": "556", "datetime": "2019-06-25 12:59:22", "author": "@porestar", "content_summary": "Can someone explain why cost this high for a SOTA model is surprising? In the end its a matter of cost-profit."}, "1141564758711242752": {"followers": "96", "datetime": "2019-06-20 04:33:42", "author": "@uamaximusua", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143531236872400897": {"followers": "156", "datetime": "2019-06-25 14:47:47", "author": "@leticiapintoa", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141832266441617409": {"followers": "90,659", "datetime": "2019-06-20 22:16:41", "author": "@stanfordnlp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143565004815384577": {"followers": "12,663", "datetime": "2019-06-25 17:01:58", "author": "@depijama", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143606094905823233": {"followers": "1,169", "datetime": "2019-06-25 19:45:14", "author": "@geofree", "content_summary": "https://t.co/eDd7SLzHAA"}, "1143011833063735298": {"followers": "93", "datetime": "2019-06-24 04:23:51", "author": "@NakanoSwitch", "content_summary": "https://t.co/kWlPKJ9AIB"}, "1142092237976870912": {"followers": "198", "datetime": "2019-06-21 15:29:43", "author": "@dahlemd", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1150778277612544001": {"followers": "12", "datetime": "2019-07-15 14:44:56", "author": "@BishnuPChowdhu1", "content_summary": "Today\u2019s read- https://t.co/1JrwnFaWJ6 Video explanation - https://t.co/SmaHp8V4G8 Reference - Bert - https://t.co/9OMLTKc0Ob Transformer XL - https://t.co/coFaDz0x7S #MachineLearning #NLP #TransformerXL"}, "1144833938285891585": {"followers": "11,930", "datetime": "2019-06-29 05:04:15", "author": "@Rosenchild", "content_summary": "RT @BioDecoded: Google Brain\u2019s XLNet bests BERT at 20 NLP tasks | VentureBeat https://t.co/o6lUeHzyDk \u2026 https://t.co/owzX13zbgg #DeepLear\u2026"}, "1141969211633238016": {"followers": "71", "datetime": "2019-06-21 07:20:51", "author": "@MatthieuPerrot", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141605976736456704": {"followers": "3,837", "datetime": "2019-06-20 07:17:29", "author": "@zehavoc", "content_summary": "I'm a bit disappointed they didn't title their paper \"Bert is Dead: Behold Optimus Prime\""}, "1141666904890925056": {"followers": "80", "datetime": "2019-06-20 11:19:35", "author": "@meriembeloucif", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1145837292487004160": {"followers": "19,019", "datetime": "2019-07-01 23:31:13", "author": "@rctatman", "content_summary": "Time to pick the next @kaggle reading group paper! Your options: - XLNet: Generalized Autoregressive Pretraining for NLU https://t.co/FNC7E62yam - Defending Against Neural Fake News (Grover) https://t.co/cMU7CBt6Uc - EfficientNet: Model Scaling for CNNs"}, "1144391166965817344": {"followers": "95", "datetime": "2019-06-27 23:44:50", "author": "@the_dismal_tide", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1219859821404487681": {"followers": "278", "datetime": "2020-01-22 05:50:19", "author": "@SERTACTR77", "content_summary": "RT @steadydee: At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will exp\u2026"}, "1141554996112056320": {"followers": "67", "datetime": "2019-06-20 03:54:54", "author": "@roholazandie", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141706021661900800": {"followers": "293", "datetime": "2019-06-20 13:55:02", "author": "@vingovan", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1205407490881359875": {"followers": "476", "datetime": "2019-12-13 08:41:55", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141700797777895424": {"followers": "157", "datetime": "2019-06-20 13:34:16", "author": "@altescy", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141731582769680384": {"followers": "136", "datetime": "2019-06-20 15:36:36", "author": "@tuzhucheng", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141867853538217987": {"followers": "388", "datetime": "2019-06-21 00:38:05", "author": "@adropboxspace", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143812909484482560": {"followers": "280", "datetime": "2019-06-26 09:27:03", "author": "@noahcse", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141892416154361856": {"followers": "190", "datetime": "2019-06-21 02:15:42", "author": "@stephanef", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143679857362030593": {"followers": "193", "datetime": "2019-06-26 00:38:21", "author": "@Mrr_Zo", "content_summary": "\u0627\u06af\u0647 \u06f4 \u0633\u0627\u0644 \u0647\u06cc\u0686\u06cc \u0646\u062e\u0648\u0631\u0645 \u0648 \u0646\u067e\u0648\u0634\u0645 \u0648 \u062a\u0648 \u062e\u06cc\u0627\u0628\u0648\u0646 \u0628\u062e\u0648\u0627\u0628\u0645\u060c \u0634\u0627\u06cc\u062f \u0628\u0634\u0647 \u06cc\u0647 \u0628\u0627\u0631 \u0627\u06cc\u0646\u0648 train \u06a9\u0646\u0645 :|"}, "1143480549010489346": {"followers": "7", "datetime": "2019-06-25 11:26:22", "author": "@paravn", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141521255113383937": {"followers": "3,500", "datetime": "2019-06-20 01:40:50", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1143380368675880960": {"followers": "224", "datetime": "2019-06-25 04:48:17", "author": "@leoneu", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141912106410729473": {"followers": "3,541", "datetime": "2019-06-21 03:33:56", "author": "@tankazunori0914", "content_summary": "BERT\u8d85\u3048\u306f\u3084\u3044\u306a\u3002\u3002 https://t.co/kQyBj1fAQv"}, "1141530728498249728": {"followers": "48", "datetime": "2019-06-20 02:18:28", "author": "@donovanOng_", "content_summary": "New pretrained models."}, "1143599779206725632": {"followers": "51,484", "datetime": "2019-06-25 19:20:09", "author": "@SmitaNairJain", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143435145329946625": {"followers": "44", "datetime": "2019-06-25 08:25:57", "author": "@FJ_82", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143610802026491906": {"followers": "83", "datetime": "2019-06-25 20:03:57", "author": "@GuXuemei", "content_summary": "the train cost is very high \ud83d\udc40"}, "1141997501827862528": {"followers": "472", "datetime": "2019-06-21 09:13:16", "author": "@kosuke_tsujino", "content_summary": "XLNet\u306fsentencepiece\u3064\u304b\u3063\u3066\u308b\u307d\u3044 https://t.co/kHwuT3myLO"}, "1141942984835043328": {"followers": "83", "datetime": "2019-06-21 05:36:38", "author": "@petersonzilli", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144275358532825088": {"followers": "1,277", "datetime": "2019-06-27 16:04:39", "author": "@maral_kh", "content_summary": "This!"}, "1141941446301216768": {"followers": "154", "datetime": "2019-06-21 05:30:31", "author": "@gaifasu", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141764460274704384": {"followers": "419", "datetime": "2019-06-20 17:47:14", "author": "@nik0spapp", "content_summary": "XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likelihood over all permutations of the factorization order. - Encodes with Transformer-XL which captures longer-term dependencies than"}, "1141707769004867584": {"followers": "561", "datetime": "2019-06-20 14:01:58", "author": "@frankcarey", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141600543971991552": {"followers": "201", "datetime": "2019-06-20 06:55:54", "author": "@aialmeida", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143704764716179456": {"followers": "3,126", "datetime": "2019-06-26 02:17:19", "author": "@MrMeritology", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1167152921043095552": {"followers": "476", "datetime": "2019-08-29 19:11:55", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141645614977077248": {"followers": "1,081", "datetime": "2019-06-20 09:55:00", "author": "@roeeaharoni", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141588151548436481": {"followers": "519", "datetime": "2019-06-20 06:06:39", "author": "@DAIBuilds", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sentiment analysis), while integrating ideas from Transformer-XL: arxiv: https://t.co/236fncXWwm code + pretrained mode"}, "1141570606187593729": {"followers": "3,416", "datetime": "2019-06-20 04:56:56", "author": "@andrey_kurenkov", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1141585185458262016": {"followers": "12,924", "datetime": "2019-06-20 05:54:52", "author": "@deliprao", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142160493815066624": {"followers": "12,765", "datetime": "2019-06-21 20:00:56", "author": "@jaguring1", "content_summary": "\u53c2\u8003\u6587\u732e3 \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Q5YEuTNKed \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9 + pretrained \u30e2\u30c7\u30eb https://t.co/56dLplYivW \u4e2d\u56fd\u306e\u4e2d\u9ad8\u751f\u5411\u3051\u306e\u82f1\u8a9e\u554f\u984c\u3092\u4e2d\u898f\u6a21\u306b\u96c6\u3081\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u300cRACE\u300d\u306e\u30ea\u30fc\u30c0\u30fc\u30dc\u30fc\u30c9 https://t.co/UBe2m5T9G7 https://t.co/1L0RqfeShN"}, "1141715601653555200": {"followers": "58", "datetime": "2019-06-20 14:33:06", "author": "@giulio_zhou", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141759621654339584": {"followers": "736", "datetime": "2019-06-20 17:28:01", "author": "@unsorsodicorda", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141694772328853504": {"followers": "640", "datetime": "2019-06-20 13:10:20", "author": "@copasta_", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141996791384047616": {"followers": "12,765", "datetime": "2019-06-21 09:10:26", "author": "@jaguring1", "content_summary": "RT @Motono_Mokuami: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067BERT\u3092\u4e0a\u56de\u308b\u6027\u80fd\u3092\u793a\u3059XLNet\u3002 https://t.co/czqX7zjfOM"}, "1141894644151644161": {"followers": "40", "datetime": "2019-06-21 02:24:33", "author": "@yakhila_04", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143399234454130688": {"followers": "81", "datetime": "2019-06-25 06:03:15", "author": "@__sudarshan__", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141883021525757952": {"followers": "207", "datetime": "2019-06-21 01:38:22", "author": "@chriskanan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141717280629198852": {"followers": "206", "datetime": "2019-06-20 14:39:46", "author": "@irodov_rg", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141905717416333312": {"followers": "33", "datetime": "2019-06-21 03:08:33", "author": "@William33712308", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142051419324846080": {"followers": "1,793", "datetime": "2019-06-21 12:47:31", "author": "@esXFdfOJxiGBFLx", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141971952334061568": {"followers": "85", "datetime": "2019-06-21 07:31:44", "author": "@minhpham", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141718286045995009": {"followers": "419", "datetime": "2019-06-20 14:43:46", "author": "@AritzBi", "content_summary": "First ELMO, then BERT and now... XLNet."}, "1143431313657081858": {"followers": "171", "datetime": "2019-06-25 08:10:43", "author": "@dgarhdez", "content_summary": "RT @vitojph: And don't forget the CO2 footprint. We should be training these models on Mars. Let's fight against climate change on Earth\u2026"}, "1141724964204138496": {"followers": "238", "datetime": "2019-06-20 15:10:18", "author": "@Santu_Shankar", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1149812818461388800": {"followers": "1,772", "datetime": "2019-07-12 22:48:33", "author": "@stfate", "content_summary": "\u00bb [1906.08237] XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/YKhnvntQgH"}, "1148170243321356288": {"followers": "222", "datetime": "2019-07-08 10:01:32", "author": "@PapersTrending", "content_summary": "[10/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 3,605 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1161941119376269313": {"followers": "222", "datetime": "2019-08-15 10:02:05", "author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 10,750 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1144140356356734976": {"followers": "35", "datetime": "2019-06-27 07:08:12", "author": "@Geek_alphaomega", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143820731102445568": {"followers": "248", "datetime": "2019-06-26 09:58:08", "author": "@estebanpdl", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141698770029047810": {"followers": "428", "datetime": "2019-06-20 13:26:13", "author": "@yashk2810", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143550753526751232": {"followers": "1,975", "datetime": "2019-06-25 16:05:20", "author": "@gdm3000", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143963808341213185": {"followers": "0", "datetime": "2019-06-26 19:26:40", "author": "@LaraCam91978012", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141730577403527171": {"followers": "53", "datetime": "2019-06-20 15:32:36", "author": "@not_a_moonman", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141665017269346304": {"followers": "243", "datetime": "2019-06-20 11:12:05", "author": "@daigo_hirooka", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1231954783642120192": {"followers": "475", "datetime": "2020-02-24 14:51:23", "author": "@mmatthew_43", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1145896397662638080": {"followers": "52", "datetime": "2019-07-02 03:26:05", "author": "@earny_joe", "content_summary": "RT @rctatman: Time to pick the next @kaggle reading group paper! Your options: - XLNet: Generalized Autoregressive Pretraining for NLU htt\u2026"}, "1141645650129543168": {"followers": "718", "datetime": "2019-06-20 09:55:08", "author": "@_Sharraf", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1171089083856084995": {"followers": "189", "datetime": "2019-09-09 15:52:50", "author": "@goodnasubi", "content_summary": "RT @overleo: BERT\u3092\u8d85\u3048\u305fXLNet\u306e\u7d39\u4ecb - akihiro_f - Medium: \u6982\u8981https://t.co/khdHR0GZWP XLNet\u306f2019/6/19\u306b\u3001\u201dXLNet: Generalized Autoregressive Pretraini\u2026"}, "1141905881409286144": {"followers": "723", "datetime": "2019-06-21 03:09:12", "author": "@ArchieIndian", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141715974321823744": {"followers": "76", "datetime": "2019-06-20 14:34:34", "author": "@PavitSankar", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142322641996517376": {"followers": "505", "datetime": "2019-06-22 06:45:15", "author": "@AUEBNLPGroup", "content_summary": "Next AUEB NLP Group meeting, Tue. **July 2**, 17:15-19:00: Discussion of \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" (https://t.co/2D23JRH7tM). Study the paper before the meeting. Central AUEB buildings, room A36. All welcome"}, "1143397685568761856": {"followers": "867", "datetime": "2019-06-25 05:57:06", "author": "@_ivana__anavi_", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143171507729752064": {"followers": "842", "datetime": "2019-06-24 14:58:21", "author": "@kazanagisora", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1141695995828289543": {"followers": "3,104", "datetime": "2019-06-20 13:15:11", "author": "@eturner303", "content_summary": "Remember BERT? Meet XLNet. Very impressive results on Squad 1.1+2.0 QA tasks and many other datasets. Impressive how fast NLProc pretraining + representations has been progressing over the past year - https://t.co/5I8wvTZv59 https://t.co/IrgyhZa2Sv"}, "1143790302999617536": {"followers": "178", "datetime": "2019-06-26 07:57:13", "author": "@schreimoz_gz", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141580698114859009": {"followers": "3,772", "datetime": "2019-06-20 05:37:02", "author": "@volkuleshov", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141730019992989696": {"followers": "12,765", "datetime": "2019-06-20 15:30:23", "author": "@jaguring1", "content_summary": "RT @smochi_pub: \u65e9\u304f\u3082BERT\u8d85\u3048\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u304d\u305f\u2026\uff01 https://t.co/86pHSazItX"}, "1141543633616658432": {"followers": "806", "datetime": "2019-06-20 03:09:45", "author": "@stomohide", "content_summary": "\u8ad6\u6587\u51fa\u3066\u307e\u3057\u305f\u3002 XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/Dl10X4T442"}, "1142066916703346690": {"followers": "414", "datetime": "2019-06-21 13:49:06", "author": "@NonLocalityGuy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143452355117092866": {"followers": "265", "datetime": "2019-06-25 09:34:20", "author": "@miguel_a_alonso", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141538225531568129": {"followers": "1,500", "datetime": "2019-06-20 02:48:16", "author": "@hn_frontpage", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding L: https://t.co/CiB9vAubwa C: https://t.co/UHZSn8lDAy"}, "1141938102426058752": {"followers": "1,581", "datetime": "2019-06-21 05:17:14", "author": "@joavanschoren", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141988807744471040": {"followers": "152", "datetime": "2019-06-21 08:38:43", "author": "@ArchismanM", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143421382040023041": {"followers": "54", "datetime": "2019-06-25 07:31:15", "author": "@SutanshuRaj", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141756381004910592": {"followers": "15", "datetime": "2019-06-20 17:15:08", "author": "@henryhenrychen", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141754994120646656": {"followers": "163,852", "datetime": "2019-06-20 17:09:38", "author": "@ceobillionaire", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1142053909071929344": {"followers": "14", "datetime": "2019-06-21 12:57:24", "author": "@LvguanInn", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143519490879229952": {"followers": "163,852", "datetime": "2019-06-25 14:01:06", "author": "@ceobillionaire", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141765643328532480": {"followers": "1,288", "datetime": "2019-06-20 17:51:57", "author": "@heghbalz", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141708406861922304": {"followers": "338", "datetime": "2019-06-20 14:04:30", "author": "@HirokiT57858674", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1141535461279559681": {"followers": "239", "datetime": "2019-06-20 02:37:17", "author": "@Dneutr0n", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1159041982146252805": {"followers": "222", "datetime": "2019-08-07 10:01:57", "author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 10,396 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1149257380678516737": {"followers": "222", "datetime": "2019-07-11 10:01:26", "author": "@PapersTrending", "content_summary": "[6/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 3,745 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141790740126679041": {"followers": "2,671", "datetime": "2019-06-20 19:31:40", "author": "@ayirpelle", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141907187578445825": {"followers": "3,179", "datetime": "2019-06-21 03:14:23", "author": "@tjmlab", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141707485746737152": {"followers": "252", "datetime": "2019-06-20 14:00:51", "author": "@_DLPBGJ80C04Z_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141660287147397120": {"followers": "56", "datetime": "2019-06-20 10:53:18", "author": "@WangcongcongCC", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144026974391635968": {"followers": "45", "datetime": "2019-06-26 23:37:40", "author": "@nullbytep", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141719313360883712": {"followers": "6,525", "datetime": "2019-06-20 14:47:51", "author": "@barneyp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141797032430047232": {"followers": "326", "datetime": "2019-06-20 19:56:40", "author": "@majortal", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141830267918782464": {"followers": "4,053", "datetime": "2019-06-20 22:08:44", "author": "@Vilinthril", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141834765391290370": {"followers": "1,564", "datetime": "2019-06-20 22:26:36", "author": "@migballesteros", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141596343581982721": {"followers": "141", "datetime": "2019-06-20 06:39:12", "author": "@Illarionov_msu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141528364467073024": {"followers": "694", "datetime": "2019-06-20 02:09:05", "author": "@santty128", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141568769946349574": {"followers": "2,927", "datetime": "2019-06-20 04:49:38", "author": "@evolvingstuff", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141744019778748424": {"followers": "2,258", "datetime": "2019-06-20 16:26:01", "author": "@annargrs", "content_summary": "It's official: #XLNet (https://t.co/iVrIJN0aRU) is a larger-than-BERT model said to do better-than-BERT. I can't reproduce it in my lab. Can you? Will you, when you could train an XXXLNet instead? Please share the poll. No offense to the authors, but the t"}, "1141720701054967808": {"followers": "144", "datetime": "2019-06-20 14:53:21", "author": "@HashemiHelia", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141512910914691074": {"followers": "574", "datetime": "2019-06-20 01:07:40", "author": "@lorenlugosch", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141632081279107072": {"followers": "254", "datetime": "2019-06-20 09:01:13", "author": "@enfageorge", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141695446492094470": {"followers": "47", "datetime": "2019-06-20 13:13:00", "author": "@trinadhiv", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/vKqUmmZGe1"}, "1141718837131272193": {"followers": "475", "datetime": "2019-06-20 14:45:57", "author": "@censored__", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1145491724518580224": {"followers": "553", "datetime": "2019-07-01 00:38:03", "author": "@kurageningen", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141523391503052802": {"followers": "40,177", "datetime": "2019-06-20 01:49:19", "author": "@iPullRank", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141701361739845632": {"followers": "842", "datetime": "2019-06-20 13:36:31", "author": "@kazanagisora", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1142028451546841089": {"followers": "1,918", "datetime": "2019-06-21 11:16:15", "author": "@DocXavi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1213488919670509568": {"followers": "385", "datetime": "2020-01-04 15:54:38", "author": "@jrriggs2124", "content_summary": "RT @arxiv_cscl: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1143392743667970053": {"followers": "2,391", "datetime": "2019-06-25 05:37:27", "author": "@sfermigier", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141614028579459072": {"followers": "114", "datetime": "2019-06-20 07:49:29", "author": "@thanhnguyentang", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141730936016396289": {"followers": "32", "datetime": "2019-06-20 15:34:02", "author": "@vincentlu073", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141518979573284864": {"followers": "85", "datetime": "2019-06-20 01:31:47", "author": "@daiquocng", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142540202608455680": {"followers": "323", "datetime": "2019-06-22 21:09:46", "author": "@xpearhead", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1151331107595378693": {"followers": "205", "datetime": "2019-07-17 03:21:41", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1147445502423818240": {"followers": "222", "datetime": "2019-07-06 10:01:40", "author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 3,559 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141579206469439489": {"followers": "41", "datetime": "2019-06-20 05:31:06", "author": "@rormandi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141677071263031296": {"followers": "34", "datetime": "2019-06-20 11:59:59", "author": "@NealPatel224", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143342609039052801": {"followers": "535", "datetime": "2019-06-25 02:18:14", "author": "@nelscorrea", "content_summary": "@mark_riedl Thank god @quocleix took a diversion from muppets with XLNet, which beats BERT on many tasks - https://t.co/vnuRfEyNPD"}, "1257284494316277762": {"followers": "205", "datetime": "2020-05-04 12:22:37", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141793724801921024": {"followers": "316", "datetime": "2019-06-20 19:43:32", "author": "@laurent_besacie", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141885020254031873": {"followers": "147", "datetime": "2019-06-21 01:46:18", "author": "@taashi_s", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141779281581752321": {"followers": "426", "datetime": "2019-06-20 18:46:08", "author": "@SeanGoldbergCS", "content_summary": "Kids these days and their newfangled Sesame Street characters like XLNet..."}, "1141886664932880384": {"followers": "670", "datetime": "2019-06-21 01:52:50", "author": "@Stack0149", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142161548661723137": {"followers": "1,765", "datetime": "2019-06-21 20:05:08", "author": "@Soufiane_Ajana", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143728462412173312": {"followers": "11,787", "datetime": "2019-06-26 03:51:29", "author": "@yutakashino", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141598822553886720": {"followers": "608", "datetime": "2019-06-20 06:49:03", "author": "@1997synohro", "content_summary": "\u6700\u8fd1\u306e\u9032\u6b69\u3059\u3054\u3059\u304e\u308b\u3067\u3057\u3087..."}, "1141932739324592128": {"followers": "11,787", "datetime": "2019-06-21 04:55:55", "author": "@yutakashino", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141635671326679042": {"followers": "611", "datetime": "2019-06-20 09:15:29", "author": "@smart_grid_fr", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1210819608459894784": {"followers": "7", "datetime": "2019-12-28 07:07:45", "author": "@27NSKEaeodmWipf", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141635826423607296": {"followers": "32", "datetime": "2019-06-20 09:16:06", "author": "@mopodono", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142211533356711936": {"followers": "599", "datetime": "2019-06-21 23:23:45", "author": "@sangwhanmoon", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141552029485211648": {"followers": "113", "datetime": "2019-06-20 03:43:07", "author": "@fsign", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143500667966971906": {"followers": "377", "datetime": "2019-06-25 12:46:19", "author": "@jbarroscandido", "content_summary": "that's a quite expensive paper right there..."}, "1143398311241637888": {"followers": "1,157", "datetime": "2019-06-25 05:59:35", "author": "@ixenario", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1157646632181104640": {"followers": "23", "datetime": "2019-08-03 13:37:19", "author": "@theHimalayan18", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141756144081227776": {"followers": "96", "datetime": "2019-06-20 17:14:12", "author": "@_Shivam_b", "content_summary": "XLNet for NLP #ai"}, "1148894966288175106": {"followers": "222", "datetime": "2019-07-10 10:01:20", "author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 3,710 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1143528962972217344": {"followers": "2,673", "datetime": "2019-06-25 14:38:45", "author": "@mosko_mule", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141893644519165953": {"followers": "27", "datetime": "2019-06-21 02:20:34", "author": "@mahesh21aug", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1141734184869945345": {"followers": "15", "datetime": "2019-06-20 15:46:56", "author": "@NusWu", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141825735683952640": {"followers": "66", "datetime": "2019-06-20 21:50:44", "author": "@alanmenegotto", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1161957413974470656": {"followers": "3,418", "datetime": "2019-08-15 11:06:50", "author": "@azriel1rf", "content_summary": "@sakura314159265 1\u5104\u3082\u304b\u3051\u306b\u3044\u304f\u3068\u304b\u305d\u306e\u4eba(\u306b\u5165\u308c\u77e5\u6075\u3057\u305f\u4eba)\"\u30ac\u30c1\"\u3058\u3083\u3093 https://t.co/OZwO8nZBic"}, "1143582011820347393": {"followers": "1,461", "datetime": "2019-06-25 18:09:33", "author": "@rfrsarmiento", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143463481036984321": {"followers": "193", "datetime": "2019-06-25 10:18:33", "author": "@hu_daa", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143445847541260288": {"followers": "96", "datetime": "2019-06-25 09:08:28", "author": "@atpoulsen", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142117219708489728": {"followers": "25", "datetime": "2019-06-21 17:08:59", "author": "@lcl4lnl", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1143494431510650881": {"followers": "27", "datetime": "2019-06-25 12:21:32", "author": "@akanyaani", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141858026531332096": {"followers": "316", "datetime": "2019-06-20 23:59:02", "author": "@liuyangumass", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141512512946483201": {"followers": "516", "datetime": "2019-06-20 01:06:06", "author": "@AGDAYWILL", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143725690551861251": {"followers": "1,097", "datetime": "2019-06-26 03:40:28", "author": "@sashimi_rawfish", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1146067004614270977": {"followers": "34", "datetime": "2019-07-02 14:44:01", "author": "@LogotMichael", "content_summary": "RT @rctatman: Time to pick the next @kaggle reading group paper! Your options: - XLNet: Generalized Autoregressive Pretraining for NLU htt\u2026"}, "1153750024075010048": {"followers": "1,230", "datetime": "2019-07-23 19:33:36", "author": "@designswinger", "content_summary": "State of Art #NLU #BERT vs #XLNET - BERT performs better but neglects dependency btn masked positions. #nlp #languagemodels Emily Pitler @GoogleAI #ainextcon https://t.co/8lg0HA48Iz https://t.co/Zlw9BQrhwA"}, "1145513396378279937": {"followers": "9", "datetime": "2019-07-01 02:04:10", "author": "@literacynor", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141515386707271680": {"followers": "4,448", "datetime": "2019-06-20 01:17:31", "author": "@pranavrajpurkar", "content_summary": "Wow!"}, "1141766532990717952": {"followers": "45", "datetime": "2019-06-20 17:55:29", "author": "@kporterrobinson", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141526083025682432": {"followers": "176", "datetime": "2019-06-20 02:00:01", "author": "@seaandoldman", "content_summary": "Two NLU SOTA results three days apart. Hard to keep up with the pace. First one:"}, "1143639725951082496": {"followers": "340", "datetime": "2019-06-25 21:58:53", "author": "@AldousBirchall", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141549621711265793": {"followers": "10,282", "datetime": "2019-06-20 03:33:33", "author": "@golbin", "content_summary": "BERT\ubcf4\ub2e4 20\uac1c\uc758 \ud0dc\uc2a4\ud06c\uc5d0\uc11c \ud070 \ud3ed\uc73c\ub85c \ub354 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778 XLNet \ucd9c\uc2dc(?!) \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378\uae4c\uc9c0 \uacf5\uac1c!!"}, "1143455289355751425": {"followers": "670", "datetime": "2019-06-25 09:46:00", "author": "@korenchkin", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141927827589091330": {"followers": "331", "datetime": "2019-06-21 04:36:24", "author": "@pywirrarika", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141998936493719552": {"followers": "2,481", "datetime": "2019-06-21 09:18:58", "author": "@shion_honda", "content_summary": "XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u4e0a\u56de\u3063\u305f\u3002\u30de\u30b9\u30af\u3092\u4f7f\u308f\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u304c\u5165\u3089\u305a\u3001\u4e88\u6e2c\u30c8\u30fc\u30af\u30f3\u9593\u306e\u4f9d\u5b58\u95a2\u4fc2\u3082\u5b66\u3079\u308b\u3002 https://t.co/uLaOqq8TBj #NowReading https://t.co/vj7kgNmRlc"}, "1141729706116390913": {"followers": "2,067", "datetime": "2019-06-20 15:29:08", "author": "@yo_ehara", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1210301818011471872": {"followers": "369", "datetime": "2019-12-26 20:50:14", "author": "@1027kg", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1153095296668196864": {"followers": "476", "datetime": "2019-07-22 00:11:56", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1143541348143894528": {"followers": "54", "datetime": "2019-06-25 15:27:58", "author": "@rouzbehf", "content_summary": "#DeepPockets needed for #DeepLearning!"}, "1141580398775812096": {"followers": "195", "datetime": "2019-06-20 05:35:51", "author": "@hereticreader", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding - https://t.co/9D1COPFWPY https://t.co/l4LGuEnFCE"}, "1143413820838170624": {"followers": "134", "datetime": "2019-06-25 07:01:13", "author": "@pamin2222", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141951200104988673": {"followers": "72", "datetime": "2019-06-21 06:09:17", "author": "@zikribayraktar", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143382577039831041": {"followers": "956", "datetime": "2019-06-25 04:57:04", "author": "@Kingwulf", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143489388728541187": {"followers": "1,049", "datetime": "2019-06-25 12:01:29", "author": "@debayan", "content_summary": "Fucked up"}, "1223348160010964992": {"followers": "205", "datetime": "2020-01-31 20:51:44", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143719704118251520": {"followers": "12,924", "datetime": "2019-06-26 03:16:41", "author": "@deliprao", "content_summary": "In the long run, withholding large parameter models on public datasets rarely prevents well resourced bad actors from weaponizing. Only guaranteed outcomes are stifling of innovation, making the rich richer, & environmental impacts from forced replicat"}, "1141756930794242048": {"followers": "100", "datetime": "2019-06-20 17:17:19", "author": "@RajaswaPatil", "content_summary": "RT @bradenjhancock: Some seriously impressive gains on popular benchmarks, with nice analysis. I'm sure the pretrained model will see a lot\u2026"}, "1141728426316857344": {"followers": "2,620", "datetime": "2019-06-20 15:24:03", "author": "@weihua916", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141583029321773062": {"followers": "160", "datetime": "2019-06-20 05:46:18", "author": "@leodambrosi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141937445304299520": {"followers": "41", "datetime": "2019-06-21 05:14:37", "author": "@shashankhalo7", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141762679822049281": {"followers": "271", "datetime": "2019-06-20 17:40:10", "author": "@davekincaid", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141649712380035073": {"followers": "72", "datetime": "2019-06-20 10:11:16", "author": "@Jbraadbaart", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141999183139889152": {"followers": "112", "datetime": "2019-06-21 09:19:57", "author": "@RoelvanEst", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141747509301112832": {"followers": "407", "datetime": "2019-06-20 16:39:53", "author": "@nishantiam", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141735876218773504": {"followers": "1,008", "datetime": "2019-06-20 15:53:39", "author": "@mugahayaliey", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141860281687126017": {"followers": "3,350", "datetime": "2019-06-21 00:08:00", "author": "@cocoweixu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141694760777736193": {"followers": "189", "datetime": "2019-06-20 13:10:17", "author": "@asa9no640511", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141523465448484865": {"followers": "5,808", "datetime": "2019-06-20 01:49:37", "author": "@AndrewLBeam", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143381201782104064": {"followers": "67", "datetime": "2019-06-25 04:51:36", "author": "@PcreteAI", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141919650260738048": {"followers": "5", "datetime": "2019-06-21 04:03:55", "author": "@Koundinya33", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141927284221382657": {"followers": "623", "datetime": "2019-06-21 04:34:15", "author": "@texttheater", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141705103990124545": {"followers": "104", "datetime": "2019-06-20 13:51:23", "author": "@Taka_baya", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143565716332769281": {"followers": "65", "datetime": "2019-06-25 17:04:47", "author": "@jjayaram7", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143735948686942209": {"followers": "73", "datetime": "2019-06-26 04:21:14", "author": "@uc7xe5t", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141633409778778113": {"followers": "5,860", "datetime": "2019-06-20 09:06:30", "author": "@bttyeo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143378657890594816": {"followers": "1,254", "datetime": "2019-06-25 04:41:29", "author": "@jmhessel", "content_summary": "I know there's some discussion of how academia should explore, companies should exploit, etc... but this is almost comical, and the trend is likely to continue"}, "1219640584702480384": {"followers": "2,475", "datetime": "2020-01-21 15:19:09", "author": "@crypto_magix", "content_summary": "RT @steadydee: At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will exp\u2026"}, "1142186021167067136": {"followers": "749", "datetime": "2019-06-21 21:42:22", "author": "@mukulmalik18", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1228157410755280903": {"followers": "205", "datetime": "2020-02-14 03:21:59", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143517750918352896": {"followers": "514", "datetime": "2019-06-25 13:54:12", "author": "@mdespriee", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141654396696244224": {"followers": "97", "datetime": "2019-06-20 10:29:53", "author": "@bdeveze", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141547902688362497": {"followers": "201", "datetime": "2019-06-20 03:26:43", "author": "@Singularity_43", "content_summary": "RT @jaguring1: pre-trained\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u3066\u308b\u3002 https://t.co/56dLplYivW \u8ad6\u6587 XLNet: Generalized Autoregressive Pretraining for Language Understanding\u2026"}, "1143761113013616640": {"followers": "868", "datetime": "2019-06-26 06:01:14", "author": "@ikuyamada", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141841103815159808": {"followers": "34", "datetime": "2019-06-20 22:51:48", "author": "@vmiliann", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141674215902498816": {"followers": "174", "datetime": "2019-06-20 11:48:39", "author": "@DX89B", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143358063518322688": {"followers": "381", "datetime": "2019-06-25 03:19:39", "author": "@shyamal_chandra", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141985835878735872": {"followers": "7", "datetime": "2019-06-21 08:26:55", "author": "@RoopalVaid", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142175658585612290": {"followers": "9", "datetime": "2019-06-21 21:01:12", "author": "@ysidereal", "content_summary": "XLNet: Bert's successor for nlp. #nlp #dl #bert https://t.co/MRGvVn7ihp https://t.co/yv2nV80Lwj"}, "1141733204602347522": {"followers": "156", "datetime": "2019-06-20 15:43:03", "author": "@garygarywang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1166289776179372034": {"followers": "222", "datetime": "2019-08-27 10:02:05", "author": "@PapersTrending", "content_summary": "[8/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 11,228 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1144688757700014080": {"followers": "1,447", "datetime": "2019-06-28 19:27:21", "author": "@__olamilekan__", "content_summary": "@farouqzaib !!! https://t.co/GIUSvRAolg"}, "1147282335483736067": {"followers": "659", "datetime": "2019-07-05 23:13:18", "author": "@swamichandra", "content_summary": "While still trying to come to terms with BERT, here comes XLNet that outperforms in almost 18/20 tasks. Can want to experiment it for a few NLP tasks. #xlnet https://t.co/exvYaFGBtQ"}, "1141604887316377600": {"followers": "1,713", "datetime": "2019-06-20 07:13:09", "author": "@arXiv__ml", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sentiment analysis), while integrating ideas from Transformer-XL: arxiv: https://t.co/ZEHC7ANQAm code + pretrained mode"}, "1141542447878557696": {"followers": "4,463", "datetime": "2019-06-20 03:05:03", "author": "@idgmatrix", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141577898395983873": {"followers": "767", "datetime": "2019-06-20 05:25:55", "author": "@bradenjhancock", "content_summary": "Some seriously impressive gains on popular benchmarks, with nice analysis. I'm sure the pretrained model will see a lot of use (even without a Sesame Street name). Hoping a Pytorch version is available soon for tinkering! (Fingers crossed that @Thom_Wolf w"}, "1143791302615547904": {"followers": "174", "datetime": "2019-06-26 08:01:11", "author": "@mizue3_1", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143476446725332998": {"followers": "323", "datetime": "2019-06-25 11:10:04", "author": "@vodkamomo", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1162666443399467008": {"followers": "222", "datetime": "2019-08-17 10:04:16", "author": "@PapersTrending", "content_summary": "[10/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 10,860 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1143728717371330560": {"followers": "106", "datetime": "2019-06-26 03:52:30", "author": "@dededen__", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141663988448735233": {"followers": "277", "datetime": "2019-06-20 11:08:00", "author": "@ZimMatthias", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141749796492828672": {"followers": "620", "datetime": "2019-06-20 16:48:58", "author": "@ykilcher", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141513353006739456": {"followers": "106", "datetime": "2019-06-20 01:09:26", "author": "@NovaIntrovert", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141761765786521600": {"followers": "615", "datetime": "2019-06-20 17:36:32", "author": "@KouroshMeshgi", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1141616607657103360": {"followers": "69", "datetime": "2019-06-20 07:59:44", "author": "@ArijRiabi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1210242417820962816": {"followers": "1,131", "datetime": "2019-12-26 16:54:12", "author": "@Olivine_Ryo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143565069936144384": {"followers": "4,149", "datetime": "2019-06-25 17:02:13", "author": "@jeremyhsu", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141710406756421633": {"followers": "326", "datetime": "2019-06-20 14:12:27", "author": "@jasonafries", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1216072364179558400": {"followers": "8,950", "datetime": "2020-01-11 19:00:19", "author": "@Deep_In_Depth", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/cgMIUDyRmD #DeepLearning #NeuralNetworks #ArtificialIntelligence #AIChips #MachineLearning #ReinforcementLearning #AGI #AutonomousCar #NewMobility #Robotics #DL #AI #DN"}, "1141602697872674816": {"followers": "1,873", "datetime": "2019-06-20 07:04:27", "author": "@milelu_ac", "content_summary": "!!!"}, "1143516281552998400": {"followers": "74", "datetime": "2019-06-25 13:48:21", "author": "@hailey_huong", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141761649193431042": {"followers": "290", "datetime": "2019-06-20 17:36:04", "author": "@dannyehb", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141750023056764929": {"followers": "17", "datetime": "2019-06-20 16:49:52", "author": "@rotwerg", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141781324400910338": {"followers": "4,891", "datetime": "2019-06-20 18:54:15", "author": "@IgorCarron", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141968046053101568": {"followers": "1,098", "datetime": "2019-06-21 07:16:13", "author": "@fredopeaud", "content_summary": "RT @lespetitescases: La course pour l'am\u00e9lioration des algos de traitement du langage naturel se poursuit \u00e0 un rythme effr\u00e9n\u00e9... BERT est s\u2026"}, "1143526978831695872": {"followers": "1,320", "datetime": "2019-06-25 14:30:52", "author": "@KrAbhinavGupta", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141814093004734464": {"followers": "1,280", "datetime": "2019-06-20 21:04:28", "author": "@__DaLong", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143745484718084097": {"followers": "554", "datetime": "2019-06-26 04:59:07", "author": "@FBWM8888", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141736746876592128": {"followers": "740", "datetime": "2019-06-20 15:57:07", "author": "@JoaoVictor_AC", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143690474923839488": {"followers": "11,078", "datetime": "2019-06-26 01:20:32", "author": "@tdualdir", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141540437510914049": {"followers": "1,565", "datetime": "2019-06-20 02:57:03", "author": "@420Cyber", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/txeTpImuU8"}, "1143447986510471168": {"followers": "375", "datetime": "2019-06-25 09:16:58", "author": "@ssshanest", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141783450002874368": {"followers": "82", "datetime": "2019-06-20 19:02:42", "author": "@stevennjui", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141693741419442176": {"followers": "7", "datetime": "2019-06-20 13:06:14", "author": "@sdrpierre", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141779438390185985": {"followers": "202", "datetime": "2019-06-20 18:46:46", "author": "@zein_shahine", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141738089901174784": {"followers": "77", "datetime": "2019-06-20 16:02:27", "author": "@JakubGlinka", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141731298320326656": {"followers": "1,409", "datetime": "2019-06-20 15:35:28", "author": "@Ryosuke0624", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141681805734625283": {"followers": "186", "datetime": "2019-06-20 12:18:48", "author": "@forozco", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141719968331026433": {"followers": "69", "datetime": "2019-06-20 14:50:27", "author": "@v_trokhymenko", "content_summary": "not bad"}, "1143810283477053440": {"followers": "1,934", "datetime": "2019-06-26 09:16:37", "author": "@mandubian", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1176327057753759745": {"followers": "11,930", "datetime": "2019-09-24 02:46:40", "author": "@Rosenchild", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141787818072707072": {"followers": "515", "datetime": "2019-06-20 19:20:03", "author": "@besanson", "content_summary": "RT @AakashDP: @besanson BERT and #NLP https://t.co/lwnabandPp"}, "1141613962854903808": {"followers": "3,595", "datetime": "2019-06-20 07:49:13", "author": "@revolunet", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141725000745070595": {"followers": "2,077", "datetime": "2019-06-20 15:10:27", "author": "@ojahnn", "content_summary": "RT @zehavoc: I'm a bit disappointed they didn't title their paper \"Bert is Dead: Behold Optimus Prime\" https://t.co/00V8BnW2Qr"}, "1141968457203740677": {"followers": "1,022", "datetime": "2019-06-21 07:17:51", "author": "@nishizawa", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141590898729295872": {"followers": "52", "datetime": "2019-06-20 06:17:34", "author": "@sharad24sc", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143553890631028736": {"followers": "260", "datetime": "2019-06-25 16:17:48", "author": "@vishnuvig", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1152298385275330569": {"followers": "186", "datetime": "2019-07-19 19:25:18", "author": "@anyanikd", "content_summary": "#XLNet outperforms #BERT https://t.co/jpRKrlTD5t"}, "1141742346331017216": {"followers": "171", "datetime": "2019-06-20 16:19:22", "author": "@pasta_jp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142080483720880129": {"followers": "4,754", "datetime": "2019-06-21 14:43:00", "author": "@rzembo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143533618066808832": {"followers": "433", "datetime": "2019-06-25 14:57:15", "author": "@miweru", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1145735457566797825": {"followers": "131", "datetime": "2019-07-01 16:46:34", "author": "@DemiourgosUA", "content_summary": "A year ago when I gave a presentation explaining the Transformer model, there were only a handful of published papers on the topic. Look at how far we've got, BERT, GPT-2, and now XLNet (https://t.co/yNutLXOpOq) are all contributing to the new leap in NLP."}, "1145653050490667008": {"followers": "69", "datetime": "2019-07-01 11:19:07", "author": "@ShabnamRashtchi", "content_summary": "Last month was another milestone in neural embedding methods. A team of researchers from Carnegie Mellon University and Google Brain proposed XLNet, end a new language model which outperforms BERT. https://t.co/d7gCeNymw9"}, "1141860554224603136": {"followers": "281", "datetime": "2019-06-21 00:09:05", "author": "@HotCompScience", "content_summary": "Most popular computer science paper of the day: \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" https://t.co/QkL9P5HTXe https://t.co/kpqjHWUIu7"}, "1143411134067879936": {"followers": "77", "datetime": "2019-06-25 06:50:32", "author": "@JakubGlinka", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1145536114616201216": {"followers": "1,331", "datetime": "2019-07-01 03:34:27", "author": "@musaprg", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141580438252560384": {"followers": "256", "datetime": "2019-06-20 05:36:00", "author": "@jcchinhui", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143378986376007681": {"followers": "285", "datetime": "2019-06-25 04:42:47", "author": "@jmrko", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1145491954878185472": {"followers": "944", "datetime": "2019-07-01 00:38:58", "author": "@abyss_paranoia", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1143358168539361280": {"followers": "2,671", "datetime": "2019-06-25 03:20:04", "author": "@ayirpelle", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141754666046369792": {"followers": "7", "datetime": "2019-06-20 17:08:19", "author": "@romehrah", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141720544036847617": {"followers": "1,092", "datetime": "2019-06-20 14:52:44", "author": "@makoFALAR1229", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143358503257595905": {"followers": "3,133", "datetime": "2019-06-25 03:21:24", "author": "@AdaptiveToolbox", "content_summary": "RT @h_thoreson: I sometimes think computing has become the modern horse. Feudal societies were organized in large part around feeding and t\u2026"}, "1152421329112465408": {"followers": "46", "datetime": "2019-07-20 03:33:50", "author": "@ChaoyangHe", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141543053406220289": {"followers": "818", "datetime": "2019-06-20 03:07:27", "author": "@ErmiaBivatan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142371957192740865": {"followers": "222", "datetime": "2019-06-22 10:01:13", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 2,121 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141548638033760256": {"followers": "311", "datetime": "2019-06-20 03:29:38", "author": "@arxiv_cs_LG", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le https://t.co/SobHICuSXS"}, "1141617473923366912": {"followers": "1,847", "datetime": "2019-06-20 08:03:10", "author": "@katsuhitosudoh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143406805529300992": {"followers": "1,089", "datetime": "2019-06-25 06:33:20", "author": "@Tweetteresearch", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1161999216266350592": {"followers": "0", "datetime": "2019-08-15 13:52:56", "author": "@dogydev", "content_summary": "Hyperparameter tuning using SHERPA (https://t.co/Kjh6MgMao3) on XLNet (https://t.co/RUTEgIVwzV) https://t.co/LBvv9DgxA8"}, "1141638264190885890": {"followers": "54", "datetime": "2019-06-20 09:25:47", "author": "@n_di_mauro", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143623302314909696": {"followers": "16,932", "datetime": "2019-06-25 20:53:37", "author": "@chrismattmann", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142714345413074944": {"followers": "220", "datetime": "2019-06-23 08:41:45", "author": "@ricangius", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141843176975527937": {"followers": "731", "datetime": "2019-06-20 23:00:02", "author": "@therealjpittman", "content_summary": "[R] XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.co/Ye8fMy5GNh #MachineLearning"}, "1141557888122413056": {"followers": "366", "datetime": "2019-06-20 04:06:24", "author": "@gamaga_ai", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143646703347744778": {"followers": "2,572", "datetime": "2019-06-25 22:26:36", "author": "@PhotoLawn", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141795359192231936": {"followers": "174", "datetime": "2019-06-20 19:50:01", "author": "@n_u_l_l_p_o", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141571363020431360": {"followers": "161", "datetime": "2019-06-20 04:59:56", "author": "@KavehHassani", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143756447391444992": {"followers": "472", "datetime": "2019-06-26 05:42:41", "author": "@jeff_weintraub", "content_summary": "\ud83e\udd14"}, "1142926927306489856": {"followers": "792", "datetime": "2019-06-23 22:46:28", "author": "@fiandola", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141709967189086208": {"followers": "271", "datetime": "2019-06-20 14:10:42", "author": "@runnlp", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141690506411433985": {"followers": "2,960", "datetime": "2019-06-20 12:53:22", "author": "@jittat", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141702599462326273": {"followers": "117", "datetime": "2019-06-20 13:41:26", "author": "@aiGuru", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141739411056386048": {"followers": "669", "datetime": "2019-06-20 16:07:42", "author": "@arunkumar_bvr", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141962710550228993": {"followers": "48", "datetime": "2019-06-21 06:55:01", "author": "@mzadrogaPL", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141865136703102982": {"followers": "956", "datetime": "2019-06-21 00:27:18", "author": "@Kingwulf", "content_summary": "RT @deeplearning4j: Better than BERT! XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/b0uBVcbv2N #dee\u2026"}, "1141598748797231112": {"followers": "608", "datetime": "2019-06-20 06:48:46", "author": "@sanjaykamath", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141913252802125824": {"followers": "141", "datetime": "2019-06-21 03:38:29", "author": "@waydegilliam", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1142949052943761408": {"followers": "229", "datetime": "2019-06-24 00:14:23", "author": "@kav_gan", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141739556024270850": {"followers": "650", "datetime": "2019-06-20 16:08:17", "author": "@timwee", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1167014607233409027": {"followers": "222", "datetime": "2019-08-29 10:02:19", "author": "@PapersTrending", "content_summary": "[10/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 11,314 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1142173332185370625": {"followers": "23", "datetime": "2019-06-21 20:51:57", "author": "@ChenChe39826259", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141599554812272640": {"followers": "134", "datetime": "2019-06-20 06:51:58", "author": "@mawatan6x", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141802528616660992": {"followers": "180", "datetime": "2019-06-20 20:18:31", "author": "@payaicha15", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141728818614480897": {"followers": "61", "datetime": "2019-06-20 15:25:37", "author": "@hrosmendez", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141941985239896064": {"followers": "21", "datetime": "2019-06-21 05:32:40", "author": "@sunghyo_chung", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141694561326161920": {"followers": "150", "datetime": "2019-06-20 13:09:29", "author": "@notthatdsw", "content_summary": "How to gain the attention of a reader: \"Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document r"}, "1141974297046597633": {"followers": "66", "datetime": "2019-06-21 07:41:03", "author": "@felschueler", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141807317349875714": {"followers": "35", "datetime": "2019-06-20 20:37:32", "author": "@tianshi5940", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141929453091557376": {"followers": "1", "datetime": "2019-06-21 04:42:52", "author": "@haradatm", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143372525625323520": {"followers": "165", "datetime": "2019-06-25 04:17:07", "author": "@marcosprevitali", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1152449069555552256": {"followers": "851", "datetime": "2019-07-20 05:24:04", "author": "@ariannabetti", "content_summary": "RT @annargrs: It's official: #XLNet (https://t.co/iVrIJN0aRU) is a larger-than-BERT model said to do better-than-BERT. I can't reproduce it\u2026"}, "1142256817151578112": {"followers": "818", "datetime": "2019-06-22 02:23:41", "author": "@ErmiaBivatan", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1143443156576419841": {"followers": "1,141", "datetime": "2019-06-25 08:57:47", "author": "@JulioGonzalo1", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141898288334921729": {"followers": "75", "datetime": "2019-06-21 02:39:02", "author": "@junji_yamato", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141594428093558785": {"followers": "55", "datetime": "2019-06-20 06:31:36", "author": "@__outlaw_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142399764773986304": {"followers": "45", "datetime": "2019-06-22 11:51:43", "author": "@artuskg", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1143648382025641985": {"followers": "311", "datetime": "2019-06-25 22:33:16", "author": "@mlennox", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141918078319648768": {"followers": "19", "datetime": "2019-06-21 03:57:40", "author": "@MassBassLol", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141988929693814785": {"followers": "6", "datetime": "2019-06-21 08:39:12", "author": "@zdr727", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141924934391697408": {"followers": "33", "datetime": "2019-06-21 04:24:54", "author": "@gumigumi4f", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141600803485917184": {"followers": "840", "datetime": "2019-06-20 06:56:56", "author": "@GreekSage", "content_summary": "\uc791\ub144\uc5d0 \ud070 \ucda9\uaca9\uc744 \uc548\uaca8\uc900 BERT \ub97c \ub2a5\uac00\ud558\ub294 \uc0c8\ub85c\uc6b4 NLP \ubaa8\ub378. BERT \uc815\ub3c4\uba74 \uc7ac\ubc0c\ub294 \uc11c\ube44\uc2a4\ub97c \ub9cc\ub4e4\uc218 \uc788\uaca0\ub2e4 \uc0dd\uac01\ud588\ub294\ub370 XLNet \uc740 \uc131\ub2a5\uc774 \ub354 \uc88b\uc544\uc84c\ub124."}, "1141609131771437056": {"followers": "1,648", "datetime": "2019-06-20 07:30:01", "author": "@28tomabou", "content_summary": "BERT\u3092\u30dc\u30b3\u30dc\u30b3\u306b\u3057\u3066\u308b\u3068\u3044\u3046\u3044\u3046\u5642\u3092\u805e\u3044\u305f https://t.co/TMlH7X3AEg"}, "1143416411227865093": {"followers": "19", "datetime": "2019-06-25 07:11:30", "author": "@dhteodoro", "content_summary": "@patrickruch @AGaudinat"}, "1141534191760232448": {"followers": "28", "datetime": "2019-06-20 02:32:14", "author": "@AbhiGoswami5393", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141956170921500672": {"followers": "35", "datetime": "2019-06-21 06:29:02", "author": "@Ahmedtronic", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/YpLZ6IlfLM outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language infer"}, "1141750067105271813": {"followers": "128", "datetime": "2019-06-20 16:50:03", "author": "@AakashDP", "content_summary": "@besanson BERT and #NLP"}, "1141858842524864512": {"followers": "90", "datetime": "2019-06-21 00:02:17", "author": "@MSuryavansh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141701985751584768": {"followers": "842", "datetime": "2019-06-20 13:38:59", "author": "@kazanagisora", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141829031135830016": {"followers": "425", "datetime": "2019-06-20 22:03:49", "author": "@iamknighton", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143631489260838914": {"followers": "67", "datetime": "2019-06-25 21:26:09", "author": "@calogerozarbo", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141800684670468096": {"followers": "4", "datetime": "2019-06-20 20:11:11", "author": "@JuneSgy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141697821265059840": {"followers": "44", "datetime": "2019-06-20 13:22:26", "author": "@Fadeevla", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1143452764648984576": {"followers": "9,212", "datetime": "2019-06-25 09:35:58", "author": "@mtrc", "content_summary": "RT @baykenney: access to compute can't be decoupled from responsible development of ml systems. https://t.co/8pAmAV39Mg"}, "1141904122498691073": {"followers": "588", "datetime": "2019-06-21 03:02:13", "author": "@usmanahmed189", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141719285099683846": {"followers": "82", "datetime": "2019-06-20 14:47:44", "author": "@RyoHWS", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141616852843393024": {"followers": "78", "datetime": "2019-06-20 08:00:42", "author": "@usct01", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143075014012309504": {"followers": "12,765", "datetime": "2019-06-24 08:34:55", "author": "@jaguring1", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1141665307037196289": {"followers": "73", "datetime": "2019-06-20 11:13:14", "author": "@anujdutt92", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141689428143943683": {"followers": "288", "datetime": "2019-06-20 12:49:05", "author": "@_ahani_", "content_summary": "Here is a new model to be added to my studies. Please .. I don\u2019t have time to study all of these new models. Waiting for a new model that will beat XLNet..."}, "1141760860550914049": {"followers": "615", "datetime": "2019-06-20 17:32:56", "author": "@KouroshMeshgi", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141706083565821952": {"followers": "49", "datetime": "2019-06-20 13:55:16", "author": "@paulomannjr", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142111858570276864": {"followers": "564", "datetime": "2019-06-21 16:47:41", "author": "@akbar_aryobee", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145547775511105536": {"followers": "67", "datetime": "2019-07-01 04:20:47", "author": "@MatsElctrcBlu", "content_summary": "\u30d5\u30a7\u30e9\u30fc\u30ea\u3082\u901f\u304b\u3063\u305f\u304c\u30d6\u30ac\u30c3\u30c6\u30a3\u306f\u66f4\u306b\u901f\u304b\u3063\u305f\u3068\u3044\u3046\u8a71\u3088\u308a\u3001\u300c\u8efd\u81ea\u52d5\u8eca\u3067\u30d6\u30ac\u30c3\u30c6\u30a3\u8d85\u3048\u308b\u624b\u6cd5\u3092\u8003\u6848\uff01\u300d\u307f\u305f\u3044\u306a\u8a71\u304c\u51fa\u3066\u304f\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\uff08\u9858\u3044\uff09\u3002\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8\u304c\u5fc5\u9808\u304b\u3002"}, "1141718986888900609": {"followers": "1,152", "datetime": "2019-06-20 14:46:33", "author": "@jd_mashiro", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141704534819049472": {"followers": "448", "datetime": "2019-06-20 13:49:07", "author": "@AldoLipani", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141578086112059392": {"followers": "7,137", "datetime": "2019-06-20 05:26:39", "author": "@anshulkundaje", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141775433051959296": {"followers": "90", "datetime": "2019-06-20 18:30:51", "author": "@Xiaolei33", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1238039551207227392": {"followers": "5,538", "datetime": "2020-03-12 09:50:05", "author": "@cackerman1", "content_summary": "https://t.co/qMZc57GnK6 https://t.co/dW0d2KRGQH https://t.co/ITCElyach3 https://t.co/ThplBQXsLq Google NLP: Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA), BERT, RoBERTa, XLNet, ALBERT, T5, others https://t.co/q0fHi"}, "1141565612432302080": {"followers": "21", "datetime": "2019-06-20 04:37:05", "author": "@Olivia_Qiqi", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1144204132334678016": {"followers": "505", "datetime": "2019-06-27 11:21:38", "author": "@AUEBNLPGroup", "content_summary": "For quick background on Transformers and BERT, consult: https://t.co/QWfUCdJ3k7"}, "1141671977138229248": {"followers": "11,293", "datetime": "2019-06-20 11:39:45", "author": "@gneubig", "content_summary": "There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street character \ud83d\ude42"}, "1153660134142484480": {"followers": "19", "datetime": "2019-07-23 13:36:24", "author": "@nybyhansen1990", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1219661825861701632": {"followers": "6", "datetime": "2020-01-21 16:43:34", "author": "@Ata84337734", "content_summary": "RT @steadydee: At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will exp\u2026"}, "1144551415269875713": {"followers": "119", "datetime": "2019-06-28 10:21:36", "author": "@sunnydayscafe", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141717950187134976": {"followers": "351", "datetime": "2019-06-20 14:42:26", "author": "@dginev", "content_summary": "The rate of improvement in the NLP state-of-art continues to amaze...."}, "1141691391715794945": {"followers": "1,011", "datetime": "2019-06-20 12:56:54", "author": "@robiriondo", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1142338981616656386": {"followers": "259", "datetime": "2019-06-22 07:50:11", "author": "@pablo_gps", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142417912096968709": {"followers": "113", "datetime": "2019-06-22 13:03:49", "author": "@tranlaman", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1153270575248367616": {"followers": "302", "datetime": "2019-07-22 11:48:26", "author": "@tan14007", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141979694813683713": {"followers": "664", "datetime": "2019-06-21 08:02:30", "author": "@crude2refined", "content_summary": "RT @ChrSzegedy: Wow! https://t.co/09A58FFsKU"}, "1141564177338773504": {"followers": "102", "datetime": "2019-06-20 04:31:23", "author": "@petrurebeja", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142389155869134848": {"followers": "9", "datetime": "2019-06-22 11:09:33", "author": "@Oleg61671386", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141690644580065280": {"followers": "2,439", "datetime": "2019-06-20 12:53:55", "author": "@kyoun", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u5206\u985e\uff0c\u30e9\u30f3\u30ad\u30f3\u30b0\u7b4918\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u304d\u304f\u8d85\u3048\u308bSOTA\uff0eAR\u306e\u826f\u3055\u3068permutation\u306b\u3088\u308b\u53cc\u65b9\u5411\u6587\u8108\u7406\u89e3\u306e\u826f\u3044\u3068\u3053\u53d6\u308a\uff0e\u30b3\u30fc\u30c9https://t.co/6xuRKXIwjs https://t.co/4zZieKH4fM"}, "1195789108154531840": {"followers": "476", "datetime": "2019-11-16 19:41:54", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141746563078746112": {"followers": "97", "datetime": "2019-06-20 16:36:07", "author": "@joobintwit", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142114715348287490": {"followers": "62", "datetime": "2019-06-21 16:59:02", "author": "@ju_chieh", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143469434725634053": {"followers": "1,429", "datetime": "2019-06-25 10:42:12", "author": "@saillabs", "content_summary": "RT @jcvasquezc1: Wowwwww #NLProc #DeepLearning https://t.co/8kf7W9M6jn"}, "1143872768489865216": {"followers": "282", "datetime": "2019-06-26 13:24:54", "author": "@manjiroukeigo", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141710341912584192": {"followers": "259", "datetime": "2019-06-20 14:12:12", "author": "@brdskggs", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141764610481119232": {"followers": "102", "datetime": "2019-06-20 17:47:50", "author": "@drChromiak", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142005200540045312": {"followers": "142", "datetime": "2019-06-21 09:43:51", "author": "@yamenajjour", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141564588766269441": {"followers": "47,691", "datetime": "2019-06-20 04:33:01", "author": "@peteskomoroch", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143643340711190530": {"followers": "122", "datetime": "2019-06-25 22:13:14", "author": "@arkinrd", "content_summary": "Being State of the Art will cost you"}, "1141875256644292608": {"followers": "1,278", "datetime": "2019-06-21 01:07:30", "author": "@jochenleidner", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141704237648187392": {"followers": "1,022", "datetime": "2019-06-20 13:47:56", "author": "@nishizawa", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141614188307066882": {"followers": "409", "datetime": "2019-06-20 07:50:07", "author": "@AmirSaffari", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143371925198163968": {"followers": "2,232", "datetime": "2019-06-25 04:14:44", "author": "@raamana_", "content_summary": "Wow! That\u2019s 5 postdoc salaries for a single big fat #DeepLearning model."}, "1143720804959449089": {"followers": "1,364", "datetime": "2019-06-26 03:21:03", "author": "@bokudentw", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141698072587714560": {"followers": "128", "datetime": "2019-06-20 13:23:26", "author": "@feedmari", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141587625838428161": {"followers": "29", "datetime": "2019-06-20 06:04:34", "author": "@ashutoshk2401", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141649366295465984": {"followers": "14", "datetime": "2019-06-20 10:09:54", "author": "@Rexhaif", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141863783045853184": {"followers": "220", "datetime": "2019-06-21 00:21:55", "author": "@iforine", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1141887994330095616": {"followers": "498", "datetime": "2019-06-21 01:58:07", "author": "@maguroIsland", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141734566056669184": {"followers": "297", "datetime": "2019-06-20 15:48:27", "author": "@jmcimula", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141729529335037956": {"followers": "166", "datetime": "2019-06-20 15:28:26", "author": "@dbparedes", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141761778793062400": {"followers": "190", "datetime": "2019-06-20 17:36:35", "author": "@GaryZZhang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145235057642094593": {"followers": "86", "datetime": "2019-06-30 07:38:09", "author": "@TONe39816151", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1143599757970907138": {"followers": "51,484", "datetime": "2019-06-25 19:20:04", "author": "@SmitaNairJain", "content_summary": "RT @h_thoreson: I sometimes think computing has become the modern horse. Feudal societies were organized in large part around feeding and t\u2026"}, "1141704939783090176": {"followers": "536", "datetime": "2019-06-20 13:50:44", "author": "@sangha_deb", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141983355983941633": {"followers": "91", "datetime": "2019-06-21 08:17:03", "author": "@sumedh_khodke", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141637207251460096": {"followers": "360", "datetime": "2019-06-20 09:21:35", "author": "@Sumenia", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141709257412358144": {"followers": "65", "datetime": "2019-06-20 14:07:53", "author": "@PathakArchita", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142081726958563328": {"followers": "301", "datetime": "2019-06-21 14:47:57", "author": "@NLP_niko", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143401490872242176": {"followers": "89", "datetime": "2019-06-25 06:12:13", "author": "@SebastianEnger", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141866988597207040": {"followers": "171", "datetime": "2019-06-21 00:34:39", "author": "@subho_mpi", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1219550844556890112": {"followers": "0", "datetime": "2020-01-21 09:22:34", "author": "@gnt538417", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1141711490153668608": {"followers": "485", "datetime": "2019-06-20 14:16:45", "author": "@Qdatalab", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143428765428649985": {"followers": "539", "datetime": "2019-06-25 08:00:36", "author": "@pschwllr", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141731514931007488": {"followers": "279", "datetime": "2019-06-20 15:36:20", "author": "@DummyWitty", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141621098854793217": {"followers": "59", "datetime": "2019-06-20 08:17:34", "author": "@liqiangniu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141896012551733248": {"followers": "637", "datetime": "2019-06-21 02:29:59", "author": "@KuraOyo", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141621061647101953": {"followers": "329", "datetime": "2019-06-20 08:17:26", "author": "@balajinix", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141999675358076928": {"followers": "435", "datetime": "2019-06-21 09:21:54", "author": "@kawauso_kun", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143720323151540225": {"followers": "267", "datetime": "2019-06-26 03:19:08", "author": "@Rpatel_15", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143455096027717632": {"followers": "2,077", "datetime": "2019-06-25 09:45:13", "author": "@ojahnn", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143371015986110465": {"followers": "99,939", "datetime": "2019-06-25 04:11:07", "author": "@jeremyphoward", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143801562797617152": {"followers": "5,251", "datetime": "2019-06-26 08:41:58", "author": "@DNAed_tech", "content_summary": "RT @AlbertVilella: The @GCPcloud #TPUv3 was used for #XLNet which comes very close to human ceiling performance of reading comprehension of\u2026"}, "1141931323604389888": {"followers": "107", "datetime": "2019-06-21 04:50:18", "author": "@gojiberries_io", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1142101322780381185": {"followers": "271", "datetime": "2019-06-21 16:05:49", "author": "@steeve__huang", "content_summary": "XLNet, the new-born baby of #CMU and #GoogleBrain, is claimed to outperform #BERT in 20 NLP tasks! The downside is that it needs too much computation power. It takes 512 TPU to train for 2.5 days !!? Feel like my imagination is limited by my poorness. :("}, "1146292423313768450": {"followers": "354", "datetime": "2019-07-03 05:39:45", "author": "@Miyaran99", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1143096680981696516": {"followers": "28", "datetime": "2019-06-24 10:01:01", "author": "@lessand_ro", "content_summary": "If you missed out on the biggest thing in NLP for 2019 so far, that\"s #XLNET. 1) Beats GPT2 and BERT 2) they are realising pretrained model, whereas GPT2 was not released for 'safety concern' over fake news generation. (Only a marketing news in my opinion)"}, "1141719008028385280": {"followers": "354", "datetime": "2019-06-20 14:46:38", "author": "@indy9000", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1142462910133919745": {"followers": "1,583", "datetime": "2019-06-22 16:02:38", "author": "@arkaitz", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142159105206956032": {"followers": "298", "datetime": "2019-06-21 19:55:25", "author": "@YasminFathy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143502676224696320": {"followers": "71", "datetime": "2019-06-25 12:54:17", "author": "@semiinvariant", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1145811220680847361": {"followers": "19", "datetime": "2019-07-01 21:47:37", "author": "@Adobur", "content_summary": "#NLP #NeuralNetworks The paper to read for what might beat BERT in natural language understanding: autoregressive pretraining https://t.co/utaidtEU70"}, "1141518329510690818": {"followers": "229", "datetime": "2019-06-20 01:29:12", "author": "@hengcherkeng", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141596123624136704": {"followers": "7", "datetime": "2019-06-20 06:38:20", "author": "@hi_minhoryu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141614822410338304": {"followers": "46", "datetime": "2019-06-20 07:52:38", "author": "@helboukkouri", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141644357281468416": {"followers": "89", "datetime": "2019-06-20 09:50:00", "author": "@SebastianEnger", "content_summary": "RT @bradenjhancock: Some seriously impressive gains on popular benchmarks, with nice analysis. I'm sure the pretrained model will see a lot\u2026"}, "1141835759206510593": {"followers": "79,158", "datetime": "2019-06-20 22:30:33", "author": "@machinelearnflx", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143809357915787265": {"followers": "177", "datetime": "2019-06-26 09:12:56", "author": "@imagineNICE", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141666261887139840": {"followers": "4,327", "datetime": "2019-06-20 11:17:02", "author": "@jekbradbury", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141633561092644866": {"followers": "29", "datetime": "2019-06-20 09:07:06", "author": "@Donatas36474150", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141773572375011330": {"followers": "647", "datetime": "2019-06-20 18:23:27", "author": "@zzprosper", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143383201160712193": {"followers": "158", "datetime": "2019-06-25 04:59:32", "author": "@ArpitJ_", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141623589155684352": {"followers": "749", "datetime": "2019-06-20 08:27:28", "author": "@mukulmalik18", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141774969904517120": {"followers": "245", "datetime": "2019-06-20 18:29:00", "author": "@yazdavar", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1143488138599378945": {"followers": "206", "datetime": "2019-06-25 11:56:31", "author": "@rcbarros", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141713830679986176": {"followers": "367", "datetime": "2019-06-20 14:26:03", "author": "@hardik1973", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141842809780985856": {"followers": "62", "datetime": "2019-06-20 22:58:34", "author": "@samhardyhey", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141734667332268032": {"followers": "297", "datetime": "2019-06-20 15:48:51", "author": "@jmcimula", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143600156883017734": {"followers": "336", "datetime": "2019-06-25 19:21:39", "author": "@TheLeanAcademic", "content_summary": "Whether this is a lot of $$$ or not really depends on its value-add to downstream tasks, services, applications, and businesses..."}, "1143357059951235072": {"followers": "8,541", "datetime": "2019-06-25 03:15:40", "author": "@rbhar90", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143623784995467266": {"followers": "666", "datetime": "2019-06-25 20:55:32", "author": "@vgholkar", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141568927081754624": {"followers": "273", "datetime": "2019-06-20 04:50:16", "author": "@_g40n_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1160972123696820225": {"followers": "205", "datetime": "2019-08-12 17:51:38", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142012983356190720": {"followers": "111", "datetime": "2019-06-21 10:14:47", "author": "@knccch", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141534089888927744": {"followers": "43", "datetime": "2019-06-20 02:31:50", "author": "@vinaysharma424", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141655930372669441": {"followers": "63", "datetime": "2019-06-20 10:35:59", "author": "@YJParkAU", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1164115427598970880": {"followers": "222", "datetime": "2019-08-21 10:02:00", "author": "@PapersTrending", "content_summary": "[9/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 11,010 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1146103655973527553": {"followers": "223", "datetime": "2019-07-02 17:09:39", "author": "@johntigue", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142093656855212032": {"followers": "71", "datetime": "2019-06-21 15:35:21", "author": "@semiinvariant", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143645329335685121": {"followers": "1,612", "datetime": "2019-06-25 22:21:09", "author": "@leonpalafox", "content_summary": "Cuesta 245,000 dolares entrenar el mejor modelo de lenguaje natural"}, "1143174828804857856": {"followers": "3,104", "datetime": "2019-06-24 15:11:33", "author": "@eturner303", "content_summary": "Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 days * $8 a TPU) - https://t.co/7OKJZHH3wI https://t.co/hvvB2C4oSN"}, "1141841429096013824": {"followers": "57", "datetime": "2019-06-20 22:53:05", "author": "@GenestClement", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1143390370644054018": {"followers": "72", "datetime": "2019-06-25 05:28:02", "author": "@jonasrbati", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142021970222915584": {"followers": "42", "datetime": "2019-06-21 10:50:30", "author": "@IAPR_TC12", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141889963442634758": {"followers": "12,765", "datetime": "2019-06-21 02:05:57", "author": "@jaguring1", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143795335040552960": {"followers": "560", "datetime": "2019-06-26 08:17:13", "author": "@yuvalmarton", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141838767747227649": {"followers": "250", "datetime": "2019-06-20 22:42:31", "author": "@TmthyDobbins", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141528051337109504": {"followers": "781", "datetime": "2019-06-20 02:07:50", "author": "@HrSaghir", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141783208087834624": {"followers": "4,531", "datetime": "2019-06-20 19:01:44", "author": "@ChrSzegedy", "content_summary": "Wow!"}, "1141581994343710721": {"followers": "78", "datetime": "2019-06-20 05:42:11", "author": "@AshenabMohammad", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141590508105605121": {"followers": "82", "datetime": "2019-06-20 06:16:01", "author": "@_g_bellard", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141669449734348800": {"followers": "12,765", "datetime": "2019-06-20 11:29:42", "author": "@jaguring1", "content_summary": "RT @28tomabou: BERT\u3092\u30dc\u30b3\u30dc\u30b3\u306b\u3057\u3066\u308b\u3068\u3044\u3046\u3044\u3046\u5642\u3092\u805e\u3044\u305f https://t.co/TMlH7X3AEg"}, "1183893610405748741": {"followers": "163,852", "datetime": "2019-10-14 23:53:27", "author": "@ceobillionaire", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141562566864527361": {"followers": "74", "datetime": "2019-06-20 04:24:59", "author": "@hailey_huong", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141815158001545216": {"followers": "4,053", "datetime": "2019-06-20 21:08:42", "author": "@Vilinthril", "content_summary": "RT @zehavoc: I'm a bit disappointed they didn't title their paper \"Bert is Dead: Behold Optimus Prime\" https://t.co/00V8BnW2Qr"}, "1141600540784246784": {"followers": "433", "datetime": "2019-06-20 06:55:53", "author": "@miweru", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143604032193224705": {"followers": "234", "datetime": "2019-06-25 19:37:03", "author": "@zahi_kakish", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1189540370759086080": {"followers": "205", "datetime": "2019-10-30 13:51:39", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142392131862061056": {"followers": "844", "datetime": "2019-06-22 11:21:23", "author": "@PiotrCzapla", "content_summary": "@Secheron1202 @NPCollapse How about a call with ppl that are interested? FYI The XLNet is a network that has better performance than BERT in 20 task and was developed on the university of Carnegie Mellon. https://t.co/fF0K3WXezh"}, "1141610883044511744": {"followers": "188", "datetime": "2019-06-20 07:36:59", "author": "@theainur", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141876373918605313": {"followers": "99,939", "datetime": "2019-06-21 01:11:57", "author": "@jeremyphoward", "content_summary": "RT @ekshakhs: New objective + more 10x data + tranf xl + more. Predict word given all permutations of words in context. Combines autoreg lm\u2026"}, "1143488428752789505": {"followers": "37", "datetime": "2019-06-25 11:57:41", "author": "@vivekchandsrc", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141610474229776384": {"followers": "4", "datetime": "2019-06-20 07:35:21", "author": "@shfaithy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141631486681997312": {"followers": "75", "datetime": "2019-06-20 08:58:51", "author": "@_shiivangii", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142001755607068672": {"followers": "6", "datetime": "2019-06-21 09:30:10", "author": "@BhayaniDebu", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141617149699448832": {"followers": "0", "datetime": "2019-06-20 08:01:53", "author": "@diam045", "content_summary": "BERT < XLNet"}, "1141737523552456704": {"followers": "118", "datetime": "2019-06-20 16:00:12", "author": "@FQCme1", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141906971118821377": {"followers": "11,078", "datetime": "2019-06-21 03:13:32", "author": "@tdualdir", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141544122819366912": {"followers": "80", "datetime": "2019-06-20 03:11:42", "author": "@HyvaHyva_", "content_summary": "\uc640 \ubc84\ud2b8\ubcf4\ub2e4 \uc131\ub2a5\uc88b\uc740 XLNet"}, "1141553626185207808": {"followers": "322", "datetime": "2019-06-20 03:49:28", "author": "@letranger14", "content_summary": "RT @rbhar90: This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a\u2026"}, "1141560362761764869": {"followers": "7,815", "datetime": "2019-06-20 04:16:14", "author": "@gdequeiroz", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141888886919946242": {"followers": "595", "datetime": "2019-06-21 02:01:40", "author": "@cddadr", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1184078421720096768": {"followers": "302", "datetime": "2019-10-15 12:07:49", "author": "@subhobrata1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143406587777671168": {"followers": "783", "datetime": "2019-06-25 06:32:28", "author": "@muktabh", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142009666756665345": {"followers": "222", "datetime": "2019-06-21 10:01:36", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 1,301 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141897486216228864": {"followers": "165", "datetime": "2019-06-21 02:35:50", "author": "@samurairodeo", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141546756280184832": {"followers": "22", "datetime": "2019-06-20 03:22:10", "author": "@seanie_12", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141815078905364491": {"followers": "559", "datetime": "2019-06-20 21:08:23", "author": "@JayChance5", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141538175330000896": {"followers": "52", "datetime": "2019-06-20 02:48:04", "author": "@Tom_Manzini", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143880100401668096": {"followers": "46", "datetime": "2019-06-26 13:54:02", "author": "@_plute", "content_summary": "RT @AIcia_Solid: \u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23 https://t.co/ri8kMFKQMY"}, "1141836172622065664": {"followers": "83", "datetime": "2019-06-20 22:32:12", "author": "@BAlhafni", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143757241352196096": {"followers": "2", "datetime": "2019-06-26 05:45:50", "author": "@Draugexa", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141939402068770816": {"followers": "2,853", "datetime": "2019-06-21 05:22:24", "author": "@JimSpohrer", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141816903142428677": {"followers": "73", "datetime": "2019-06-20 21:15:38", "author": "@ursachi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141655122025549826": {"followers": "123", "datetime": "2019-06-20 10:32:46", "author": "@jeary_talking", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1216190023826997250": {"followers": "230", "datetime": "2020-01-12 02:47:52", "author": "@krishzen", "content_summary": "RT @Deep_In_Depth: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/cgMIUDyRmD #DeepLearning #NeuralNe\u2026"}, "1176327000425979904": {"followers": "11,930", "datetime": "2019-09-24 02:46:26", "author": "@Rosenchild", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141780657678573569": {"followers": "31", "datetime": "2019-06-20 18:51:36", "author": "@ChaoJiang_Neuro", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143511786110369793": {"followers": "475", "datetime": "2019-06-25 13:30:29", "author": "@censored__", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142112844806156288": {"followers": "205", "datetime": "2019-06-21 16:51:36", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143699709564817410": {"followers": "2,671", "datetime": "2019-06-26 01:57:14", "author": "@ayirpelle", "content_summary": "RT @IAmSamFin: This is a lot of $$$, and one could raise solid q's about the cost-benefit to society if ML research increasingly focuses on\u2026"}, "1141663916759457793": {"followers": "0", "datetime": "2019-06-20 11:07:43", "author": "@Niladri58710940", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1175158089575583744": {"followers": "205", "datetime": "2019-09-20 21:21:36", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141955073662181376": {"followers": "436", "datetime": "2019-06-21 06:24:40", "author": "@krtk", "content_summary": "RT @Sam_Witteveen: This looks super impressive!! https://t.co/m5D4XVkX82"}, "1141563699171229696": {"followers": "23", "datetime": "2019-06-20 04:29:29", "author": "@deehzee", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1219752003691139072": {"followers": "476", "datetime": "2020-01-21 22:41:54", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141626240794710016": {"followers": "7,503", "datetime": "2019-06-20 08:38:00", "author": "@HigeponJa", "content_summary": "\u307e\u305f\u52c9\u5f37\u3057\u306a\u304f\u3061\u3083\u3002"}, "1141770868806340608": {"followers": "321", "datetime": "2019-06-20 18:12:42", "author": "@Soria_Emilio", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141874773686771713": {"followers": "86", "datetime": "2019-06-21 01:05:35", "author": "@Arctan2", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142142628798128128": {"followers": "124", "datetime": "2019-06-21 18:49:57", "author": "@tamilyn", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141559523800313856": {"followers": "226", "datetime": "2019-06-20 04:12:54", "author": "@sagarpath", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141738864093962240": {"followers": "2,379", "datetime": "2019-06-20 16:05:32", "author": "@tedgreenwald", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/YiSm2ByKhd"}, "1142034474588233728": {"followers": "23", "datetime": "2019-06-21 11:40:11", "author": "@airOfBengal", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143417920455892992": {"followers": "273", "datetime": "2019-06-25 07:17:30", "author": "@SunriseAtTheSky", "content_summary": "RT @vitojph: And don't forget the CO2 footprint. We should be training these models on Mars. Let's fight against climate change on Earth\u2026"}, "1143704605471068160": {"followers": "107", "datetime": "2019-06-26 02:16:41", "author": "@merajatgupta", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141833201381457920": {"followers": "332", "datetime": "2019-06-20 22:20:24", "author": "@pabloibieta", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141980263733321729": {"followers": "257", "datetime": "2019-06-21 08:04:46", "author": "@chajeongwon", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142010906982912000": {"followers": "744", "datetime": "2019-06-21 10:06:32", "author": "@pdrmtaheri", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1216076981571325954": {"followers": "217", "datetime": "2020-01-11 19:18:40", "author": "@ShyBOT7", "content_summary": "RT @Deep_In_Depth: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/cgMIUDyRmD #DeepLearning #NeuralNe\u2026"}, "1141851075407781890": {"followers": "1,251", "datetime": "2019-06-20 23:31:25", "author": "@nikhilbd", "content_summary": "XLNet > BERT. Biggest surprise: It's not called Big Bird."}, "1141713073415348224": {"followers": "56", "datetime": "2019-06-20 14:23:03", "author": "@hbzhang", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141741608867684352": {"followers": "213", "datetime": "2019-06-20 16:16:26", "author": "@matt_vowels", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1234323089086771203": {"followers": "476", "datetime": "2020-03-02 03:42:11", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141610114073468929": {"followers": "381", "datetime": "2019-06-20 07:33:55", "author": "@shyamal_chandra", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141700765020385281": {"followers": "44", "datetime": "2019-06-20 13:34:08", "author": "@kazu_mimu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1258482512516374528": {"followers": "476", "datetime": "2020-05-07 19:43:07", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141597643757576193": {"followers": "46", "datetime": "2019-06-20 06:44:22", "author": "@angelsalamanca", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141656847264432133": {"followers": "127", "datetime": "2019-06-20 10:39:38", "author": "@DanilBaibak", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141966040760553473": {"followers": "1,098", "datetime": "2019-06-21 07:08:15", "author": "@drago_carlo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141726833882173440": {"followers": "2,671", "datetime": "2019-06-20 15:17:44", "author": "@ayirpelle", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141752193239998464": {"followers": "844", "datetime": "2019-06-20 16:58:30", "author": "@PiotrCzapla", "content_summary": "Awesome to see the work of Zhilin Yang, @ZihangDai, being appreciated even though their previous work on transformer-xl was rejected on ICLR because \"reviewers found it difficult to see the merit of the proposed approach\" https://t.co/deMjNqmxLv"}, "1142031428219211776": {"followers": "1,530", "datetime": "2019-06-21 11:28:05", "author": "@domyoriginal", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141732552534679552": {"followers": "4", "datetime": "2019-06-20 15:40:27", "author": "@fr_kirska", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142481336420036608": {"followers": "1,220", "datetime": "2019-06-22 17:15:51", "author": "@chahan69", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143670685270716416": {"followers": "1,110", "datetime": "2019-06-26 00:01:54", "author": "@Elliot_M_Jones", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1143648101393149952": {"followers": "5,761", "datetime": "2019-06-25 22:32:09", "author": "@BenMazer", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1213485333410656256": {"followers": "3,500", "datetime": "2020-01-04 15:40:23", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141918541580341248": {"followers": "316", "datetime": "2019-06-21 03:59:30", "author": "@XzPFhGXhc8nDTIP", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141695187015651328": {"followers": "175", "datetime": "2019-06-20 13:11:58", "author": "@plison2", "content_summary": "Thanks, highly interesting!"}, "1141894885877792768": {"followers": "44", "datetime": "2019-06-21 02:25:30", "author": "@satyamurty69", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1164840238562369536": {"followers": "222", "datetime": "2019-08-23 10:02:09", "author": "@PapersTrending", "content_summary": "[10/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 11,113 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1141769940321689600": {"followers": "21,221", "datetime": "2019-06-20 18:09:01", "author": "@DataSciNews", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141833997338718209": {"followers": "6,858", "datetime": "2019-06-20 22:23:33", "author": "@abhi1thakur", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142019408086233088": {"followers": "110", "datetime": "2019-06-21 10:40:19", "author": "@Raquel1934", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145539596299542529": {"followers": "346", "datetime": "2019-07-01 03:48:17", "author": "@ATGCU2501", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141615730821087232": {"followers": "491", "datetime": "2019-06-20 07:56:15", "author": "@ale_suglia", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141900549408735232": {"followers": "685", "datetime": "2019-06-21 02:48:01", "author": "@satoshihirose", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141560917630619648": {"followers": "41", "datetime": "2019-06-20 04:18:26", "author": "@ElectroDod", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141523903560462336": {"followers": "17", "datetime": "2019-06-20 01:51:21", "author": "@vincentperot", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143604954919309312": {"followers": "416", "datetime": "2019-06-25 19:40:43", "author": "@evan_cofer", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143474834627846146": {"followers": "11,291", "datetime": "2019-06-25 11:03:39", "author": "@marypcbuk", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143571099852136449": {"followers": "611", "datetime": "2019-06-25 17:26:11", "author": "@anat_elhalal", "content_summary": "Long live #MIGarage @DigiCatapult\u2019s programme to support startups with access to computation. Att:@DigiCatNI"}, "1141714694698221569": {"followers": "69", "datetime": "2019-06-20 14:29:29", "author": "@include_j", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1213847982799491073": {"followers": "3,500", "datetime": "2020-01-05 15:41:25", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141769409947717634": {"followers": "149", "datetime": "2019-06-20 18:06:55", "author": "@parker_brydon", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/k2zsU9YDw2 XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms #BERT on 20 tasks, ofte"}, "1152704714989101058": {"followers": "202", "datetime": "2019-07-20 22:19:54", "author": "@AngryEgyptianX", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143672064605212672": {"followers": "484", "datetime": "2019-06-26 00:07:23", "author": "@dgarijov", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141993464407740417": {"followers": "442", "datetime": "2019-06-21 08:57:13", "author": "@AdrianB82", "content_summary": "RT @arxiv_cscl: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141728385661460480": {"followers": "1,286", "datetime": "2019-06-20 15:23:54", "author": "@savvyRL", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141567753813467136": {"followers": "756", "datetime": "2019-06-20 04:45:36", "author": "@assthiam19", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143776413742501889": {"followers": "3,314", "datetime": "2019-06-26 07:02:02", "author": "@AndySugs", "content_summary": "RT: DataSciNews: RT quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: https://t.co/q4HOf56qLB github (code + pretrained models): https://t.co/DIdOVc6p7z with Zhilin Y\u2026 ht"}, "1141846652954656768": {"followers": "484", "datetime": "2019-06-20 23:13:51", "author": "@timothy_lkh_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141668405851938816": {"followers": "15", "datetime": "2019-06-20 11:25:33", "author": "@nilazak", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141892768320512000": {"followers": "52", "datetime": "2019-06-21 02:17:05", "author": "@salil_23", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1143011048464646144": {"followers": "11", "datetime": "2019-06-24 04:20:44", "author": "@OcYucy", "content_summary": "RT @hillbig: \u4e8b\u524d\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u308bBERT\u306f\u30de\u30b9\u30af\u3055\u308c\u305f\u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u7121\u8996\u3057\u3066\u3044\u305f\u3002XLNet\u306f\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u5358\u8a9e\u3092\u9806\u306b\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u306f\u65e2\u306b\u751f\u6210\u3057\u305f\u5358\u8a9e\u306e\u307f\u3067\u6761\u4ef6\u4ed8\u3059\u308b\u30de\u30b9\u30af\u5316\u6ce8\u610f\u3092\u4f7f\u3063\u305fTransformer\u3067\u4e88\u6e2c\u3002\u591a\u304f\u306eNLP\u2026"}, "1141707057805946881": {"followers": "824", "datetime": "2019-06-20 13:59:09", "author": "@morioka", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141688604093296647": {"followers": "3,885", "datetime": "2019-06-20 12:45:49", "author": "@LTIatCMU", "content_summary": "Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldcmu, and @GoogleAI. The gains are due to a new permutation-based pre-training objective, and models that capture longer context than"}, "1219571253947895808": {"followers": "410", "datetime": "2020-01-21 10:43:40", "author": "@MatrixSpain", "content_summary": "RT @crypto_magix: In few months everyone can rent decentralized & cheap processing #AI power in @MatrixAINetwork (GPU powered) #blockchain\u2026"}, "1141683776013119490": {"followers": "531", "datetime": "2019-06-20 12:26:38", "author": "@mpuig", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143831355551207424": {"followers": "110", "datetime": "2019-06-26 10:40:21", "author": "@Raquel1934", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143417425565757440": {"followers": "531", "datetime": "2019-06-25 07:15:32", "author": "@vitojph", "content_summary": "And don't forget the CO2 footprint. We should be training these models on Mars. Let's fight against climate change on Earth while we help terraform the red planet"}, "1143759218857562112": {"followers": "294", "datetime": "2019-06-26 05:53:42", "author": "@rose_miura", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1141907712113987584": {"followers": "27", "datetime": "2019-06-21 03:16:28", "author": "@chenifeng", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1151426865661403136": {"followers": "537", "datetime": "2019-07-17 09:42:11", "author": "@albarjip", "content_summary": "BERT for language modelling was good, but we already have something better: XLNet. Joining a clever factorization permutation trick and Transformer-XL layers produces new state of the art results https://t.co/BwqQP32QBp https://t.co/I0yuDBzoh0"}, "1142094974726467589": {"followers": "6", "datetime": "2019-06-21 15:40:35", "author": "@FumingGuo", "content_summary": "Fabulous work!"}, "1141555635395260417": {"followers": "19,073", "datetime": "2019-06-20 03:57:27", "author": "@xamat", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141707199275651073": {"followers": "216", "datetime": "2019-06-20 13:59:42", "author": "@e0en", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142708789197594624": {"followers": "946", "datetime": "2019-06-23 08:19:40", "author": "@zach_wambugu", "content_summary": "RT @NegruEduard: Great news for #NLP enthusiasts. XLnet[1] is said to outperform Bert on certain nlp tasks according to this article [2]. #\u2026"}, "1141697551755649024": {"followers": "1,309", "datetime": "2019-06-20 13:21:22", "author": "@akihiro_akichan", "content_summary": "RT @shion_honda: Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj https://t.co/uPBPdOnoXm"}, "1143096673331273728": {"followers": "222", "datetime": "2019-06-24 10:00:59", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 2,375 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141518003869106176": {"followers": "8,541", "datetime": "2019-06-20 01:27:55", "author": "@rbhar90", "content_summary": "This is a beautiful paper. A new language pretraining method that achieves compelling improvements over BERT. Large jumps on a number of benchmarks. Uses a clever permutation invariant autoregressive formulation plus Transformer-XL for handling long sequen"}, "1141938183736909824": {"followers": "679", "datetime": "2019-06-21 05:17:33", "author": "@ThomasKoller", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143580256080728064": {"followers": "63", "datetime": "2019-06-25 18:02:34", "author": "@Krelix", "content_summary": "RT @baykenney: access to compute can't be decoupled from responsible development of ml systems. https://t.co/8pAmAV39Mg"}, "1142502997907451905": {"followers": "66", "datetime": "2019-06-22 18:41:56", "author": "@shubh_300595", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141630050707660801": {"followers": "2,250", "datetime": "2019-06-20 08:53:09", "author": "@CharlotteHase", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1142034836296630272": {"followers": "917", "datetime": "2019-06-21 11:41:37", "author": "@chronologic1", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143845431006261248": {"followers": "61", "datetime": "2019-06-26 11:36:17", "author": "@Soubh1k", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141828103129272322": {"followers": "710", "datetime": "2019-06-20 22:00:08", "author": "@snehaark", "content_summary": "There 8 copies of this paper lying around the nearest office printer already. \ud83e\udd13"}, "1141691664873865216": {"followers": "361", "datetime": "2019-06-20 12:57:59", "author": "@jadhavamitb", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1142073513844461568": {"followers": "536", "datetime": "2019-06-21 14:15:19", "author": "@sangha_deb", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1143706700282597376": {"followers": "323", "datetime": "2019-06-26 02:25:01", "author": "@vodkamomo", "content_summary": "RT @IAmSamFin: This is a lot of $$$, and one could raise solid q's about the cost-benefit to society if ML research increasingly focuses on\u2026"}, "1141952952740880385": {"followers": "91", "datetime": "2019-06-21 06:16:15", "author": "@lanyboy", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143390041718304769": {"followers": "7", "datetime": "2019-06-25 05:26:43", "author": "@panteon_college", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1145806263269199872": {"followers": "949", "datetime": "2019-07-01 21:27:55", "author": "@hkawaguc", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141701714476769280": {"followers": "123", "datetime": "2019-06-20 13:37:55", "author": "@almoslmi", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141956508508479491": {"followers": "1,167", "datetime": "2019-06-21 06:30:22", "author": "@cylon7", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143832314901753856": {"followers": "1,017", "datetime": "2019-06-26 10:44:09", "author": "@YutaroTomoto", "content_summary": "RT @kazunori_279: TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u2026"}, "1143588422218633216": {"followers": "39", "datetime": "2019-06-25 18:35:01", "author": "@alymenbr", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1161496842133266433": {"followers": "26", "datetime": "2019-08-14 04:36:41", "author": "@Yugi_NHC", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1141788861527416832": {"followers": "11", "datetime": "2019-06-20 19:24:12", "author": "@bananadata48", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1200689459273977857": {"followers": "12,765", "datetime": "2019-11-30 08:14:09", "author": "@jaguring1", "content_summary": "Meta-Learning Update Rules for Unsupervised Representation Learning https://t.co/BJsGsZxbeL On the Variance of the Adaptive Learning Rate and Beyond https://t.co/1edwZFySqt XLNet: Generalized Autoregressive Pretraining for Language Understanding https://"}, "1153718364054917121": {"followers": "100", "datetime": "2019-07-23 17:27:47", "author": "@KarimiRabeeh", "content_summary": "A Fair Comparison Study of XLNet and BERT with Large Models, what I exactly was expecting to see in XLNet paper(https://t.co/XfPcvr9jIT), awesome. https://t.co/7I2IVQS0sM"}, "1143789566010118144": {"followers": "84", "datetime": "2019-06-26 07:54:17", "author": "@vanhuyz", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143854630448828424": {"followers": "1", "datetime": "2019-06-26 12:12:50", "author": "@Daniil11628237", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141728688989495297": {"followers": "197", "datetime": "2019-06-20 15:25:06", "author": "@MatthewTeschke", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1143074342323945473": {"followers": "183", "datetime": "2019-06-24 08:32:15", "author": "@Synekt16", "content_summary": "RT @MikiBear_: \uc544\ub2c8 \ubbf8\uce5c \u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b\u314b https://t.co/ixHqPog8Yu https://t.co/DSxHpesHSN"}, "1141903177882583041": {"followers": "135", "datetime": "2019-06-21 02:58:27", "author": "@partialf", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143376802418700290": {"followers": "4,989", "datetime": "2019-06-25 04:34:07", "author": "@cigitalgem", "content_summary": "ML economics https://t.co/OWee0ifYGc"}, "1141563282592940032": {"followers": "2,671", "datetime": "2019-06-20 04:27:50", "author": "@ayirpelle", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143624411511025664": {"followers": "1,613", "datetime": "2019-06-25 20:58:01", "author": "@keskkyla", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143213828416692229": {"followers": "37", "datetime": "2019-06-24 17:46:31", "author": "@jamesethatcher", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141516271764176896": {"followers": "2,204", "datetime": "2019-06-20 01:21:02", "author": "@MarkNeumannnn", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143398598106853377": {"followers": "58", "datetime": "2019-06-25 06:00:43", "author": "@moret1788", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143446309606744065": {"followers": "589", "datetime": "2019-06-25 09:10:19", "author": "@ArthurCamara", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141710781534392320": {"followers": "322", "datetime": "2019-06-20 14:13:56", "author": "@jesusfbes", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141710335801532418": {"followers": "2,031", "datetime": "2019-06-20 14:12:10", "author": "@PMZepto", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1246105246448807936": {"followers": "320", "datetime": "2020-04-03 16:00:16", "author": "@urvishp33140920", "content_summary": "RT @carlolepelaars: Finally got around to reading up on some recent NLP papers. Currently reading: ALBERT: https://t.co/AfVSLTf866 RoBERT\u2026"}, "1143579921748504576": {"followers": "13", "datetime": "2019-06-25 18:01:14", "author": "@jvlochter", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142225885115891713": {"followers": "28,418", "datetime": "2019-06-22 00:20:47", "author": "@ogrisel", "content_summary": "RT @deliprao: PSA for #NLProc folks. XLM is not XLNet (https://t.co/G5JZTVXD1A) that was released couple days ago. They both beat BERT. You\u2026"}, "1141605088638537730": {"followers": "28", "datetime": "2019-06-20 07:13:57", "author": "@hey_kishore", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1232966624942018560": {"followers": "205", "datetime": "2020-02-27 09:52:05", "author": "@fudoumyousan", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1142284434479222785": {"followers": "58", "datetime": "2019-06-22 04:13:26", "author": "@aptr322", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141682559958622208": {"followers": "962", "datetime": "2019-06-20 12:21:48", "author": "@bgalbraith", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141719950228246528": {"followers": "4,884", "datetime": "2019-06-20 14:50:22", "author": "@martin_wicke", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1151516388671885312": {"followers": "0", "datetime": "2019-07-17 15:37:55", "author": "@yuco49574109", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1142066651707379712": {"followers": "283", "datetime": "2019-06-21 13:48:02", "author": "@kowalskithomas", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143723281297448961": {"followers": "12,345", "datetime": "2019-06-26 03:30:54", "author": "@kazunori_279", "content_summary": "TPU v3 Pod\u306f32 core\u3042\u305f\u308a$32\u306a\u306e\u3067\u3001512 core\u306e\u6599\u91d1\u306f\uff08\u672a\u516c\u8868\u3067\u3059\u304c\u540c\u3058\u6bd4\u7387\u3068\u60f3\u5b9a\u3059\u308b\u3068\uff09$512 x 24 x 2.5 = $30K\uff08340\u4e07\u5186\u304f\u3089\u3044\uff09\u3067\u3059\u306d\u3002\u4f55\u5343\u4e07\u3082\u3059\u308b\u30b9\u30d1\u30b3\u30f3\u8cb7\u308f\u306a\u304f\u3066\u3082\u3053\u306e\u6027\u80fd\u304c\u3059\u3050\u624b\u306b\u5165\u308b\u3002"}, "1141516145847128065": {"followers": "564", "datetime": "2019-06-20 01:20:32", "author": "@kalpeshk2011", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141760329853968384": {"followers": "515", "datetime": "2019-06-20 17:30:50", "author": "@belizgunel", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142006224751222785": {"followers": "45", "datetime": "2019-06-21 09:47:56", "author": "@andromeda_aaa1", "content_summary": "RT @shion_honda: XLNet [Yang+, 2019] context\u306e\u9806\u5e8f\u3092\u5909\u3048\u306a\u304c\u3089\u3001\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092 Two-Stream Self-Attention(TrfmXL\u306e\u6d3e\u751f)\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3002RACE\u3001SQuAD\u3001GLUE\u306a\u306920\u30bf\u30b9\u30af\u3067BERT\u3092\u5927\u5e45\u306b\u2026"}, "1143488280744222721": {"followers": "98", "datetime": "2019-06-25 11:57:05", "author": "@paul_conyngham", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141929360598827008": {"followers": "426", "datetime": "2019-06-21 04:42:30", "author": "@okamoto0409", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143410106178121728": {"followers": "796", "datetime": "2019-06-25 06:46:27", "author": "@ayyar", "content_summary": "https://t.co/MEeuMGw869"}, "1141741965136027649": {"followers": "2,785", "datetime": "2019-06-20 16:17:51", "author": "@murisliver", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141610193156861952": {"followers": "230", "datetime": "2019-06-20 07:34:14", "author": "@shashank_bits", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141945072621015040": {"followers": "6,124", "datetime": "2019-06-21 05:44:56", "author": "@StephenPiment", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141740569875701760": {"followers": "1,713", "datetime": "2019-06-20 16:12:19", "author": "@arXiv__ml", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1141704536626798592": {"followers": "329", "datetime": "2019-06-20 13:49:08", "author": "@MarcoAlmada", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141623686136360961": {"followers": "21", "datetime": "2019-06-20 08:27:51", "author": "@mukosame", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141746917791227904": {"followers": "258", "datetime": "2019-06-20 16:37:32", "author": "@arezae", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1143795618789433344": {"followers": "38", "datetime": "2019-06-26 08:18:20", "author": "@d10nator", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141844029337767936": {"followers": "668", "datetime": "2019-06-20 23:03:25", "author": "@fulmicoton", "content_summary": "RT @n0mad_0: \ud83d\ude31 https://t.co/uRioG1LkqW"}, "1141880005959413760": {"followers": "34", "datetime": "2019-06-21 01:26:23", "author": "@DavidRen555", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143651194126184448": {"followers": "3,107", "datetime": "2019-06-25 22:44:27", "author": "@samweston", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1143799827823595522": {"followers": "200", "datetime": "2019-06-26 08:35:04", "author": "@overleo", "content_summary": "BERT\u3092\u8d85\u3048\u305fXLNet\u306e\u7d39\u4ecb - akihiro_f - Medium: \u6982\u8981https://t.co/khdHR0GZWP XLNet\u306f2019/6/19\u306b\u3001\u201dXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u3068\u984c\u3057\u3066Arxiv\u306b\u6295\u7a3f\u3055\u308c\u305f\u8ad6\u6587\u3067\u3059\u3002\u4e00\u8a00(?)\u3067\u3044\u3046\u3068\u2026 https://t.co/p03Aej2nmw [ml]"}, "1143827932999245824": {"followers": "5,910", "datetime": "2019-06-26 10:26:45", "author": "@dirkvandenpoel", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143403953432006657": {"followers": "363", "datetime": "2019-06-25 06:22:00", "author": "@PfeiffJo", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1216072892359827457": {"followers": "513", "datetime": "2020-01-11 19:02:25", "author": "@PasqualeDeMar", "content_summary": "RT @Deep_In_Depth: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/cgMIUDyRmD #DeepLearning #NeuralNe\u2026"}, "1142257934866345985": {"followers": "3", "datetime": "2019-06-22 02:28:08", "author": "@FabeiWang", "content_summary": "awesome"}, "1143721710924881921": {"followers": "1,831", "datetime": "2019-06-26 03:24:39", "author": "@Vengineer", "content_summary": "\u6642\u9593\u3092\u304a\u91d1\u3067\u8cb7\u3046\u3002"}, "1141710216909557760": {"followers": "1,375", "datetime": "2019-06-20 14:11:42", "author": "@AjitGaddam", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143413624511315968": {"followers": "443", "datetime": "2019-06-25 07:00:26", "author": "@Keemedes", "content_summary": "Jesu"}, "1141635965397524480": {"followers": "1,543", "datetime": "2019-06-20 09:16:39", "author": "@deadcatbouncepn", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1143356986530181128": {"followers": "322", "datetime": "2019-06-25 03:15:22", "author": "@letranger14", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143499178615672832": {"followers": "113", "datetime": "2019-06-25 12:40:24", "author": "@_SalvorHardin", "content_summary": "Who cares? They took home top prize at the arcade! Mind and money well spent."}, "1143481010937585664": {"followers": "236", "datetime": "2019-06-25 11:28:12", "author": "@flowing", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141512256552755206": {"followers": "291", "datetime": "2019-06-20 01:05:04", "author": "@QizheXie", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141535422285139968": {"followers": "54", "datetime": "2019-06-20 02:37:08", "author": "@ttnam93", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141598784629149702": {"followers": "25,741", "datetime": "2019-06-20 06:48:54", "author": "@Miles_Brundage", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141700669499482112": {"followers": "64", "datetime": "2019-06-20 13:33:46", "author": "@sinchani", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143188036445855744": {"followers": "1,176", "datetime": "2019-06-24 16:04:01", "author": "@lesv", "content_summary": "Looks interesting"}, "1142014270776856576": {"followers": "556", "datetime": "2019-06-21 10:19:54", "author": "@puneethmishra", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143595841417953280": {"followers": "555", "datetime": "2019-06-25 19:04:30", "author": "@jonathankoren", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141589325060009984": {"followers": "1,187", "datetime": "2019-06-20 06:11:19", "author": "@ivanprado", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141638504423677952": {"followers": "25", "datetime": "2019-06-20 09:26:44", "author": "@NLP_Grayming", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143356839712690176": {"followers": "322", "datetime": "2019-06-25 03:14:47", "author": "@letranger14", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141559823328141312": {"followers": "1,868", "datetime": "2019-06-20 04:14:05", "author": "@Beeeender", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141660278376914945": {"followers": "2,481", "datetime": "2019-06-20 10:53:16", "author": "@shion_honda", "content_summary": "Transformer-XL\u304cXLNet\u306b\u9032\u5316\u3057\u3066BERT\u3092\u8d85\u3048\u3066\u304d\u307e\u3057\u305f\u3002 https://t.co/uLaOqq8TBj"}, "1142036812728340485": {"followers": "17", "datetime": "2019-06-21 11:49:28", "author": "@ayanmaj1992", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141802811190992897": {"followers": "2,737", "datetime": "2019-06-20 20:19:38", "author": "@sivareddyg", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142167540786978821": {"followers": "", "datetime": "2019-06-21 20:28:56", "author": "@AaaLee", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143777294541975553": {"followers": "2", "datetime": "2019-06-26 07:05:32", "author": "@sivia89024", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141661349400813569": {"followers": "6,414", "datetime": "2019-06-20 10:57:31", "author": "@pmjoshi_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1177523907122946048": {"followers": "222", "datetime": "2019-09-27 10:02:31", "author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 561 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141778264421089280": {"followers": "111", "datetime": "2019-06-20 18:42:06", "author": "@tavoaguilar91", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143386682374406144": {"followers": "315", "datetime": "2019-06-25 05:13:22", "author": "@_yayan24", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141699980240908288": {"followers": "795", "datetime": "2019-06-20 13:31:01", "author": "@yuji9511_compro", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143692908010201088": {"followers": "441", "datetime": "2019-06-26 01:30:12", "author": "@geinfituesd01", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142271105635217408": {"followers": "274", "datetime": "2019-06-22 03:20:28", "author": "@cighos", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1151796311349444609": {"followers": "778", "datetime": "2019-07-18 10:10:14", "author": "@pommedeterre33", "content_summary": "@kchonyc @macavaney @srchvrs @CharlotteHase At the end of XLnet paper there is a part about ad hoc search and Bert is included in the exp https://t.co/sl8wrNjaDB"}, "1141999528477974528": {"followers": "99", "datetime": "2019-06-21 09:21:19", "author": "@wjwwilliams", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141574500531597313": {"followers": "182", "datetime": "2019-06-20 05:12:25", "author": "@NilayShri", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141835764151570434": {"followers": "29", "datetime": "2019-06-20 22:30:35", "author": "@liuhan2074198", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143685316680802307": {"followers": "8,435", "datetime": "2019-06-26 01:00:02", "author": "@ngkabra", "content_summary": "RT @timhwang: the inherent structure of markets in artificial intelligence: oligopoly https://t.co/ZVphlI7jpc"}, "1141774933078532100": {"followers": "1,145", "datetime": "2019-06-20 18:28:51", "author": "@niku9Tenhou", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141732467243720704": {"followers": "116", "datetime": "2019-06-20 15:40:07", "author": "@xerofdv", "content_summary": "RT @ML_NLP: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.c\u2026"}, "1141527895166390277": {"followers": "3,881", "datetime": "2019-06-20 02:07:13", "author": "@BrundageBot", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le https://t.co/TW02SATyIe"}, "1141514182967250945": {"followers": "2,663", "datetime": "2019-06-20 01:12:44", "author": "@sigitpurnomo", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1176627860565413889": {"followers": "476", "datetime": "2019-09-24 22:41:57", "author": "@yasuokajihei", "content_summary": "XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1141756333550637057": {"followers": "1,380", "datetime": "2019-06-20 17:14:57", "author": "@BrainRoaring", "content_summary": "RT @_Shivam_b: XLNet for NLP #ai https://t.co/cXVWzvG3Ii"}, "1141645351637721089": {"followers": "346", "datetime": "2019-06-20 09:53:57", "author": "@lhenault", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143580044830355456": {"followers": "7", "datetime": "2019-06-25 18:01:44", "author": "@romehrah", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142094020513955846": {"followers": "30", "datetime": "2019-06-21 15:36:48", "author": "@chenfeiyang2018", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1142328584658280450": {"followers": "739", "datetime": "2019-06-22 07:08:52", "author": "@vmpmember", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141936631672385539": {"followers": "154", "datetime": "2019-06-21 05:11:23", "author": "@joanSolCom", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141951408884867075": {"followers": "26", "datetime": "2019-06-21 06:10:06", "author": "@LorenzoOstano", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143700808610926593": {"followers": "816", "datetime": "2019-06-26 02:01:36", "author": "@oiorsme9k1", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141567347376832513": {"followers": "63", "datetime": "2019-06-20 04:43:59", "author": "@kuanchen22", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1143398347866173440": {"followers": "1,157", "datetime": "2019-06-25 05:59:44", "author": "@ixenario", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1144068236255698944": {"followers": "45", "datetime": "2019-06-27 02:21:37", "author": "@yoquankara", "content_summary": "A closer estimation for XLNet is $245,000 : 8 (cores) x 100 (tuning cycles) ~ $3M \ud83e\udd13 Bad news: uphill battle against big players is getting harder. Good news: they are (still) open sourcing the works."}, "1143425409478004736": {"followers": "222", "datetime": "2019-06-25 07:47:16", "author": "@sannikpatel", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142247858411732992": {"followers": "467", "datetime": "2019-06-22 01:48:06", "author": "@arisbw", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1141560729780346880": {"followers": "0", "datetime": "2019-06-20 04:17:41", "author": "@ahmaurya", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1210214644847001603": {"followers": "2,802", "datetime": "2019-12-26 15:03:50", "author": "@masahirok_jp", "content_summary": "RT @yasuokajihei: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/7UY32yRgUD"}, "1145615272109174795": {"followers": "171", "datetime": "2019-07-01 08:49:00", "author": "@pasta_jp", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1143782569558724608": {"followers": "2", "datetime": "2019-06-26 07:26:29", "author": "@sivia89024", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1141701150640500736": {"followers": "701", "datetime": "2019-06-20 13:35:40", "author": "@refine_P", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1141724416147120133": {"followers": "296", "datetime": "2019-06-20 15:08:07", "author": "@folaraz", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141620544426692609": {"followers": "199", "datetime": "2019-06-20 08:15:22", "author": "@mathpluscode", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1148562850983473153": {"followers": "169", "datetime": "2019-07-09 12:01:37", "author": "@motor_ai", "content_summary": "https://t.co/fsnDbGUCtM"}, "1143619186511556609": {"followers": "227", "datetime": "2019-06-25 20:37:16", "author": "@Ukerzel", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1219861533548826624": {"followers": "216", "datetime": "2020-01-22 05:57:08", "author": "@ayytact", "content_summary": "RT @steadydee: At $245,000 to train an an AI model in 2.5 days, cloud computing costs are out of control! \ud83d\udc40 Demand for blockchain will exp\u2026"}, "1141564031179902981": {"followers": "17", "datetime": "2019-06-20 04:30:48", "author": "@chandanbhatte", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142671294632251392": {"followers": "888", "datetime": "2019-06-23 05:50:41", "author": "@RichmanRonald", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145844981149515776": {"followers": "118", "datetime": "2019-07-02 00:01:46", "author": "@JunsongW", "content_summary": "RT @rctatman: Time to pick the next @kaggle reading group paper! Your options: - XLNet: Generalized Autoregressive Pretraining for NLU htt\u2026"}, "1141716839447388162": {"followers": "330", "datetime": "2019-06-20 14:38:01", "author": "@SerhiiHavrylov", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143585344383836160": {"followers": "1,000", "datetime": "2019-06-25 18:22:47", "author": "@danieltomasz", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1145916824346742785": {"followers": "9", "datetime": "2019-07-02 04:47:15", "author": "@crpersher", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1161216158479069185": {"followers": "222", "datetime": "2019-08-13 10:01:21", "author": "@PapersTrending", "content_summary": "[6/10] \ud83d\udcc8 - pytorch-pretrained-BERT - 10,656 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/RdEWpPLOJR"}, "1141864354502983681": {"followers": "220", "datetime": "2019-06-21 00:24:11", "author": "@NorikazuYamada", "content_summary": "RT @kyoun: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google&CMU) https://t.co/AjPxZX6tdX \u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\uff0eGLUE\uff0c\u8aad\u89e3\uff0c\u2026"}, "1143620263537516546": {"followers": "1,033", "datetime": "2019-06-25 20:41:32", "author": "@catherinehavasi", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143459859766972416": {"followers": "34", "datetime": "2019-06-25 10:04:09", "author": "@_sumitsaha_", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1141540978492264448": {"followers": "157", "datetime": "2019-06-20 02:59:12", "author": "@ankurbpn", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143575683970416641": {"followers": "158", "datetime": "2019-06-25 17:44:24", "author": "@ArpitJ_", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141644434561519616": {"followers": "89", "datetime": "2019-06-20 09:50:18", "author": "@SebastianEnger", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1153605886197342208": {"followers": "4,774", "datetime": "2019-07-23 10:00:50", "author": "@alex_combessie", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/J5xMtPsRgR"}, "1145343406144540672": {"followers": "10", "datetime": "2019-06-30 14:48:42", "author": "@brianwithaneye", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141617544844812288": {"followers": "4,019", "datetime": "2019-06-20 08:03:27", "author": "@ballforest", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142173811065798656": {"followers": "763", "datetime": "2019-06-21 20:53:51", "author": "@wasp_dragon", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143293115760930817": {"followers": "99", "datetime": "2019-06-24 23:01:34", "author": "@m_khurram_amin", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141709563202134017": {"followers": "4,577", "datetime": "2019-06-20 14:09:06", "author": "@prem_k", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1147792378327195649": {"followers": "1,834", "datetime": "2019-07-07 09:00:02", "author": "@j2blather", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141858465385594881": {"followers": "29", "datetime": "2019-06-21 00:00:47", "author": "@chenlailin", "content_summary": "RT @nik0spapp: XLNet, a new acronym to remember in #NLProc\ud83d\udc47 Two key differences to BERT: - Learns with an objective which maximizes likel\u2026"}, "1143857382180687872": {"followers": "315", "datetime": "2019-06-26 12:23:46", "author": "@_yayan24", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143535229446971393": {"followers": "310", "datetime": "2019-06-25 15:03:39", "author": "@sasikiran_m", "content_summary": "The dataset is 10x larger and the tokens processed are 4x larger than BERT. However, this is a huge step forward as we get GPT like LM while retaining the bi-directionality of BERT. Exciting times ahead. https://t.co/bCyYAS8Py9 #XLNet #BERT"}, "1143454677759123457": {"followers": "152", "datetime": "2019-06-25 09:43:34", "author": "@FourmyleCircus", "content_summary": "RT @zarawesome: it costs two hundred thousand dollars to teach this artificial intelligence for twelve seconds https://t.co/ylz5YUVSWl"}, "1141692369907019778": {"followers": "363", "datetime": "2019-06-20 13:00:47", "author": "@sudharsan2020", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141531695197446145": {"followers": "509", "datetime": "2019-06-20 02:22:19", "author": "@ritheshkumar_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141705743126675456": {"followers": "363", "datetime": "2019-06-20 13:53:55", "author": "@rodoume", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1237848480862703616": {"followers": "755", "datetime": "2020-03-11 21:10:50", "author": "@mitsuki20Gin", "content_summary": "RT @fudoumyousan: XLNet\u304c\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u30b9\u30b3\u30a2\u3067BERT\u3092\u5927\u5e45\u306b\u8d85\u3048\u308b https://t.co/AvpNLsSBrV"}, "1143653920343879680": {"followers": "1,471", "datetime": "2019-06-25 22:55:17", "author": "@matseinarsen", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142032914974171143": {"followers": "9", "datetime": "2019-06-21 11:33:59", "author": "@daniyalsyed788", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142132554033594369": {"followers": "180", "datetime": "2019-06-21 18:09:55", "author": "@sriharshams", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141820272040972289": {"followers": "36", "datetime": "2019-06-20 21:29:01", "author": "@lfdharo", "content_summary": "Awesome"}, "1141768889631727617": {"followers": "595", "datetime": "2019-06-20 18:04:50", "author": "@radevd", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1142062764590030850": {"followers": "34", "datetime": "2019-06-21 13:32:36", "author": "@dalmeidalorra", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144596695881097217": {"followers": "335", "datetime": "2019-06-28 13:21:32", "author": "@kobi78", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141573113273966593": {"followers": "621", "datetime": "2019-06-20 05:06:54", "author": "@dolaoseb", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1144077585279680512": {"followers": "5,147", "datetime": "2019-06-27 02:58:46", "author": "@shigekzishihara", "content_summary": "RT @yutakashino: XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R \u307b\u3093\u3068\u3067\u3059\u306d\uff0c512 TPU 2.5\u65e5\u3060\u3068\uff0cClo\u2026"}, "1141757657935749120": {"followers": "69", "datetime": "2019-06-20 17:20:13", "author": "@be_izzi", "content_summary": "RT @LTIatCMU: Today sees a major advance in the state of the art on English language understanding tasks by researchers at @LTIatCMU, @mldc\u2026"}, "1142831405413519362": {"followers": "15", "datetime": "2019-06-23 16:26:54", "author": "@uthamkamath", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141972813684912128": {"followers": "309", "datetime": "2019-06-21 07:35:10", "author": "@Daniel_J_Im", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141757821354160128": {"followers": "45", "datetime": "2019-06-20 17:20:52", "author": "@MEzraAragon", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142005465666088960": {"followers": "3", "datetime": "2019-06-21 09:44:55", "author": "@vanducng", "content_summary": "RT @lmthang: I thought SQuAD is solved with BERT, but XLNet team doesn't want to stop there:) Great results on SQuAD, GLUE, and RACE! @Ziha\u2026"}, "1141520101000134656": {"followers": "15", "datetime": "2019-06-20 01:36:15", "author": "@hahnz_", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143612776952291330": {"followers": "1,188", "datetime": "2019-06-25 20:11:47", "author": "@joeddav", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141574386966773760": {"followers": "96", "datetime": "2019-06-20 05:11:57", "author": "@uamaximusua", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141776223129034754": {"followers": "313", "datetime": "2019-06-20 18:33:59", "author": "@ravo", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141983594476462080": {"followers": "170", "datetime": "2019-06-21 08:18:00", "author": "@DriesBuyck", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143403033075707904": {"followers": "391", "datetime": "2019-06-25 06:18:21", "author": "@mafqcm", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141519237904777216": {"followers": "4,526", "datetime": "2019-06-20 01:32:49", "author": "@CShorten30", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141539269565132800": {"followers": "77,472", "datetime": "2019-06-20 02:52:25", "author": "@rsalakhu", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sentiment analysis), while integrating ideas from Transformer-XL: arxiv: https://t.co/Xjkh9rbYPX code + pretrained models: https://t."}, "1143690403331235842": {"followers": "5,475", "datetime": "2019-06-26 01:20:15", "author": "@AIcia_Solid", "content_summary": "\u8a71\u984c\u306e XLNet \u304f\u3093\u3001 GCP \u3060\u3068\u5b66\u7fd2\u3055\u305b\u308b\u306e\u306b3000\u4e07\u304f\u3089\u3044\u304b\u304b\u308b\u307f\u305f\u3044\u3067\u3059\u306d\ud83e\udd23\ud83e\udd23\ud83e\udd23"}, "1143637003671498752": {"followers": "80", "datetime": "2019-06-25 21:48:04", "author": "@abhiiitr", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141814884809809920": {"followers": "4,053", "datetime": "2019-06-20 21:07:37", "author": "@Vilinthril", "content_summary": "RT @gneubig: There are a number of nice aspects to this method, but perhaps the nicest thing is that it's not named after a Sesame Street c\u2026"}, "1163837528631197699": {"followers": "1,495", "datetime": "2019-08-20 15:37:44", "author": "@tommy19970714", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143793401948594176": {"followers": "24", "datetime": "2019-06-26 08:09:32", "author": "@ponzislavish", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143590687289597952": {"followers": "207", "datetime": "2019-06-25 18:44:01", "author": "@TechRonic9876", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141919233720254469": {"followers": "620", "datetime": "2019-06-21 04:02:15", "author": "@apsdehal", "content_summary": "Impressive super human performance on GLUE and a lot of other tasks. I am curious to see how it performs on SuperGLUE."}, "1141933900211609601": {"followers": "5,219", "datetime": "2019-06-21 05:00:32", "author": "@omarsar0", "content_summary": "Paper: https://t.co/BC4mfHr9gP Code: https://t.co/6ZXwOfg3GV"}, "1141635702465146880": {"followers": "976", "datetime": "2019-06-20 09:15:36", "author": "@Edeediong", "content_summary": "RT @rsalakhu: XLNet: Generalized Autoregressive Pretraining for Language Understanding: outperforming BERT on 20 tasks (SQuAD, GLUE, sent\u2026"}, "1141790072490090500": {"followers": "60", "datetime": "2019-06-20 19:29:01", "author": "@airandoust", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143550488471900161": {"followers": "275", "datetime": "2019-06-25 16:04:17", "author": "@EricOndenyi", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1143760404478402565": {"followers": "71", "datetime": "2019-06-26 05:58:25", "author": "@GGozzoli", "content_summary": "RT @IAmSamFin: This is a lot of $$$, and one could raise solid q's about the cost-benefit to society if ML research increasingly focuses on\u2026"}, "1141533287879499779": {"followers": "523", "datetime": "2019-06-20 02:28:39", "author": "@bensprecher", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141884381595721728": {"followers": "2,187", "datetime": "2019-06-21 01:43:46", "author": "@gepuro", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143008343541211137": {"followers": "18,250", "datetime": "2019-06-24 04:09:59", "author": "@hillbig", "content_summary": "BERT ignores dependency between the masked positions. XLNet uses a random permutation of the factorization and the transformer with the masked attention to incorporate the dependency, achieving new SOTA on 18 NLP tasks. https://t.co/TSBzCY75rY"}, "1142477460329717760": {"followers": "19,900", "datetime": "2019-06-22 17:00:27", "author": "@dawnieando", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1153311248798429184": {"followers": "166", "datetime": "2019-07-22 14:30:03", "author": "@maurobennici", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1210215306305536001": {"followers": "2,802", "datetime": "2019-12-26 15:06:28", "author": "@masahirok_jp", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141661815816040450": {"followers": "26", "datetime": "2019-06-20 10:59:22", "author": "@dataquidnunc", "content_summary": "Just when we thought BERT was the answer to our NLP tasks...."}, "1143474972146278400": {"followers": "534", "datetime": "2019-06-25 11:04:12", "author": "@krishnamrith12", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141527355955044354": {"followers": "338", "datetime": "2019-06-20 02:05:04", "author": "@__snehal__", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1168443993216020480": {"followers": "350", "datetime": "2019-09-02 08:42:11", "author": "@vochicong", "content_summary": "BERT\u8d85\u3048\u306eXLNet\u8a00\u8a9e\u30e2\u30c7\u30eb\u306epretrain\u8cbb\u7528\uff08TPU v3\uff09\u304c 512 (chips) * 2.5 (days) * 24 (hours) * ($8/4) = $61,440 \u3067\u7d04 650 \u4e07\u5186 \u4e0b\u6d41\u8ee2\u79fb\u5b66\u7fd2 finetuning \u3067\u6d3b\u7528\u3057\u305f\u3044\u306e\u3060\u304c\u3001\u65e5\u672c\u8a9e pretrain \u30e2\u30c7\u30eb\u306f\u516c\u958b\u3055\u308c\u3066\u3044\u306a\u3044"}, "1141601196022435841": {"followers": "8,478", "datetime": "2019-06-20 06:58:29", "author": "@ai_news_jp", "content_summary": "RT @yoquankara: BERT\u3092\u8d85\u3048\u305fXLNet (by CMU & Google) https://t.co/m1tiKe3tow"}, "1141893921833934851": {"followers": "22", "datetime": "2019-06-21 02:21:40", "author": "@Tsingggg", "content_summary": "RT @mark_riedl: RIP BERT The problem is naming models after Sesame Street characters is that it artificially adds significance to then mod\u2026"}, "1141717718015631361": {"followers": "51,260", "datetime": "2019-06-20 14:41:30", "author": "@ML_NLP", "content_summary": "XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) https://t.co/AG9dsvdaJ2 #NLProc"}, "1141543643737726977": {"followers": "2,280", "datetime": "2019-06-20 03:09:48", "author": "@AfroRoboticist", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142058551147294724": {"followers": "841", "datetime": "2019-06-21 13:15:51", "author": "@neibc", "content_summary": "Great!"}, "1141612550129405952": {"followers": "1,168", "datetime": "2019-06-20 07:43:36", "author": "@mark_cummins", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141582531994750977": {"followers": "422", "datetime": "2019-06-20 05:44:19", "author": "@jcvasquezc1", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141738540906110977": {"followers": "649", "datetime": "2019-06-20 16:04:15", "author": "@lucy3_li", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1154551224772911104": {"followers": "14", "datetime": "2019-07-26 00:37:17", "author": "@elevenyeast", "content_summary": "#note #bert #xlnet https://t.co/xCnPJOBHnO"}, "1142190704937988097": {"followers": "12,924", "datetime": "2019-06-21 22:00:59", "author": "@deliprao", "content_summary": "XLNet is this \ud83d\udc47\ud83c\udffc https://t.co/qptPgeTKMN"}, "1213107969157713920": {"followers": "3,500", "datetime": "2020-01-03 14:40:52", "author": "@arxiv_cscl", "content_summary": "XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/5gqnArmO0a"}, "1141741047569158145": {"followers": "530", "datetime": "2019-06-20 16:14:12", "author": "@MichaBreakstone", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141771699660214273": {"followers": "90", "datetime": "2019-06-20 18:16:00", "author": "@LeoApolonio", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141762560099663872": {"followers": "356", "datetime": "2019-06-20 17:39:41", "author": "@saruftw", "content_summary": "Weekend read!"}, "1143646744196067328": {"followers": "729", "datetime": "2019-06-25 22:26:46", "author": "@aagahi", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141547592955731968": {"followers": "1,246", "datetime": "2019-06-20 03:25:29", "author": "@JonClarkSeattle", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141612333887889408": {"followers": "150", "datetime": "2019-06-20 07:42:45", "author": "@sejchr", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141990659173146625": {"followers": "22", "datetime": "2019-06-21 08:46:04", "author": "@YungSungChuang", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1145519954965360641": {"followers": "45", "datetime": "2019-07-01 02:30:14", "author": "@andromeda_aaa1", "content_summary": "RT @icoxfog417: BERT\u3092\u8d85\u3048\u305f\u3068\u8a71\u984c\u306b\u306a\u3063\u305fXLNet\u306e\u30b3\u30b9\u30c8\u306b\u3064\u3044\u3066\u306e\u8a71\u3002\u8ad6\u6587\u4e2d\u3067\u306f\"512 TPU v3 chips\"\u30672.5 days\u3068\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u30021TPU\u306f4chips\u3067\u69cb\u6210\u3055\u308c\u308b\u306e\u3067128TPU\u30922.5days=$61,440\u307b\u3069\u306b\u306a\u308b\u3068\u306e\u8a66\u7b97(\u2026"}, "1141900566978654210": {"followers": "323", "datetime": "2019-06-21 02:48:05", "author": "@chase0213", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143655124151230466": {"followers": "1,454", "datetime": "2019-06-25 23:00:04", "author": "@jussikarlgren", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141686684280598528": {"followers": "2", "datetime": "2019-06-20 12:38:11", "author": "@thai_hoang_pham", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1142103073331875841": {"followers": "4,347", "datetime": "2019-06-21 16:12:46", "author": "@CordwainersCat", "content_summary": "RT @smochi_pub: \u65e9\u304f\u3082BERT\u8d85\u3048\u30e2\u30c7\u30eb\u304c\u51fa\u3066\u304d\u305f\u2026\uff01 https://t.co/86pHSazItX"}, "1142268695693144065": {"followers": "1", "datetime": "2019-06-22 03:10:54", "author": "@mimi_zhao", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141838436678217728": {"followers": "14", "datetime": "2019-06-20 22:41:12", "author": "@ArshadShaik29", "content_summary": "Looking forward to implementation in pytorch ;) @huggingface #NLP @PyTorch"}, "1141741049552998408": {"followers": "19", "datetime": "2019-06-20 16:14:13", "author": "@Pramodith7", "content_summary": "Now we have something better than BERT!!!"}, "1142734249818304512": {"followers": "222", "datetime": "2019-06-23 10:00:50", "author": "@PapersTrending", "content_summary": "[1/10] \ud83d\udcc8 - XLNet: Generalized Autoregressive Pretraining for Language Understanding - 2,246 \u2b50 - \ud83d\udcc4 https://t.co/KbJ1JSRqVM - \ud83d\udd17 https://t.co/vLGhKjwqTN"}, "1141534129047183360": {"followers": "98", "datetime": "2019-06-20 02:31:59", "author": "@Delfox29", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143485105421324289": {"followers": "399", "datetime": "2019-06-25 11:44:28", "author": "@djsaunde", "content_summary": "\ud83d\ude10"}, "1141914812407934976": {"followers": "294", "datetime": "2019-06-21 03:44:41", "author": "@rose_miura", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1143508903378006016": {"followers": "223", "datetime": "2019-06-25 13:19:02", "author": "@krishnanpc", "content_summary": "\"AI\" Research. \ud83d\ude48"}, "1143403008493047810": {"followers": "531", "datetime": "2019-06-25 06:18:15", "author": "@vitojph", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143813404035026945": {"followers": "566", "datetime": "2019-06-26 09:29:01", "author": "@dmn001", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141838191508373504": {"followers": "264", "datetime": "2019-06-20 22:40:13", "author": "@dreamware", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141604678221914112": {"followers": "0", "datetime": "2019-06-20 07:12:19", "author": "@thomas80066043", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143485586595971072": {"followers": "8", "datetime": "2019-06-25 11:46:23", "author": "@boo_o1", "content_summary": "RT @mark_riedl: That is 4x the average salary in the US and 9.5x the poverty line. https://t.co/3OpWKfHZ8H"}, "1143414719530844166": {"followers": "72", "datetime": "2019-06-25 07:04:47", "author": "@o_pm_o", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1141860281682993152": {"followers": "3,350", "datetime": "2019-06-21 00:08:00", "author": "@cocoweixu", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141955405880238080": {"followers": "993", "datetime": "2019-06-21 06:25:59", "author": "@taniokah", "content_summary": "RT @icoxfog417: BERT\u306e\u5f31\u70b9\u3092\u4fee\u6b63\u3057\u305fXLNet\u304c\u516c\u958b\u3002BERT\u3067\u306fMask\u7b87\u6240\u3092\u4e88\u6e2c\u3059\u308b\u304c\u3001\"Mask\"\u306f\u901a\u5e38\u767a\u751f\u3057\u306a\u3044\u305f\u3081\u30ce\u30a4\u30ba\u306b\u306a\u308b\u3002\u305d\u3053\u3067\u5358\u8a9e\u306e\u4e88\u6e2c\u6642\u306b\u4f7f\u7528\u3059\u308bContext\u306e\u9806\u5e8f\u3092\u5909\u3048\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3002Self\u3092\u542b\u307e\u306a\u3044Context\u304b\u3089\u4e88\u6e2c\u3059\u308b\u4e00\u65b9\u3001C\u2026"}, "1141544606619901952": {"followers": "485", "datetime": "2019-06-20 03:13:37", "author": "@FabienCampagne", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1141739453020381184": {"followers": "669", "datetime": "2019-06-20 16:07:52", "author": "@arunkumar_bvr", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}, "1143560117570478080": {"followers": "1,323", "datetime": "2019-06-25 16:42:33", "author": "@stephen_oman", "content_summary": "RT @eturner303: Holy crap: It costs $245,000 to train the XLNet model (the one that's beating BERT on NLP tasks..512 TPU v3 chips * 2.5 day\u2026"}, "1142099046334550016": {"followers": "36", "datetime": "2019-06-21 15:56:46", "author": "@Yo_Lucky_Rockz", "content_summary": "RT @quocleix: XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) arxiv: h\u2026"}}}