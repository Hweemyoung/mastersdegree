{"citation_id": "55055144", "tab": "twitter", "twitter": {"1093866422231388160": {"author": "@prototechno", "followers": "4,825", "datetime": "2019-02-08 13:37:32", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093453014013140992": {"author": "@cocchr", "followers": "285", "datetime": "2019-02-07 10:14:48", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093341739367260161": {"author": "@BrundageBot", "followers": "3,891", "datetime": "2019-02-07 02:52:38", "content_summary": "Are All Layers Created Equal?. Chiyuan Zhang, Samy Bengio, and Yoram Singer https://t.co/LQZEbg4yRX"}, "1093407322083192833": {"author": "@deliprao", "followers": "12,982", "datetime": "2019-02-07 07:13:14", "content_summary": "RT @roydanroy: Another paper citing papers that build off Gintare and my work but not citing us. Stingy citations certainly haven't helped.\u2026"}, "1093691124017811456": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-02-08 02:00:58", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093597408896794624": {"author": "@ScriptShade", "followers": "239", "datetime": "2019-02-07 19:48:34", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093866008215904261": {"author": "@yasuokajihei", "followers": "478", "datetime": "2019-02-08 13:35:54", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093776314236948480": {"author": "@ReedRoof", "followers": "28", "datetime": "2019-02-08 07:39:29", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1104872115910565890": {"author": "@oldspiderdude", "followers": "0", "datetime": "2019-03-10 22:30:14", "content_summary": "RT @roydanroy: Another paper citing papers that build off Gintare and my work but not citing us. Stingy citations certainly haven't helped.\u2026"}, "1093808686592655361": {"author": "@mvaldenegro", "followers": "1,066", "datetime": "2019-02-08 09:48:07", "content_summary": "RT @hillbig: The layers of NN have different robustness; the input layer and the first layer of ResBlock after shape change are sensitive t\u2026"}, "1093337196772216832": {"author": "@syinari0123", "followers": "1,610", "datetime": "2019-02-07 02:34:35", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1094068757297737728": {"author": "@vigi99", "followers": "145", "datetime": "2019-02-09 03:01:33", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093804677752270848": {"author": "@hillbig", "followers": "18,280", "datetime": "2019-02-08 09:32:11", "content_summary": "The layers of NN have different robustness; the input layer and the first layer of ResBlock after shape change are sensitive to re-initialize their values, but other layers are not sensitive and also not necessarily close to initial values. https://t.co/"}, "1139955023700516865": {"author": "@heghbalz", "followers": "1,306", "datetime": "2019-06-15 17:57:11", "content_summary": "RT @maithra_raghu: Intriguing invited talk at #DeepPhenomena from Chiyuan Zhang on the effect of resetting different layers: Are all layers\u2026"}, "1093535789642862592": {"author": "@JeanMarcJAzzi", "followers": "365", "datetime": "2019-02-07 15:43:43", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093831431652929536": {"author": "@daisuzu", "followers": "1,244", "datetime": "2019-02-08 11:18:30", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093452793279664128": {"author": "@nsaphra", "followers": "2,898", "datetime": "2019-02-07 10:13:55", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093344719055278080": {"author": "@roydanroy", "followers": "14,923", "datetime": "2019-02-07 03:04:29", "content_summary": "Another paper citing papers that build off Gintare and my work but not citing us. Stingy citations certainly haven't helped. The basic message of this paper is obvious to anyone who's actually obtained a numerically nonvacuous bound."}, "1150499220035555328": {"author": "@vinayprabhu", "followers": "382", "datetime": "2019-07-14 20:16:03", "content_summary": "@Tim_Dettmers @LukeZettlemoyer Curious if the layers in these models will still retain the same critical / robust dichotomy as proposed in 'Are All Layers Created Equal?' : https://t.co/EShM0IOmwh"}, "1093334750654287873": {"author": "@kchonyc", "followers": "24,110", "datetime": "2019-02-07 02:24:52", "content_summary": "\"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization of deep models\""}, "1140252168345260033": {"author": "@Daniel_J_Im", "followers": "309", "datetime": "2019-06-16 13:37:56", "content_summary": "RT @maithra_raghu: Intriguing invited talk at #DeepPhenomena from Chiyuan Zhang on the effect of resetting different layers: Are all layers\u2026"}, "1093353666461855744": {"author": "@irodov_rg", "followers": "220", "datetime": "2019-02-07 03:40:02", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093809501826801664": {"author": "@jaialkdanel", "followers": "1,770", "datetime": "2019-02-08 09:51:21", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093393625319460865": {"author": "@mducoffe", "followers": "108", "datetime": "2019-02-07 06:18:49", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093336761110011905": {"author": "@ethancaballero", "followers": "587", "datetime": "2019-02-07 02:32:51", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093404669962002437": {"author": "@IgorCarron", "followers": "4,893", "datetime": "2019-02-07 07:02:42", "content_summary": "RT @roydanroy: Another paper citing papers that build off Gintare and my work but not citing us. Stingy citations certainly haven't helped.\u2026"}, "1093758307347644416": {"author": "@blissunplugged", "followers": "24", "datetime": "2019-02-08 06:27:56", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093349744267517952": {"author": "@sivia89024", "followers": "2", "datetime": "2019-02-07 03:24:27", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093694466643247110": {"author": "@owltrainlab", "followers": "45", "datetime": "2019-02-08 02:14:15", "content_summary": "Are All Layers Created Equal?. (arXiv:1902.01996v1 [https://t.co/9mpyAAFgFS]) https://t.co/UTmps8SWge"}, "1093575305124552705": {"author": "@tarantulae", "followers": "3,755", "datetime": "2019-02-07 18:20:45", "content_summary": "These two plots from \"Are All Layers Created Equal?\" (https://t.co/XY8ctrt7Cs) are amazing. They clearly show that ResNets are completely different beasts. Some layers from ResNets are robust even in re-randomization scenarios and sensitive layers are dist"}, "1093349757638987776": {"author": "@PJ_Muncaster", "followers": "1,974", "datetime": "2019-02-07 03:24:30", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093803810760216581": {"author": "@omitawo", "followers": "129", "datetime": "2019-02-08 09:28:45", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093866111529934849": {"author": "@yasuokajihei", "followers": "478", "datetime": "2019-02-08 13:36:18", "content_summary": "RT @hillbig: The layers of NN have different robustness; the input layer and the first layer of ResBlock after shape change are sensitive t\u2026"}, "1093805345820925952": {"author": "@morinosuke__p", "followers": "273", "datetime": "2019-02-08 09:34:51", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093840909098074112": {"author": "@toshihiro_yama", "followers": "146", "datetime": "2019-02-08 11:56:09", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1094149198339960835": {"author": "@cghosh_", "followers": "279", "datetime": "2019-02-09 08:21:11", "content_summary": "RT @hillbig: The layers of NN have different robustness; the input layer and the first layer of ResBlock after shape change are sensitive t\u2026"}, "1093325569209573376": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-02-07 01:48:23", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/w7DVSUhFUm Are All Layers Created Equal?. (arXiv:1902.01996v1 [https://t.co/7W25u7yuPe])"}, "1093333608700497920": {"author": "@Miles_Brundage", "followers": "25,873", "datetime": "2019-02-07 02:20:20", "content_summary": "\"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093325659093504000": {"author": "@StatMLPapers", "followers": "9,721", "datetime": "2019-02-07 01:48:44", "content_summary": "Are All Layers Created Equal?. (arXiv:1902.01996v1 [https://t.co/zjV5HgYw5a]) https://t.co/NYgx6GmIPu"}, "1093470830636326913": {"author": "@ayirpelle", "followers": "2,670", "datetime": "2019-02-07 11:25:36", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093379568369655815": {"author": "@MuhammadPuter12", "followers": "180", "datetime": "2019-02-07 05:22:57", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1139946433501491200": {"author": "@maithra_raghu", "followers": "5,629", "datetime": "2019-06-15 17:23:03", "content_summary": "Intriguing invited talk at #DeepPhenomena from Chiyuan Zhang on the effect of resetting different layers: Are all layers created equal? https://t.co/I8kUsk5Afv #ICML2019 @icmlconf https://t.co/ogd3VD7613"}, "1093335296886792192": {"author": "@MelroLeandro", "followers": "57", "datetime": "2019-02-07 02:27:02", "content_summary": "Are All Layers Created Equal?. (arXiv:1902.01996v1 [https://t.co/qMCMMD3AHu]) https://t.co/aHr9hb1qcH"}, "1125677682094637056": {"author": "@TusharJain_007", "followers": "122", "datetime": "2019-05-07 08:24:08", "content_summary": "@alfcnz @jasonyo Yes, found it to be really interesting with surprisingly interesting behaviour from the first & last layers, corroborating \"robust\" behaviour of the final layer with \"Are all Layers created equal?\" paper: https://t.co/CtVkQGROhj."}, "1093554458938826753": {"author": "@CShorten30", "followers": "4,582", "datetime": "2019-02-07 16:57:54", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093336666780192769": {"author": "@tarantulae", "followers": "3,755", "datetime": "2019-02-07 02:32:29", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093342120050638848": {"author": "@EricSchles", "followers": "2,077", "datetime": "2019-02-07 02:54:09", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1094984717714128898": {"author": "@TuXinming", "followers": "161", "datetime": "2019-02-11 15:41:15", "content_summary": "A great paper to understand generalization of deep architectures and the difference between layers!"}, "1093516125080453120": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-02-07 14:25:35", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093865530908241920": {"author": "@SoyMilkBayesian", "followers": "1,371", "datetime": "2019-02-08 13:34:00", "content_summary": "\u3075\u3057\u304e\u3075\u3057\u304e"}, "1093462353591631875": {"author": "@arxivml", "followers": "780", "datetime": "2019-02-07 10:51:55", "content_summary": "\"Are All Layers Created Equal?\", Chiyuan Zhang, Samy Bengio, Yoram Singer https://t.co/J8uBL4NpS1"}, "1093525417091227649": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-02-07 15:02:30", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093430616627003392": {"author": "@iugoaoj", "followers": "368", "datetime": "2019-02-07 08:45:48", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1093802991029604353": {"author": "@hillbig", "followers": "18,280", "datetime": "2019-02-08 09:25:29", "content_summary": "\u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 https://t.co/966NUiZhTS"}, "1093377917151834117": {"author": "@Tsingggg", "followers": "22", "datetime": "2019-02-07 05:16:24", "content_summary": "RT @roydanroy: Another paper citing papers that build off Gintare and my work but not citing us. Stingy citations certainly haven't helped.\u2026"}, "1093626622840918016": {"author": "@sharad24sc", "followers": "52", "datetime": "2019-02-07 21:44:40", "content_summary": "RT @kchonyc: \"Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization\u2026"}, "1093325308797902848": {"author": "@helioRocha_", "followers": "629", "datetime": "2019-02-07 01:47:21", "content_summary": "\"Are All Layers Created Equal?. (arXiv:1902.01996v1 [https://t.co/rnjY1tRrwN])\" #arXiv https://t.co/FL6Ybh5Tzo"}, "1093803271926370304": {"author": "@kuz44ma69", "followers": "36", "datetime": "2019-02-08 09:26:36", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093847100180258818": {"author": "@KSKSKSKS2", "followers": "216", "datetime": "2019-02-08 12:20:46", "content_summary": "RT @hillbig: \u5b66\u7fd2\u5f8c\u306eNN\u306e\u5165\u529b\u5c64\u3084ResNet\u306e\u30b5\u30a4\u30ba\u304c\u5909\u308f\u308b\u6700\u521d\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u305f\u5834\u5408\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5927\u5e45\u306b\u60aa\u5316\u3059\u308b\u304c\u3001\u4ed6\u306e\u5c64\u3092\u521d\u671f\u5024\u306b\u623b\u3057\u3066\u3082\u30c6\u30b9\u30c8\u8aa4\u5dee\u306f\u5c11\u3057\u3057\u304b\u60aa\u5316\u305b\u305a\u3001\u3055\u3089\u306b\u5fc5\u305a\u3057\u3082\u521d\u671f\u5024\u306b\u8fd1\u3044\u308f\u3051\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u3042\u308b\u7a2e\u306e\u5909\u5316\u306b\u5bfe\u3057\u30ed\u30d0\u30b9\u30c8\u3068\u3044\u3048\u308b\u3002 http\u2026"}, "1093339453060534273": {"author": "@deep_rl", "followers": "858", "datetime": "2019-02-07 02:43:33", "content_summary": "Are All Layers Created Equal? - Chiyuan Zhang https://t.co/0HUCCBWnEY"}, "1093474603773685761": {"author": "@PhilCorlett1", "followers": "3,433", "datetime": "2019-02-07 11:40:35", "content_summary": "RT @Miles_Brundage: \"Are All Layers Created Equal?,\" Zhang et al.: https://t.co/wC43mM3P56"}, "1150502492746076160": {"author": "@Tim_Dettmers", "followers": "5,599", "datetime": "2019-07-14 20:29:04", "content_summary": "Thank you for sharing \u2014 I did not know about this work. Looking at the heatmap, it looks almost identical to the distribution that I see with sparse momentum. That would suggest that sparse momentum can identify these critical layers and how they change as"}, "1093587291048685569": {"author": "@patxangas", "followers": "2,227", "datetime": "2019-02-07 19:08:22", "content_summary": "RT @tarantulae: These two plots from \"Are All Layers Created Equal?\" (https://t.co/XY8ctrt7Cs) are amazing. They clearly show that ResNets\u2026"}}, "completed": "1", "queriedAt": "2020-06-03 01:44:52"}