{"citation_id": "60874810", "queriedAt": "2020-05-09 12:36:32", "completed": "0", "twitter": {"1249119807192137731": {"followers": "13", "content_summary": "This unsupervised pretraining is also called contrastive predictive coding (CPC): https://t.co/MYr9ZnU2js Same concept, three different names. Nonetheless, embeddings (as I like to call them) are now being used in reinforcement learning scenarios: https", "author": "@AugustKarlstedt", "datetime": "2020-04-11 23:39:04"}, "1248249880092397569": {"followers": "593", "content_summary": "RT @olivierhenaff: Very happy to share our latest unsupervised representation learning work! In addition to SOTA linear classification, we\u2026", "author": "@soumen_eclectic", "datetime": "2020-04-09 14:02:17"}, "1234269445775855617": {"followers": "163", "content_summary": "RT @avdnoord: Unsupervised pre-training now outperforms supervised learning on ImageNet for any data regime (see figure) and also for trans\u2026", "author": "@ChurchillMic", "datetime": "2020-03-02 00:09:02"}, "1238778267689050113": {"followers": "13", "content_summary": "RT @olivierhenaff: Very happy to share our latest unsupervised representation learning work! In addition to SOTA linear classification, we\u2026", "author": "@EricKuy", "datetime": "2020-03-14 10:45:29"}}, "tab": "twitter"}