{"tab": "twitter", "completed": "1", "twitter": {"1097312355627732993": {"author": "@StatMLPapers", "followers": "9,717", "datetime": "2019-02-18 01:50:27", "content_summary": "Off-Policy Actor-Critic in an Ensemble: Achieving Maximum General Entropy and Effective Environment Exploration in Deep Reinforcement Learning. (arXiv:1902.05551v1 [cs.LG]) https://t.co/pqD1ryd3ck"}, "1097324145975705605": {"author": "@deep_rl", "followers": "858", "datetime": "2019-02-18 02:37:18", "content_summary": "Off-Policy Actor-Critic in an Ensemble: Achieving Maximum General Entropy and Effective Environment Exploration in Deep Reinforcement Learning - Gang Chen https://t.co/6VbrM0v6Ya"}, "1097348139588177921": {"author": "@yapp1e", "followers": "49", "datetime": "2019-02-18 04:12:38", "content_summary": "Off-Policy Actor-Critic in an Ensemble: Achieving Maximum General Entropy and Effective Environment Exploration in Deep Reinforcement Learning. (arXiv:1902.05551v1 [cs.LG]) https://t.co/qXDKmllpG6 We propose a new policy iteration theory as an important e"}, "1097982840296345600": {"author": "@arxiv_in_review", "followers": "1,311", "datetime": "2019-02-19 22:14:43", "content_summary": "#ICML2019 Off-Policy Actor-Critic in an Ensemble: Achieving Maximum General Entropy and Effective Environment Exploration in Deep Reinforcement Learning. (arXiv:1902.05551v1 [cs\\.LG]) https://t.co/tP7i7mI7Nr"}, "1097383159551926272": {"author": "@arxivml", "followers": "780", "datetime": "2019-02-18 06:31:48", "content_summary": "\"Off-Policy Actor-Critic in an Ensemble: Achieving Maximum General Entropy and Effective Environment Exploration in\u2026 https://t.co/RZ9LqOTLUi"}}, "citation_id": "55656356", "queriedAt": "2020-06-03 23:30:52"}