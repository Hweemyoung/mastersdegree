{"queriedAt": "2020-06-03 14:52:39", "completed": "1", "citation_id": "54960770", "tab": "twitter", "twitter": {"1093410328426102788": {"author": "@cathalhoran", "datetime": "2019-02-07 07:25:11", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "514"}, "1093269977254240257": {"author": "@_AntreasAntonio", "datetime": "2019-02-06 22:07:29", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "642"}, "1093529319962361856": {"author": "@hereticreader", "datetime": "2019-02-07 15:18:01", "content_summary": "Parameter-Efficient Transfer Learning for NLP - https://t.co/9D1COPFWPY https://t.co/r10uD8MFLU", "followers": "195"}, "1093326394921168896": {"author": "@mamoruk", "datetime": "2019-02-07 01:51:40", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "8,966"}, "1092707524976001024": {"author": "@arxivml", "datetime": "2019-02-05 08:52:30", "content_summary": "\"Parameter-Efficient Transfer Learning for NLP\", Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone\u2026 https://t.co/MyGenaHRvk", "followers": "793"}, "1094447211205849088": {"author": "@ShivamKotwalia", "datetime": "2019-02-10 04:05:23", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "186"}, "1093453814957514752": {"author": "@__kolesnikov__", "datetime": "2019-02-07 10:17:59", "content_summary": "Impressive demonstration of how to successfully adapt the BERT model for solving custom NLP tasks by tuning a small amount of parameters.", "followers": "527"}, "1130397376521412608": {"author": "@mrjreid", "datetime": "2019-05-20 08:58:30", "content_summary": "https://t.co/08ny1qGRRm", "followers": "31"}, "1093326997021806593": {"author": "@imenurok", "datetime": "2019-02-07 01:54:03", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "2,050"}, "1093586484198703104": {"author": "@JeremiahHarmsen", "datetime": "2019-02-07 19:05:10", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1,090"}, "1093355904882360320": {"author": "@aglooka", "datetime": "2019-02-07 03:48:55", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "323"}, "1093231458335051776": {"author": "@mathpluscode", "datetime": "2019-02-06 19:34:25", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "205"}, "1092626562334093312": {"author": "@sigitpurnomo", "datetime": "2019-02-05 03:30:47", "content_summary": "RT @arxiv_cs_cl: https://t.co/dzzekh0SvI Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) #NLProc", "followers": "2,668"}, "1093249791608336385": {"author": "@ashwinids89", "datetime": "2019-02-06 20:47:16", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "175"}, "1121028018359754753": {"author": "@arinzechukwu_c", "datetime": "2019-04-24 12:28:01", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "39"}, "1093296769448869888": {"author": "@letranger14", "datetime": "2019-02-06 23:53:56", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "329"}, "1093694388562124800": {"author": "@PerthMLGroup", "datetime": "2019-02-08 02:13:56", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "456"}, "1093525568090501120": {"author": "@Montreal_AI", "datetime": "2019-02-07 15:03:06", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "176,918"}, "1093270286114398209": {"author": "@AmineKorchiMD", "datetime": "2019-02-06 22:08:42", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "3,234"}, "1093480015981797376": {"author": "@morioka", "datetime": "2019-02-07 12:02:06", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "823"}, "1093253319517192192": {"author": "@yrikk", "datetime": "2019-02-06 21:01:17", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "229"}, "1093146181105000448": {"author": "@random_forest75", "datetime": "2019-02-06 13:55:33", "content_summary": "RT @ML_NLP: Parameter-Efficient Transfer Learning for NLP https://t.co/y9HVgUtNaE #NLProc", "followers": "64"}, "1094605724674637824": {"author": "@obousquet", "datetime": "2019-02-10 14:35:16", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "1,809"}, "1092606469357359104": {"author": "@arxiv_cs_cl", "datetime": "2019-02-05 02:10:56", "content_summary": "https://t.co/dzzekh0SvI Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) #NLProc", "followers": "4,223"}, "1139679033837273088": {"author": "@arxiv_cscl", "datetime": "2019-06-14 23:40:30", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyCSSpR", "followers": "3,541"}, "1120158781403353094": {"author": "@morioka", "datetime": "2019-04-22 02:53:59", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "823"}, "1093874239030013953": {"author": "@ayirpelle", "datetime": "2019-02-08 14:08:36", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "2,675"}, "1187426656216080392": {"author": "@siddhadev", "datetime": "2019-10-24 17:52:30", "content_summary": "RT @PapersTrending: [4/10] \ud83d\udcc8 - Parameter-Efficient Transfer Learning for NLP - 146 \u2b50 - \ud83d\udcc4 https://t.co/VaZ5SW08i4 - \ud83d\udd17 https://t.co/TSGDS2voJt", "followers": "6"}, "1093283911998849024": {"author": "@danielrock", "datetime": "2019-02-06 23:02:51", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1,346"}, "1120164430656999424": {"author": "@kdexd", "datetime": "2019-04-22 03:16:26", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "692"}, "1093466408816648197": {"author": "@Miles_Brundage", "datetime": "2019-02-07 11:08:02", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "25,873"}, "1093578341741780992": {"author": "@Even_Oldridge", "datetime": "2019-02-07 18:32:49", "content_summary": "Seems like this also has big implications for multi task learning. Very cool paper.", "followers": "1,164"}, "1139528278841155585": {"author": "@arxiv_cscl", "datetime": "2019-06-14 13:41:27", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1093666343939801089": {"author": "@evolvingstuff", "datetime": "2019-02-08 00:22:30", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "2,929"}, "1093257539884646406": {"author": "@MarioLucic_", "datetime": "2019-02-06 21:18:03", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "1,475"}, "1120170381254569984": {"author": "@nguyenvo09", "datetime": "2019-04-22 03:40:05", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "58"}, "1120318275525726210": {"author": "@pragmaticml", "datetime": "2019-04-22 13:27:46", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1,251"}, "1093454929556094976": {"author": "@FrancescoLocat8", "datetime": "2019-02-07 10:22:25", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "829"}, "1092618637540802560": {"author": "@arXiv__ml", "datetime": "2019-02-05 02:59:17", "content_summary": "#arXiv #machinelearning [cs.LG] Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) https://t.co/BMDWLEOF2K Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream", "followers": "1,804"}, "1093545411501912064": {"author": "@irhumshafkat", "datetime": "2019-02-07 16:21:57", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "460"}, "1093506267623182336": {"author": "@esvhd", "datetime": "2019-02-07 13:46:25", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "146"}, "1092637977988063232": {"author": "@msarozz", "datetime": "2019-02-05 04:16:08", "content_summary": "RT @mlmemoirs: #arXiv #machinelearning [cs.LG] Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) https://t.co/5LH\u2026", "followers": "1,880"}, "1093477772880691200": {"author": "@JeremiahHarmsen", "datetime": "2019-02-07 11:53:11", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "1,090"}, "1093370860792991744": {"author": "@Shujian_Liu", "datetime": "2019-02-07 04:48:21", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "297"}, "1120230553914105856": {"author": "@Steinhafenn", "datetime": "2019-04-22 07:39:11", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "242"}, "1120310341890727936": {"author": "@AsaCoopStick", "datetime": "2019-04-22 12:56:14", "content_summary": "Code at https://t.co/5pcAW6rAvE. See also https://t.co/wyMoX3n8Xb, also in ICML, which examines almost the same setting, except they freeze BERT parameters, and we train them jointly across all tasks.", "followers": "273"}, "1092988792514867200": {"author": "@ukyo_ho", "datetime": "2019-02-06 03:30:09", "content_summary": "RT @arxiv_cscl: Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "73"}, "1139935751595069440": {"author": "@arxiv_cscl", "datetime": "2019-06-15 16:40:36", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1120368158450364416": {"author": "@battle8500", "datetime": "2019-04-22 16:45:59", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "65"}, "1093299894889009152": {"author": "@AssistedEvolve", "datetime": "2019-02-07 00:06:22", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "216"}, "1093582028648390656": {"author": "@EdwardDixon3", "datetime": "2019-02-07 18:47:28", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "412"}, "1120378065819963393": {"author": "@EdwardDixon3", "datetime": "2019-04-22 17:25:21", "content_summary": "@seb_ruder Hope we'll get to read your synopsis! Excited about applications for ever-deeper #LanguageModels, all the more so if we can get there cheaper!", "followers": "412"}, "1093622912689549312": {"author": "@chiheb_tr", "datetime": "2019-02-07 21:29:55", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "826"}, "1093404909813317637": {"author": "@ibrahim7rasim", "datetime": "2019-02-07 07:03:39", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1"}, "1093231175764664320": {"author": "@deliprao", "datetime": "2019-02-06 19:33:18", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "13,116"}, "1093629092610666496": {"author": "@deeplearning4j", "datetime": "2019-02-07 21:54:28", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/Ya6DJZqbrK", "followers": "25,311"}, "1093231967779340288": {"author": "@blauigris", "datetime": "2019-02-06 19:36:27", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "243"}, "1092607309824647168": {"author": "@StatMLPapers", "datetime": "2019-02-05 02:14:16", "content_summary": "Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) https://t.co/ZM2ttta9eq", "followers": "9,764"}, "1140116996060827648": {"author": "@arxiv_cscl", "datetime": "2019-06-16 04:40:48", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1093560030761226251": {"author": "@unsorsodicorda", "datetime": "2019-02-07 17:20:03", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "737"}, "1092839826926104576": {"author": "@deliprao", "datetime": "2019-02-05 17:38:13", "content_summary": "Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer learning into production #NLProc https://t.co/cmKPMfwwcD", "followers": "13,116"}, "1093480868969152512": {"author": "@MarioLucic_", "datetime": "2019-02-07 12:05:29", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "1,475"}, "1093417018424848385": {"author": "@kotti_sasikanth", "datetime": "2019-02-07 07:51:46", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "272"}, "1227206868172406786": {"author": "@abhimanyu7c2", "datetime": "2020-02-11 12:24:52", "content_summary": "Thrilled to win my first @kaggle medal, a silver in the Google Q&A Labeling comp! I used a lot of luck, @huggingface 's pretrained RoBERTa-base (modified as https://t.co/yRvv9BZzkg), pre/post-processing, Cyclical LRs (@lnsmith613 h/t @jeremyphoward). T", "followers": "11"}, "1093410811186417664": {"author": "@jongen87", "datetime": "2019-02-07 07:27:06", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "103"}, "1120130033438339072": {"author": "@BADRINATHJAYAK1", "datetime": "2019-04-22 00:59:45", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "15"}, "1093332961330528256": {"author": "@d9sugiha", "datetime": "2019-02-07 02:17:45", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "89"}, "1120278142491381760": {"author": "@neilhoulsby", "datetime": "2019-04-22 10:48:17", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "623"}, "1092864576989945859": {"author": "@drdeanjones", "datetime": "2019-02-05 19:16:34", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "178"}, "1093232119831248897": {"author": "@stealth_hex", "datetime": "2019-02-06 19:37:03", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "17"}, "1187308189810941952": {"author": "@PapersTrending", "datetime": "2019-10-24 10:01:46", "content_summary": "[4/10] \ud83d\udcc8 - Parameter-Efficient Transfer Learning for NLP - 146 \u2b50 - \ud83d\udcc4 https://t.co/VaZ5SW08i4 - \ud83d\udd17 https://t.co/TSGDS2voJt", "followers": "229"}, "1123535571568041984": {"author": "@PerthMLGroup", "datetime": "2019-05-01 10:32:09", "content_summary": "RT @AndrewLBeam: @zacharylipton Are you thinking of adapters? https://t.co/sag3MlwqFI", "followers": "456"}, "1140269080391241728": {"author": "@ElectronNest", "datetime": "2019-06-16 14:45:08", "content_summary": "RT @arxiv_cscl: Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "231"}, "1093231605601132544": {"author": "@jeremyphoward", "datetime": "2019-02-06 19:35:00", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "100,543"}, "1093990557947355136": {"author": "@plusTOMMY", "datetime": "2019-02-08 21:50:49", "content_summary": "RT @__kolesnikov__: Impressive demonstration of how to successfully adapt the BERT model for solving custom NLP tasks by tuning a small amo\u2026", "followers": "106"}, "1093564780865093633": {"author": "@johnnyprothero", "datetime": "2019-02-07 17:38:55", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "198"}, "1093503359200284672": {"author": "@mosko_mule", "datetime": "2019-02-07 13:34:51", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "2,702"}, "1187006897104269318": {"author": "@siddhadev", "datetime": "2019-10-23 14:04:32", "content_summary": "RT @PapersTrending: [9/10] \ud83d\udcc8 - Parameter-Efficient Transfer Learning for NLP - 132 \u2b50 - \ud83d\udcc4 https://t.co/VaZ5SW08i4 - \ud83d\udd17 https://t.co/TSGDS2voJt", "followers": "6"}, "1120416485120331776": {"author": "@AISC_TO", "datetime": "2019-04-22 19:58:01", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/Cr2fhdTUSv \"Adapter modules... add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones.\" Houlsby et al.", "followers": "1,040"}, "1093229567622500358": {"author": "@jmtomczak", "datetime": "2019-02-06 19:26:54", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "2,795"}, "1214063995822493701": {"author": "@trurom", "datetime": "2020-01-06 05:59:47", "content_summary": "@nickwalton00 @zarzuelazen @karpathy @gdb Please tell me, is AI Dungeon still learning from gamers inputs? I came up with idea that you can use Adapters layers (ads) (https://t.co/tYQh2NdOBP) in order fast [re]training. And than one more ads insert in prev", "followers": "39"}, "1093455790357299201": {"author": "@sylvain_gelly", "datetime": "2019-02-07 10:25:50", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "887"}, "1261261445150339074": {"author": "@norihitoishida", "datetime": "2020-05-15 11:45:36", "content_summary": "#xpaperchallenge MobileBERT(Sun+ 2020) https://t.co/1KL19fns30 \u306e\u6a5f\u69cb\u306e\u5de5\u592b\u306f\u3001 Parameter-Efficient Transfer Learning for NLP https://t.co/QKafTUFFRc \u306eAdapter module\u3092\u601d\u3044\u51fa\u3057\u307e\u3059\u306d(\u3053\u308c\u306f\u30a2\u30d5\u30a3\u30f3\u3067\u6b21\u5143\u843d\u3068\u3057\u3066skip-connect\u3055\u305b\u3066\u308b\u3060\u3051\u3067\u3059\u304c)", "followers": "794"}, "1092795339579510787": {"author": "@arxiv_cscl", "datetime": "2019-02-05 14:41:26", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1120244331783360512": {"author": "@theo_matussiere", "datetime": "2019-04-22 08:33:56", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "133"}, "1093747108677484545": {"author": "@veerendra_b", "datetime": "2019-02-08 05:43:26", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "11"}, "1093470114467344386": {"author": "@tejscript", "datetime": "2019-02-07 11:22:45", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "73"}, "1093462323065348097": {"author": "@OlivierBachem", "datetime": "2019-02-07 10:51:48", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "1,557"}, "1093874078648279041": {"author": "@helboukkouri", "datetime": "2019-02-08 14:07:58", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "46"}, "1103762755717230592": {"author": "@iamarocks1", "datetime": "2019-03-07 21:02:02", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "4"}, "1093248131779592192": {"author": "@JeremiahHarmsen", "datetime": "2019-02-06 20:40:40", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "1,090"}, "1095610682211426304": {"author": "@santhoshkolloju", "datetime": "2019-02-13 09:08:36", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "43"}, "1139754539979067392": {"author": "@arxiv_cscl", "datetime": "2019-06-15 04:40:32", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1227420110639337474": {"author": "@santhoshkolloju", "datetime": "2020-02-12 02:32:13", "content_summary": "RT @abhimanyu7c2: Thrilled to win my first @kaggle medal, a silver in the Google Q&A Labeling comp! I used a lot of luck, @huggingface 's p\u2026", "followers": "43"}, "1093782718549237760": {"author": "@EvpokPadding", "datetime": "2019-02-08 08:04:56", "content_summary": "Great findings! I freaked out a bit when I saw where the adapters layers were meant to ve inserted but it actually makes a lot of sense", "followers": "935"}, "1093537153219321863": {"author": "@midasIIITD", "datetime": "2019-02-07 15:49:08", "content_summary": "RT @NilayShri: @midasIIITD @the_dhumketu this seems to be the 'trend' https://t.co/m6YfyTtwfX", "followers": "854"}, "1094034964557877249": {"author": "@arxiv_pop", "datetime": "2019-02-09 00:47:16", "content_summary": "2019/02/02 \u6295\u7a3f 1\u4f4d LG(Machine Learning) Parameter-Efficient Transfer Learning for NLP https://t.co/l5nDo0X8HE 5 Tweets 41 Retweets 114 Favorites", "followers": "715"}, "1093226505784549376": {"author": "@iam_wkr", "datetime": "2019-02-06 19:14:44", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "649"}, "1120395476866617344": {"author": "@thembani_p", "datetime": "2019-04-22 18:34:32", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "193"}, "1227732376148639750": {"author": "@FelipeV14702764", "datetime": "2020-02-12 23:13:03", "content_summary": "RT @abhimanyu7c2: Thrilled to win my first @kaggle medal, a silver in the Google Q&A Labeling comp! I used a lot of luck, @huggingface 's p\u2026", "followers": "4"}, "1123334734912704512": {"author": "@sarahwiegreffe", "datetime": "2019-04-30 21:14:06", "content_summary": "@zacharylipton These two do it for BERT: https://t.co/4teWsISYu3 and https://t.co/th5vuDAasv", "followers": "528"}, "1092636777226166273": {"author": "@mlmemoirs", "datetime": "2019-02-05 04:11:22", "content_summary": "#arXiv #machinelearning [cs.LG] Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) https://t.co/5LHIbumoeZ Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream", "followers": "1,287"}, "1093165388278517760": {"author": "@sussenglish", "datetime": "2019-02-06 15:11:53", "content_summary": "RT @ML_NLP: Parameter-Efficient Transfer Learning for NLP https://t.co/y9HVgUtNaE #NLProc", "followers": "631"}, "1139377148437729281": {"author": "@arxiv_cscl", "datetime": "2019-06-14 03:40:55", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyCSSpR", "followers": "3,541"}, "1120336483104247811": {"author": "@JoaoVictor_AC", "datetime": "2019-04-22 14:40:07", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "740"}, "1121124597984661504": {"author": "@kgwmath", "datetime": "2019-04-24 18:51:48", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "101"}, "1093234517278547968": {"author": "@alanmelling", "datetime": "2019-02-06 19:46:34", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "235"}, "1093791189524987904": {"author": "@mattmcd", "datetime": "2019-02-08 08:38:35", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "613"}, "1093507387892219904": {"author": "@poolio", "datetime": "2019-02-07 13:50:52", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "7,640"}, "1140267948751446018": {"author": "@arxiv_cscl", "datetime": "2019-06-16 14:40:38", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1093253876294152192": {"author": "@derekchen14", "datetime": "2019-02-06 21:03:30", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "249"}, "1092638125728432128": {"author": "@machine_ml", "datetime": "2019-02-05 04:16:44", "content_summary": "RT @mlmemoirs: #arXiv #machinelearning [cs.LG] Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs.LG]) https://t.co/5LH\u2026", "followers": "9,800"}, "1093270151980433408": {"author": "@akshaysharma", "datetime": "2019-02-06 22:08:10", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "423"}, "1093498662607499264": {"author": "@NilayShri", "datetime": "2019-02-07 13:16:12", "content_summary": "@midasIIITD @the_dhumketu this seems to be the 'trend'", "followers": "181"}, "1227260602822905858": {"author": "@huggingface", "datetime": "2020-02-11 15:58:23", "content_summary": "RT @abhimanyu7c2: Thrilled to win my first @kaggle medal, a silver in the Google Q&A Labeling comp! I used a lot of luck, @huggingface 's p\u2026", "followers": "22,306"}, "1144632258428624897": {"author": "@siddhadev", "datetime": "2019-06-28 15:42:51", "content_summary": "https://t.co/QkzM9wNKRI supports now the alsome #GoogleResearch adapter-BERT (Parameter-Efficient Transfer Learning for NLP, #arXiv: https://t.co/K8E3fCabhX) - which is a great alternative to task specific fine-tuning all of #BERT weights. #NLProc #Tenso", "followers": "6"}, "1093277084657451008": {"author": "@Kevin_Duu", "datetime": "2019-02-06 22:35:43", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "35"}, "1120158041205198853": {"author": "@twitoogle", "datetime": "2019-04-22 02:51:03", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "134"}, "1139346953152733185": {"author": "@arxiv_cscl", "datetime": "2019-06-14 01:40:56", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1196498513841967104": {"author": "@pinoystartup", "datetime": "2019-11-18 18:40:50", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/I4oFIbzog9", "followers": "595"}, "1093631143403053057": {"author": "@jastner109", "datetime": "2019-02-07 22:02:37", "content_summary": "RT @andrey_kurenkov: woah \ud83d\ude32 https://t.co/AQ7Qaltgto", "followers": "194"}, "1093301434567733248": {"author": "@treasured_write", "datetime": "2019-02-07 00:12:29", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "101"}, "1093278087012331520": {"author": "@dannyehb", "datetime": "2019-02-06 22:39:42", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "307"}, "1093351436715405312": {"author": "@prajjwal_1", "datetime": "2019-02-07 03:31:10", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "381"}, "1093242030182944769": {"author": "@codekee", "datetime": "2019-02-06 20:16:26", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "595"}, "1123313195379089409": {"author": "@StrongDuality", "datetime": "2019-04-30 19:48:30", "content_summary": "@zacharylipton @furlanel Perhaps https://t.co/TSGeEqp9Rg (or https://t.co/gPLlDSlBhu for vision)? Also, https://t.co/LjiOIZEUiO and https://t.co/4uNx1X9CCk?", "followers": "1,976"}, "1093090391144853505": {"author": "@cdathuraliya", "datetime": "2019-02-06 10:13:52", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "327"}, "1093493138625310720": {"author": "@giffmana", "datetime": "2019-02-07 12:54:15", "content_summary": "Transferring gigantic source models like BERT to many different target tasks. My colleagues in Brain and @GoogleAI in Z\u00fcrich show how to do it in an efficient way.", "followers": "539"}, "1120211357557841920": {"author": "@ialuronico", "datetime": "2019-04-22 06:22:54", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "996"}, "1102799254593056769": {"author": "@sai_prasanna", "datetime": "2019-03-05 05:13:25", "content_summary": "Introducing small trainable bottleneck layers with skip connection (intialized first to make it approx identity function before training) to huge pre trained models like BERT with pre-trained weights fixed yields similar performance to fine-tuning. https:/", "followers": "345"}, "1092845118896435200": {"author": "@diegovogeid", "datetime": "2019-02-05 17:59:15", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "69"}, "1093522292062056448": {"author": "@AssistedEvolve", "datetime": "2019-02-07 14:50:05", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "216"}, "1123343090822078464": {"author": "@AndrewLBeam", "datetime": "2019-04-30 21:47:18", "content_summary": "@zacharylipton Are you thinking of adapters? https://t.co/sag3MlwqFI", "followers": "5,831"}, "1093341659948109825": {"author": "@EricSchles", "datetime": "2019-02-07 02:52:19", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "2,138"}, "1093374806802624517": {"author": "@ArpitJ_", "datetime": "2019-02-07 05:04:02", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "150"}, "1254249552107851776": {"author": "@norihitoishida", "datetime": "2020-04-26 03:22:50", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/QKafTUFFRc Adapter module(AutoEncoder+skip-connection)\u306f\u51c4\u304f\u826f\u3044\u30a2\u30a4\u30c7\u30a2\u3060\u3068\u601d\u3046\u3093\u3060\u3051\u3069\u3001\u5727\u7e2e\u5f8c\u306e\u7a7a\u9593\u3067(batch norm\u306a\u308a)\u4f55\u3089\u304b\u306enorm\u3092\u30d6\u30c1\u8fbc\u307e\u306a\u3044\u7406\u7531\u304c\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u2026 \u3054\u5b58\u77e5\u306e\u65b9\u3044\u305f\u3089\u6559\u3048\u3066\u4e0b\u3055\u3044\ud83d\ude47\u200d\u2642\ufe0f https://t.co/5jR6GwxyUh", "followers": "794"}, "1147817711604514816": {"author": "@cackerman1", "datetime": "2019-07-07 10:40:42", "content_summary": "https://t.co/rUJK6ooZ6r https://t.co/R00bDms3V7 https://t.co/VouEO04JLN https://t.co/I5OrknqTeU transfer learning for NLP https://t.co/nShZW8A9Mf https://t.co/vu6Tuk3JFi https://t.co/ObpDUvEkM8 https://t.co/9a1RK6yhGV https://t.co/0gJb1zdAH4 https://t.co/C", "followers": "5,544"}, "1094595294761758727": {"author": "@terrible_coder", "datetime": "2019-02-10 13:53:49", "content_summary": "This one: https://t.co/8o7FgCSauc", "followers": "432"}, "1140328479654862848": {"author": "@omarsar0", "datetime": "2019-06-16 18:41:10", "content_summary": "paper: https://t.co/IyTOEREJDa (Houlsby et al., 2019)", "followers": "5,513"}, "1093398805100212224": {"author": "@sigitpurnomo", "datetime": "2019-02-07 06:39:24", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "2,668"}, "1139022750117453826": {"author": "@ialuronico", "datetime": "2019-06-13 04:12:40", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "996"}, "1093303133143355392": {"author": "@ayirpelle", "datetime": "2019-02-07 00:19:14", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "2,675"}, "1093467279386439681": {"author": "@LenzBelzner", "datetime": "2019-02-07 11:11:29", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "107"}, "1120204633920684033": {"author": "@GuptaRajat033", "datetime": "2019-04-22 05:56:11", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "260"}, "1262531607300202496": {"author": "@lao_ni", "datetime": "2020-05-18 23:52:46", "content_summary": "How can human learn so fast? Maybe we are always adapting our existing network?", "followers": "59"}, "1093428645396389889": {"author": "@neilhoulsby", "datetime": "2019-02-07 08:37:58", "content_summary": "It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is on arXiv; work @GoogleAI Z\u00fcrich and Kirkland. https://t.co/ud3eIPSan7 #GoogleZurich #GoogleKirkland https://t.co/qcPxBe9pZm", "followers": "623"}, "1123313235522936832": {"author": "@vp834", "datetime": "2019-04-30 19:48:40", "content_summary": "@zacharylipton Is this the one - Parameter-Efficient Transfer Learning for NLP https://t.co/oLgQsp73B9", "followers": "2"}, "1227263602995761157": {"author": "@Bharath09095865", "datetime": "2020-02-11 16:10:19", "content_summary": "RT @abhimanyu7c2: Thrilled to win my first @kaggle medal, a silver in the Google Q&A Labeling comp! I used a lot of luck, @huggingface 's p\u2026", "followers": "16"}, "1139238917662638080": {"author": "@NogaRot", "datetime": "2019-06-13 18:31:38", "content_summary": "Parameter-Efficient Transfer Learning for NLP, presented by Stanislaw Jastrzebski @kudkudakpl . Full paper: https://t.co/VRlTcRVi3j #ICML2019 #NLP https://t.co/Www3W6kYjR", "followers": "310"}, "1093616564350275586": {"author": "@DataScienceMD", "datetime": "2019-02-07 21:04:42", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "2"}, "1120603786609750016": {"author": "@tcs2711", "datetime": "2019-04-23 08:22:17", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "56"}, "1092616298562052096": {"author": "@BrundageBot", "datetime": "2019-02-05 02:50:00", "content_summary": "Parameter-Efficient Transfer Learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly https://t.co/LLZrV5IQyJ", "followers": "3,913"}, "1093934167157628928": {"author": "@RickGalbo", "datetime": "2019-02-08 18:06:44", "content_summary": "in a world where a few is 10^5", "followers": "293"}, "1093462551818653696": {"author": "@m__dehghani", "datetime": "2019-02-07 10:52:42", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "1,649"}, "1123490908362760195": {"author": "@ialuronico", "datetime": "2019-05-01 07:34:40", "content_summary": "RT @sarahwiegreffe: @zacharylipton These two do it for BERT: https://t.co/4teWsISYu3 and https://t.co/th5vuDAasv", "followers": "996"}, "1092860246702338048": {"author": "@roeeaharoni", "datetime": "2019-02-05 18:59:21", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "1,086"}, "1093226212992929792": {"author": "@kudkudakpl", "datetime": "2019-02-06 19:13:34", "content_summary": "Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params per-task is needed. Applications: e.g. multi-task learning and production.", "followers": "820"}, "1093454627507449856": {"author": "@karol_kurach", "datetime": "2019-02-07 10:21:13", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "926"}, "1093355244333854720": {"author": "@su_9qu", "datetime": "2019-02-07 03:46:18", "content_summary": "\" transfer with adapter modules \" \" Tuning with adapter modules \" https://t.co/X75klZH5Wr (BERT_013)(fine-tuning\u90e8\u5206\u306e\u5468\u8fba)", "followers": "110"}, "1093232432495644672": {"author": "@loretoparisi", "datetime": "2019-02-06 19:38:17", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1,074"}, "1093542391678525440": {"author": "@raamnaathn", "datetime": "2019-02-07 16:09:57", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "19"}, "1093147478571012096": {"author": "@machinelearnflx", "datetime": "2019-02-06 14:00:43", "content_summary": "RT @ML_NLP: Parameter-Efficient Transfer Learning for NLP https://t.co/y9HVgUtNaE #NLProc", "followers": "80,883"}, "1093454222790742016": {"author": "@cbaziotis", "datetime": "2019-02-07 10:19:36", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "126"}, "1093551651728674817": {"author": "@Jeeva_G", "datetime": "2019-02-07 16:46:45", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "576"}, "1093456062160822272": {"author": "@v_trokhymenko", "datetime": "2019-02-07 10:26:55", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "68"}, "1094217811096936448": {"author": "@MpatsisCS", "datetime": "2019-02-09 12:53:50", "content_summary": "Cool paper!!!", "followers": "81"}, "1093511811167469569": {"author": "@kmotohas", "datetime": "2019-02-07 14:08:26", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "339"}, "1093799459526459392": {"author": "@Ar_Douillard", "datetime": "2019-02-08 09:11:27", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "285"}, "1120117579077554176": {"author": "@kudkudakpl", "datetime": "2019-04-22 00:10:16", "content_summary": "Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject)", "followers": "820"}, "1093673577201561605": {"author": "@sidbrahma", "datetime": "2019-02-08 00:51:14", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "179"}, "1093642391590973442": {"author": "@vladpaunescu", "datetime": "2019-02-07 22:47:19", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "102"}, "1092626196272160770": {"author": "@arxiv_cs_LG", "datetime": "2019-02-05 03:29:19", "content_summary": "Parameter-Efficient Transfer Learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly https://t.co/PvKdqA4lhQ", "followers": "324"}, "1093851680746000384": {"author": "@Underflow404", "datetime": "2019-02-08 12:38:58", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "268"}, "1093269553885466625": {"author": "@esvhd", "datetime": "2019-02-06 22:05:48", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "146"}, "1093931852728029184": {"author": "@diegovogeid", "datetime": "2019-02-08 17:57:32", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "69"}, "1093557283005628417": {"author": "@giovenko", "datetime": "2019-02-07 17:09:08", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "165"}, "1186945740209295360": {"author": "@PapersTrending", "datetime": "2019-10-23 10:01:31", "content_summary": "[9/10] \ud83d\udcc8 - Parameter-Efficient Transfer Learning for NLP - 132 \u2b50 - \ud83d\udcc4 https://t.co/VaZ5SW08i4 - \ud83d\udd17 https://t.co/TSGDS2voJt", "followers": "229"}, "1093925469181759491": {"author": "@0xhexhex", "datetime": "2019-02-08 17:32:10", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "95"}, "1093290035577790466": {"author": "@battle8500", "datetime": "2019-02-06 23:27:11", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "65"}, "1120512831953285123": {"author": "@Tonoid117", "datetime": "2019-04-23 02:20:51", "content_summary": "RT @AISC_TO: Parameter-Efficient Transfer Learning for NLP https://t.co/Cr2fhdTUSv \"Adapter modules... add only a few trainable parameter\u2026", "followers": "13"}, "1092984028188364802": {"author": "@terrible_coder", "datetime": "2019-02-06 03:11:13", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "432"}, "1123373743936458752": {"author": "@AssistedEvolve", "datetime": "2019-04-30 23:49:06", "content_summary": "RT @vp834: @zacharylipton Is this the one - Parameter-Efficient Transfer Learning for NLP https://t.co/oLgQsp73B9", "followers": "216"}, "1123373714047782912": {"author": "@AssistedEvolve", "datetime": "2019-04-30 23:48:59", "content_summary": "RT @AndrewLBeam: @zacharylipton Are you thinking of adapters? https://t.co/sag3MlwqFI", "followers": "216"}, "1093432542630498304": {"author": "@amir_shehzd", "datetime": "2019-02-07 08:53:27", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "34"}, "1093822007609827328": {"author": "@agesmundo", "datetime": "2019-02-08 10:41:03", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "53"}, "1092882166139236354": {"author": "@oren_data", "datetime": "2019-02-05 20:26:27", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "751"}, "1093485851223285760": {"author": "@__jm", "datetime": "2019-02-07 12:25:17", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "245"}, "1120268315354468352": {"author": "@chajeongwon", "datetime": "2019-04-22 10:09:14", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "257"}, "1093785810359500800": {"author": "@FrancescoLocat8", "datetime": "2019-02-08 08:17:13", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "829"}, "1093443523280556032": {"author": "@elyase", "datetime": "2019-02-07 09:37:05", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "64"}, "1120194950531100672": {"author": "@e0en", "datetime": "2019-04-22 05:17:43", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "213"}, "1093245404651634689": {"author": "@nicholdav", "datetime": "2019-02-06 20:29:50", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1,549"}, "1093239475369594880": {"author": "@aCraigPfeifer", "datetime": "2019-02-06 20:06:16", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "770"}, "1093406246668038145": {"author": "@ionandrou", "datetime": "2019-02-07 07:08:58", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "655"}, "1095953282134990848": {"author": "@jianmo_ni", "datetime": "2019-02-14 07:49:58", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "12"}, "1093538944539594753": {"author": "@GillesMoyse", "datetime": "2019-02-07 15:56:16", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "1,068"}, "1093144721822830592": {"author": "@ML_NLP", "datetime": "2019-02-06 13:49:45", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/y9HVgUtNaE #NLProc", "followers": "51,405"}, "1093554952075689985": {"author": "@pragmaticml", "datetime": "2019-02-07 16:59:52", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "1,251"}, "1120164766792597505": {"author": "@KouroshMeshgi", "datetime": "2019-04-22 03:17:46", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "621"}, "1093242524661997568": {"author": "@RewaSood21", "datetime": "2019-02-06 20:18:24", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "6"}, "1187670600317358081": {"author": "@PapersTrending", "datetime": "2019-10-25 10:01:51", "content_summary": "[5/10] \ud83d\udcc8 - Parameter-Efficient Transfer Learning for NLP - 150 \u2b50 - \ud83d\udcc4 https://t.co/VaZ5SW08i4 - \ud83d\udd17 https://t.co/TSGDS2voJt", "followers": "229"}, "1120190730289459200": {"author": "@dannyehb", "datetime": "2019-04-22 05:00:56", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "307"}, "1093034249060519936": {"author": "@cghosh_", "datetime": "2019-02-06 06:30:47", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "281"}, "1093527066216198146": {"author": "@yuvalmarton", "datetime": "2019-02-07 15:09:03", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "574"}, "1093531228853231616": {"author": "@EricSchles", "datetime": "2019-02-07 15:25:36", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "2,138"}, "1093504519638405125": {"author": "@morioka", "datetime": "2019-02-07 13:39:28", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "823"}, "1093594424045641731": {"author": "@alfcnz", "datetime": "2019-02-07 19:36:43", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "6,525"}, "1093568428193996800": {"author": "@andrey_kurenkov", "datetime": "2019-02-07 17:53:25", "content_summary": "woah \ud83d\ude32", "followers": "3,466"}, "1093508938304184321": {"author": "@jaguring1", "datetime": "2019-02-07 13:57:01", "content_summary": "RT @__kolesnikov__: Impressive demonstration of how to successfully adapt the BERT model for solving custom NLP tasks by tuning a small amo\u2026", "followers": "13,497"}, "1115654999504244736": {"author": "@NeslihanCaniken", "datetime": "2019-04-09 16:37:34", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "999"}, "1093234539336392704": {"author": "@A_K_Nain", "datetime": "2019-02-06 19:46:40", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "3,392"}, "1093597348977086472": {"author": "@Cedias_", "datetime": "2019-02-07 19:48:20", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "340"}, "1093240829777506305": {"author": "@daniel_heres", "datetime": "2019-02-06 20:11:39", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "50"}, "1093805041297829888": {"author": "@indy9000", "datetime": "2019-02-08 09:33:38", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "358"}, "1093699036077412352": {"author": "@PerthMLGroup", "datetime": "2019-02-08 02:32:24", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "456"}, "1093233489766477824": {"author": "@b_cavello", "datetime": "2019-02-06 19:42:29", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "3,045"}, "1093530667709853696": {"author": "@ClementDelangue", "datetime": "2019-02-07 15:23:22", "content_summary": "RT @Thom_Wolf: Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between eac\u2026", "followers": "7,620"}, "1093916255202013184": {"author": "@NusWu", "datetime": "2019-02-08 16:55:33", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "15"}, "1093572964996575232": {"author": "@codekee", "datetime": "2019-02-07 18:11:27", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "595"}, "1123535577398169601": {"author": "@PerthMLGroup", "datetime": "2019-05-01 10:32:10", "content_summary": "RT @vp834: @zacharylipton Is this the one - Parameter-Efficient Transfer Learning for NLP https://t.co/oLgQsp73B9", "followers": "456"}, "1120196098658672641": {"author": "@HomoSapienLCY", "datetime": "2019-04-22 05:22:16", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "79"}, "1093914726705364992": {"author": "@ErmiaBivatan", "datetime": "2019-02-08 16:49:29", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "818"}, "1093557309509390336": {"author": "@giovenko", "datetime": "2019-02-07 17:09:14", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "165"}, "1093499033224663041": {"author": "@jmsl", "datetime": "2019-02-07 13:17:40", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "489"}, "1093761696878022656": {"author": "@giovenko", "datetime": "2019-02-08 06:41:24", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "165"}, "1227288016424775681": {"author": "@joeddav", "datetime": "2020-02-11 17:47:19", "content_summary": "RT @abhimanyu7c2: Thrilled to win my first @kaggle medal, a silver in the Google Q&A Labeling comp! I used a lot of luck, @huggingface 's p\u2026", "followers": "1,778"}, "1120204664392302593": {"author": "@GuptaRajat033", "datetime": "2019-04-22 05:56:19", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "260"}, "1093274844320464897": {"author": "@feedmari", "datetime": "2019-02-06 22:26:49", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "128"}, "1093326104511664128": {"author": "@chris_brockett", "datetime": "2019-02-07 01:50:30", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "1,641"}, "1093367803674066945": {"author": "@puneethmishra", "datetime": "2019-02-07 04:36:12", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "565"}, "1093292889679921152": {"author": "@KSKSKSKS2", "datetime": "2019-02-06 23:38:31", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "216"}, "1093326650631221248": {"author": "@pczzy", "datetime": "2019-02-07 01:52:41", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "46"}, "1093452650069266432": {"author": "@joapuipe", "datetime": "2019-02-07 10:13:21", "content_summary": "RT @neilhoulsby: It turns out that only a few parameters need to be trained to fine-tune huge text transformer models. Our latest paper is\u2026", "followers": "293"}, "1093495857939431427": {"author": "@jastner109", "datetime": "2019-02-07 13:05:03", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "194"}, "1120367800307146753": {"author": "@battle8500", "datetime": "2019-04-22 16:44:33", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "65"}, "1093595835428995072": {"author": "@FlorencePoirel", "datetime": "2019-02-07 19:42:19", "content_summary": "Congratulations! Nice work!", "followers": "2,020"}, "1093786008477417472": {"author": "@Lidinwise", "datetime": "2019-02-08 08:18:00", "content_summary": "I can't belive it's true. But I'm tery out", "followers": "1,348"}, "1120192534133841920": {"author": "@HrSaghir", "datetime": "2019-04-22 05:08:06", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "784"}, "1093189985283371010": {"author": "@0xhimako", "datetime": "2019-02-06 16:49:37", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/eW927y9JKM", "followers": "144"}, "1093610906745884672": {"author": "@etherealkatana", "datetime": "2019-02-07 20:42:13", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "49"}, "1093401744833499136": {"author": "@siddheshrane24", "datetime": "2019-02-07 06:51:05", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "51"}, "1120122922918481920": {"author": "@mosko_mule", "datetime": "2019-04-22 00:31:30", "content_summary": "RT @kudkudakpl: Accepted to #ICML2019! :) (didn't expect this, we got both a strong accept and a strong reject) https://t.co/vdDZaU6Ov2", "followers": "2,702"}, "1093583312399073281": {"author": "@neilhoulsby", "datetime": "2019-02-07 18:52:34", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "623"}, "1093785176927334402": {"author": "@arxiv_in_review", "datetime": "2019-02-08 08:14:42", "content_summary": "#ICML2019 Parameter-Efficient Transfer Learning for NLP. (arXiv:1902.00751v1 [cs\\.LG]) https://t.co/SnLWX7IkZh", "followers": "1,345"}, "1092614119096832000": {"author": "@arxiv_cscl", "datetime": "2019-02-05 02:41:20", "content_summary": "Parameter-Efficient Transfer Learning for NLP https://t.co/T6AeyDatOr", "followers": "3,541"}, "1093406889055014912": {"author": "@KlimZaporojets", "datetime": "2019-02-07 07:11:31", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "26"}, "1093390747737370624": {"author": "@evolvingstuff", "datetime": "2019-02-07 06:07:23", "content_summary": "RT @kudkudakpl: Our paper on fine-tuning BERT is out: https://t.co/k3WaeAu0QR :) We found that for BERT a surprisingly small # of params pe\u2026", "followers": "2,929"}, "1092974611765940224": {"author": "@saikrishnaklu", "datetime": "2019-02-06 02:33:48", "content_summary": "RT @deliprao: Adapter modules for fine-tuning BERT models in a parameter efficient way \ud83d\udc4d ... Stuff like this is important for SOTA transfer\u2026", "followers": "388"}, "1093551076526182401": {"author": "@pragmaticml", "datetime": "2019-02-07 16:44:28", "content_summary": "Excellent work by Holsby, Giurgiu, Jastrzebski, et. al on param. efficient transfer learning. Results nearly equivalent to model finetuning while learning only 4% new params. Awesome to see a totally different take on transfer learning pan out well. http", "followers": "1,251"}, "1093521839496880129": {"author": "@Thom_Wolf", "datetime": "2019-02-07 14:48:17", "content_summary": "Modifying the inside of a pre-trained model for transfer learning: @neilhoulsby and co insert \"adapter modules\" between each module of a pre-trained model (e.g. BERT) with: - a small number of parameters, - a near-identity initialization. Nice parameter/", "followers": "20,061"}}}