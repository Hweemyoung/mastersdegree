[{"idx_paper": 24, "q": "arxiv.org/abs/1901.00516", "items": [{"id": "qmMUEsWEXN8", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "77", "dislikeCount": "0"}, "snippet": {"channelTitle": "Simon C", "title": "Intel's Distributed DL Platform and Inside-Out Tracking Camera, ML vs Honey Frauds | AI Radar #4", "tags": ["AI", "ML", "Machine Learning", "Deep Learning", "Robots", "Drones", "automation", "tracking", "food frauds"], "channelId": "UC6Us86VfsJkdHQkXzaWl5gg", "publishedAt": "2019-01-30T09:58:55Z", "defaultAudioLanguage": "en", "description": "AI Radar #4 \n\n1) Intel's Distributed Deep Learning Platform for Kubernetes, NAUTA \n- https://venturebeat.com/2019/01/23/intel-debuts-nauta-for-distributed-deep-learning-with-kubernetes/\n- https://www.intel.ai/introducing-nauta/#gs.wurgeSvA\n- code repo - https://github.com/intelAI/Nauta\n\n2) Intel's stand-alone inside-out tracking camera T265\n- https://siliconangle.com/2019/01/23/intel-debuts-ai-powered-camera-system-robots-ar-applications/\n- https://newsroom.intel.com/news/intel-realsense-stand-alone-inside-out-tracking-camera/#gs.HZ5NSLsi\n\n3) Fighting honey frauds with ML \n- https://www.fastcompany.com/90294831/machine-learning-is-a-sweet-way-to-tell-if-your-honey-is-fake\n\n\u201cHoney Authentication with Machine Learning Augmented Bright-Field Microscopy.\u201d - https://arxiv.org/abs/1901.00516\n\n\"Development and Application of a Database of Food Ingredient Fraud and Economically Motivated Adulteration from 1980 to 2010\" - https://onlinelibrary.wiley.com/doi/full/10.1111/j.1750-3841.2012.02657.x"}, "contentDetails": {"duration": "PT6M"}}]}, {"idx_paper": 31, "q": "arxiv.org/abs/1901.00596", "items": [{"id": "vaTnR08nVv8", "statistics": {"likeCount": "43", "favoriteCount": "0", "commentCount": "7", "viewCount": "3268", "dislikeCount": "4"}, "snippet": {"channelTitle": "AI Tech", "title": "[Meetup] Introduction to GNN - \u675c\u5cb3\u83ef", "channelId": "UCEOqmzElFcyjK-JE21ifJGQ", "publishedAt": "2019-04-28T15:50:40Z", "defaultAudioLanguage": "zh-TW", "description": "\u65e5\u671f\uff1a2019/04/28\n\n\u6458\u8981\uff1a\n\u5f88\u591a\u7814\u7a76\u8cc7\u6599\u6709\u5176\u5167\u5728\u7684\u7d50\u69cb\uff0c\u800c\u5f80\u5f80\u662f\u67b6\u69cb\u5728\u975e\u6b50\u6c0f\u7a7a\u9593\u3002\u4f8b\u5982\u793e\u6703\u79d1\u5b78\u4e2d\u7684\u4eba\u969b\u7db2\u8def\u5206\u6790\uff0c\u751f\u7269\u8cc7\u8a0a\u7576\u4e2d\u7684\u57fa\u56e0\u8868\u73fe\u7db2\u8def\uff0c\u6216\u662f\u96fb\u8166\u5716\u5b78\u7576\u4e2d\u7684\u8868\u9762\u7db2\u683c\u3002\u5716\uff08graph\uff09\u53ef\u4ee5\u4f5c\u70ba\u4e00\u500b\u904b\u7b97\u5143\u88ab\u795e\u7d93\u7db2\u8def\u8655\u7406\uff0c\u800c\u5c07\u8cc7\u6599\u4e2d\u7684\u5e7e\u4f55\u7d50\u69cb\u5b9a\u7fa9\u65bc\u908a\u7684\u5c6c\u6027\u4e0a\u3002\u5716\u795e\u7d93\u7db2\u8def\uff08graph neural network, GNN\uff09\u662f\u795e\u7d93\u7db2\u8def\u6a21\u578b\u7684\u5b50\u96c6\u3002\u8fd1\u5e74\u4f86\uff0c\u8d8a\u4f86\u8d8a\u591a\u5b78\u8005\u5728\u9019\u500b\u9818\u57df\u4e0a\u505a\u51fa\u8ca2\u737b\uff0c\u4e5f\u5728 Arxiv \u4e0a\u7d2f\u7a4d\u4e86\u6578\u7bc7\u7684\u56de\u9867\u6587\u737b\u3002\u672c\u6b21\u7684\u6f14\u8b1b\u5c07\u6703\u8ddf\u5927\u5bb6\u4ecb\u7d39\u4ec0\u9ebc\u662f\u5716\u795e\u7d93\u7db2\u8def\uff0c\u4e26\u4e14\u5e36\u51fa\u5716\u795e\u7d93\u7db2\u8def\u7684\u7a2e\u985e\u3002\u672c\u6b21\u7684\u4ecb\u7d39\u5c07\u6703\u878d\u5408\u6578\u7bc7\u56de\u9867\u6587\u737b\u7684\u5167\u5bb9\uff0c\u7531\u65bc\u5167\u5bb9\u975e\u5e38\u7684\u591a\u800c\u7e41\u96dc\uff0c\u672c\u6b21\u7684\u4ecb\u7d39\u6703\u4ee5\u5927\u67b6\u69cb\u8ddf\u65b9\u5411\u70ba\u4e3b\uff0c\u6b61\u8fce\u8207\u6703\u4f86\u8cd3\u4e00\u8d77\u8a0e\u8ad6\u7d30\u7bc0\u3002\n\n\u7c21\u5831\uff1ahttps://yuehhua.github.io/slides/graph-theory/\nhttps://yuehhua.github.io/slides/intro-to-gnn/\n\n\u76f8\u95dc\u8ad6\u6587\uff1a\nA Comprehensive Survey on Graph Neural Networks\uff1ahttps://arxiv.org/abs/1901.00596\nRelational inductive biases, deep learning, and graph networks\nGeometric deep learning: going beyond Euclidean data\nThe Graph Neural Network Model\nVariational Graph Auto-Encoders\nNeural Message Passing for Quantum Chemistry\nDIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING\nGRAPH ATTENTION NETWORKS\nMolGAN: An implicit generative model for small molecular graphs", "defaultLanguage": "zh-TW"}, "contentDetails": {"duration": "PT1H27M8S"}}]}, {"idx_paper": 57, "q": "arxiv.org/abs/1901.01365", "items": [{"id": "y7lxGo3wHZk", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "58", "dislikeCount": "1"}, "snippet": {"channelTitle": "Takayuki Osa", "publishedAt": "2019-03-06T04:30:41Z", "title": "Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization", "description": "A policy learned on Walker2D task with four options.\n\nHierarchical Reinforcement Learning via Advantage-Weighted Information Maximization\nT. Osa, V. Tangkaratt, M. Sugiyama, ICLR 2019\n\nDetails can be found in https://arxiv.org/abs/1901.01365", "channelId": "UCzZ38mRPXlWF-9PY9jtFSsw"}, "contentDetails": {"duration": "PT9S"}}]}, {"idx_paper": 74, "q": "arxiv.org/abs/1901.01751", "items": [{"id": "mtQJwooVuvA", "statistics": {"likeCount": "9", "favoriteCount": "0", "commentCount": "1", "viewCount": "422", "dislikeCount": "0"}, "snippet": {"channelTitle": "UCL Financial Computing", "title": "Financial Computing Seminar / Adriano Koshiyama - cGANs for Trading Strategies", "tags": ["machinelearning", "finance", "trading", "networks"], "channelId": "UCAQMZUQYgeQ-s_Cal_O_WNw", "publishedAt": "2019-03-13T14:41:46Z", "description": "Title:\nConditional Generative Adversarial Networks for Trading Strategies\n\nLink:\nhttps://arxiv.org/abs/1901.01751\n\nAbstract: \nSystematic trading strategies are algorithmic procedures that allocate assets aiming to optimize a certain performance criterion. To obtain an edge in a highly competitive environment, the analyst needs to proper fine-tune its strategy, or discover how to combine weak signals in novel alpha creating manners. Both aspects, namely fine-tuning and combination, have been extensively researched using several methods, but emerging techniques such as Generative Adversarial Networks can have an impact into such aspects. Therefore, our work proposes the use of Conditional Generative Adversarial Networks (cGANs) for trading strategies calibration and aggregation. To this purpose, we provide a full methodology on: (i) the training and selection of a cGAN for time series data; (ii) how each sample is used for strategies calibration; and (iii) how all generated samples can be used for ensemble modelling. To provide evidence that our approach is well grounded, we have designed an experiment with multiple trading strategies, encompassing 579 assets. We compared cGAN with an ensemble scheme and model validation methods, both suited for time series. Our results suggest that cGANs are a suitable alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha"}, "contentDetails": {"duration": "PT44M48S"}}]}, {"idx_paper": 94, "q": "arxiv.org/abs/1901.02161", "items": [{"id": "yYFae_Obp-0", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "84", "dislikeCount": "0"}, "snippet": {"channelTitle": "Daniel Brown", "publishedAt": "2019-01-08T05:37:48Z", "title": "Risk-Aware Active Inverse Reinforcement Learning", "description": "Supplementary video for \"Risk-Aware Active Inverse Reinforcement Learning\" by Daniel S. Brown, Yuchen Cui, and Scott Niekum. Published in Conference on Robot Learning 2018 (https://arxiv.org/abs/1901.02161). This work seeks to allow a robot to ask for demonstrations in a way that actively reduces its risk of violating the demonstrators intention.", "channelId": "UCxNESajoQCX_36rrobL1ECA"}, "contentDetails": {"duration": "PT4M56S"}}]}, {"idx_paper": 126, "q": "arxiv.org/abs/1901.02860", "items": [{"id": "lSTljZy8ag4", "statistics": {"likeCount": "29", "favoriteCount": "0", "commentCount": "3", "viewCount": "1458", "dislikeCount": "0"}, "snippet": {"channelTitle": "\ubc15\uc131\ub0a8", "publishedAt": "2019-05-13T10:43:40Z", "title": "PR-161: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "description": "Paper: https://arxiv.org/abs/1901.02860", "channelId": "UCWDE4MKQk8CsRy2vM0S0dOg"}, "contentDetails": {"duration": "PT20M9S"}}]}, {"idx_paper": 127, "q": "arxiv.org/abs/1901.02878", "items": [{"id": "yP4MeQCSeiI", "statistics": {"likeCount": "3", "favoriteCount": "0", "commentCount": "0", "viewCount": "44", "dislikeCount": "0"}, "snippet": {"channelTitle": "platform ai", "publishedAt": "2019-03-16T19:16:31Z", "title": "A Constructive Approach for One-Shot Training: Hypercube-Based Topological Coverings", "description": "https://arxiv.org/abs/1901.02878v1", "channelId": "UCErD53gCA7PJdCTQQTIDglg"}, "contentDetails": {"duration": "PT45M58S"}}]}, {"idx_paper": 150, "q": "arxiv.org/abs/1901.03887", "items": [{"id": "P9XWdpmsEy8", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "54", "dislikeCount": "0"}, "snippet": {"channelTitle": "Data Science Group WMG", "title": "Memory-driven MADDPG (MD-MADDPG)", "tags": ["Reinforcement Learning", "Multi-agent", "Communication", "AI"], "channelId": "UCkDyucGZSYrSbBefntPZVHg", "publishedAt": "2019-10-25T10:31:12Z", "description": "A demonstration of MD-MADDPG on four different scenarios.\n\nRelated article:\n\"Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement Learning through Memory-driven Communication\"\n(https://arxiv.org/abs/1901.03887)\n\nAbstract:\nDeep reinforcement learning algorithms have recently been used to train multiple interacting agents in a centralised manner whilst keeping their execution decentralised. When the agents can only acquire partial observations and are faced with tasks requiring coordination and synchronisation skills, inter-agent communication plays an essential role. In this work, we propose a framework for multi-agent training using deep deterministic policy gradients that enables concurrent, end-to-end learning of an explicit communication protocol through a memory device. During training, the agents learn to perform read and write operations enabling them to infer a shared representation of the world. We empirically demonstrate that concurrent learning of the communication device and individual policies can improve inter-agent coordination and performance in small-scale systems. We illustrate how different communication patterns can emerge on six different tasks of increasing complexity. Furthermore, we study the effects of corrupting the communication channel, provide a visualisation of the time-varying memory content as the underlying task is being solved and validate the building blocks of the proposed memory device through ablation studies."}, "contentDetails": {"duration": "PT46S"}}]}, {"idx_paper": 287, "q": "arxiv.org/abs/1901.08159", "items": [{"id": "NZouarYfLms", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "51", "dislikeCount": "0"}, "snippet": {"channelTitle": "Amr Sharaf", "publishedAt": "2019-12-14T04:45:12Z", "title": "Meta-Learning Contextual Bandit Exploration", "description": "My spotlight talk for the meta-learning workshop at NeurIPS 2019. More details are in the paper: https://arxiv.org/abs/1901.08159", "channelId": "UCi7H4hjPLJDn2Qpbnf44ykA"}, "contentDetails": {"duration": "PT3M33S"}}]}, {"idx_paper": 320, "q": "arxiv.org/abs/1901.08544", "items": [{"id": "XsejJGhgKO4", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "56", "dislikeCount": "0"}, "snippet": {"channelTitle": "CMU Theory", "publishedAt": "2020-02-13T00:56:15Z", "title": "Ilya Razenshteyn on Random space partitions and optimal transport: beyond metric embeddings", "description": "CMU Theory lunch talk from October 16, 2019 by Ilya Razenshteyn on Random space partitions and optimal transport: beyond metric embeddings.\n\n\nI will talk about two new results (both joint with Yihe Dong, Piotr Indyk and Tal Wagner):\n\n* A new algorithm that finds nearest-neighbor-friendly partitions of a high-dimensional space using graph partitioning coupled with supervised learning (e.g., neural networks) https://arxiv.org/abs/1901.08544\n\n* A new algorithm for optimal transport based on tree approximations https://arxiv.org/abs/1910.04126\n\n\n\nThe unifying theme is going beyond low-distortion embeddings into simple metric spaces.", "channelId": "UCWFp4UWNiOv71j0sPbdNiqw"}, "contentDetails": {"duration": "PT58M6S"}}]}, {"idx_paper": 416, "q": "arxiv.org/abs/1901.09491", "items": [{"id": "NYNrF1scwt4", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "12", "dislikeCount": "0"}, "snippet": {"channelTitle": "Data Science at Home", "title": "What if I train a neural network with random data? (with Stanis\u0142aw Jastrz\u0119bski) (Ep. 87)", "tags": ["podbean", "Artificial Intelligence", "Deep Learning", "optimisation..."], "channelId": "UCS3-Fuwaty1mc8MH6vJTVTw", "publishedAt": "2019-11-12T06:38:27Z", "description": "Source:\nhttps://www.podbean.com/eau/pb-rukqb-c6b5f6\n\nWhat happens to a neural network trained with random data? \nAre massive neural networks just lookup tables or do they truly learn something?\u00a0\nToday\u2019s episode will be about memorisation and generalisation in deep learning, with Stanislaw Jastrz\u0119bski from New York University.\nStan spent two summers as a visiting student with Prof. Yoshua Bengio\u00a0and has been working on\u00a0\nUnderstanding and improving how deep network generalise\nRepresentation Learning\nNatural Language Processing\nComputer Aided Drug Design\n\u00a0\nWhat makes deep learning unique?\nI have asked him a few questions for which I was looking for an answer for a long time. For instance, what is deep learning bringing to the table that other methods don\u2019t or are not capable of?\u00a0Stan believe that the one thing that makes deep learning special is representation learning. All the other competing methods, be it kernel machines, or random forests, do not have this capability. Moreover, optimisation (SGD) lies at the heart of representation learning in the sense that it allows finding good representations.\u00a0\n\u00a0\nWhat really improves the training quality of a neural network?\nWe discussed about the accuracy of neural networks depending pretty much on how good the Stochastic Gradient Descent method is at finding minima of the loss function. What would influence such minima?Stan's answer has revealed that training set accuracy or loss value is not that interesting actually. It is relatively easy to overfit data (i.e. achieve the lowest loss possible), provided a large enough network, and a large enough computational budget. However, shape of the minima, or performance on validation sets are in a quite fascinating way influenced by optimisation. Optimisation in the beginning of the trajectory, steers such trajectory towards minima of certain properties that go much further than just training accuracy.\nAs always we spoke about the future of AI and the role deep learning will play.\nI hope you enjoy the show!\nDon't forget to join the conversation on our new Discord channel. See you there!\n\u00a0\nReferences\n\u00a0\nHomepage of\u00a0Stanis\u0142aw Jastrz\u0119bski\u00a0https://kudkudak.github.io/\nA Closer Look at Memorization in Deep Networks\u00a0https://arxiv.org/abs/1706.05394\nThree Factors Influencing Minima in SGD\u00a0https://arxiv.org/abs/1711.04623\nDon't Decay the Learning Rate, Increase the Batch Size\u00a0https://arxiv.org/abs/1711.00489\nStiffness: A New Perspective on Generalization in Neural Networks\u00a0https://arxiv.org/abs/1901.09491"}, "contentDetails": {"duration": "PT19M40S"}}]}, {"idx_paper": 456, "q": "arxiv.org/abs/1901.10002", "items": [{"id": "IRWaE1u9mDM", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "12", "dislikeCount": "0"}, "snippet": {"channelTitle": "Machine Learning and AI Meetup", "title": "\"A Framework for Understanding Unintended Consequences of Machine Learning\" Laura Summers", "tags": ["mlai", "machine learning", "ai", "meetup", "melbourne", "deep learning", "papers", "research", "ethics", "ethical ai", "unintended consequences"], "channelId": "UCBbqjp6ndv4TAZVS44avl6w", "publishedAt": "2020-05-06T22:38:01Z", "description": "For our February 2020 Meetup we had a series of talks on papers covered in local reading groups. We had four presenters sharing their synopsis and review on a paper they've read and liked recently.\n\nThis video features \n\"A Framework for Understanding Unintended Consequences of Machine Learning\"\n- Covered by Laura Summers Link to paper -  https://arxiv.org/abs/1901.10002\n\nCheck out the other talks here\n\"FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\"\n- Covered by Adel Foda\nhttps://youtu.be/gSMI5wZHe9w \n\n\"Hindsight Experience Replay\"\n- Covered by Kristian Holsheimer\nhttps://youtu.be/9Nl49LLV62w\n\n\"Causal Induction from Visual Observations for Goal Directed Tasks\"\n- Covered by Lizzie Silver\nhttps://youtu.be/cDXaAq0noPs\n\nFor more Melbourne MLAI goodness head to http://mlai.melbourne/"}, "contentDetails": {"duration": "PT22M26S"}}]}, {"idx_paper": 524, "q": "arxiv.org/abs/1901.10946", "items": [{"id": "eoiK42w02w0", "statistics": {"likeCount": "0", "favoriteCount": "0", "viewCount": "94", "dislikeCount": "0"}, "snippet": {"channelTitle": "Yukai Liu", "publishedAt": "2019-10-27T07:03:07Z", "title": "Demo: NAOMI: Non-Autoregressive Multiresolution Sequence Imputation", "description": "Video to accompany paper titled \"NAOMI: Non-Autoregressive Multiresolution Sequence Imputation\", https://arxiv.org/abs/1901.10946", "channelId": "UC4KvLPEW6t1tWV8Q_5Tjo-w"}, "contentDetails": {"duration": "PT54S"}}]}, {"idx_paper": 527, "q": "arxiv.org/abs/1901.10995", "items": [{"id": "EbFosdOi5SY", "statistics": {"likeCount": "57", "favoriteCount": "0", "commentCount": "4", "viewCount": "1057", "dislikeCount": "0"}, "snippet": {"channelTitle": "Yannic Kilcher", "title": "Go-Explore: a New Approach for Hard-Exploration Problems", "tags": ["machine learning", "ml", "reinforcement learning", "rl", "ai", "artificial intelligence", "uber", "exploration", "hard exploration", "research", "novelty", "graph", "robustify", "explore", "montezuma", "montezuma's revenge", "pitfall", "atari"], "channelId": "UCZHmQk67mSJgfCCTn7xBfew", "publishedAt": "2020-01-10T12:36:40Z", "defaultAudioLanguage": "en-US", "description": "This algorithm solves the hardest games in the Atari suite and makes it look so easy! This modern version of Dijkstra's shortest path algorithm is outperforming everything else by orders of magnitude, and all based on random exploration.\n\nhttps://arxiv.org/abs/1901.10995\nhttps://eng.uber.com/go-explore/\nhttps://github.com/uber-research/go-explore\n\nAbstract:\nA grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of \"superhuman\" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).\n\nAuthors: Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune", "defaultLanguage": "en"}, "contentDetails": {"duration": "PT19M3S"}}]}, {"idx_paper": 531, "q": "arxiv.org/abs/1901.11117", "items": [{"id": "Axo0EtMUK90", "statistics": {"likeCount": "74", "favoriteCount": "0", "commentCount": "11", "viewCount": "1232", "dislikeCount": "0"}, "snippet": {"channelTitle": "Henry AI Labs", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw", "publishedAt": "2020-04-23T19:13:27Z", "defaultAudioLanguage": "en-US", "description": "This video explores the T5 large-scale study on Transfer Learning. This paper takes apart many different factors of the Pre-Training then Fine-Tuning pipeline for NLP. This involves Auto-Regressive Language Modeling vs. BERT-Style Masked Language Modeling and XLNet-style shuffling, as well as the impact of dataset composition, size, and how to best use more computation. Thanks for watching and please check out Machine Learning Street Talk where Tim Scarfe, Yannic Kilcher and I discuss this paper!\n\nMachine Learning Street Talk: https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ\n\nPaper Links:\nT5: https://arxiv.org/abs/1910.10683\nGoogle AI Blog Post on T5: https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\nTrain Large, Then Compress: https://arxiv.org/pdf/2002.11794.pdf\nScaling Laws for Neural Language Models: https://arxiv.org/pdf/2001.08361.pdf\nThe Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\nELECTRA: https://arxiv.org/pdf/2003.10555.pdf\nTransformer-XL: https://arxiv.org/pdf/1901.02860.pdf\nReformer: The Efficient Transformer: https://openreview.net/pdf?id=rkgNKkHtvB\nThe Evolved Transformer: https://arxiv.org/pdf/1901.11117.pdf\nDistilBERT: https://arxiv.org/pdf/1910.01108.pdf\nHow to generate text (HIGHLY RECOMMEND): https://huggingface.co/blog/how-to-generate\nTokenizers: https://blog.floydhub.com/tokenization-nlp/\n\nThanks for watching! Please Subscribe!"}, "contentDetails": {"duration": "PT23M43S"}}]}, {"idx_paper": 583, "q": "arxiv.org/abs/1902.00040", "items": [{"id": "BPRlQzHEGk4", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "96", "dislikeCount": "0"}, "snippet": {"channelTitle": "University of Malta", "title": "Research collaboration between Ubisoft and the Institute of Digital Games", "tags": ["University of Malta", "Digital Games", "Games", "Institute", "Research", "Projects", "Players", "2019", "data", "motivation"], "channelId": "UCpHDpzlHMmuDq4Dbz7U8VrQ", "publishedAt": "2019-08-16T08:47:50Z", "description": "\"Unlocking Player Motivation\"\n\nIs it possible to predict the motivation of players just by observing their gameplay data? Even if so, how should we measure motivation in the first place? To address the above questions, on the one end, we collect a large dataset of gameplay data from players of the popular game Tom Clancy's The Division. \n\nOn the other end, we ask them to report their levels of competence, autonomy, relatedness and presence using the Ubisoft Perceived Experience Questionnaire. After processing the survey responses in an ordinal fashion we employ preference learning methods based on support vector machines to infer the mapping between gameplay and the reported four motivation factors. Our key findings suggest that gameplay features are strong predictors of player motivation as the best obtained models reach accuracies of near certainty, from 92% up to 94% on unseen players. \n\nSee the full version accepted for IEEE Conference on Games, 2019 here: https://arxiv.org/abs/1902.00040 - Your Gameplay Says It All: Modelling Motivation in Tom Clancy's The Division - David Melhart, Ahmad Azadvar, Alessandro Canossa, Antonios Liapis, Georgios N. Yannakakis"}, "contentDetails": {"duration": "PT3M4S"}}]}, {"idx_paper": 648, "q": "arxiv.org/abs/1902.00506", "items": [{"id": "cD-eXjf854Q", "statistics": {"likeCount": "1886", "favoriteCount": "0", "commentCount": "146", "viewCount": "56671", "dislikeCount": "26"}, "snippet": {"channelTitle": "Two Minute Papers", "title": "DeepMind: The Hanabi Card Game Is the Next Frontier for AI Research", "tags": ["two minute papers", "deep learning", "ai", "deepmind hanabi", "hanabi ai", "hanabi challenge", "deepmind card game", "card game ai"], "channelId": "UCbfYPyITQ-7l4upoX8nvctg", "publishedAt": "2019-03-19T16:51:19Z", "defaultAudioLanguage": "en", "description": "\ud83d\udcdd The paper \"The Hanabi Challenge: A New Frontier for AI Research\" and a blog post is available here:\nhttps://arxiv.org/abs/1902.00506\nhttp://www.marcgbellemare.info/blog/a-cooperative-benchmark-announcing-the-hanabi-learning-environment/\n\n\u2764\ufe0f Pick up cool perks on our Patreon page: https://www.patreon.com/TwoMinutePapers\n\n\ud83d\ude4f We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\n313V, Alex Haro, Andrew Melnychuk, Angelos Evripiotis, Anthony Vdovitchenko, Brian Gilman, Christian Ahlin, Christoph Jadanowski, Claudio Fernandes, Dennis Abts, Eric Haddad, Eric Martel, Evan Breznyik, Geronimo Moralez, Javier Bustamante, John De Witt, Kaiesh Vohra, Kasia Hayden, Kjartan Olason, Levente Szabo, Lorin Atzberger, Marcin Dukaczewski, Marten Rauschenberg, Maurits van Mastrigt, Michael Albrecht, Michael Jensen, Morten Punnerud Engelstad, Nader Shakerin, Owen Campbell-Moore, Owen Skarpness, Raul Ara\u00fajo da Silva, Richard Reis, Rob Rowe, Robin Graham, Ryan Monsurate, Shawn Azman, Steef, Steve Messina, Sunil Kim, Thomas Krcmar, Torsten Reil, Zach Boldyga, Zach Doty.\nhttps://www.patreon.com/TwoMinutePapers\n\nThumbnail background image credit: https://pixabay.com/images/id-591631/\nThe Hanabi card game images are owned by the published and one image by Drentsoft Media - https://www.youtube.com/watch?v=Ofzg71qHh8k\nPoker image: https://commons.wikimedia.org/wiki/File:Two_poker_cards_and_poker_chips_20170611.jpg\nSplash screen/thumbnail design: Fel\u00edcia Feh\u00e9r - http://felicia.hu\n\nK\u00e1roly Zsolnai-Feh\u00e9r's links:\nFacebook: https://www.facebook.com/TwoMinutePapers/\nTwitter: https://twitter.com/karoly_zsolnai\nWeb: https://cg.tuwien.ac.at/~zsolnai/\n\n#DeepMind #Hanabi #AI"}, "contentDetails": {"duration": "PT3M55S"}}]}, {"idx_paper": 907, "q": "arxiv.org/abs/1902.02476", "items": [{"id": "5WOj_ZZJ2wM", "statistics": {"likeCount": "49", "favoriteCount": "0", "commentCount": "2", "viewCount": "1916", "dislikeCount": "0"}, "snippet": {"channelTitle": "Timur Garipov", "title": "[NeurIPS 2019] A Simple Baseline for Bayesian Uncertainty in Deep Learning", "channelId": "UCtl3dZUPOsZrXJFT1ABMUxQ", "publishedAt": "2019-12-03T08:12:57Z", "defaultAudioLanguage": "en", "description": "This short video summarizes our NeurIPS'19 paper \"A Simple Baseline for Bayesian Uncertainty in Deep Learning\" (https://arxiv.org/abs/1902.02476)\n\nPyTorch Code:\nhttps://github.com/wjmaddox/swa_gaussian\n\nVisualizations shown in this video were created in collaboration with Javier Ideami \nhttps://losslandscape.com"}, "contentDetails": {"duration": "PT3M32S"}}]}, {"idx_paper": 952, "q": "arxiv.org/abs/1902.02918", "items": [{"id": "O84mcq7P_es", "statistics": {"likeCount": "33", "favoriteCount": "0", "commentCount": "2", "viewCount": "955", "dislikeCount": "0"}, "snippet": {"channelTitle": "Sebastien Bubeck", "publishedAt": "2020-03-30T10:00:11Z", "title": "Randomized smoothing for certified robustness", "description": "We give a short proof of the Cohen-Rosenfeld-Kolter theorem on the certified robustness of randomized smoothing.\n\nCohen-Rosenfeld-Kolter paper: https://arxiv.org/abs/1902.02918\nPresented proof can be found in Salman-Yang-Li-Zhang-Zhang-Razenshteyn-Bubeck: https://arxiv.org/abs/1906.04584\nFor more background on adversarial examples see e.g., this video: https://www.youtube.com/watch?v=QWjPGHVrcEQ&list=PLgKuh-lKre12c2Il9mNX0Cmp9Z4oFNrQh", "channelId": "UC-UC-nE8B33UGnC-NRaSfug"}, "contentDetails": {"duration": "PT32M27S"}}]}, {"idx_paper": 1008, "q": "arxiv.org/abs/1902.03393", "items": [{"id": "ueUAtFLtukM", "statistics": {"likeCount": "72", "favoriteCount": "0", "commentCount": "9", "viewCount": "1247", "dislikeCount": "0"}, "snippet": {"channelTitle": "Henry AI Labs", "publishedAt": "2019-11-20T18:13:28Z", "title": "Knowledge Distillation with TAs", "description": "This video presents a paper exploring the effectiveness of introducing intermediate teaching networks (Teacher Assistants) to the knowledge distillation pipeline! Thanks for watching, please subscribe!\n\nImproved Knowledge Distillation via Teacher Assistant:\nhttps://arxiv.org/pdf/1902.03393.pdf\nOn the Efficacy of Knowledge Distillation:\nhttps://arxiv.org/abs/1910.01348\nEfficientNet:\nhttps://arxiv.org/abs/1905.11946\nSelf-Training with Noisy Student:\nhttps://arxiv.org/abs/1911.04252", "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw"}, "contentDetails": {"duration": "PT13M29S"}}]}, {"idx_paper": 1015, "q": "arxiv.org/abs/1902.03466", "items": [{"id": "mBOEnStGNwo", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "95", "dislikeCount": "0"}, "snippet": {"channelTitle": "FordDeepDSP FordDeepDSP", "title": "Hierarchical Multi Task DNN for End-to-End Driving.", "tags": ["Deep Neural Network", "DNN", "End-to-End", "End2End", "Multi Tasks", "autonomous", "driving", "1/6th scale", "AV"], "channelId": "UCrydv6TcnK6cxtxoyXb-BDA", "publishedAt": "2019-01-15T07:58:04Z", "description": "This video shows a novel multi-network hierarchical model can be implemented to address the task of end-to-end driving. Please view the full article for details;\nhttps://arxiv.org/abs/1902.03466\n\nThe model consists of a master classifier network which determines the driving task required from an input stereo image and directs said image to one of a set of subservient regression network models that perform inference and output a steering command."}, "contentDetails": {"duration": "PT2M16S"}}]}, {"idx_paper": 1051, "q": "arxiv.org/abs/1902.03701", "items": [{"id": "Rb2a6lSQSas", "statistics": {"likeCount": "25", "favoriteCount": "0", "commentCount": "0", "viewCount": "1457", "dislikeCount": "0"}, "snippet": {"channelTitle": "Greg Kahn", "publishedAt": "2019-02-10T22:13:08Z", "title": "Generalization through Simulation", "description": "Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight\n\nKatie Kang*, Suneel Belkhale*, Gregory Kahn*, Pieter Abbeel, Sergey Levine\n\nBerkeley AI Research (BAIR), University of California, Berkeley\n\nhttps://arxiv.org/abs/1902.03701\nhttps://github.com/gkahn13/GtS", "channelId": "UCe2X23GXwNgreBOCjbXXFgA"}, "contentDetails": {"duration": "PT2M58S"}}]}, {"idx_paper": 1768, "q": "arxiv.org/abs/1902.04615", "items": [{"id": "wZWn7Hm8osA", "statistics": {"likeCount": "106", "favoriteCount": "0", "commentCount": "9", "viewCount": "2536", "dislikeCount": "0"}, "snippet": {"channelTitle": "Yannic Kilcher", "title": "Gauge Equivariant Convolutional Networks and the Icosahedral CNN", "tags": ["machine learning", "deep learning", "artificial intelligence", "ai", "data science", "convolution", "convolutional neural networks", "cnn", "manifolds", "curvature", "parallel transport", "gauge", "gauge transformation", "icosahedron", "weight sharing", "coordinate frame", "invariant", "coordinate system", "equivariance", "sphere", "spherical"], "channelId": "UCZHmQk67mSJgfCCTn7xBfew", "publishedAt": "2019-08-13T14:03:03Z", "defaultAudioLanguage": "en-US", "description": "Ever wanted to do a convolution on a Klein Bottle? This paper defines CNNs over manifolds such that they are independent of which coordinate frame you choose. Amazingly, this then results in an efficient practical method to achieve state-of-the-art in several tasks!\n\nhttps://arxiv.org/abs/1902.04615\n\nAbstract:\nThe principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.\n\nAuthors: Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling", "defaultLanguage": "en"}, "contentDetails": {"duration": "PT21M50S"}}]}, {"idx_paper": 1788, "q": "arxiv.org/abs/1902.04742", "items": [{"id": "o3GfnEjTdIQ", "statistics": {"likeCount": "28", "favoriteCount": "0", "commentCount": "0", "viewCount": "2114", "dislikeCount": "0"}, "snippet": {"channelTitle": "CMU Locus Lab", "title": "Uniform convergence may be unable to explain generalization in deep learning (NeurIPS19 oral paper)", "tags": ["deep learning", "generalization", "learning theory"], "channelId": "UCFq0sKBQodUIjS3qjaXCYjg", "publishedAt": "2019-10-27T17:29:59Z", "description": "This video provides a brief outline of our NeurIPS '19 Oral paper titled \"Uniform convergence may be unable to explain generalization in deep learning\".  This paper won the Outstanding New Direction award.\n\nCheck out our paper: https://arxiv.org/abs/1902.04742\nOr check out our blog *with code* downloadable as a Jupyter notebook: https://locuslab.github.io/2019-07-09-uniform-convergence/\n\nOur blog also has an FAQ section that addresses some of the most commonly asked technical questions. Feel free to email us if you have any other questions!\n\n========\nAuthors: Vaishnavh Nagarajan, J. Zico Kolter\n\nAbstract: Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning,  all  based on the fundamental learning-theoretic technique of uniform convergence. While\nit is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice,  these bounds can {\\em increase} with the training dataset size. Guided by our observations,\nwe then present examples of overparameterized linear classifiers and neural networks trained by  gradient descent (GD) where uniform convergence provably cannot ``explain generalization'' -- even if we take into account the implicit bias of GD {\\em to the fullest extent possible}.  More precisely, even if we consider only the set of classifiers output by SGD that have test errors less than some small \u03f5, applying (two-sided) uniform convergence on this set of classifiers yields a generalization guarantee that is larger than 1\u2212\u03f5 and is therefore nearly vacuous. Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well. \n========="}, "contentDetails": {"duration": "PT2M52S"}}]}, {"idx_paper": 1805, "q": "arxiv.org/abs/1902.04818", "items": [{"id": "sbKaUc0tPaY", "statistics": {"likeCount": "30", "favoriteCount": "0", "commentCount": "3", "viewCount": "556", "dislikeCount": "0"}, "snippet": {"channelTitle": "Yannic Kilcher", "title": "The Odds are Odd: A Statistical Test for Detecting Adversarial Examples", "channelId": "UCZHmQk67mSJgfCCTn7xBfew", "publishedAt": "2019-02-20T05:12:21Z", "defaultAudioLanguage": "en-US", "description": "https://arxiv.org/abs/1902.04818\n\nAbstract:\nWe investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.\n\nAuthors:\nKevin Roth, Yannic Kilcher, Thomas Hofmann", "defaultLanguage": "en"}, "contentDetails": {"duration": "PT30M26S"}}]}, {"idx_paper": 1819, "q": "arxiv.org/abs/1903.00374", "items": [{"id": "ik9WrxPCamQ", "statistics": {"likeCount": "9", "favoriteCount": "0", "commentCount": "1", "viewCount": "447", "dislikeCount": "0"}, "snippet": {"channelTitle": "deepsense_ai", "title": "deepsense.ai and Google Brain design artificial imagination for reinforcement learning", "channelId": "UCPzx2mFggm_Fv5BJYUJOpnA", "publishedAt": "2019-03-21T11:31:52Z", "defaultAudioLanguage": "en", "description": "deepsense.ai, Google Brain, the University of Warsaw and the University of Illinois at Urbana-Champaign have concluded a collaborative research project, building neural networks that mimic a simulated environment and effectively enabling artificial intelligence to perform a mental simulation. \n\nMore detailed information about the research, outcomes and possible uses can be found in the Arxiv paper (https://arxiv.org/abs/1903.00374) and a detailed blog post about artificial imagination on deepsense.ai\u2019s blog (https://deepsense.ai/building-a-matrix-reinforcement-learning-and-artificial-imagination/)."}, "contentDetails": {"duration": "PT5M39S"}}]}, {"idx_paper": 1869, "q": "arxiv.org/abs/1902.05522", "items": [{"id": "pl8540HN8L8", "liveStreamingDetails": {"actualStartTime": "2019-07-19T17:13:06.864000Z"}, "statistics": {"likeCount": "23", "favoriteCount": "0", "commentCount": "2", "viewCount": "601", "dislikeCount": "0"}, "snippet": {"channelTitle": "Numenta", "title": "Review: Superposition of many models into one", "tags": ["nupic", "numenta", "HTM", "hierarchical temporal memory", "jeff hawkins", "artificial intelligence", "computational neuroscience", "journal club", "paper review", "neuroscience"], "channelId": "UC8-ttzWLgXZOGuhUyrPlUuA", "publishedAt": "2019-07-19T18:03:49Z", "defaultAudioLanguage": "en", "description": "For this Friday\u2019s journal club, we will be looking at a recently published paper by Bruno Olshausen at the Redwood Institute. It is about a more plausible algorithm in terms of biological constraints. It also ties well with the latest discussions on Continuous Learning, it provides an ingenious and elegant approach to the catastrophic forgetting problem in multitask learning.\n\nHere is the link to the paper: https://arxiv.org/abs/1902.05522"}, "contentDetails": {"duration": "PT43M55S"}}]}, {"idx_paper": 2090, "q": "arxiv.org/abs/1902.07198", "items": [{"id": "_IXj6kXPdq8", "liveStreamingDetails": {"actualStartTime": "2019-12-07T19:59:13Z"}, "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "45", "dislikeCount": "0"}, "snippet": {"channelTitle": "Rishabh Agarwal", "title": "Learning to Generalize from Sparse and Underspecified Rewards (ICML'19)", "tags": ["Reinforcement learning; Deep Learning; ICML; Generalization"], "channelId": "UC-HUaHerGem0kvW-xhZXt2A", "publishedAt": "2019-12-07T19:59:13Z", "defaultAudioLanguage": "en", "description": "Short presentation for our ICML 2019 paper, \"Learning to Generalize from Sparse and Underspecified Rewards\". More details about this work can be found at https://bit.ly/merl2019\n\n- Link to Paper: https://arxiv.org/abs/1902.07198\n- Link to Google AI blog: https://ai.googleblog.com/2019/02/learning-to-generalize-from-sparse-and.html\n- Link to open-source code: https://github.com/google-research/google-research/tree/master/meta_reward_learning\n- Link to Poster: https://postersession.ai/poster/learning-to-generalize-from-sparse-and-u/\n\nAbstract:  We consider the problem of learning from sparse and underspecified rewards. This task structure arises in interpretation problems where an agent receives a complex input, such as a natural language command, and needs to generate a complex response, such as an action sequence, but only receives binary success-failure feedback. Rewards of this kind are usually underspecified because they do not distinguish between purposeful and accidental success. To learn in these scenarios, effective exploration is critical to find successful trajectories, but generalization also depends on discounting spurious trajectories that achieve accidental success. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We address reward underspecification by using Meta-Learning and Bayesian Optimization to construct an auxiliary reward function, which provides more accurate feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of the trained policy. Without using expert demonstrations, ground truth programs, our Meta Reward-Learning (MeRL) achieves state-of-the-art results on weakly-supervised semantic parsing, improving upon prior work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL."}, "contentDetails": {"duration": "PT4M17S"}}]}, {"idx_paper": 2151, "q": "arxiv.org/abs/1902.07638", "items": [{"id": "k-Ibrnd88Es", "statistics": {"likeCount": "2", "favoriteCount": "0", "commentCount": "0", "viewCount": "121", "dislikeCount": "0"}, "snippet": {"channelTitle": "Yisong Yue", "publishedAt": "2020-05-20T00:58:43Z", "title": "Lecture by Debadeepta Dey (CS 159 Spring 2020)", "description": "Neural Architecture Search\n\nSlides: https://drive.google.com/file/d/1pZ0jULB0TFocbilgTYv1kYpGZU7WySRw/view?usp=sharing\n\nCourse: https://sites.google.com/view/cs-159-spring-2020/\n\nRelevant Papers:\nhttps://arxiv.org/abs/1808.05377\nhttps://arxiv.org/abs/1806.09055\nhttps://arxiv.org/abs/1902.07638\nhttps://arxiv.org/abs/1905.13360", "channelId": "UCYztb8yVDT02TyeKoMiL6kg"}, "contentDetails": {"duration": "PT1H25M34S"}}]}, {"idx_paper": 2272, "q": "arxiv.org/abs/1903.03698", "items": [{"id": "DWSZHEvZO4o", "statistics": {"likeCount": "9", "favoriteCount": "0", "commentCount": "2", "viewCount": "622", "dislikeCount": "0"}, "snippet": {"channelTitle": "Vitchyr Pong", "publishedAt": "2019-06-21T20:14:11Z", "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning", "description": "Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, Sergey Levine\n\nPaper: https://arxiv.org/abs/1903.03698\nCode: https://github.com/vitchyr/rlkit/\nWebsite: https://sites.google.com/view/skew-fit", "channelId": "UC1Vp5OM9bIXLnNmYYAhLlxg"}, "contentDetails": {"duration": "PT9M31S"}}]}, {"idx_paper": 2298, "q": "arxiv.org/abs/1902.08727", "items": [{"id": "VbIZrb-x7RU", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "69", "dislikeCount": "0"}, "snippet": {"channelTitle": "pritish sahu", "publishedAt": "2019-07-09T14:39:22Z", "title": "Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach", "description": "This is joint work with Minyoung Kim, Behnam Gholami, Vladimir Pavlovic.\n\nLink to paper: https://arxiv.org/abs/1902.08727\nLink to code:  https://github.com/seqam-lab/GPDA", "channelId": "UC0orUzfMfJLj2V-4I3g_Yxg"}, "contentDetails": {"duration": "PT4M54S"}}]}, {"idx_paper": 2330, "q": "arxiv.org/abs/1903.04193", "items": [{"id": "Ha4Jbqr3sTM", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "12", "dislikeCount": "0"}, "snippet": {"channelTitle": "VUB AI Lab", "title": "A Robotic Wheelchair that Learns", "channelId": "UCwJa-qixhgnY9XToXiMdADw", "publishedAt": "2020-02-27T12:34:36Z", "description": "Short movie that shows how the Bootstrapped Dual Policy Iteration algorithm, developed at the Vrije Universiteit Brussel, works and allows a robot to learn to accomplish tasks by trial and error.\n\n\nPaper: https://arxiv.org/abs/1903.04193\n\n\n\nCode: https://github.com/vub-ai-lab/bdpi\n\n\n\nSee our other videos (on this channel) for more examples of what this wheelchair can do.", "defaultLanguage": "en"}, "contentDetails": {"duration": "PT3M44S"}}]}, {"idx_paper": 2348, "q": "arxiv.org/abs/1903.04476", "items": [{"id": "7GbjCcJ-OSI", "liveStreamingDetails": {"actualStartTime": "2019-09-27T17:22:42.443000Z"}, "statistics": {"likeCount": "24", "favoriteCount": "0", "commentCount": "0", "viewCount": "480", "dislikeCount": "0"}, "snippet": {"channelTitle": "Numenta", "title": "Selfless Sequential Learning", "tags": ["nupic", "numenta", "HTM", "hierarchical temporal memory", "jeff hawkins", "artificial intelligence", "computational neuroscience"], "channelId": "UC8-ttzWLgXZOGuhUyrPlUuA", "publishedAt": "2019-09-27T19:40:25Z", "description": "The very interesting and recently published paper at ICLR2019 studying the impact of sparsity in the context of Continual Learning:\nhttps://openreview.net/forum?id=Bkxbrn0cYX\n\nRelated: Continual Learning via Neural Pruning\nSiavash Golkar, Michael Kagan, Kyunghyun Cho  https://arxiv.org/abs/1903.04476"}, "contentDetails": {"duration": "PT2H6M40S"}}]}, {"idx_paper": 2413, "q": "arxiv.org/abs/1902.09866", "items": [{"id": "gMC8lZJQSWw", "statistics": {"likeCount": "21", "favoriteCount": "0", "viewCount": "864", "dislikeCount": "0"}, "snippet": {"channelTitle": "PapersWeLove", "title": "Is Program Analysis the Silver Bullet Against Software Bugs? by Karim Ali [PWLConf 2019]", "tags": ["paperswelove", "pwlconf"], "channelId": "UCoj4eQh_dZR37lL78ymC6XA", "publishedAt": "2019-09-19T13:37:19Z", "defaultAudioLanguage": "en", "description": "PWLConf 2019 Link: https://pwlconf.org/2019/karim-ali/\nSlides / Captions: https://github.com/papers-we-love/pwlconf-info/tree/master/2019/karim-ali\n\nIs Program Analysis the Silver Bullet Against Software Bugs?\nKarim Ali, Assistant Professor of Computing Science at the University of Alberta\n\nProgram analysis is the art of reasoning about the run-time behavior of a program without necessarily executing it. This information is useful for various real-life applications such as supporting software developers (e.g., bug-finding tools, code refactoring tools, and code recommenders) and compiler optimizations. Program analysis is also used to ensure complex software adheres to standards and regulations (e.g., medical devices, car industry, and aviation industry).\n\nIn this talk, I will discuss the three main properties that enable program analyses to be useful in practice: scalability, precision, and usability. I will relate that to various papers that have been published in the field of program analysis, as well as some of the work that my group has done. I will conclude with where I see program analysis research going and the challenges that we aim to solve in the field. \n\nReferences\n------------------\n\nStrictly declarative specification of sophisticated points-to analyses by Martin Bravenboer and Yannis Smaragdakis (https://people.cs.umass.edu/~yannis/doop-oopsla09prelim.pdf)\n\nDimensions of Precision in Reference Analysis of Object-Oriented Programming Languages by Barbara G. Ryder (http://prolangs.cs.vt.edu/refs/docs/cc03.pdf)\n\nAverroes: Whole-Program Analysis without the Whole Program by Karim Ali and Ond\u0159ej Lhot\u00e1k (https://karimali.ca/resources/papers/averroes.pdf)\n\nFlowDroid: Precise Context, Flow, Field, Object-Sensitive and Lifecycle-Aware Taint Analysis for Android Apps by Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bartel, Jacques Klein, Yves Le Traon, Damien Octeau, and Patrick McDaniel (https://orbilu.uni.lu/bitstream/1099)\n\nPrecise Interprocedural Dataflow Analysis with Applications to Constant Propagation by Shmuel Sagiv, Thomas W. Reps, and Susan Horwitz (https://link.springer.com/content/pdf/10.1007/3-540-59293-8_226.pdf)\n\nContext-, Flow-, and Field-Sensitive Data-Flow Analysis using Synchronized Pushdown Systems by Johannes Sp\u00e4th, Karim Ali, and Eric Bodden (https://karimali.ca/resources/papers/spds.pdf)\n\nDebugging Static Analysis by Lisa Nguyen Quang Do, Stefan Kr\u00fcger, Patrick Hill, Karim Ali, and Eric Bodden (https://karimali.ca/resources/papers/visuflow.pdf)\n\nTowards Better Inlining Decisions Using Inlining Trials by Jeffrey Dean and Craig Chamber (https://courses.cs.washington.edu/courses/cse501/06wi/reading/dean-lfp94.pdf)\n\nReluplex: An Efficient SMT Solver for Verifying Deep Neural Networks by Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer (https://theory.stanford.edu/~barrett/pubs/KBD+17.pdf)\n\nAnalyzing Deep Neural Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification by Pengfei Yang, Jiangchao Liu, Jianlin Li, Liqian Chen, and Xiaowei Huang (https://arxiv.org/abs/1902.09866)\n\nSoot (https://github.com/Sable/soot)\n\nIBM. T.J. Watson Libraries for Analysis WALA (http://wala.sourceforge.net/)\n\nBiography\n----------------\n\nKarim Ali is an Assistant Professor in the Department of Computing Science at the University of Alberta. His research interests are in programming languages and software engineering, particularly in scalability, precision, and usability of program analysis tools. His work ranges from developing new theories for scalable and precise program analyses to applications of program analysis in security and Just-in-Time compilers.\n\nKarim obtained his PhD degree from the University of Waterloo in 2014, and was a postdoctoral research at Technische Universit\u00e4t Darmstadt (TU Darmstadt) 2014\u20132016.\n\n    Twitter https://twitter.com/karimhamdanali\n    Site: https://karimali.ca/\n    DBLP: https://dblp.org/pers/hd/a/Ali_0001:Karim\n\n-----------------------------------------------------------------------------------------------------------\nVideo Sponsor: Comcast (https://jobs.comcast.com/)\nCaptioning Sponsor: Two Sigma (https://www.twosigma.com/careers/)\n-----------------------------------------------------------------------------------------------------------"}, "contentDetails": {"duration": "PT50M39S"}}]}, {"idx_paper": 2480, "q": "arxiv.org/abs/1902.10565", "items": [{"id": "saSLTjQKuN0", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "1", "viewCount": "11", "dislikeCount": "0"}, "snippet": {"channelTitle": "Full Papers", "publishedAt": "2020-05-22T05:24:58Z", "title": "Full Paper - Accelerating Self-Play Learning in Go", "description": "This is a full reading of the paper: Accelerating Self-Play Learning in Go\n\nBy introducing several improvements to the AlphaZero process and architecture, this paper greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, the KataGo bot only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF\u2019s final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero.\n\n-----------------\nYou can find the full paper at: https://arxiv.org/abs/1902.10565\nNote the paper has an appendix section which isn't covered in this video due to its length.\n\nThe paper\u2019s author is David J. Wu\n-----------------\nAbout the Full Papers youtube  channel:\n\nReading research papers takes a lot of time. This channel makes it easier.\n\nYou might enjoy this channel if you:\n* Prefer to learn through hearing rather than reading alone.\n* Like to take a first-pass through a paper while commuting.\n* Spend long days staring at a screen and want to give your eyes a rest, lay back and hear a research paper at the end of the day.\n* Have a backlog of research papers to read and would like to catch up by reading some of them more casually.\n* Want to take a first-pass through a paper before deciding if it\u2019s worth sitting down and working through the details.\n\nI tend to post papers I\u2019m interested in, but if you have a paper you\u2019d like me to generate a video for just send me a message on twitter at https://twitter.com/marcsto or email me at marcsto@gmail.com and I should be able to help.\n\nThis video was created by Marc Stogaitis. Just to make sure it\u2019s clear, I did not write the paper, I\u2019m just trying to help you learn it.\n\nYou can visit my website at https://marc.ai", "channelId": "UCqTc8u62Cf-V5qNM6mk7J1A"}, "contentDetails": {"duration": "PT29M6S"}}]}, {"idx_paper": 2494, "q": "arxiv.org/abs/1903.06496", "items": [{"id": "MuJpHR1CpTc", "statistics": {"likeCount": "8", "favoriteCount": "0", "commentCount": "0", "viewCount": "427", "dislikeCount": "0"}, "snippet": {"channelTitle": "Sunghoon Joo", "title": "PR-218 : MFAS: Multimodal Fusion Architecture Search", "tags": ["Deep learning", "Machine learning", "Multimodal", "PR12", "Paper review"], "channelId": "UCrcRz2PZ0akJ4IoYCYgwlEg", "publishedAt": "2020-01-14T15:44:16Z", "defaultAudioLanguage": "ko", "description": "PR-218 : MFAS: Multimodal Fusion Architecture Search\nMultimodal data\ub97c \uc774\uc6a9\ud558\uae30 \uc704\ud55c \ucd5c\uc120\uc758 fusion architecture\ub97c \uc124\uacc4\ud558\ub294 \ubb38\uc81c\ub97c\nNeural architecture search (NAS) \ubc29\ubc95\uc911 \ud558\ub098\uc778 Sequential model based optimization (SMBO) \ub97c \uc801\uc6a9\ud574\uc11c \ud47c \ub17c\ubb38\uc785\ub2c8\ub2e4.\n\nreviewed by Sunghoon Joo\n\n*Paper url : https://arxiv.org/abs/1903.06496\n*slide url : https://www.slideshare.net/SunghoonJoo2/pr218-mfas-multimodal-fusion-architecture-search"}, "contentDetails": {"duration": "PT29M29S"}}]}, {"idx_paper": 2508, "q": "arxiv.org/abs/1903.06754", "items": [{"id": "4Ptq9CEySww", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "44", "dislikeCount": "0"}, "snippet": {"channelTitle": "wootcrisp", "title": "Ridealongs, p. 0+BeHs3u t.0+0020: Atari-HEAD, \\textcite{Zhang2019, Automatism2018}", "tags": ["wootcrisp"], "channelId": "UCjDdGVf_p5UbLSEJYWbgQcw", "publishedAt": "2019-09-24T03:24:06Z", "description": "Music: Automatism - From the Lake (Full Album 2018)\n\nhttps://www.youtube.com/watch?v=v73uUojSRAM\n\nPaper: Zhang, R., Liu, Z., Guan, L., Zhang, L., Hayhoe, M. M., & Ballard, D. H. (2019). Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset. Retrieved from http://arxiv.org/abs/1903.06754"}, "contentDetails": {"duration": "PT31M17S"}}]}, {"idx_paper": 2537, "q": "arxiv.org/abs/1903.07227", "items": [{"id": "Tiu5GzkjKeY", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "12", "dislikeCount": "0"}, "snippet": {"channelTitle": "Junjie Chen", "publishedAt": "2019-04-28T22:36:10Z", "title": "Music Generation and Convolutional RBM - BME 49500 Deep Learning Project", "description": "Technical blog by Google team:\nhttps://magenta.tensorflow.org/coconet\n\nBach Generator Doodle:\nhttps://www.google.com/doodles/celebrating-johann-sebastian-bach\n\nCoconet:\nhttps://arxiv.org/abs/1903.07227\n\nNADE:\nhttps://arxiv.org/abs/1605.02226\n\nMusic Generation using Convolutional RBM:\n(theirs actually works)\nhttps://arxiv.org/abs/1612.04742\n\nDataset:\nhttps://github.com/czhuang/JSB-Chorales-dataset", "channelId": "UCas4RSTyvN7Wm2GZNh0ap_Q"}, "contentDetails": {"duration": "PT4M11S"}}]}, {"idx_paper": 2557, "q": "arxiv.org/abs/1903.07400", "items": [{"id": "4ZHcBo7006Y", "statistics": {"likeCount": "3", "favoriteCount": "0", "commentCount": "0", "viewCount": "177", "dislikeCount": "0"}, "snippet": {"channelTitle": "Jingwei Zhang", "publishedAt": "2019-03-16T15:10:30Z", "title": "Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration", "description": "https://arxiv.org/abs/1903.07400\n\nExploration in sparse reward reinforcement learning remains a difficult open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Most commonly, these signals are added as bonus rewards, which results in the mixture policy faithfully conducting neither exploration nor task fulfillment for an extended amount of time. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and OpenAI Gym classic control from pixels. The results show a greatly improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives.", "channelId": "UCQE2omNXJ1ZvjfvPTvj7hHw"}, "contentDetails": {"duration": "PT3M4S"}}]}, {"idx_paper": 2646, "q": "arxiv.org/abs/1903.08297", "items": [{"id": "2hauTQm5otg", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "33", "dislikeCount": "0"}, "snippet": {"channelTitle": "\u9673\u5b78\u84b2", "title": "\u985e\u795e\u7d93\u7db2\u8def\u671f\u672b\u4f5c\u696d", "channelId": "UCTejYyaGljshM8qBRsJbzMQ", "publishedAt": "2020-01-01T07:29:52Z", "defaultAudioLanguage": "zh-TW", "description": "\u4ecb\u7d39\u7684\u8ad6\u6587:\nhttps://arxiv.org/abs/1903.08297\n\n\u5728\u9304\u7684\u6642\u5019\uff0c\u6211\u4ee5\u70ba\u53ef\u4ee5\u9304\u5230\u6211\u6253\u7684\u8a3b\u91cb\uff0c\u6240\u4ee5\u6709\u4e9b\u5730\u65b9\u6211\u6c92\u6709\u8b1b\u5b8c\u6574\uff0c\u8acb\u8001\u5e2b\u591a\u591a\u5305\u6db5\u3002"}, "contentDetails": {"duration": "PT13M16S"}}]}, {"idx_paper": 2671, "q": "arxiv.org/abs/1903.08738", "items": [{"id": "yIuDU7HT7s8", "statistics": {"likeCount": "9", "favoriteCount": "0", "commentCount": "2", "viewCount": "335", "dislikeCount": "0"}, "snippet": {"channelTitle": "Hoang Le", "title": "Batch Policy Learning under Constraints", "tags": ["Machine learning", "Reinforcement Learning", "Batch Policy Learning", "Constrained Policy Learning"], "channelId": "UC4TKZ-kqyaw-ApUnSYpagyw", "publishedAt": "2019-03-22T05:53:10Z", "description": "https://arxiv.org/abs/1903.08738\n\nHoang M. Le, Cameron Voloshin, Yisong Yue\nCalifornia Institute of Technology\n\nAbstract:  When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting."}, "contentDetails": {"duration": "PT12M42S"}}]}, {"idx_paper": 2864, "q": "arxiv.org/abs/1903.11257", "items": [{"id": "uKA2Tfd3s-4", "statistics": {"likeCount": "139", "favoriteCount": "0", "commentCount": "36", "viewCount": "1966", "dislikeCount": "0"}, "snippet": {"channelTitle": "Numenta", "title": "Numenta 2020: The Year in Review, The Decade Ahead", "tags": ["nupic", "numenta", "HTM", "hierarchical temporal memory", "jeff hawkins", "artificial intelligence", "computational neuroscience"], "channelId": "UC8-ttzWLgXZOGuhUyrPlUuA", "publishedAt": "2020-01-15T19:55:50Z", "defaultAudioLanguage": "en", "description": "A conversation with Jeff Hawkins and Subutai Ahmad.\n\nChristy Maver, VP Marketing, sits down with Jeff Hawkins, Co-Founder, and Subutai Ahmad, VP Research, and asks them to reflect on the previous year while making predictions about the coming decade.  Subutai shares the results of the effort he is leading to apply our neuroscience research to machine learning problems, and Jeff talks about continuing the neuroscience work and his upcoming book, which he spent most of 2019 writing. \n\nThe sparsity paper Subutai referenced is \"How Can We Be So Dense? The Benefits of Using Highly Sparse Representations\" - https://arxiv.org/abs/1903.11257\n\nThe \"Frameworks paper Jeff and Subutai referenced is, \"A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex\" - https://numenta.com/neuroscience-research/research-publications/papers/a-framework-for-intelligence-and-cortical-function-based-on-grid-cells-in-the-neocortex/\n\nMusic: https://freemusicarchive.org/music/Ryan_Andersen/MORE/Telling_Me_Yes", "defaultLanguage": "en"}, "contentDetails": {"duration": "PT18M39S"}}]}, {"idx_paper": 2884, "q": "arxiv.org/abs/1903.11524", "items": [{"id": "NCpyXBNqNmw", "statistics": {"likeCount": "19", "favoriteCount": "0", "commentCount": "2", "viewCount": "990", "dislikeCount": "2"}, "snippet": {"channelTitle": "Kindred AI", "publishedAt": "2019-03-27T19:25:26Z", "title": "Autoregressive Policies for Continuous Control Deep Reinforcement Learning", "description": "Autoregressive Policies for Continuous Control Deep Reinforcement Learning\nBy: Dmytro Korenkevych, A. Rupam Mahmood, Gautham Vasan, James Bergstra, (2019).\n\nThe full paper can be found at: https://arxiv.org/abs/1903.11524\n\nReinforcement learning algorithms rely on exploration to discover new behaviors, which is typically achieved by following a stochastic policy. In continuous control tasks, policies with a Gaussian distribution have been widely adopted. Gaussian exploration however does not result in smooth trajectories that generally correspond to safe and rewarding behaviors in practical tasks. In addition, Gaussian policies do not result in an effective exploration of an environment and become increasingly inefficient as the action rate increases. This contributes to a low sample efficiency often observed in learning continuous control tasks. We introduce a family of stationary autoregressive (AR) stochastic processes to facilitate exploration in continuous control domains. We show that proposed processes possess two desirable features: subsequent process observations are temporally coherent with continuously adjustable degree of coherence, and the process stationary distribution is standard normal. We derive an autoregressive policy (ARP) that implements such processes maintaining the standard agent-environment interface. We show how ARPs can be easily used with the existing off-the-shelf learning algorithms. Empirically we demonstrate that using ARPs results in improved exploration and sample efficiency in both simulated and real world domains, and, furthermore, provides smooth exploration trajectories that enable safe operation of robotic hardware.", "channelId": "UCjeevuqcVsC5o-Kgc6QaCVQ"}, "contentDetails": {"duration": "PT2M50S"}}]}, {"idx_paper": 2914, "q": "arxiv.org/abs/1903.11981", "items": [{"id": "UjJU13GdL94", "statistics": {"likeCount": "95", "favoriteCount": "0", "commentCount": "18", "viewCount": "1896", "dislikeCount": "1"}, "snippet": {"channelTitle": "Yannic Kilcher", "title": "Regularizing Trajectory Optimization with Denoising Autoencoders (Paper Explained)", "tags": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "model predictive control", "dae", "denoising autoencoders", "trajectory", "trajectory optimization", "planning", "adversarial attack", "errors", "open loop", "closed loop", "joint", "probability", "derivative", "gaussian", "experience", "learned model", "world model", "model predictive", "mpc"], "channelId": "UCZHmQk67mSJgfCCTn7xBfew", "publishedAt": "2020-05-24T15:31:04Z", "defaultAudioLanguage": "en-US", "description": "Can you plan with a learned model of the world? Yes, but there's a catch: The better your planning algorithm is, the more the errors of your world model will hurt you! This paper solves this problem by regularizing the planning algorithm to stay in high probability regions, given its experience.\n\nhttps://arxiv.org/abs/1903.11981\n\nInterview w/ Harri: https://youtu.be/HnZDmxYnpg4\n\nAbstract:\nTrajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.\n\nAuthors: Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "defaultLanguage": "en"}, "contentDetails": {"duration": "PT29M58S"}}]}, {"idx_paper": 2948, "q": "arxiv.org/abs/1903.12436", "items": [{"id": "TiIuFt1KvJ4", "statistics": {"likeCount": "17", "favoriteCount": "0", "commentCount": "2", "viewCount": "877", "dislikeCount": "0"}, "snippet": {"channelTitle": "Michael Black", "title": "From Variational to Deterministic Autoencoders", "tags": ["VAE", "autoencoder", "deep learning", "machine learning", "ICLR", "variational", "regularization"], "channelId": "UCqNJuPO0tyV6eWfYB7lcsvw", "publishedAt": "2020-04-19T15:09:12Z", "defaultAudioLanguage": "en", "description": "paper: https://arxiv.org/abs/1903.12436\ncode: https://github.com/ParthaEth/Regularized_autoencoders-RAE-\n\nabstract:\nVariational Autoencoders (VAEs) provide a theoretically-backed framework for deep generative models. However, they often produce \u201cblurry\u201d images, which is linked to their training objective. Sampling in the most popular implementation, the Gaussian VAE, can be interpreted as simply injecting noise to the input of a deterministic decoder. In practice, this simply enforces a smooth latent space structure. We challenge the adoption of the full VAE framework on this specific point in favor of a simpler, deterministic one. Specifically, we investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism for sampling new data points, we propose to employ an efficient ex-post density estimation step that can be readily adopted both for the proposed deterministic autoencoders as well as to improve sample quality of existing VAEs. We show in a rigorous empirical study that regularized deterministic autoencoding achieves state-of-the-art sample quality on the common MNIST, CIFAR-10 and CelebA datasets.\n\ncitation:\n@inproceedings{ghosh2020from,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}"}, "contentDetails": {"duration": "PT4M56S"}}]}, {"idx_paper": 2965, "q": "arxiv.org/abs/1903.12650", "items": [{"id": "GuzM1ayDKQo", "statistics": {"likeCount": "5", "favoriteCount": "0", "commentCount": "0", "viewCount": "463", "dislikeCount": "0"}, "snippet": {"channelTitle": "\u9234\u6728\u826f\u592a", "title": "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e BERT \u306b\u3064\u3044\u3066\u8a71\u3057\u3066\u307f\u308b(/\u30fb\u03c9\u30fb)/", "tags": ["\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "\u4eba\u5de5\u77e5\u80fd", "AI", "\u6a5f\u68b0\u5b66\u7fd2", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0"], "channelId": "UCn_zjj8ymGmwZo9aukVfUCA", "publishedAt": "2019-04-09T16:03:24Z", "description": "\u4eca\u56de\u306f\u3001\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u4eba\u6c17\u306e\u624b\u6cd5 BERT\u306b\u3064\u3044\u3066\u8a71\u3057\u3066\u307f\u305f(/\u30fb\u03c9\u30fb)/\n\n\u88dc\u8db3\uff1aglue\u306e\u30ea\u30fc\u30c0\u30fc\u30dc\u30fc\u30c9\u3092\u78ba\u8a8d\u3057\u305f\u3068\u3053\u308d\u3001\u3053\u306e\u52d5\u753b\u3092\u6295\u7a3f\u3057\u3066\u304b\u3089\u4e8c\u304b\u6708\u305f\u305f\u305a\u306bglue\u3067\u30d2\u30c8\u306e\u30b9\u30b3\u30a2\u3092\u8d85\u3048\u3066\u3044\u307e\u3057\u305f\u30021\u5e74\u4ee5\u5185\u3063\u3066\u52d5\u753b\u5185\u3067\u306f\u8a00\u3044\u307e\u3057\u305f\u304c\u30012\u304b\u6708\u4ee5\u5185\u3067\u305d\u3053\u307e\u3067\u3044\u3063\u305f\u3063\u307d\u3044\u3067\u3059(/\u30fb\u03c9\u30fb)/\n\n\u30c4\u30a4\u30c3\u30bf\u30fc\u3084\u3063\u3066\u307e\u3059\u3002\nhttps://twitter.com/yowaijibunntot1\n\nGLUE\u306e\u30ea\u30fc\u30c0\u30fc\u30dc\u30fc\u30c9\nhttps://gluebenchmark.com/leaderboard\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nhttps://arxiv.org/abs/1810.04805\n\nBERT\u306ePre-training\u6642\u9593\u3092\u7206\u901f\u5316\u3057\u305f\u8ad6\u6587\nReducing BERT Pre-Training Time from 3 Days to 76 Minutes\nhttps://arxiv.org/abs/1904.00962\n\nBERT\uff08\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\uff09\u306b\u3088\u308b\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306f\u3001\u3069\u3093\u306a\u30c7\u30fc\u30bf\u3067\u8a55\u4fa1\u3055\u308c\u305f\u306e\uff1f\u3069\u3093\u306a\u5fdc\u7528\u304c\u3067\u304d\u305d\u3046\uff1f\nhttps://it-mint.com/2018/11/06/datasets-of-deep-learning-bert-1603.html\n\nImageNet\u306e\u5b66\u7fd2\u6642\u9593\u304c\u5927\u5e45\u306b\u77ed\u7e2e\u3055\u308c\u3001\u73fe\u5728\u306e\u4e16\u754c\u8a18\u9332\nYet Another Accelerated SGD: ResNet-50 Training on ImageNet in 74.7 seconds\nhttps://arxiv.org/abs/1903.12650"}, "contentDetails": {"duration": "PT14M15S"}}]}, {"idx_paper": 31, "q": "arxiv.org/pdf/1901.00596.pdf", "items": [{"id": "NbxSzyTnLTQ", "statistics": {"likeCount": "17", "favoriteCount": "0", "commentCount": "5", "viewCount": "780", "dislikeCount": "0"}, "snippet": {"channelTitle": "BASIRA Lab", "publishedAt": "2019-12-19T13:49:43Z", "title": "Graph Theory Blink 10 (3 rules of geometric deep learning: locality, aggregation, and composition).", "description": "#graphNeuralNetworks #geometricDeepLearning #graphConvolutionalNetworks \n\nLecture 10 is a brief introduction to geometric deep learning: an exciting research field intersecting graph theory and and deep learning.\n\nIn this lecture, I cover the three fundamental rules driving the field of deep learning including:\n1) Locality: \u201ctell me who your neighbours are, I will tell you who you are\u201d,\n2) Aggregation: \u201chow to integrate information or messages you get from your neighbour?\u201d, and\n3) Composition: \u201chow deep you want to learn from your neighbours\u2019 messages?\u201d\n\n**** Resources and further readings ****\n\n1. Stanford course \u201cCS224W: Machine Learning with Graphs\u201d, offered by Jure Leskovec:\nhttp://web.stanford.edu/class/cs224w/slides/08-GNN.pdf\nA special thanks to my students Alin Banka and Inis Buzi for sharing this with me :) \n2. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C. and Yu, P.S., 2019. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596.\nhttps://arxiv.org/pdf/1901.00596.pdf\n4. Graph-based deep learning literature in top conferences:\nhttps://github.com/basiralab/graph-based-deep-learning-literature", "channelId": "UCxsqJMTD-yOe277vtQIRjgw"}, "contentDetails": {"duration": "PT55M43S"}}]}, {"idx_paper": 61, "q": "arxiv.org/pdf/1901.01484.pdf", "items": [{"id": "R4tTkI2vnE8", "statistics": {"likeCount": "11", "favoriteCount": "0", "commentCount": "0", "viewCount": "327", "dislikeCount": "0"}, "snippet": {"channelTitle": "BASIRA Lab", "title": "Best Project Presentation on Lanczosnet (multi-scale deep graph convolutional networks #ICLR2019)", "channelId": "UCxsqJMTD-yOe277vtQIRjgw", "publishedAt": "2020-01-07T14:10:28Z", "defaultAudioLanguage": "en", "description": "#BestProjectPresentation #GraphLaplacian #GraphConvolutionalNetworks\n\nPresenters: Elif AK, G\u00fcl\u00e7in BAYKAL, Oguzhan CAN, and Talip Tolga SARI; Istanbul Technical University (Graph Theory course project, Best Project Presentation).\n\nIn this video, we are presenting the following research paper:\nLiao R, Zhao Z, Urtasun R, Zemel RS. Lanczosnet: Multi-scale deep graph convolutional networks, ICLR 2019 (arXiv preprint arXiv:1901.01484).\n\n** Link to the research paper: https://arxiv.org/pdf/1901.01484.pdf\n** Link to Github code of the paper: https://github.com/lrjconan/LanczosNetwork\n\n************************\nPaper abstract (Liao et al., ICLR 2019): We propose the Lanczos network (LanczosNet), which uses the Lanczos algorithm to construct low rank approximations of the graph Laplacian for graph convolution. Relying on the tridiagonal decomposition of the Lanczos algorithm, we not only efficiently exploit multi-scale information via fast approximated computation of matrix power but also design learnable spectral filters. Being fully differentiable, LanczosNet facilitates both graph kernel learning as well as learning node embeddings. We show the connection between our LanczosNet and graph based manifold learning methods, especially the diffusion maps. We benchmark our model against several recent deep graph networks on citation networks and QM8 quantum chemistry dataset. Experimental results show that our model achieves the state-of-the-art performance in most tasks.\n************************"}, "contentDetails": {"duration": "PT8M34S"}}]}, {"idx_paper": 94, "q": "arxiv.org/pdf/1901.02161.pdf", "items": [{"id": "87agi0-DBk4", "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "1", "viewCount": "12", "dislikeCount": "0"}, "snippet": {"channelTitle": "Scott Niekum", "publishedAt": "2020-05-18T21:27:12Z", "title": "Probabilistically Safe and Correct Imitation Learning -- Scott Niekum", "description": "ICRA 2020 Workshop on Interactive Robot Learning talk by Scott Niekum on Probabilistically Safe and Correct Imitation Learning  \n\n\n\nFor papers discussed in this video, see:\nhttps://arxiv.org/pdf/2002.09089.pdf\nhttps://arxiv.org/pdf/1907.03976.pdf\nhttps://arxiv.org/pdf/1904.06387.pdf\nhttps://arxiv.org/pdf/1901.02161.pdf\nhttp://www.cs.utexas.edu/users/sniekum/pubs/CuiICRA2018.pdf", "channelId": "UCt117N84AwfKVo5Ts8aO66w"}, "contentDetails": {"duration": "PT29M9S"}}]}, {"idx_paper": 126, "q": "arxiv.org/pdf/1901.02860.pdf", "items": [{"id": "Axo0EtMUK90", "statistics": {"likeCount": "74", "favoriteCount": "0", "commentCount": "11", "viewCount": "1232", "dislikeCount": "0"}, "snippet": {"channelTitle": "Henry AI Labs", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw", "publishedAt": "2020-04-23T19:13:27Z", "defaultAudioLanguage": "en-US", "description": "This video explores the T5 large-scale study on Transfer Learning. This paper takes apart many different factors of the Pre-Training then Fine-Tuning pipeline for NLP. This involves Auto-Regressive Language Modeling vs. BERT-Style Masked Language Modeling and XLNet-style shuffling, as well as the impact of dataset composition, size, and how to best use more computation. Thanks for watching and please check out Machine Learning Street Talk where Tim Scarfe, Yannic Kilcher and I discuss this paper!\n\nMachine Learning Street Talk: https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ\n\nPaper Links:\nT5: https://arxiv.org/abs/1910.10683\nGoogle AI Blog Post on T5: https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\nTrain Large, Then Compress: https://arxiv.org/pdf/2002.11794.pdf\nScaling Laws for Neural Language Models: https://arxiv.org/pdf/2001.08361.pdf\nThe Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\nELECTRA: https://arxiv.org/pdf/2003.10555.pdf\nTransformer-XL: https://arxiv.org/pdf/1901.02860.pdf\nReformer: The Efficient Transformer: https://openreview.net/pdf?id=rkgNKkHtvB\nThe Evolved Transformer: https://arxiv.org/pdf/1901.11117.pdf\nDistilBERT: https://arxiv.org/pdf/1910.01108.pdf\nHow to generate text (HIGHLY RECOMMEND): https://huggingface.co/blog/how-to-generate\nTokenizers: https://blog.floydhub.com/tokenization-nlp/\n\nThanks for watching! Please Subscribe!"}, "contentDetails": {"duration": "PT23M43S"}}]}, {"idx_paper": 273, "q": "arxiv.org/pdf/1901.07957.pdf", "items": [{"id": "x1IAPgvKUmM", "statistics": {"likeCount": "32", "favoriteCount": "0", "commentCount": "2", "viewCount": "1084", "dislikeCount": "0"}, "snippet": {"channelTitle": "Ali Yektaie", "publishedAt": "2019-05-01T03:13:54Z", "title": "Phoneme Detection with CNN-RNN-CTC Loss Function - Machine Learning", "description": "This is the report for the final project of the Advanced Machine Learning course by professor Jeremy Bolton.\n\nGitHub Repository for the code:\nData Gatherer (C#): https://github.com/aliyektaie/PhonemeDetectionDataGatherer\nMain Model (python): \nhttps://github.com/aliyektaie/PhonemeDetectionDeepNerualNetwork\n\nReferences:\nhttps://cmusphinx.github.io/wiki/phonemerecognition/\nhttps://pdfs.semanticscholar.org/3866/55ee41444c75fd04ea116a05b4a20423d55a.pdf\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a\nhttp://www.cs.toronto.edu/~graves/icml_2006.pdf\nhttps://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\nhttps://www.dlology.com/blog/how-to-train-a-keras-model-to-recognize-variable-length-text/\nhttps://arxiv.org/pdf/1901.07957.pdf?\nhttp://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\nhttps://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\nhttp://www.odyssey2016.org/papers/slides/24_Fri/02%20-%20Keynote_3/Keynote_3.pdf\nhttp://luthuli.cs.uiuc.edu/~daf/courses/cs-498-daf-ps/lecture%208%20-%20audio%20features2.pdf\nhttps://arxiv.org/pdf/1603.00982.pdf\nhttps://arxiv.org/pdf/1712.02898.pdf\nhttps://www.youtube.com/watch?v=9dXiAecyJrY\nhttp://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4272376/\nhttps://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\nhttps://skymind.ai/wiki/open-datasets\nhttps://www.youtube.com/watch?v=HyUtT_z-cms&t=4s\nhttps://distill.pub/2017/ctc/", "channelId": "UCW-gIPbFkDwgup8_VWF4-Mg"}, "contentDetails": {"duration": "PT11M43S"}}]}, {"idx_paper": 320, "q": "arxiv.org/pdf/1901.08544.pdf", "items": [{"id": "I8rCMwN-fQw", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "38", "dislikeCount": "0"}, "snippet": {"channelTitle": "Kolmogorov-Seminar-Moscow", "title": "\u0420\u0430\u0437\u0435\u043d\u0448\u0442\u0435\u0439\u043d \u0418\u043b\u044c\u044f. \u041e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u0438 \u043f\u043e\u0438\u0441\u043a\u0435 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439 (06.05.2020)", "tags": ["Ilya Razenshteyn", "nearest neighbors", "machine learning"], "channelId": "UC20xHyxD6FqItj2N6y3eSZg", "publishedAt": "2020-05-09T01:03:12Z", "defaultAudioLanguage": "ru", "description": "\u0417\u0430\u0434\u0430\u0447\u0430 \u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u044f\u0445 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c: \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0447\u0435\u043a (\u0432 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435) \u0438 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u0438\u0445, \u0430 \u0437\u0430\u0442\u0435\u043c \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0441\u0442\u0440\u043e \u043e\u0442\u0432\u0435\u0447\u0430\u0442\u044c \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0442\u0430\u043a\u043e\u0433\u043e \u0442\u0438\u043f\u0430: \u0434\u0430\u043d\u0430 \u0442\u043e\u0447\u043a\u0430, \u043d\u0430\u0439\u0442\u0438 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439 \u043a \u044d\u0442\u043e\u0439 \u0442\u043e\u0447\u043a\u0435. \u0411\u0443\u0434\u0435\u0442 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u043d\u043e \u043f\u0440\u043e \u044d\u0442\u0443 \u0437\u0430\u0434\u0430\u0447\u0443 \u0438 \u043f\u0440\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0432 \u043d\u0435\u0439:\n\nNeural Locality-Sensitive Hashing\nhttps://arxiv.org/pdf/1901.08544.pdf\n\n____________________\n\u041a\u043e\u043b\u043c\u043e\u0433\u043e\u0440\u043e\u0432\u0441\u043a\u0438\u0439 \u0441\u0435\u043c\u0438\u043d\u0430\u0440 \u043f\u043e \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439.\r\n\u041a\u0430\u0444\u0435\u0434\u0440\u0430 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043b\u043e\u0433\u0438\u043a\u0438 \u0438 \u0442\u0435\u043e\u0440\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432.\r\n\u041c\u0435\u0445\u0430\u043d\u0438\u043a\u043e-\u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442.\r\n\u041c\u0413\u0423 \u0438\u043c. \u041c.\u0412. \u041b\u043e\u043c\u043e\u043d\u043e\u0441\u043e\u0432\u0430."}, "contentDetails": {"duration": "PT1H55M14S"}}]}, {"idx_paper": 456, "q": "arxiv.org/pdf/1901.10002.pdf", "items": [{"id": "7cgYig-Vekk", "statistics": {"likeCount": "2", "favoriteCount": "0", "commentCount": "0", "viewCount": "61", "dislikeCount": "0"}, "snippet": {"channelTitle": "Sean Mullery", "title": "Algorithmic Bias", "tags": ["C-Programming"], "channelId": "UCoIlaxeETOcT_gPvxNqsEyg", "publishedAt": "2019-12-10T11:45:01Z", "defaultAudioLanguage": "en-IE", "description": "This is part of a lecture series in Machine Learning at IT Sligo.\nThis is part of the Online Master of Engineering in Connected and Autonomous Vehicles https://www.itsligo.ie/courses/meng-c...\nYou can study this Masters Programme from anywhere in the world.\n\nTopics covered.\nBias by proxy\nBias in models\nStatic Vs Dynamic Systems\nBias in the data set\nFemale representation in the literature\nBias in Image Data sets - Geo-diversity\nBias in autonomous vehicles\nThe Uber crash\nTypes of Bias\nAvoiding bias when detecting Humans\nAvoiding Bias in general\nGeneral advice\n\nAcknowledgements\nRachel Thomas https://www.usfca.edu/faculty/rachel-thomas\n\nHospital Algorithms Are Biased Against Black Patients https://onezero.medium.com/hospital-algorithms-are-biased-against-black-patients-new-research-shows-7ab4cc896fb3\n\nCanadian Immigration algorithmic bias article https://ihrp.law.utoronto.ca/sites/default/files/media/IHRP-Automated-Systems-Report-Web.pdf\n\nEthics and Datascience https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7 \n\nWomen in libraries http://sappingattention.blogspot.com/2012/05/women-in-libraries.html\n\nBen Blatt Simon & Schuster https://www.npr.org/2017/03/31/521836700/nabokovs-favorite-word-is-mauve-crunches-the-literary-numbers?t=1575394447256\n\nIBM Mitigating Bias https://www.ibm.com/blogs/research/2018/02/mitigating-bias-ai-models/\n\nPredictive Inequity in Object detection.\n https://arxiv.org/pdf/1902.11097.pdf\n\nNo Classification without Representation https://arxiv.org/pdf/1711.08536.pdf\n\nA Framework for Understanding Unintended Consequences of Machine Learning by Harini Suresh and John V. Guttag  https://arxiv.org/pdf/1901.10002.pdf \n\nThe Privacy Implications of Autonomous Vehicles - www.dataprotectionreport.com. https://www.dataprotectionreport.com/2017/07/the-privacy-implications-of-autonomous-vehicles/\n\nBlack Box Thinking by Matthew Syed https://www.amazon.com/Black-Box-Thinking-People-Mistakes-But/dp/1591848229\n\nFCC report: Protecting Consumer Privacy in an Era of rapid change. https://www.ftc.gov/sites/default/files/documents/reports/federal-trade-commission-report-protecting-consumer-privacy-era-rapid-change-recommendations/120326privacyreport.pdf \n\nConsumer Privacy Protection Principles https://cryptome.org/2014/11/auto-privacy-principles.pdf\n\nEthics and Datascience https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7\n\n16 things you can do to make tech more ethical  https://www.fast.ai/2019/04/22/ethics-action-1/#checklist\n\nAmazon's Ring Doorbells \n https://www.cnet.com/features/amazons-helping-police-build-a-surveillance-network-with-ring-doorbells/"}, "contentDetails": {"duration": "PT45M45S"}}]}, {"idx_paper": 527, "q": "arxiv.org/pdf/1901.10995.pdf", "items": [{"id": "0hgarA3EvqA", "statistics": {"likeCount": "50", "favoriteCount": "0", "commentCount": "15", "viewCount": "1065", "dislikeCount": "1"}, "snippet": {"channelTitle": "Henry AI Labs", "title": "First Return Then Explore", "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw", "publishedAt": "2020-04-29T19:02:06Z", "defaultAudioLanguage": "en-US", "description": "This video explores \"First Return Then Explore\", the latest advancement of the Go-Explore algorithm. This paper introduces Policy-based Go-Explore where the agent is trained to return to the frontier of explored states, rather than just resetting the simulator state. This helps with stochasticity during training, removes the need for a second robustify phase, and provides a better policy for exploration from the most promising state.\n\nThanks for watching! Please Subscribe!\n\nPaper Links:\nFirst return then explore: https://arxiv.org/pdf/2004.12919.pdf\nGo-Explore: https://arxiv.org/pdf/1901.10995.pdf\nThe Ingredients of Real-World RL: https://bair.berkeley.edu/blog/2020/04/27/ingredients/\nDomain Randomization for Sim2Real Transfer: https://lilianweng.github.io/lil-log/2019/05/05/domain-randomization.html\nBeyond Domain Randomization: http://josh-tobin.com/assets/pdf/BeyondDomainRandomization_Tobin_RSS19.pdf\nJeff Clune at Rework on Go-Explore: https://www.youtube.com/watch?v=SWcuTgk2di8&t=862s\nWorld models: https://worldmodels.github.io/\nSolving Rubik's Cube with a Robot Hand: https://openai.com/blog/solving-rubiks-cube/\nExploration based language learning for text-based games: https://arxiv.org/abs/2001.08868\nAbandoning Objectives: https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf\nSpecification Gaming: https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity\nUpside-Down RL: https://arxiv.org/pdf/1912.02875.pdf\nChip Design with Deep Reinforcement Learning: https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html\n\nThanks for watching! Please Subscribe!"}, "contentDetails": {"duration": "PT23M26S"}}]}, {"idx_paper": 531, "q": "arxiv.org/pdf/1901.11117.pdf", "items": [{"id": "khA-fiC1Wa0", "statistics": {"likeCount": "63", "favoriteCount": "0", "commentCount": "6", "viewCount": "1336", "dislikeCount": "0"}, "snippet": {"channelTitle": "Henry AI Labs", "title": "The Evolved Transformer", "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw", "publishedAt": "2020-01-29T17:53:26Z", "defaultAudioLanguage": "en-US", "description": "This video explains the Evolved Transformer model! The Evolved Transformer has been applied to the Meena bot, one of the most impressive chatbots to date. This paper explores this study on Neural Architecture Search for transformer models and presents their interesting encoding strategy of these models.\nPaper Link:\nhttps://arxiv.org/pdf/1901.11117.pdf\nThanks for watching! Please Subscribe!"}, "contentDetails": {"duration": "PT15M1S"}}]}, {"idx_paper": 764, "q": "arxiv.org/pdf/1902.01385.pdf", "items": [{"id": "5E5zGtsa3As", "statistics": {"likeCount": "5", "favoriteCount": "0", "commentCount": "0", "viewCount": "166", "dislikeCount": "0"}, "snippet": {"channelTitle": "Devendra Chaplot", "title": "Embodied Multimodal Multitask Learning | No Aux | Test", "tags": ["Artificial Intelligence", "Reinforcement Learning", "Navigation", "Semantic Goal Navigation", "Embodied Question Answering"], "channelId": "UCePx8jw8Fc23KuWP1MTrBtw", "publishedAt": "2019-02-06T06:50:19Z", "description": "Embodied Multimodal Multitask Learning: https://arxiv.org/pdf/1902.01385.pdf\nThis video shows a demo of the Dual-Attention model trained without Auxiliary tasks on the test set in the Doom Hard environment. It also shows spatial attention, convolutional output, answer distribution visualizations. The Dual-Attention model is trained with Reinforcement Learning to follow instructions and answer questions."}, "contentDetails": {"duration": "PT58S"}}]}, {"idx_paper": 872, "q": "arxiv.org/pdf/1902.02263.pdf", "items": [{"id": "C_gqZhEKggs", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "7", "dislikeCount": "0"}, "snippet": {"channelTitle": "Simon C", "title": "AI Polyglot, ML Blocks Even More Spam and Deletes Suspicious Accounts | AI Radar #20", "tags": ["AI", "ML", "Artificial Intelligence", "Machine Learning", "automation", "Google", "Whatsapp", "Facebook", "Gmail", "Spam", "fake news", "India", "polyglot", "unsupervised learning", "translation", "Tensorflow", "TTS system", "VoiceLoop", "text-to-speech"], "channelId": "UC6Us86VfsJkdHQkXzaWl5gg", "publishedAt": "2019-02-10T22:18:12Z", "defaultAudioLanguage": "en", "description": "#AIPolyglot #TextToSpeech #MLvsSpam #MLvsFakeNews\n\n1) Facebook AI Polyglot Speaks 3 Languages\n- https://venturebeat.com/2019/02/07/facebooks-polyglot-ai-speaks-english-german-and-spanish/\n\"Unsupervised Polyglot Text To Speech\" - https://arxiv.org/pdf/1902.02263.pdf\n- https://twitter.com/Kyle_L_Wiggers\n\n2) Google ML Blocks even more spam\n- https://www.theverge.com/2019/2/6/18213453/gmail-tensorflow-machine-learning-spam-100-million\n- https://cloud.google.com/blog/products/g-suite/ridding-gmail-of-100-million-more-spam-messages-with-tensorflow\n\n3) Whatsapp ML Deletes 2 M Accounts per month\n- https://www.theguardian.com/technology/2019/feb/06/whatsapp-deleting-two-million-accounts-per-month-to-stop-fake-news\n- https://twitter.com/safimichael\n- https://venturebeat.com/2019/02/06/whatsapp-on-how-its-fighting-bulk-messaging-and-fake-accounts/\n\n--\n\nYou can record videos better than me using a GitUp2 Git2 action camera like this: \n- https://www.gitup.com/cameras/12-49-gitup-git2-action-camera-pro-packing.html"}, "contentDetails": {"duration": "PT6M33S"}}]}, {"idx_paper": 1008, "q": "arxiv.org/pdf/1902.03393.pdf", "items": [{"id": "ueUAtFLtukM", "statistics": {"likeCount": "72", "favoriteCount": "0", "commentCount": "9", "viewCount": "1247", "dislikeCount": "0"}, "snippet": {"channelTitle": "Henry AI Labs", "publishedAt": "2019-11-20T18:13:28Z", "title": "Knowledge Distillation with TAs", "description": "This video presents a paper exploring the effectiveness of introducing intermediate teaching networks (Teacher Assistants) to the knowledge distillation pipeline! Thanks for watching, please subscribe!\n\nImproved Knowledge Distillation via Teacher Assistant:\nhttps://arxiv.org/pdf/1902.03393.pdf\nOn the Efficacy of Knowledge Distillation:\nhttps://arxiv.org/abs/1910.01348\nEfficientNet:\nhttps://arxiv.org/abs/1905.11946\nSelf-Training with Noisy Student:\nhttps://arxiv.org/abs/1911.04252", "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw"}, "contentDetails": {"duration": "PT13M29S"}}]}, {"idx_paper": 1693, "q": "arxiv.org/pdf/1902.04043.pdf", "items": [{"id": "r5zlu7A1Om8", "statistics": {"likeCount": "6", "favoriteCount": "0", "commentCount": "4", "viewCount": "150", "dislikeCount": "0"}, "snippet": {"channelTitle": "Victor Gouet", "title": "AI learns to play Starcraft 2 with QMix #1", "tags": ["Gym", "Machine Learning", "ML", "RL", "Reinforcement Learning", "pytorch", "torch", "qmix", "QMIX", "Starcraft 2", "sc2"], "channelId": "UChT3kPSMIFW0aQSJ2EU7k7w", "publishedAt": "2019-11-30T17:27:17Z", "description": "Using Reinforcement Learning (Machine Learning) in the SMAC environment (Starcraft 2).\nThe SMAC environment is here : https://github.com/oxwhirl/smac\nThe project is open source on my GitHub : https://github.com/Gouet/QMIX-Starcraft\nSMAC : https://arxiv.org/pdf/1902.04043.pdf\nQMIX : https://arxiv.org/pdf/1803.11485.pdf\nUsing pytorch 1.3.0"}, "contentDetails": {"duration": "PT26S"}}]}, {"idx_paper": 1768, "q": "arxiv.org/pdf/1902.04615.pdf", "items": [{"id": "hwWeAd5_K20", "liveStreamingDetails": {"actualStartTime": "2020-03-31T12:59:52.632000Z"}, "statistics": {"likeCount": "1", "favoriteCount": "0", "commentCount": "0", "viewCount": "85", "dislikeCount": "0"}, "snippet": {"channelTitle": "m-dml", "publishedAt": "2020-03-31T15:12:30Z", "title": "ML@HZG episode 2: \"Don't Fear the Sphere\"", "description": "0:00:01 Announcements\n\n5 minute presentations\n0:01:42 Julianna Carvalho\n0:07:42 Tobias Finn\n0:15:49 Lennart Marien\n\n0:25:15 We cover \"Spherical CNNs on Unstructured Grids,\" Jiang et al. 2019, arXiv.\nhttps://arxiv.org/pdf/1901.02039.pdf\nOpen review: https://openreview.net/forum?id=Bkl-43C9FQ\nCode: https://github.com/maxjiang93/ugscnn\n\nThe paper describes how convolution-like operations can be learned and applied when input data exist on an irregularly spaced 3D mesh instead of a flat plane. We discuss how this approach works, and why taking into account the sphere's curved geometry can be so important when designing an ML architecture. The discussion is mostly non-technical, but we mention a few equations and try to give intuitions as to what's going on.\n\nSome links relevant to our discussion:\n-Convolutional nets https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n-The regular icosahedron https://en.wikipedia.org/wiki/Regular_icosahedron\n\nWe cover additional approaches for 1-2 minutes each:\n1:21:31 The cubed sphere https://arxiv.org/pdf/2003.11927.pdf\nand its application to molecular modeling https://papers.nips.cc/paper/6935-spherical-convolutions-and-their-application-in-molecular-modelling.pdf\n\n1:25:17 Adjusting the convolutional filters to compensate distortions\nhttp://www.cvlibs.net/publications/Coors2018ECCV.pdf\n\n1:27:01 Point cloud-based approaches https://arxiv.org/pdf/1801.07829.pdf\nhttps://arxiv.org/pdf/1706.02413.pdf\n\n1:32:49 Gauge equivariant networks https://arxiv.org/pdf/1902.04615.pdf\nMax Welling's podcast interview https://twimlai.com/twiml-talk-267-gauge-equivariant-cnns-generative-models-and-the-future-of-ai-with-max-welling/\nhttps://towardsdatascience.com/an-easy-guide-to-gauge-equivariant-convolutional-networks-9366fb600b70\n\n1:38:43 Spherical correlation https://arxiv.org/pdf/1801.10130.pdf\nhttps://medium.com/@samramasinghe/spherical-convolution-a-theoretical-walk-through-98e98ee64655", "channelId": "UCyXAYFO3h-tBIEbPEqMnNKw"}, "contentDetails": {"duration": "PT1H53M23S"}}]}, {"idx_paper": 1788, "q": "arxiv.org/pdf/1902.04742.pdf", "items": [{"id": "JzwsiYfg_GA", "statistics": {"likeCount": "11", "favoriteCount": "0", "commentCount": "1", "viewCount": "239", "dislikeCount": "0"}, "snippet": {"channelTitle": "AI Pursuit - Angus Tay", "title": "NeurIPS 2019 Outstanding New Directions Paper Award w/ slides", "tags": ["9-03-2020"], "channelId": "UCe_QLqna7cFtTCfZ0a8pycg", "publishedAt": "2020-03-09T02:20:00Z", "description": "As this is a non- profit  channel, videos are shared for educational purpose for the wider audiences to know the current state of the art \ud83d\ude03\n\nVideo is reposted for educational purpose.\n-----------------------------------------------------------------------------------------\nSubscribe \u21e2 https://www.youtube.com/c/AIPursuit?sub_confirmation=1\n-----------------------------------------------------------------------------------------\n\nNo Ads!\nContinue to support the channel: https://paypal.me/aipursuit\n \nFor full NeurIPS coverage, please visit https://slideslive.com/t/neurips-2019 \nfor more videos with slides.\n\n\nPaper: https://arxiv.org/pdf/1902.04742.pdf\n\nSlides: http://www.cs.cmu.edu/~vaishnan/talks/neurips19_uc_slides.pdf\n\n\nSource: https://slideslive.com/38922600/uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning\n\n\nNeurIPS 2019 Outstanding New Directions Paper Award: Uniform convergence may be unable to explain generalization in deep learning\nVaishnavh Nagarajan, J. Zico Kolter\n\nAbstract:\nWe cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well. While it is well-known that many existing bounds are numerically large, through a variety of experiments, we first bring to light another crucial and more concerning aspect of these bounds: in practice, these bounds can {\\em increase} with the dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by stochastic gradient descent (SGD) where uniform convergence provably cannot `explain generalization,\u2019 even if we take into account implicit regularization {\\em to the fullest extent possible}. More precisely, even if we consider only the set of classifiers output by SGD that have test errors less than some small \u03f5, applying (two-sided) uniform convergence on this set of classifiers yields a generalization guarantee that is larger than 1\u2212\u03f5 and is therefore nearly vacuous."}, "contentDetails": {"duration": "PT18M58S"}}]}, {"idx_paper": 1819, "q": "arxiv.org/pdf/1903.00374.pdf", "items": [{"id": "MXcaufqBua4", "statistics": {"likeCount": "4", "favoriteCount": "0", "commentCount": "0", "viewCount": "44", "dislikeCount": "0"}, "snippet": {"channelTitle": "DeepPavlov", "title": "RL#9: Model-Based Reinforcement Learning", "tags": ["reinforcement learning", "machine learning", "deeppavlov", "artificial intelligence"], "channelId": "UCJ-6K2HGA0hpQytlSM7FBVQ", "publishedAt": "2020-05-27T16:01:42Z", "description": "Advanced Topics in Reinforcement Learning\nhttps://deeppavlov.ai/rl_course_2020\n\n\u041d\u0430 \u043b\u0435\u043a\u0446\u0438\u0438 \u0415\u0432\u0433\u0435\u043d\u0438\u0439 \u041a\u0430\u0448\u0438\u043d \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043e \u0442\u043e\u043c \u0437\u0430\u0447\u0435\u043c \u043d\u0443\u0436\u0435\u043d Model-Based RL \u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c Model-Free. \u0412\u0441\u043f\u043e\u043c\u043d\u0438\u043c, \u0447\u0442\u043e \u0442\u0430\u043a\u043e\u0435 \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435, \u0438 \u043a\u0430\u043a \u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c, \n\u0435\u0441\u043b\u0438 \u0437\u043d\u0430\u0435\u043c \"\u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0440\u0435\u0434\u044b\". \u0418 \u0442\u0430\u043a \u0436\u0435 \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0442\u044c, \u0435\u0441\u043b\u0438 \u043d\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u0440\u0435\u0434\u044b. \u0412 \u043a\u043e\u043d\u0446\u0435 \u0440\u0430\u0437\u0431\u0435\u0440\u0435\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u0430\u043d\u043e\u043d\u0438\u0447\u043d\u044b\u0445 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0441\u0442\u0430\u0442\u0435\u0439 \u0438 \u0432\u0441\u043f\u043e\u043c\u043d\u0438\u043c \u0428\u043c\u0438\u0434\u0445\u0443\u0431\u0435\u0440\u0430\n\nC\u0441\u044b\u043b\u043a\u0438 \u043d\u0430 \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u0441\u0442\u0430\u0442\u044c\u0438:\n\nDeep Learning for Real-Time Atari Game Play Using Offline MCTS Planning [https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf]\nMastering Chess and Shogi by Self-Play with a General RL Algorithm [https://arxiv.org/pdf/1712.01815.pdf]\nWorld Models [https://arxiv.org/abs/1803.10122]\nModel-Based Reinforcement Learning for Atari [https://arxiv.org/pdf/1903.00374.pdf]\nLearning Latent Dynamics for Planning from Pixels [https://arxiv.org/pdf/1811.04551]\n\n\u0421\u043b\u0430\u0439\u0434\u044b:\nhttps://docs.google.com/presentation/d/1j1HzFdeqIUelpi5vJUay-yzL8ihERKLfLEdJDJtiQlo/edit#slide=id.g7fa1059486_1_50"}, "contentDetails": {"duration": "PT2H15M45S"}}]}, {"idx_paper": 1869, "q": "arxiv.org/pdf/1902.05522.pdf", "items": [{"id": "06_iBtEeUTc", "statistics": {"likeCount": "30", "favoriteCount": "0", "commentCount": "0", "viewCount": "1769", "dislikeCount": "1"}, "snippet": {"channelTitle": "Carnegie Mellon University Deep Learning", "publishedAt": "2019-03-06T04:41:24Z", "title": "Continual Learning in Neural Networks by Pulkit Agarwal", "description": "This is a guest lecture by Pulkit Agarwal, a Ph.D. student whose current research is on Superposition of Many Models Into One.\n\nCarnegie Mellon University\nCourse: 11-785, Intro to Deep Learning\nOffering: Spring 2019\nSlides: \n\nThe related paper co-authored by Dr. Pulkit Agarwal can be found here: https://arxiv.org/pdf/1902.05522.pdf\nFor more information, please visit: http://deeplearning.cs.cmu.edu/\n\nContent:\n\u2022 Continual Learning \n\u2022 Superposition of Many Models Into One", "channelId": "UC8hYZGEkI2dDO8scT8C5UQA"}, "contentDetails": {"duration": "PT1H13M42S"}}]}, {"idx_paper": 2058, "q": "arxiv.org/pdf/1902.06965.pdf", "items": [{"id": "RY2bEjnNBpY", "statistics": {"likeCount": "6", "favoriteCount": "0", "commentCount": "0", "viewCount": "332", "dislikeCount": "0"}, "snippet": {"channelTitle": "\u0421\u0435\u043c\u0438\u043d\u0430\u0440\u044b \u043f\u043e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c\u0443 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e JetBrains Research", "title": "Positive-Unlabeled Learning", "tags": ["\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0431\u0435\u0437 \u0443\u0447\u0438\u0442\u0435\u043b\u044f"], "channelId": "UCdeSxuESqLOxuuwXNnqqbrA", "publishedAt": "2019-04-09T13:20:17Z", "description": "\u041c\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u043c \u043e \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\u0439 \u043a\u0430\u043a Positive-Unlabeled learning. \u0421\u043f\u0435\u0440\u0432\u0430 \u043e\u0431\u0441\u0443\u0434\u0438\u043c, \u0447\u0442\u043e \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0438\u0437 \u0441\u0435\u0431\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0438 \u0433\u0434\u0435 \u043c\u043e\u0436\u0435\u0442 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0442\u044c\u0441\u044f. \u0417\u0430\u0442\u0435\u043c, \u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044e \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0439 \u043c\u043d\u043e\u0439 \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0439 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b. \u0414\u043b\u044f \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442\u0441\u044f \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f \u043e \u0442\u0435\u043e\u0440\u0438\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438. \n\n\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043f\u0440\u0435\u043f\u0440\u0438\u043d\u0442: https://arxiv.org/pdf/1902.06965.pdf\n\n\u0414\u043e\u043a\u043b\u0430\u0434\u0447\u0438\u043a: \u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u0418\u0432\u0430\u043d\u043e\u0432.\n\n\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0441\u043b\u0430\u0439\u0434\u044b: https://research.jetbrains.org/files/material/5cac988ca384a.pdf"}, "contentDetails": {"duration": "PT51M44S"}}]}, {"idx_paper": 2079, "q": "arxiv.org/pdf/1903.02271.pdf", "items": [{"id": "4VqV31TCsZk", "statistics": {"likeCount": "20", "favoriteCount": "0", "commentCount": "3", "viewCount": "237", "dislikeCount": "0"}, "snippet": {"channelTitle": "Henry AI Labs", "title": "GANs with Fewer Labels", "tags": ["GANs", "High-Fidelity Image Generation With Fewer Labels", "Deep Learning", "Generative Adversarial Networks", "Semi-Supervised Learning", "Self-Supervised Learning", "Machine Learning", "Data Science", "Artificial Intelligence", "Computer Vision", "ImageNet"], "channelId": "UCHB9VepY6kYvZjj0Bgxnpbw", "publishedAt": "2019-07-22T20:43:10Z", "description": "State of the art GAN results using only 20% of the labels! This is a great technique utilizing semi-supervised learning and self-supervised learning to dramatically reduce the need for manual labeling in training Generative Adversarial Networks!\nPlease Subscribe! Thanks for watching!\n\nPaper Link: https://arxiv.org/pdf/1903.02271.pdf\n\"High-Fidelity Image Generation with Fewer Labels\""}, "contentDetails": {"duration": "PT7M4S"}}]}, {"idx_paper": 2192, "q": "arxiv.org/pdf/1902.08160.pdf", "items": [{"id": "Uox2olcefQ8", "statistics": {"likeCount": "14", "favoriteCount": "0", "commentCount": "1", "viewCount": "569", "dislikeCount": "1"}, "snippet": {"channelTitle": "Vipul Vaibhaw", "title": "Understanding how neural network learns via Topology", "tags": ["deep learning", "Machine Learning", "Artificial Intelligence", "Deep Neural Networks", "Neural Networks", "Artificial Neural Networks", "Data Science", "Education", "Research"], "channelId": "UCoDo2nCm0z-OL23whMcuryA", "publishedAt": "2019-02-24T11:39:34Z", "defaultAudioLanguage": "en", "description": "Link to the paper - https://arxiv.org/pdf/1902.08160.pdf\n\nGithub Repo(work in progress) - https://github.com/vaibhawvipul/Topology-of-Learning-in-Artificial-Neural-Networks\n\nFeel free to contribute to Github Repo."}, "contentDetails": {"duration": "PT15M47S"}}]}, {"idx_paper": 2243, "q": "arxiv.org/pdf/1903.03386.pdf", "items": [{"id": "VR0e276VMeI", "statistics": {"likeCount": "2", "favoriteCount": "0", "commentCount": "0", "viewCount": "117", "dislikeCount": "0"}, "snippet": {"channelTitle": "Florian Dubost", "title": "Modeling Spatial Progression of Dementia caused by Alzheimer's disease with Deep Learning", "tags": ["deep learning", "dementia", "alzheimer", "autoencoder", "varitonal autoencoder", "machine learning", "neuroimaging", "artificial intelligence", "https://www.youtube.com/channel/UC_cpXuriimXpQeom8JrqGeg?view_as=subscriber"], "channelId": "UC_cpXuriimXpQeom8JrqGeg", "publishedAt": "2019-08-25T11:51:09Z", "defaultAudioLanguage": "en", "description": "In this video, I show a simulation of the evolution of gray matter density in MRI scans over time in the brain of people who suffer from dementia caused by Alzheimer's disease. We used methods based on convolutional neural network to model the spatial evolution of multiple brain regions separately.  Convolutional neural networks are methods used for data analysis and are especially relevant for image analysis. More precisely, we generated the images used variational auto-encoders. This deep learning simulation shows that the brain regions, such as the hippocampus, the medial and inferior temporal gyri, the amygdala, the anterior temporal lobe, etc, are affected at different degrees. This method can be used to evaluate the performance of event-based models (EBM), a class of disease progression models that can be used to estimate temporal ordering of neuropathological changes from cross-sectional data. This model was optimized using data from 1737 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI). In the article below, we use the simulation method to evaluate a novel method that exploits high-dimensional voxel-wise imaging biomarkers: n-dimensional discriminative EBM. In the future this methodological development could aid improve the early diagnosis of Alzheimer's disease and contribute to a more efficient disease prevention.\n\nFor the international conference IPMI, held in Hong Kong.\n\n\nThe full article is available here: https://arxiv.org/pdf/1903.03386.pdf\nTo get the latest news: https://twitter.com/fpgDubost \nTo see all our publications: https://scholar.google.com/citations?user=_yNBmx8AAAAJ&hl=fr\nPersonal website (under construction): https://floriandubost.com\nLinkedIn: https://www.linkedin.com/in/florian-dubost-11b60a6a/ \nResearchGate: https://www.researchgate.net/profile/Florian_Dubost"}, "contentDetails": {"duration": "PT1M29S"}}]}, {"idx_paper": 2413, "q": "arxiv.org/pdf/1902.09866.pdf", "items": [{"id": "gMC8lZJQSWw", "statistics": {"likeCount": "21", "favoriteCount": "0", "viewCount": "864", "dislikeCount": "0"}, "snippet": {"channelTitle": "PapersWeLove", "title": "Is Program Analysis the Silver Bullet Against Software Bugs? by Karim Ali [PWLConf 2019]", "tags": ["paperswelove", "pwlconf"], "channelId": "UCoj4eQh_dZR37lL78ymC6XA", "publishedAt": "2019-09-19T13:37:19Z", "defaultAudioLanguage": "en", "description": "PWLConf 2019 Link: https://pwlconf.org/2019/karim-ali/\nSlides / Captions: https://github.com/papers-we-love/pwlconf-info/tree/master/2019/karim-ali\n\nIs Program Analysis the Silver Bullet Against Software Bugs?\nKarim Ali, Assistant Professor of Computing Science at the University of Alberta\n\nProgram analysis is the art of reasoning about the run-time behavior of a program without necessarily executing it. This information is useful for various real-life applications such as supporting software developers (e.g., bug-finding tools, code refactoring tools, and code recommenders) and compiler optimizations. Program analysis is also used to ensure complex software adheres to standards and regulations (e.g., medical devices, car industry, and aviation industry).\n\nIn this talk, I will discuss the three main properties that enable program analyses to be useful in practice: scalability, precision, and usability. I will relate that to various papers that have been published in the field of program analysis, as well as some of the work that my group has done. I will conclude with where I see program analysis research going and the challenges that we aim to solve in the field. \n\nReferences\n------------------\n\nStrictly declarative specification of sophisticated points-to analyses by Martin Bravenboer and Yannis Smaragdakis (https://people.cs.umass.edu/~yannis/doop-oopsla09prelim.pdf)\n\nDimensions of Precision in Reference Analysis of Object-Oriented Programming Languages by Barbara G. Ryder (http://prolangs.cs.vt.edu/refs/docs/cc03.pdf)\n\nAverroes: Whole-Program Analysis without the Whole Program by Karim Ali and Ond\u0159ej Lhot\u00e1k (https://karimali.ca/resources/papers/averroes.pdf)\n\nFlowDroid: Precise Context, Flow, Field, Object-Sensitive and Lifecycle-Aware Taint Analysis for Android Apps by Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bartel, Jacques Klein, Yves Le Traon, Damien Octeau, and Patrick McDaniel (https://orbilu.uni.lu/bitstream/1099)\n\nPrecise Interprocedural Dataflow Analysis with Applications to Constant Propagation by Shmuel Sagiv, Thomas W. Reps, and Susan Horwitz (https://link.springer.com/content/pdf/10.1007/3-540-59293-8_226.pdf)\n\nContext-, Flow-, and Field-Sensitive Data-Flow Analysis using Synchronized Pushdown Systems by Johannes Sp\u00e4th, Karim Ali, and Eric Bodden (https://karimali.ca/resources/papers/spds.pdf)\n\nDebugging Static Analysis by Lisa Nguyen Quang Do, Stefan Kr\u00fcger, Patrick Hill, Karim Ali, and Eric Bodden (https://karimali.ca/resources/papers/visuflow.pdf)\n\nTowards Better Inlining Decisions Using Inlining Trials by Jeffrey Dean and Craig Chamber (https://courses.cs.washington.edu/courses/cse501/06wi/reading/dean-lfp94.pdf)\n\nReluplex: An Efficient SMT Solver for Verifying Deep Neural Networks by Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer (https://theory.stanford.edu/~barrett/pubs/KBD+17.pdf)\n\nAnalyzing Deep Neural Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification by Pengfei Yang, Jiangchao Liu, Jianlin Li, Liqian Chen, and Xiaowei Huang (https://arxiv.org/abs/1902.09866)\n\nSoot (https://github.com/Sable/soot)\n\nIBM. T.J. Watson Libraries for Analysis WALA (http://wala.sourceforge.net/)\n\nBiography\n----------------\n\nKarim Ali is an Assistant Professor in the Department of Computing Science at the University of Alberta. His research interests are in programming languages and software engineering, particularly in scalability, precision, and usability of program analysis tools. His work ranges from developing new theories for scalable and precise program analyses to applications of program analysis in security and Just-in-Time compilers.\n\nKarim obtained his PhD degree from the University of Waterloo in 2014, and was a postdoctoral research at Technische Universit\u00e4t Darmstadt (TU Darmstadt) 2014\u20132016.\n\n    Twitter https://twitter.com/karimhamdanali\n    Site: https://karimali.ca/\n    DBLP: https://dblp.org/pers/hd/a/Ali_0001:Karim\n\n-----------------------------------------------------------------------------------------------------------\nVideo Sponsor: Comcast (https://jobs.comcast.com/)\nCaptioning Sponsor: Two Sigma (https://www.twosigma.com/careers/)\n-----------------------------------------------------------------------------------------------------------"}, "contentDetails": {"duration": "PT50M39S"}}]}, {"idx_paper": 2646, "q": "arxiv.org/pdf/1903.08297.pdf", "items": [{"id": "wC8V29S3roY", "statistics": {"likeCount": "0", "favoriteCount": "0", "commentCount": "0", "viewCount": "5", "dislikeCount": "0"}, "snippet": {"channelTitle": "\u8a31\u52dd\u9023", "publishedAt": "2020-01-09T10:12:29Z", "title": "\u985e\u795e\u7d93\u7db2\u8def\u671f\u672b\u5831\u544a", "description": "\u8ad6\u6587:https://arxiv.org/pdf/1903.08297.pdf", "channelId": "UCFJ6bCPOJsBjzd0lLQeoTbA"}, "contentDetails": {"duration": "PT12M38S"}}]}]