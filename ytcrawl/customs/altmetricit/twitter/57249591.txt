{"tab": "twitter", "completed": "1", "twitter": {"1107816653038014464": {"author": "@arXiv__ml", "followers": "1,689", "datetime": "2019-03-19 01:30:46", "content_summary": "#arXiv #machinelearning [cs.LG] Training Over-parameterized Deep ResNet Is almost as Easy as Training a Two-layer Network. (arXiv:1903.07120v1 [cs.LG]) https://t.co/C1STA5V3hq It has been proved that gradient descent converges linearly to the global minim"}, "1107917658446417920": {"author": "@ComputerPapers", "followers": "613", "datetime": "2019-03-19 08:12:08", "content_summary": "Training Over-parameterized Deep ResNet Is almost as Easy as Training a Two-layer Network. https://t.co/HeNzYGxBCv"}, "1107812860317569026": {"author": "@StatMLPapers", "followers": "9,658", "datetime": "2019-03-19 01:15:42", "content_summary": "Training Over-parameterized Deep ResNet Is almost as Easy as Training a Two-layer Network. (arXiv:1903.07120v1 [cs.LG]) https://t.co/k18uaVf2bT"}, "1125799881522728963": {"author": "@unsorsodicorda", "followers": "734", "datetime": "2019-05-07 16:29:42", "content_summary": "This seems interesting and apparently I missed it: https://t.co/SgsPBjLQBA extends @zhuzeyuan et al. results to ResNets. Cc @TheGregYang @prfsanjeevarora"}, "1107819282182914048": {"author": "@deep_rl", "followers": "857", "datetime": "2019-03-19 01:41:13", "content_summary": "Training Over-parameterized Deep ResNet Is almost as Easy as Training a Two-layer Network - Huishuai Zhang https://t.co/wPj4Wv7f7v"}, "1107922621960732678": {"author": "@arxivml", "followers": "768", "datetime": "2019-03-19 08:31:51", "content_summary": "\"Training Over-parameterized Deep ResNet Is almost as Easy as Training a Two-layer Network\", Huishuai Zhang, Da Yu,\u2026 https://t.co/ohUd6vNg7G"}}, "citation_id": "57249591", "queriedAt": "2020-06-04 00:52:32"}