{"citation_id": "3527586", "completed": "1", "queriedAt": "2020-05-14 12:37:00", "tab": "twitter", "twitter": {"565713952131526661": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "334", "datetime": "2015-02-12 03:28:11", "author": "@nel215"}, "1171994221789810688": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "159", "datetime": "2019-09-12 03:49:31", "author": "@matasramon"}, "565964266407493632": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "739", "datetime": "2015-02-12 20:02:50", "author": "@sandmangil"}, "1172163089753493510": {"content_summary": "Really great thread on \"why batch-norm\" works/speeds up training for CNNs. Write-up to follow soon.", "followers": "114", "datetime": "2019-09-12 15:00:32", "author": "@joungMax"}, "1172250043111346190": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "168", "datetime": "2019-09-12 20:46:04", "author": "@dr_levan"}, "1056015369800835073": {"content_summary": "@Peter_shirley There was a recent paper that introduced a batch norm layer to account for this https://t.co/lr7b6sDMGX", "followers": "268", "datetime": "2018-10-27 02:50:58", "author": "@n0madsky"}, "1172296786742001665": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "140", "datetime": "2019-09-12 23:51:48", "author": "@ferpineda"}, "1172257919322198022": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "7", "datetime": "2019-09-12 21:17:22", "author": "@LYTiQ_ai"}, "1171883893089296384": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "75", "datetime": "2019-09-11 20:31:07", "author": "@cutearguments"}, "566386519766614017": {"content_summary": "[toread] untitled: http://t.co/qOfK8PoZ8M", "followers": "21", "datetime": "2015-02-14 00:00:43", "author": "@pmn_"}, "1171889326004822023": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "904", "datetime": "2019-09-11 20:52:42", "author": "@Mentalo"}, "1171975818983067649": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "282", "datetime": "2019-09-12 02:36:24", "author": "@Ar_Douillard"}, "1172029820814811136": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "115", "datetime": "2019-09-12 06:10:59", "author": "@viktor_m81"}, "1171932748908396544": {"content_summary": "A must for thread all DL practitioners. One more reason to love this community.", "followers": "46", "datetime": "2019-09-11 23:45:15", "author": "@5sigma"}, "1172210552036311041": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "14", "datetime": "2019-09-12 18:09:08", "author": "@BLJ11232689"}, "1172380738227855360": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "215", "datetime": "2019-09-13 05:25:24", "author": "@carlfm01"}, "1172062276754395138": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "169", "datetime": "2019-09-12 08:19:57", "author": "@m_iqbal2307"}, "1171971000470102016": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "392", "datetime": "2019-09-12 02:17:15", "author": "@nachiketiyengar"}, "1172264550692990981": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "2,467", "datetime": "2019-09-12 21:43:43", "author": "@sindero"}, "1172454813369397249": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "303", "datetime": "2019-09-13 10:19:45", "author": "@subhobrata1"}, "1172175453571862528": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "1,063", "datetime": "2019-09-12 15:49:40", "author": "@miguelalonsojr"}, "566388331693764608": {"content_summary": "RT newsycombinator: Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/KXyC1cUEjx", "followers": "379", "datetime": "2015-02-14 00:07:55", "author": "@afiqio"}, "567747924856422400": {"content_summary": "@googleresearch beat humans experts on #imagenet challenge http://t.co/FHMB8iX9l4 #MachineLearning,#bigdata, #deeplearning, #computervision", "followers": "105", "datetime": "2015-02-17 18:10:28", "author": "@Data88Geek"}, "1171955454488064000": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "161", "datetime": "2019-09-12 01:15:28", "author": "@rajarishis"}, "1171999942476361728": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "19", "datetime": "2019-09-12 04:12:15", "author": "@manishankar_v"}, "566129731108491264": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "205", "datetime": "2015-02-13 07:00:20", "author": "@icrtiou"}, "1172244713551912961": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "647", "datetime": "2019-09-12 20:24:53", "author": "@zzprosper"}, "1224937276435656705": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "41", "datetime": "2020-02-05 06:06:19", "author": "@Kkushaj"}, "761928564610654208": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/wxiUfiOqJn", "followers": "1,215", "datetime": "2016-08-06 14:15:00", "author": "@Data4Bots"}, "565699675496652801": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "1,711", "datetime": "2015-02-12 02:31:27", "author": "@Keiku"}, "1172085914606002176": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "50", "datetime": "2019-09-12 09:53:53", "author": "@Dmitry77162374"}, "1172113780165218305": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "295", "datetime": "2019-09-12 11:44:36", "author": "@ahsante7"}, "767364780227047424": {"content_summary": "Batch Normalization, much more efficient learning with normalization that reduces internal covariate shift. https://t.co/cKM99kfnWW", "followers": "428", "datetime": "2016-08-21 14:16:35", "author": "@kamonama"}, "1172163151057408000": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "5,073", "datetime": "2019-09-12 15:00:47", "author": "@ankurhandos"}, "1172158784233512960": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "993", "datetime": "2019-09-12 14:43:26", "author": "@rzeta0"}, "1172362652540006400": {"content_summary": "RT @fdellaert: Wondering about batch norm? Read this amazing thread. https://t.co/y5oXJNBXsq", "followers": "822", "datetime": "2019-09-13 04:13:32", "author": "@JaredHeinly"}, "1171898981032611848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "27", "datetime": "2019-09-11 21:31:04", "author": "@TMVector"}, "1171895166933905413": {"content_summary": "Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time of its publication", "followers": "4,418", "datetime": "2019-09-11 21:15:55", "author": "@abursuc"}, "1171967291820408833": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "149", "datetime": "2019-09-12 02:02:31", "author": "@krunal_wrote"}, "1172173071148445697": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "105", "datetime": "2019-09-12 15:40:12", "author": "@alfo_512"}, "1177313609258590208": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "255", "datetime": "2019-09-26 20:06:52", "author": "@christian_hudon"}, "565696982300524545": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "840", "datetime": "2015-02-12 02:20:45", "author": "@warrenhenning"}, "565757322140463105": {"content_summary": "New state of the art on image classification is 10x faster than before and more accurate than well-trained humans. http://t.co/Ni66gaAEFO", "followers": "691", "datetime": "2015-02-12 06:20:31", "author": "@olcan"}, "565924645719642112": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "395", "datetime": "2015-02-12 17:25:24", "author": "@robotbugs"}, "1171878752986574848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "37", "datetime": "2019-09-11 20:10:41", "author": "@FrancoisPlesse"}, "565742188827398145": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "625", "datetime": "2015-02-12 05:20:23", "author": "@nova77t"}, "1172424588145225729": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,023", "datetime": "2019-09-13 08:19:39", "author": "@desertnaut"}, "1171929081673183232": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "52", "datetime": "2019-09-11 23:30:41", "author": "@earny_joe"}, "581473386943156224": {"content_summary": "Batch Normalization #BN http://t.co/kB746kQQlS", "followers": "473", "datetime": "2015-03-27 15:10:33", "author": "@Udibr"}, "1172107817051930625": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2,614", "datetime": "2019-09-12 11:20:54", "author": "@gar1t"}, "565852190057984000": {"content_summary": "V pond\u011bl\u00ed um\u011bl Microsoft rozpozn\u00e1vat obr\u00e1zky l\u00e9pe ne\u017e \u010dlov\u011bk, dnes to oznamuje i Google: http://t.co/eoOIAchgb4 (4,8%, lid\u00e9 maj\u00ed 5,1%)", "followers": "45", "datetime": "2015-02-12 12:37:29", "author": "@milosjanda"}, "1172106968678273024": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "0", "datetime": "2019-09-12 11:17:32", "author": "@shekharml"}, "1172616880588124161": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "41", "datetime": "2019-09-13 21:03:45", "author": "@aiton5"}, "1173329942098018304": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "135", "datetime": "2019-09-15 20:17:12", "author": "@raghavgoyal14"}, "1172027447526793216": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "44", "datetime": "2019-09-12 06:01:33", "author": "@MLAIwithPuneet"}, "1172165219914641411": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "25", "datetime": "2019-09-12 15:09:00", "author": "@DeepHindsight"}, "1172293579798020096": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "825", "datetime": "2019-09-12 23:39:04", "author": "@morioka"}, "1172229887916044289": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "16", "datetime": "2019-09-12 19:25:58", "author": "@hitesh_golchha"}, "1171919289928036353": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "200", "datetime": "2019-09-11 22:51:46", "author": "@mewopean"}, "1173521986888318976": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "40", "datetime": "2019-09-16 09:00:19", "author": "@thegreatfubini"}, "566393317697400832": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/RZbm4CfXUB (cmts http://t.co/GBaMlBdqNb)", "followers": "172", "datetime": "2015-02-14 00:27:44", "author": "@_uranium_"}, "1171947060477747200": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "818", "datetime": "2019-09-12 00:42:07", "author": "@deepgradient"}, "1172269351782821889": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "204", "datetime": "2019-09-12 22:02:47", "author": "@tritemio_sc"}, "1171925035549954048": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "99", "datetime": "2019-09-11 23:14:36", "author": "@m_khurram_amin"}, "1173221998006329344": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "963", "datetime": "2019-09-15 13:08:16", "author": "@FlorianWilhelm"}, "565707647761252354": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "469", "datetime": "2015-02-12 03:03:08", "author": "@sotetsuk"}, "1056266167269113857": {"content_summary": "BatchNorm: https://t.co/gPtN2EcKcS Reduce Internal Covariate Shift, Faster training, Enable higher Learning Rates, Jacobian closer to identity matrix, Model Regularization, Linear transformation for learning identity, Improve gradient propagation #OneMLPap", "followers": "51", "datetime": "2018-10-27 19:27:33", "author": "@antbrl"}, "565889288496480256": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "77", "datetime": "2015-02-12 15:04:54", "author": "@Miguel_Molero"}, "1172029455658868736": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "139", "datetime": "2019-09-12 06:09:32", "author": "@mmeierer"}, "1172022941334327296": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "10,021", "datetime": "2019-09-12 05:43:39", "author": "@prrgutierrez"}, "565773145529786368": {"content_summary": "[\u306f\u3066\u30d6\uff1aIT] [1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/VM6heUncxP", "followers": "348", "datetime": "2015-02-12 07:23:24", "author": "@wahoonews"}, "1172233126095007747": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "29", "datetime": "2019-09-12 19:38:50", "author": "@hassaanrafique"}, "1172183800966762496": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "650", "datetime": "2019-09-12 16:22:50", "author": "@timwee"}, "566391597906923520": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/oiXdT961Qi", "followers": "276", "datetime": "2015-02-14 00:20:54", "author": "@bartezzini"}, "1237144152812523522": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "36", "datetime": "2020-03-09 22:32:05", "author": "@causal_mind"}, "566455506407919619": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "770", "datetime": "2015-02-14 04:34:51", "author": "@ivanfzy"}, "565687548878802944": {"content_summary": "A new approach to training gives a big boost to deep vision networks - http://t.co/0TD2Y4DvQh - 4.9% is 0.96 @Karpathy's, to coin a new unit", "followers": "13,014", "datetime": "2015-02-12 01:43:16", "author": "@ThugMetricsNews"}, "565839445564592129": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "573", "datetime": "2015-02-12 11:46:51", "author": "@lelayf"}, "1172138334715568129": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "271", "datetime": "2019-09-12 13:22:10", "author": "@bysd_a"}, "565831071422750720": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "161", "datetime": "2015-02-12 11:13:34", "author": "@AloisGruson"}, "566384431380697088": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/UyZvPBfPSn", "followers": "1,983", "datetime": "2015-02-13 23:52:26", "author": "@kristijohnson69"}, "1171908378836135936": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "222", "datetime": "2019-09-11 22:08:25", "author": "@deepak_s_rawat"}, "1172165243767861248": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "133", "datetime": "2019-09-12 15:09:06", "author": "@theo_matussiere"}, "565714739289137152": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "18,231", "datetime": "2015-02-12 03:31:18", "author": "@hillbig"}, "1124485212916740097": {"content_summary": "RT @hackerfriendly: TIL that some ML researchers had an idea (batch normalization) that worked, then tacked on a post-hoc mathematical anal\u2026", "followers": "6,193", "datetime": "2019-05-04 01:25:41", "author": "@kscottz"}, "1173247328112861185": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "4", "datetime": "2019-09-15 14:48:55", "author": "@haznaitak"}, "1082057872521994240": {"content_summary": "RT @parker_brydon: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Sergey Ioffe, Christian Sze\u2026", "followers": "75", "datetime": "2019-01-06 23:34:35", "author": "@_Artemisa_v"}, "1172114577875722240": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "595", "datetime": "2019-09-12 11:47:46", "author": "@popedaniels"}, "565716504227086337": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "5,890", "datetime": "2015-02-12 03:38:19", "author": "@startupml"}, "1172305854592208896": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "46", "datetime": "2019-09-13 00:27:50", "author": "@XiangmingMeng"}, "1229924858789654528": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "23", "datetime": "2020-02-19 00:25:11", "author": "@SiaAhmadi1"}, "680328632947048448": {"content_summary": "Batch normalization to speed deep learning training https://t.co/kmOJnntVPw", "followers": "1,348", "datetime": "2015-12-25 10:06:00", "author": "@Lidinwise"}, "1127390897925005312": {"content_summary": "Batch Normalization [Ioffe+, 2015, ICML] \u30df\u30cb\u30d0\u30c3\u30c1\u5b66\u7fd2\u306b\u304a\u3044\u3066\u5404\u5c64\u306e\u6d3b\u6027\u5316\u524d\u306b\u30d0\u30c3\u30c1\u65b9\u5411\u306e\u6b63\u898f\u5316\u3068\u30a2\u30d5\u30a3\u30f3\u5909\u63db\u3092\u304b\u3051\u308b\u3053\u3068\u3067\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u8efd\u6e1b\u3057\u3001\u3088\u308a\u9ad8\u3044\u5b66\u7fd2\u7387\u304c\u63a1\u7528\u53ef\u80fd\u306b\u3002\u6b63\u5247\u5316\u306e\u52b9\u679c\u3082\u78ba\u8a8d\u3067\u304d\u305f\u3002\u5f93\u6765\u306e\u69cb\u9020\u306b\u7d44\u307f\u8fbc\u3080\u3060\u3051\u3067\u6a5f\u80fd\u3059\u308b\u3002 https://t.co/3lQRyz1NmI #NowReading https://t.co/3BzPeAxzzy", "followers": "2,478", "datetime": "2019-05-12 01:51:50", "author": "@shion_honda"}, "566386072213790721": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/DE8HDi736o", "followers": "11,131", "datetime": "2015-02-13 23:58:57", "author": "@pwebmedia"}, "1171898195913445376": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "29", "datetime": "2019-09-11 21:27:57", "author": "@yass_ouali"}, "566393536736927744": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/RdwEDSpwjq", "followers": "74", "datetime": "2015-02-14 00:28:36", "author": "@kaahdar"}, "1172203317994360832": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "885", "datetime": "2019-09-12 17:40:24", "author": "@RichmanRonald"}, "565705323546443776": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "1,307", "datetime": "2015-02-12 02:53:54", "author": "@dna_nerd"}, "566385941275619328": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/R8VtclrZtx #hackernews #feedly", "followers": "74", "datetime": "2015-02-13 23:58:25", "author": "@Yun_kaax"}, "1172032125685653505": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "218", "datetime": "2019-09-12 06:20:08", "author": "@maheshkkreddy"}, "1172200668624556035": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "441", "datetime": "2019-09-12 17:29:52", "author": "@Bouh___"}, "578191128861777920": {"content_summary": "A recent paper on DNNs which shows that a technique called Batch Normalization dramatically decreases training time. http://t.co/mBvQSP3tl6", "followers": "57", "datetime": "2015-03-18 13:48:02", "author": "@ZobreusPatrick"}, "1172092966736945152": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "208", "datetime": "2019-09-12 10:21:54", "author": "@Don_Rubiel"}, "566669674403098624": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "335", "datetime": "2015-02-14 18:45:53", "author": "@gh0std4ncer"}, "1172010322254815233": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "277", "datetime": "2019-09-12 04:53:30", "author": "@ZimMatthias"}, "1172353592885297152": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "883", "datetime": "2019-09-13 03:37:32", "author": "@yoshi_and_aki"}, "1172491174227189761": {"content_summary": "RT jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about training mechanics by studying this thread and the links it contains. https://t.co/DfIUQ9JEMy", "followers": "76", "datetime": "2019-09-13 12:44:14", "author": "@software_tic"}, "565830625479753729": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "227", "datetime": "2015-02-12 11:11:48", "author": "@HowIMetYourData"}, "565920921383211008": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "23,517", "datetime": "2015-02-12 17:10:36", "author": "@johnplattml"}, "1172057543935311872": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by R https://t.co/RtFjLrJiOw (Popularity:60.0) #Machine_Learning", "followers": "53", "datetime": "2019-09-12 08:01:08", "author": "@PoQaa_cs"}, "1172169858030690306": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "913", "datetime": "2019-09-12 15:27:26", "author": "@AndreaBanino"}, "566392305343426563": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/EsYb3pKdtr", "followers": "377", "datetime": "2015-02-14 00:23:43", "author": "@CyberMithun"}, "1172316947775508480": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "62", "datetime": "2019-09-13 01:11:55", "author": "@iyucao"}, "1223336237596938240": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "99,904", "datetime": "2020-01-31 20:04:22", "author": "@jeremyphoward"}, "1158166316416733185": {"content_summary": "@jerofad Mine here: https://t.co/8gyAqEiC1s", "followers": "2,911", "datetime": "2019-08-05 00:02:22", "author": "@WaleAkinfaderin"}, "1172118705460920320": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "230", "datetime": "2019-09-12 12:04:10", "author": "@kimbustion"}, "1171966274538102784": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "32", "datetime": "2019-09-12 01:58:28", "author": "@Amplituhedr0n"}, "963087870809567233": {"content_summary": "Thank gods (and Ioffe, and Szegedy) for batch normalization https://t.co/NRjkxFHLGw", "followers": "97", "datetime": "2018-02-12 16:30:16", "author": "@ideaferace"}, "1171971319316930560": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "119", "datetime": "2019-09-12 02:18:31", "author": "@mlai_"}, "565927624678318081": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "665", "datetime": "2015-02-12 17:37:14", "author": "@adelong"}, "1172024324959526912": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "180", "datetime": "2019-09-12 05:49:08", "author": "@sriharshams"}, "1172399997167390720": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "26", "datetime": "2019-09-13 06:41:56", "author": "@ravi220644"}, "1172002054757343232": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "263", "datetime": "2019-09-12 04:20:39", "author": "@raphaelmeudec"}, "1172503606093705216": {"content_summary": "RT @fdellaert: Wondering about batch norm? Read this amazing thread. https://t.co/y5oXJNBXsq", "followers": "14", "datetime": "2019-09-13 13:33:38", "author": "@Gatsby_23"}, "1171874769706409984": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "484", "datetime": "2019-09-11 19:54:52", "author": "@felbalazard"}, "1171978102815084544": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "626", "datetime": "2019-09-12 02:45:28", "author": "@alisher_ai"}, "1171951321467015173": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "566", "datetime": "2019-09-12 00:59:03", "author": "@dmn001"}, "565847977906814976": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "71", "datetime": "2015-02-12 12:20:45", "author": "@HwanyJung"}, "1172015139056508928": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "89", "datetime": "2019-09-12 05:12:38", "author": "@mgrankin"}, "1172004385095856129": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "316", "datetime": "2019-09-12 04:29:54", "author": "@ikeondata"}, "1185534313598722049": {"content_summary": "RT @_brohrer_: @dcpage3 offers keen insights into how batch norm works. I particularly resonated with his meta-conclusions: 1) When studyin\u2026", "followers": "588", "datetime": "2019-10-19 12:33:01", "author": "@flavioclesio"}, "1172291264215052288": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "28", "datetime": "2019-09-12 23:29:52", "author": "@jai00271"}, "1172309382694133760": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "602", "datetime": "2019-09-13 00:41:51", "author": "@fjsosah"}, "1172201708426076160": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "55", "datetime": "2019-09-12 17:34:00", "author": "@LReinel"}, "624752427997138949": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "2,927", "datetime": "2015-07-25 01:26:00", "author": "@evolvingstuff"}, "1172196051421413379": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "316", "datetime": "2019-09-12 17:11:31", "author": "@jehosafet"}, "565717647988645888": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "68", "datetime": "2015-02-12 03:42:52", "author": "@rytido"}, "566383989733474304": {"content_summary": "Interesting paper on improved deep network training by Google http://t.co/6mkrDKk65q", "followers": "1,883", "datetime": "2015-02-13 23:50:40", "author": "@ithilgore"}, "565908745377431552": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (pdf) | http://t.co/eOpN0lMIAY", "followers": "10,439", "datetime": "2015-02-12 16:22:13", "author": "@YhatHQ"}, "566380642192338944": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/RZbm4CfXUB (cmts http://t.co/GBaMlBdqNb)", "followers": "23,238", "datetime": "2015-02-13 23:37:22", "author": "@newsycbot"}, "1171919980079665152": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1", "datetime": "2019-09-11 22:54:31", "author": "@ZhiWeiLee3"}, "566379640001142784": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/oCBuqHJMUl #business #startups", "followers": "6,246", "datetime": "2015-02-13 23:33:23", "author": "@StartUpTool"}, "1172146920246013955": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "615", "datetime": "2019-09-12 13:56:17", "author": "@KouroshMeshgi"}, "1172026608481382401": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "39", "datetime": "2019-09-12 05:58:13", "author": "@blue_kiji"}, "1172017019694112768": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2", "datetime": "2019-09-12 05:20:07", "author": "@_kalngyk"}, "1172186387770159104": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "674", "datetime": "2019-09-12 16:33:07", "author": "@marcusabrubaker"}, "759371600428802048": {"content_summary": "https://t.co/r6DPxVoSfd", "followers": "95", "datetime": "2016-07-30 12:54:32", "author": "@seiyab_"}, "1171873849908060160": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "15", "datetime": "2019-09-11 19:51:12", "author": "@darelimit"}, "1172254537517867008": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "264", "datetime": "2019-09-12 21:03:55", "author": "@mustafa_alghali"}, "1171875326797983744": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "45", "datetime": "2019-09-11 19:57:04", "author": "@RamiroJC_83"}, "566478103577628673": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "105", "datetime": "2015-02-14 06:04:39", "author": "@svenpilz"}, "1172072154029912066": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "655", "datetime": "2019-09-12 08:59:12", "author": "@ruiwang2uiuc"}, "1174116667544096768": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "609", "datetime": "2019-09-18 00:23:22", "author": "@2prime_PKU"}, "565791263736487937": {"content_summary": "Ensemble of batch-normalized #deeplearning networks http://t.co/nvrdzbKz0l improves accuracy of image recognition and reduces time 10 fold.", "followers": "934", "datetime": "2015-02-12 08:35:23", "author": "@pavelkordik"}, "1172094964521676800": {"content_summary": "@iugoaoj Hello, there is your unroll: Thread by @dcpage3: \"The paper that introduced Batch Norm https://t.co/ybxv8LzEMy combines clear intuition with compelling experiments (14x [\u2026]\" https://t.co/2q3jQMeVyP Enjoy :) \ud83e\udd16", "followers": "321,539", "datetime": "2019-09-12 10:29:50", "author": "@threadreaderapp"}, "1171943432144732163": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "23,866", "datetime": "2019-09-12 00:27:42", "author": "@kchonyc"}, "566392299488165888": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/MwAvmPvG9s", "followers": "178", "datetime": "2015-02-14 00:23:41", "author": "@AWLITF"}, "1172130344838918146": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1,134", "datetime": "2019-09-12 12:50:25", "author": "@arunprakashml"}, "1171904328971378688": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "281", "datetime": "2019-09-11 21:52:19", "author": "@manideep2510"}, "1172391909400014848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "11", "datetime": "2019-09-13 06:09:47", "author": "@AbhayGa22429023"}, "1172068317739270146": {"content_summary": "@johannes_laute Hola you can read it here: Thread by @dcpage3: \"The paper that introduced Batch Norm https://t.co/ybxv8LzEMy combines clear intuition with compelling experiments (14x [\u2026]\" https://t.co/2q3jQMeVyP See you soon. \ud83e\udd16", "followers": "321,539", "datetime": "2019-09-12 08:43:57", "author": "@threadreaderapp"}, "1172268041545146371": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "4,890", "datetime": "2019-09-12 21:57:35", "author": "@IgorCarron"}, "1172504076367605760": {"content_summary": "@karthikabinav Thoughts?", "followers": "12", "datetime": "2019-09-13 13:35:30", "author": "@manasijv"}, "1171963621909204992": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3", "datetime": "2019-09-12 01:47:56", "author": "@jujy361"}, "1171903843434749952": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "347", "datetime": "2019-09-11 21:50:23", "author": "@madelain777"}, "1172092168862949378": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "147", "datetime": "2019-09-12 10:18:44", "author": "@JPSmithNL"}, "1172390687997775873": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "61", "datetime": "2019-09-13 06:04:56", "author": "@ChristosVotskos"}, "1171924174861950977": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "118", "datetime": "2019-09-11 23:11:11", "author": "@matthewopala"}, "1171986956814733314": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "86", "datetime": "2019-09-12 03:20:39", "author": "@piyushkp27"}, "1172474178873843713": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "157", "datetime": "2019-09-13 11:36:42", "author": "@data_pink"}, "1172147091264524290": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,663", "datetime": "2019-09-12 13:56:58", "author": "@sigitpurnomo"}, "1172597031359197185": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "6,125", "datetime": "2019-09-13 19:44:52", "author": "@Roger_M_Taylor"}, "1172029087931621379": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,370", "datetime": "2019-09-12 06:08:04", "author": "@Pimp_Fada"}, "1171928639727951874": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "311", "datetime": "2019-09-11 23:28:55", "author": "@jbarriosx"}, "1172656520288440321": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "3", "datetime": "2019-09-13 23:41:15", "author": "@snowdirt7"}, "566045555986427904": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "229", "datetime": "2015-02-13 01:25:51", "author": "@ochiai_tetu"}, "1172511702006620160": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "29", "datetime": "2019-09-13 14:05:48", "author": "@nikhilt_1997"}, "566396912765378560": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/U6w7klOosy \u2026", "followers": "314", "datetime": "2015-02-14 00:42:01", "author": "@255_255_255_255"}, "566384434857791488": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/DYQzh2DFqd", "followers": "1,347", "datetime": "2015-02-13 23:52:26", "author": "@fnkey"}, "1171945748453150721": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "17,768", "datetime": "2019-09-12 00:36:54", "author": "@ericjang11"}, "1172027572164579328": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "158", "datetime": "2019-09-12 06:02:03", "author": "@ArpitJ_"}, "566387945364799489": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "1,838", "datetime": "2015-02-14 00:06:23", "author": "@kragen"}, "570034443622043649": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "4,403", "datetime": "2015-02-24 01:36:16", "author": "@mrkn"}, "565858619309428736": {"content_summary": "Train ConvNets 14x faster with the same accuracy by Reducing Internal Covariate Shift http://t.co/XNPNhZPXMu #Pubs", "followers": "42,874", "datetime": "2015-02-12 13:03:02", "author": "@DeepLearningHub"}, "566746644914241536": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/1vHxF0pLoK", "followers": "197", "datetime": "2015-02-14 23:51:44", "author": "@icymihn"}, "565736752858423296": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "5,104", "datetime": "2015-02-12 04:58:47", "author": "@smly"}, "1171972957326827520": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "39", "datetime": "2019-09-12 02:25:01", "author": "@AnandkrishAK"}, "1172187779024842752": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "13", "datetime": "2019-09-12 16:38:39", "author": "@4krageoverlord"}, "1171913025072193536": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "109", "datetime": "2019-09-11 22:26:52", "author": "@ID9112150632304"}, "566617450939445250": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Ioffe & Szegedy http://t.co/MJUUJ6Vi9e", "followers": "4,336", "datetime": "2015-02-14 15:18:22", "author": "@gchrupala"}, "624806716731998208": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "28,418", "datetime": "2015-07-25 05:01:44", "author": "@ogrisel"}, "1172015324893343744": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "60", "datetime": "2019-09-12 05:13:23", "author": "@SanghyukChun"}, "565803074107629568": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "825", "datetime": "2015-02-12 09:22:19", "author": "@fly51fly"}, "1172094836847104000": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "365", "datetime": "2019-09-12 10:29:20", "author": "@iugoaoj"}, "566383849928941568": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/uhZh446Bx8", "followers": "332", "datetime": "2015-02-13 23:50:07", "author": "@davidcamachoj"}, "1172157868952477696": {"content_summary": "Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyVjAMd", "followers": "194,979", "datetime": "2019-09-12 14:39:48", "author": "@ylecun"}, "1172013610371026945": {"content_summary": "A great thread recollecting the past and present work related to batch normalisation, and why and how it works.", "followers": "50", "datetime": "2019-09-12 05:06:34", "author": "@TommyLofstedt"}, "1172066952027791361": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "103", "datetime": "2019-09-12 08:38:31", "author": "@urosn"}, "1171906363544539136": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "137", "datetime": "2019-09-11 22:00:24", "author": "@ralphbrooks"}, "566443179893731328": {"content_summary": "batch normalization http://t.co/sLehTuCSBF", "followers": "640", "datetime": "2015-02-14 03:45:52", "author": "@akkikiki"}, "1172064404776468481": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "415", "datetime": "2019-09-12 08:28:24", "author": "@__nggih"}, "565707548930891778": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "506", "datetime": "2015-02-12 03:02:44", "author": "@cocomoff"}, "566197845435637760": {"content_summary": "Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/eE5W3juQjF #DeepLearning #datascience", "followers": "336", "datetime": "2015-02-13 11:31:00", "author": "@WhiteRavenPL"}, "1172392947188322305": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "34", "datetime": "2019-09-13 06:13:55", "author": "@MatRazor"}, "566436951234859008": {"content_summary": "\u5f8c\u3067\u8aad\u3080 - Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/8BOTilI6LK", "followers": "1,297", "datetime": "2015-02-14 03:21:07", "author": "@nozaq"}, "1175018127442288651": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "4,865", "datetime": "2019-09-20 12:05:27", "author": "@SwarmTogether"}, "566454426916110336": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/PRSdPixPyG via @newsycombinator", "followers": "606", "datetime": "2015-02-14 04:30:34", "author": "@UNLMTD1"}, "565868677799874560": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "7,210", "datetime": "2015-02-12 13:43:00", "author": "@fastml_extra"}, "709688184385478656": {"content_summary": "RT @mjhirn: @itchyankles @txustice In Deep Learning this is called Batch Normalization. 14x learning speed up https://t.co/Yr4R6j8HCv", "followers": "2,346", "datetime": "2016-03-15 10:30:23", "author": "@itchyankles"}, "1171946608541306880": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,670", "datetime": "2019-09-12 00:40:19", "author": "@ayirpelle"}, "1172499732175785984": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,405", "datetime": "2019-09-13 13:18:14", "author": "@testdrivenio"}, "1172336152675639296": {"content_summary": "RT @fdellaert: Wondering about batch norm? Read this amazing thread. https://t.co/y5oXJNBXsq", "followers": "2", "datetime": "2019-09-13 02:28:14", "author": "@dj_jiang"}, "566379613291831296": {"content_summary": "HN: Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/2gUE8AGlgX", "followers": "174", "datetime": "2015-02-13 23:33:17", "author": "@cascadesandiego"}, "566876247196581888": {"content_summary": "This batch-normalization (http://t.co/8YdyFYMBpX) prevent random reshuffling of minibatches?", "followers": "561", "datetime": "2015-02-15 08:26:44", "author": "@sergecell"}, "1171896679672483840": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "382", "datetime": "2019-09-11 21:21:55", "author": "@yassersouri"}, "1172082946842529793": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "34", "datetime": "2019-09-12 09:42:05", "author": "@MatRazor"}, "565946011550351360": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "3,141", "datetime": "2015-02-12 18:50:18", "author": "@niam"}, "566215402074542080": {"content_summary": "Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/eE5W3juQjF #DeepLearning #datascience", "followers": "4,520", "datetime": "2015-02-13 12:40:46", "author": "@hire_ai"}, "1172159027331174401": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "1,165", "datetime": "2019-09-12 14:44:24", "author": "@sarkerabeed"}, "821730557109280768": {"content_summary": "\u3088\u3093\u3067\u308b: https://t.co/5LFqqmHGtA", "followers": "1,795", "datetime": "2017-01-18 14:46:46", "author": "@xanxys_"}, "1172142429920739330": {"content_summary": "@miguelalonsojr Bonjour there is your unroll: Thread by @dcpage3: \"The paper that introduced Batch Norm https://t.co/ybxv8LzEMy combines clear intuition with compelling experiments (14x [\u2026]\" https://t.co/2q3jQMeVyP See you soon. \ud83e\udd16", "followers": "321,539", "datetime": "2019-09-12 13:38:27", "author": "@threadreaderapp"}, "572630235264487424": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. (arXiv:1502.03167v3 \u2026 http://t.co/Ypi6ocC9my", "followers": "1,546", "datetime": "2015-03-03 05:31:01", "author": "@animesh1977"}, "1171954648812597248": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "98", "datetime": "2019-09-12 01:12:16", "author": "@shpotes"}, "1185946804640071680": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift How normalization applied to layers helps to reach faster convergence. ArXiV: https://t.co/NoX6XBmrJe #NeuralNetwork #nn #normalization #DL", "followers": "523", "datetime": "2019-10-20 15:52:06", "author": "@W3hack"}, "1171958631488577537": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "36", "datetime": "2019-09-12 01:28:06", "author": "@aurko_roy"}, "1230752780530245632": {"content_summary": "\u6b63\u898f\u5316\u3059\u308b\u76ee\u7684\uff1a https://t.co/v4WnEDqCOu \u3044\u308d\u3093\u306a\u6b63\u898f\u5316\uff08\u6982\u8981\uff09\uff1a https://t.co/TdK7nVQGE0 batch\uff1ahttps://t.co/czTb83KO6n Layer\uff1ahttps://t.co/2PdGGJfLFD Instance\uff1ahttps://t.co/tHwjGjGiU7 Group\uff1ahttps://t.co/z70sXj5EIs", "followers": "402", "datetime": "2020-02-21 07:15:03", "author": "@bblomiyasan"}, "1172086275416870913": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "270", "datetime": "2019-09-12 09:55:19", "author": "@kotti_sasikanth"}, "1171877942512771074": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "7", "datetime": "2019-09-11 20:07:28", "author": "@TH_Wars"}, "1172159498053701634": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "79", "datetime": "2019-09-12 14:46:16", "author": "@AndrewTouchet"}, "565786594305933313": {"content_summary": "Everybody seems to be taking on @karpathy (the human not the network) these days. New ImageNet record by Google folks http://t.co/H0do4P63GN", "followers": "4,418", "datetime": "2015-02-12 08:16:50", "author": "@abursuc"}, "1172111942573600768": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,162", "datetime": "2019-09-12 11:37:18", "author": "@YangKevinK"}, "566224050964676609": {"content_summary": "Paper on improving the efficiency of deep learning networks by batched normalization #ml http://t.co/hvhuxNYZva", "followers": "911", "datetime": "2015-02-13 13:15:08", "author": "@chriseko"}, "1171876311914045440": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "61", "datetime": "2019-09-11 20:00:59", "author": "@Laxya16"}, "1172009887682842624": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,146", "datetime": "2019-09-12 04:51:46", "author": "@sudeeppillai"}, "1172110115614285824": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,412", "datetime": "2019-09-12 11:30:02", "author": "@cournape"}, "1172169527133708288": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "50", "datetime": "2019-09-12 15:26:07", "author": "@_GR4HAM"}, "566380625545154560": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/ZakRM6Ac77", "followers": "24", "datetime": "2015-02-13 23:37:18", "author": "@DeShawn_Mann"}, "1172246350907478024": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "457", "datetime": "2019-09-12 20:31:23", "author": "@SingingData"}, "1172171233816768513": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "292", "datetime": "2019-09-12 15:32:54", "author": "@NogaRot"}, "565808827334139905": {"content_summary": "Almost there. :) http://t.co/3wAudVVDsL", "followers": "14,211", "datetime": "2015-02-12 09:45:11", "author": "@karoly_zsolnai"}, "1171922646126084096": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "164", "datetime": "2019-09-11 23:05:06", "author": "@ZachBessinger"}, "565884411187445764": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "2,130", "datetime": "2015-02-12 14:45:31", "author": "@mattthemathman"}, "1172342022629285888": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "13", "datetime": "2019-09-13 02:51:33", "author": "@BBatErdene5"}, "1172046492636459008": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2", "datetime": "2019-09-12 07:17:14", "author": "@mf_side"}, "1172284476472332289": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "19", "datetime": "2019-09-12 23:02:53", "author": "@MassBassLol"}, "1173159889339912192": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "11,002", "datetime": "2019-09-15 09:01:28", "author": "@data_hpz"}, "1171892733960687621": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "34", "datetime": "2019-09-11 21:06:15", "author": "@therealaseifert"}, "1172495519110901761": {"content_summary": "RT @_brohrer_: A fun deep dive on \u201cWhy Batch Norm?\u201d https://t.co/8GD0oVqhZ5", "followers": "2,067", "datetime": "2019-09-13 13:01:30", "author": "@EricSchles"}, "1172250299970523140": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "72", "datetime": "2019-09-12 20:47:05", "author": "@jonasrbati"}, "674822227204825088": {"content_summary": "@quantombone https://t.co/3oFL5j9mVS Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "followers": "12,600", "datetime": "2015-12-10 05:25:31", "author": "@quantombone"}, "1171970083364564992": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "82", "datetime": "2019-09-12 02:13:36", "author": "@mzeid4real"}, "749911673054568448": {"content_summary": "Is it only me who has this on Batch Nomalization paper? https://t.co/mPI6gmDBJl https://t.co/4i5EO3Qji2", "followers": "1,913", "datetime": "2016-07-04 10:24:10", "author": "@y0b1byte"}, "1184706697501110272": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "303", "datetime": "2019-10-17 05:44:22", "author": "@subhobrata1"}, "1171901832563765249": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "19", "datetime": "2019-09-11 21:42:24", "author": "@AVSave"}, "1171961326480105477": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "180", "datetime": "2019-09-12 01:38:48", "author": "@SanjeethVeigas"}, "1172192375118024704": {"content_summary": "Reading this thread while listening to https://t.co/yWj5QqRGsG makes it sound like an educational Youtube video", "followers": "214", "datetime": "2019-09-12 16:56:55", "author": "@Varal7"}, "1171978198646571008": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1", "datetime": "2019-09-12 02:45:51", "author": "@reksieapp"}, "1171946014359486464": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "22", "datetime": "2019-09-12 00:37:58", "author": "@mofas223"}, "810213601940279296": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/FgzGitFuRd", "followers": "852", "datetime": "2016-12-17 20:02:30", "author": "@FMarradi"}, "1172067633270800384": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,623", "datetime": "2019-09-12 08:41:14", "author": "@tarantulae"}, "1172101258586578946": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "196", "datetime": "2019-09-12 10:54:51", "author": "@aNatureTech"}, "1172128837183819776": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "120", "datetime": "2019-09-12 12:44:26", "author": "@sengkim123"}, "1172158686292135941": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "403", "datetime": "2019-09-12 14:43:03", "author": "@DSPonFPGA"}, "1172052674201575424": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "0", "datetime": "2019-09-12 07:41:47", "author": "@benbenfiol"}, "1171917742351499264": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "49", "datetime": "2019-09-11 22:45:37", "author": "@RyRodri"}, "565803985492541440": {"content_summary": "V pond\u011bl\u00ed um\u011bl Microsoft rozpozn\u00e1vat obr\u00e1zky l\u00e9pe ne\u017e \u010dlov\u011bk, dnes to oznamuje i Google: http://t.co/eoOIAchgb4 (4,8%, lid\u00e9 maj\u00ed 5,1%)", "followers": "4,772", "datetime": "2015-02-12 09:25:56", "author": "@michalillich"}, "1172012471898370048": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "772", "datetime": "2019-09-12 05:02:02", "author": "@theabhimanyu"}, "1172346422760333312": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "15", "datetime": "2019-09-13 03:09:02", "author": "@prashil_t"}, "1172166392218439681": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "208", "datetime": "2019-09-12 15:13:40", "author": "@sathya04"}, "566714450020139009": {"content_summary": "Batch #Normalization: #Accelerating #Deep #Network #Training by Reducing Internal Covariate Shift : http://t.co/sfLVdhLAjo", "followers": "1,527", "datetime": "2015-02-14 21:43:48", "author": "@chorfa672m"}, "565767357646503936": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "245", "datetime": "2015-02-12 07:00:24", "author": "@joanfihu"}, "1172308755804852226": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,911", "datetime": "2019-09-13 00:39:22", "author": "@WaleAkinfaderin"}, "566392401686589440": {"content_summary": "#hackernews Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/VwlOryo82c", "followers": "91", "datetime": "2015-02-14 00:24:06", "author": "@Yelow79"}, "565720319600918528": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "16,735", "datetime": "2015-02-12 03:53:29", "author": "@mjcavaretta"}, "680268102219644928": {"content_summary": "\u3088\u3080 https://t.co/D7DdLVSUki", "followers": "432", "datetime": "2015-12-25 06:05:28", "author": "@tsuzukamakiri"}, "565729053102309377": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "105", "datetime": "2015-02-12 04:28:11", "author": "@Data88Geek"}, "1171984128725487618": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "47", "datetime": "2019-09-12 03:09:25", "author": "@UPPALANSHUK"}, "566380711872327680": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/rGflD39mql #news", "followers": "40", "datetime": "2015-02-13 23:37:39", "author": "@NLUlite"}, "565849000281378818": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "290", "datetime": "2015-02-12 12:24:49", "author": "@ButterKaffee"}, "1172089174628790272": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "415", "datetime": "2019-09-12 10:06:50", "author": "@__nggih"}, "1172398991956312064": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "0", "datetime": "2019-09-13 06:37:56", "author": "@chiayu_h"}, "566930592844234752": {"content_summary": "\u201c@sergecell: This batch-normalization (http://t.co/b5kx9WxDnt) prevent random reshuffling of minibatches?\u201d But it doesn't work batch size=1.", "followers": "1,092", "datetime": "2015-02-15 12:02:41", "author": "@bittnt"}, "1171959020380467206": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "202", "datetime": "2019-09-12 01:29:39", "author": "@gnperdue"}, "1171867587417952260": {"content_summary": "The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup on ImageNet!!) So why has 'internal covariate shift' remained controversial to this day? Thread \ud83d\udc47 https://t.co/L0BBmo0q4t", "followers": "2,420", "datetime": "2019-09-11 19:26:19", "author": "@dcpage3"}, "1171949112704393216": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "259", "datetime": "2019-09-12 00:50:16", "author": "@Ahmeda77"}, "1172034428106936320": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "41", "datetime": "2019-09-12 06:29:17", "author": "@kazenokizi"}, "566093758916096000": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "144", "datetime": "2015-02-13 04:37:24", "author": "@DhimantJayswal"}, "1171917798353690625": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2,670", "datetime": "2019-09-11 22:45:50", "author": "@ayirpelle"}, "565760289363476480": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "341", "datetime": "2015-02-12 06:32:18", "author": "@bsnrs"}, "1172001512475643905": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "304", "datetime": "2019-09-12 04:18:29", "author": "@flrgsr"}, "575274882822373376": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "2,826", "datetime": "2015-03-10 12:39:54", "author": "@tetsuroito"}, "566409235198517248": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/HlcxfeolPo", "followers": "486", "datetime": "2015-02-14 01:30:59", "author": "@val314159"}, "1171897355504779264": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "4,327", "datetime": "2019-09-11 21:24:37", "author": "@jekbradbury"}, "1172399664764555264": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "281", "datetime": "2019-09-13 06:40:36", "author": "@NuitBlog"}, "1172170378447220737": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "45", "datetime": "2019-09-12 15:29:30", "author": "@nullbytep"}, "681873996216971264": {"content_summary": "RT @newsycombinator: Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/RdwEDSpwjq", "followers": "4", "datetime": "2015-12-29 16:26:43", "author": "@yoyo_sarun"}, "1171867958018285569": {"content_summary": "Great thread on batchnorm and ICS https://t.co/HiOIrTBFQN", "followers": "1,083", "datetime": "2019-09-11 19:27:48", "author": "@rosstaylor90"}, "565696276583694338": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "269,218", "datetime": "2015-02-12 02:17:57", "author": "@karpathy"}, "1172119001863876610": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "15", "datetime": "2019-09-12 12:05:21", "author": "@henryhenrychen"}, "1172157067173535744": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "65", "datetime": "2019-09-12 14:36:37", "author": "@dave_co_dev"}, "1172952229134917637": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3", "datetime": "2019-09-14 19:16:18", "author": "@wndiros"}, "1172392536427515905": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "278", "datetime": "2019-09-13 06:12:17", "author": "@son_ayong"}, "1172132067175481344": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,719", "datetime": "2019-09-12 12:57:16", "author": "@bjh_ip"}, "565947625774780417": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "674", "datetime": "2015-02-12 18:56:43", "author": "@jonathanrraiman"}, "1172020257743151104": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,456", "datetime": "2019-09-12 05:32:59", "author": "@danilobzdok"}, "1171946933772050433": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "65", "datetime": "2019-09-12 00:41:37", "author": "@dave_co_dev"}, "1172339216044326913": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "203", "datetime": "2019-09-13 02:40:24", "author": "@python4living"}, "1171978730668867584": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,436", "datetime": "2019-09-12 02:47:58", "author": "@chrisemoody"}, "1172226534855196677": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "78", "datetime": "2019-09-12 19:12:39", "author": "@mishraka"}, "566397047847518209": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/U6w7klOosy \u2026", "followers": "662", "datetime": "2015-02-14 00:42:34", "author": "@foxtrotfourwbm"}, "1171875175555461120": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "31", "datetime": "2019-09-11 19:56:28", "author": "@alwyn_mathew"}, "1172048405725700096": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "52", "datetime": "2019-09-12 07:24:50", "author": "@r_lepert"}, "566477119266357251": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "1,158", "datetime": "2015-02-14 06:00:44", "author": "@sschinke"}, "566465331149828098": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "7,176", "datetime": "2015-02-14 05:13:54", "author": "@HackSysTeam"}, "566379570539294721": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] | http://t.co/80chpK4MTg #geeknews via ycombinator", "followers": "270", "datetime": "2015-02-13 23:33:07", "author": "@TechnologyNomad"}, "565969968580157440": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "418", "datetime": "2015-02-12 20:25:30", "author": "@PaulTozour"}, "565956792883359744": {"content_summary": "Train ConvNets 14x faster with the same accuracy by Reducing Internal Covariate Shift http://t.co/XNPNhZPXMu #Pubs", "followers": "363", "datetime": "2015-02-12 19:33:09", "author": "@aachigorin"}, "1171980447032565760": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "59", "datetime": "2019-09-12 02:54:47", "author": "@flapdoodle_sand"}, "1172212641445683207": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "40", "datetime": "2019-09-12 18:17:27", "author": "@thedatafrog"}, "1171930114281615361": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "448", "datetime": "2019-09-11 23:34:47", "author": "@chrix2"}, "566385656100712448": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "18,879", "datetime": "2015-02-13 23:57:18", "author": "@dinabass"}, "1172020320892571649": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "40", "datetime": "2019-09-12 05:33:14", "author": "@black_sailer"}, "1172239541488537600": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "5", "datetime": "2019-09-12 20:04:20", "author": "@xelda1234"}, "1171891029689208834": {"content_summary": "@curtwehrley Hello the unroll you asked for: Thread by @dcpage3: \"The paper that introduced Batch Norm https://t.co/ybxv8LzEMy combines clear intuition with compelling experiments (14x [\u2026]\" https://t.co/2q3jQMeVyP Talk to you soon. \ud83e\udd16", "followers": "321,539", "datetime": "2019-09-11 20:59:28", "author": "@threadreaderapp"}, "746457078957473792": {"content_summary": "\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\": https://t.co/1aiLCaCwUG #cs #ml #ann #2015", "followers": "2,132", "datetime": "2016-06-24 21:36:50", "author": "@onepaperperday"}, "1172172710962745344": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "255", "datetime": "2019-09-12 15:38:46", "author": "@christian_hudon"}, "1171903560147058688": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "99", "datetime": "2019-09-11 21:49:16", "author": "@DSaience"}, "794387220329209856": {"content_summary": "Batch Normalization\u306e\u5143\u8ad6\u6587\u306e3.1\u3092\u898b\u308b\u3068\u5b66\u7fd2\u6642\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u6bce\u306e\u5e73\u5747\u5206\u6563\u3092\u3055\u3089\u306b\u5e73\u5747\u5316\u3057\u3066\u4e88\u6e2c\u6642\u306e\u6b63\u898f\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u4f7f\u3046\u307f\u305f\u3044\u306a\u3053\u3068\u306a\u306e\u304b\u306a / \u201c1502.03167v3.pdf\u201d https://t.co/s3Hd6TDAiY", "followers": "567", "datetime": "2016-11-04 03:54:06", "author": "@masatoi0"}, "1172368231270502400": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "160", "datetime": "2019-09-13 04:35:42", "author": "@shasta_kr"}, "614274408333488128": {"content_summary": "Batch normalization appears to be the current state of the art in CNN training. http://t.co/x0cMDUuMeu #machinelearning", "followers": "362", "datetime": "2015-06-26 03:30:06", "author": "@Alrecenk"}, "1172016330788212737": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "74", "datetime": "2019-09-12 05:17:22", "author": "@huyunwei"}, "1171989434360877056": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,152", "datetime": "2019-09-12 03:30:30", "author": "@iskander"}, "1171904288785911809": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "281", "datetime": "2019-09-11 21:52:10", "author": "@manideep2510"}, "1172178076366589952": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "65", "datetime": "2019-09-12 16:00:06", "author": "@_dongkwan_kim"}, "565776794708893696": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "459", "datetime": "2015-02-12 07:37:54", "author": "@eabrilm"}, "1171912278121185280": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "167", "datetime": "2019-09-11 22:23:54", "author": "@danielmurfet"}, "1172304233074429952": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "912", "datetime": "2019-09-13 00:21:24", "author": "@ChikaObuah"}, "566537339543879681": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/BC41Llo3VR, discussion: http://t.co/5hsPFR7pQI", "followers": "917", "datetime": "2015-02-14 10:00:02", "author": "@hackernewsfeed"}, "1171954079397892096": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "277", "datetime": "2019-09-12 01:10:01", "author": "@__clang__"}, "566450046829228032": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "111,562", "datetime": "2015-02-14 04:13:09", "author": "@thegrugq"}, "1172312807557087232": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "43", "datetime": "2019-09-13 00:55:28", "author": "@criz5Castro"}, "596258802871226369": {"content_summary": "I wonder how batch normalization compares to max-norm as a regularization technique http://t.co/bl68ruT7Er #deeplearning #machinelearning", "followers": "1,752", "datetime": "2015-05-07 10:22:31", "author": "@vincent_spruyt"}, "589980609805557761": {"content_summary": "Images fool Recognition system http://t.co/gfJIZ8HCtU viz google-batch normalization-http://t.co/QEiBfor8NQ, msft-http://t.co/X1Zmun5AWr", "followers": "1,133", "datetime": "2015-04-20 02:35:13", "author": "@govindk"}, "1172405819226243073": {"content_summary": "RT @fdellaert: Wondering about batch norm? Read this amazing thread. https://t.co/y5oXJNBXsq", "followers": "378", "datetime": "2019-09-13 07:05:04", "author": "@aalperino"}, "1013434873158209536": {"content_summary": "@greenmaree @karpathy It's just that Batch Normalization already includes a bias term. https://t.co/0NLNUH1oxj in algorithm 1, last line of code (scale and shift), the beta term, is the one that shifts, i.e., the bias term.", "followers": "252", "datetime": "2018-07-01 14:51:17", "author": "@inigo_zgz"}, "1171886461978726400": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "57", "datetime": "2019-09-11 20:41:19", "author": "@kuttykousik"}, "565784460827037696": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "4,945", "datetime": "2015-02-12 08:08:21", "author": "@bitemyapp"}, "566392294970908673": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/dAWGaGCmSS", "followers": "7", "datetime": "2015-02-14 00:23:40", "author": "@HaxNewsFeed"}, "1172067870899167232": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "600", "datetime": "2019-09-12 08:42:11", "author": "@mundt_martin"}, "838773986280402944": {"content_summary": "\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u5143\u8ad6\u6587\u8aad\u3080\u305e(\uff9f\u0414\uff9f)\uff7a\uff9e\uff99\uff67! https://t.co/RPzuaFSRp3", "followers": "650", "datetime": "2017-03-06 15:31:16", "author": "@karino2012"}, "1171878829905907712": {"content_summary": "@soldni Hola there is your unroll: Thread by @dcpage3: \"The paper that introduced Batch Norm https://t.co/ybxv8LzEMy combines clear intuition with compelling experiments (14x [\u2026]\" https://t.co/2q3jQMeVyP Talk to you soon. \ud83e\udd16", "followers": "321,539", "datetime": "2019-09-11 20:11:00", "author": "@threadreaderapp"}, "567013162604834816": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/uHDcCzBbuX", "followers": "1,830", "datetime": "2015-02-15 17:30:47", "author": "@philadev"}, "1172092124122234881": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "147", "datetime": "2019-09-12 10:18:33", "author": "@JPSmithNL"}, "1172163014600077313": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "2,700", "datetime": "2019-09-12 15:00:15", "author": "@npetit"}, "1172160018428252161": {"content_summary": "Nice thread", "followers": "3,195", "datetime": "2019-09-12 14:48:20", "author": "@A_K_Nain"}, "1172019404437147648": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "103", "datetime": "2019-09-12 05:29:35", "author": "@jongen87"}, "1171938892498513922": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "47", "datetime": "2019-09-12 00:09:40", "author": "@cdannery_doo"}, "1172380566156505091": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "23", "datetime": "2019-09-13 05:24:43", "author": "@deehzee"}, "1171872535367094277": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "431", "datetime": "2019-09-11 19:45:59", "author": "@equiparable"}, "1171873567362953217": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "3,763", "datetime": "2019-09-11 19:50:05", "author": "@IAmSamFin"}, "1172189229188812801": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "101", "datetime": "2019-09-12 16:44:25", "author": "@ozanpolatbilek"}, "1172415104966545408": {"content_summary": "RT @_brohrer_: A fun deep dive on \u201cWhy Batch Norm?\u201d https://t.co/8GD0oVqhZ5", "followers": "40", "datetime": "2019-09-13 07:41:58", "author": "@pktippa"}, "566507662842818560": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/gHXyUTCgC1", "followers": "194", "datetime": "2015-02-14 08:02:06", "author": "@betterhn100"}, "1171899808971448322": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "133", "datetime": "2019-09-11 21:34:21", "author": "@Nijumich"}, "1172117893129101314": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "290", "datetime": "2019-09-12 12:00:57", "author": "@dannyehb"}, "565724967011352577": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "2,125", "datetime": "2015-02-12 04:11:57", "author": "@SalinaHendricks"}, "1171937329012854784": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "65", "datetime": "2019-09-12 00:03:27", "author": "@kli_nlpr"}, "1171939264583438336": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "167", "datetime": "2019-09-12 00:11:08", "author": "@gmravi2003"}, "1172031874631262208": {"content_summary": "RT @lavanyaai: Cool thread on the intuition behind why BatchNorm works so well in practice! \ud83e\udd37\ud83c\udffc\u200d\u2640\ufe0f https://t.co/n96wmcJ1ZH", "followers": "208", "datetime": "2019-09-12 06:19:08", "author": "@AtnurShabbir"}, "1171931125733703680": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "28", "datetime": "2019-09-11 23:38:48", "author": "@picofanta"}, "1173357873323077633": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "4", "datetime": "2019-09-15 22:08:11", "author": "@HaozhiQ"}, "1199145522117758976": {"content_summary": "@dpp @paulkrugman A 2015 paper by Ioffe, Szegedy that proposed batch normalization, masivley reducing the vanishing gradients problem. And finally in 2016 the RMSProp optimization algorithm. https://t.co/LKr50wLLw9 https://t.co/K0fTVaAhXx https://t.co/", "followers": "7,934", "datetime": "2019-11-26 01:59:06", "author": "@AgathaBacelar"}, "1172049012389765121": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "45", "datetime": "2019-09-12 07:27:14", "author": "@artuskg"}, "1172049765657468929": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "50", "datetime": "2019-09-12 07:30:14", "author": "@remiconnesson"}, "566383831864070144": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/PMAP3tbT12 (https://t.co/SmthwJqUao)", "followers": "285", "datetime": "2015-02-13 23:50:03", "author": "@hn_bot_top1"}, "802309618835734528": {"content_summary": "Batch normalization for neural networks https://t.co/PBkK0Eu9q9", "followers": "349", "datetime": "2016-11-26 00:34:53", "author": "@collinalexbell"}, "1171938558476787713": {"content_summary": "Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recent empirical evidence that strongly supports it.", "followers": "5,469", "datetime": "2019-09-12 00:08:20", "author": "@RogerGrosse"}, "566752979945275393": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/RdwEDSpwjq", "followers": "1,062", "datetime": "2015-02-15 00:16:54", "author": "@maymounkov"}, "1171984290894274560": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "771", "datetime": "2019-09-12 03:10:04", "author": "@aCraigPfeifer"}, "1172195947197214721": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "0", "datetime": "2019-09-12 17:11:06", "author": "@benbenfiol"}, "1171932401259429888": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "494", "datetime": "2019-09-11 23:43:52", "author": "@jbohnslav"}, "1172144522714898432": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "58", "datetime": "2019-09-12 13:46:46", "author": "@IGoytom"}, "1172168867713409025": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "1,095", "datetime": "2019-09-12 15:23:30", "author": "@SuhnyllaKler"}, "1172158677870137345": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "4,740", "datetime": "2019-09-12 14:43:01", "author": "@rzembo"}, "1172156406771830785": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "99", "datetime": "2019-09-12 14:33:59", "author": "@treasured_write"}, "1171895159967113216": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "86", "datetime": "2019-09-11 21:15:53", "author": "@tiagomluis"}, "1171965120781537281": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "30", "datetime": "2019-09-12 01:53:53", "author": "@gitlostmurali"}, "1172133839071657986": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "3,374", "datetime": "2019-09-12 13:04:19", "author": "@renato_umeton"}, "1172019483671584770": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "129", "datetime": "2019-09-12 05:29:54", "author": "@aks_phenom"}, "1172163426270896129": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "15", "datetime": "2019-09-12 15:01:53", "author": "@NawazAD17"}, "1172879833967353857": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "52", "datetime": "2019-09-14 14:28:38", "author": "@Gaarv1911"}, "1172164178338963457": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3", "datetime": "2019-09-12 15:04:52", "author": "@VuTerminalh2t3"}, "1172471026183106560": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "137", "datetime": "2019-09-13 11:24:10", "author": "@ralphbrooks"}, "565824439401455617": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/wwHeHwUlLx", "followers": "52", "datetime": "2015-02-12 10:47:13", "author": "@sergeypod"}, "1172018698606874624": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "3,416", "datetime": "2019-09-12 05:26:47", "author": "@andrey_kurenkov"}, "1172101992258256897": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "79", "datetime": "2019-09-12 10:57:46", "author": "@junjungoal"}, "1171889878113619969": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "80", "datetime": "2019-09-11 20:54:54", "author": "@ricklentz"}, "1171914370965803010": {"content_summary": "Why batch norm works.", "followers": "309", "datetime": "2019-09-11 22:32:13", "author": "@nafiz_h"}, "1172029142771982336": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "39", "datetime": "2019-09-12 06:08:17", "author": "@TsainGra"}, "1172014581696188416": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1,456", "datetime": "2019-09-12 05:10:25", "author": "@Gabriel_Oguna"}, "626543618061307904": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "230", "datetime": "2015-07-30 00:03:33", "author": "@chuchaba"}, "1172002315001139200": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,920", "datetime": "2019-09-12 04:21:41", "author": "@stucchio"}, "1172205898720112640": {"content_summary": "I give myself time till Dec 31st to understand this thread entirely", "followers": "220", "datetime": "2019-09-12 17:50:39", "author": "@ilayarendi"}, "1171882120815489025": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "712", "datetime": "2019-09-11 20:24:04", "author": "@JohnDobermann"}, "1172280271934087168": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "3,633", "datetime": "2019-09-12 22:46:11", "author": "@HirokatuKataoka"}, "1172315939078983680": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "59", "datetime": "2019-09-13 01:07:55", "author": "@avnish_ks"}, "1171943198996013057": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "75", "datetime": "2019-09-12 00:26:46", "author": "@grhwatson"}, "566392321382432769": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/ipuGePbX2n", "followers": "1,819", "datetime": "2015-02-14 00:23:47", "author": "@PhooDotID_JR"}, "1209323395776598016": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "216", "datetime": "2019-12-24 04:02:20", "author": "@udaykovur"}, "565699812373577728": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "126", "datetime": "2015-02-12 02:32:00", "author": "@nakaet"}, "1173960656002961408": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "21", "datetime": "2019-09-17 14:03:26", "author": "@GersonVizcarra"}, "565765327762776064": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "442", "datetime": "2015-02-12 06:52:20", "author": "@moreymat"}, "1171955670754762752": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,474", "datetime": "2019-09-12 01:16:20", "author": "@Pac_Kd"}, "1172007648926916609": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1,346", "datetime": "2019-09-12 04:42:53", "author": "@DSakya"}, "1172204342172143616": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "163", "datetime": "2019-09-12 17:44:28", "author": "@ChurchillMic"}, "565812632582492160": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/ymmkn8u8EP #arxiv", "followers": "787", "datetime": "2015-02-12 10:00:18", "author": "@arXivStats"}, "565923693939793920": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "8,425", "datetime": "2015-02-12 17:21:37", "author": "@debasishg"}, "1172500535368458240": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "807", "datetime": "2019-09-13 13:21:26", "author": "@MASSIMOFERRO19"}, "566316782659829760": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "363", "datetime": "2015-02-13 19:23:37", "author": "@jamisjohnson"}, "1172216643012792320": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "44", "datetime": "2019-09-12 18:33:21", "author": "@iamadarsh_k"}, "1173197129101107201": {"content_summary": "RT @A_K_Nain: Nice thread https://t.co/UzN1kzlxH3", "followers": "303", "datetime": "2019-09-15 11:29:27", "author": "@subhobrata1"}, "1081990910060183553": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Sergey Ioffe, Christian Szegedy https://t.co/t3dzFsKshT", "followers": "149", "datetime": "2019-01-06 19:08:30", "author": "@parker_brydon"}, "1171876432202493953": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,213", "datetime": "2019-09-11 20:01:28", "author": "@sampathweb"}, "1172186209688465408": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "171", "datetime": "2019-09-12 16:32:25", "author": "@AmirHadifar"}, "565948124737585152": {"content_summary": "Title: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/HwcPQf6TTu #datascience #\u2026", "followers": "666", "datetime": "2015-02-12 18:58:42", "author": "@thiakx"}, "1172132156996497408": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "21,221", "datetime": "2019-09-12 12:57:38", "author": "@DataSciNews"}, "566385192462725120": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "39,692", "datetime": "2015-02-13 23:55:27", "author": "@mappingbabel"}, "565745811003174912": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "1,073", "datetime": "2015-02-12 05:34:47", "author": "@terashimahiroki"}, "1171875387741048832": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "41", "datetime": "2019-09-11 19:57:19", "author": "@VenkyMsd"}, "1021972359115415554": {"content_summary": "@3blue1brown @minutephysics I like to suggest, make a video about batch normalization and its importance in cnn... maybe take a look at these papers https://t.co/uOzDPEpncP and https://t.co/0y21Y75jnD", "followers": "152", "datetime": "2018-07-25 04:16:12", "author": "@alirg1"}, "1171961754479464449": {"content_summary": "A nice thread on why batch norm helps DNNs train faster", "followers": "410", "datetime": "2019-09-12 01:40:30", "author": "@JuanpaMF"}, "1172005777290063872": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1", "datetime": "2019-09-12 04:35:26", "author": "@junyittc"}, "1172068544940531712": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,056", "datetime": "2019-09-12 08:44:51", "author": "@viperale"}, "1171929434955235328": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "425", "datetime": "2019-09-11 23:32:05", "author": "@iamknighton"}, "1172386004155527168": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "414", "datetime": "2019-09-13 05:46:19", "author": "@akhwasim"}, "1172424499490217984": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "1,023", "datetime": "2019-09-13 08:19:17", "author": "@desertnaut"}, "1172049912638427136": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "1,118", "datetime": "2019-09-12 07:30:49", "author": "@ziyuwang"}, "1172420094128820227": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "311", "datetime": "2019-09-13 08:01:47", "author": "@ssakares"}, "809100486229913600": {"content_summary": "Groks general concept of BN. Reads paper https://t.co/Se5bRWmptM again. Head explodes.", "followers": "1,058", "datetime": "2016-12-14 18:19:22", "author": "@BigsnarfDude"}, "1171882943872151552": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "837", "datetime": "2019-09-11 20:27:21", "author": "@MarklDouthwaite"}, "1172204242209165312": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "41", "datetime": "2019-09-12 17:44:04", "author": "@Shreyas298"}, "1172049355546812416": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1,088", "datetime": "2019-09-12 07:28:36", "author": "@GonzaloBarria"}, "1172307514102145025": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "103", "datetime": "2019-09-13 00:34:26", "author": "@sfjccz"}, "1172205383881879552": {"content_summary": "Closer look at the mysteriously famous \"batch normalization\".", "followers": "157", "datetime": "2019-09-12 17:48:36", "author": "@anshulheaven"}, "581561407918882816": {"content_summary": "Batch Normalization #BN http://t.co/kB746kQQlS", "followers": "825", "datetime": "2015-03-27 21:00:19", "author": "@fly51fly"}, "1171874218289463296": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "52", "datetime": "2019-09-11 19:52:40", "author": "@gpsbhargav"}, "896438637591527424": {"content_summary": "WE catchup: batch normalization, in \u2197 depth https://t.co/r5JkV7LNOH https://t.co/xxu7fNddcf https://t.co/eFMhoWZbmz", "followers": "1,266", "datetime": "2017-08-12 18:30:00", "author": "@huitseeker"}, "1172154827717013504": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "72", "datetime": "2019-09-12 14:27:43", "author": "@adamlineberryML"}, "1171886752618954759": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "72", "datetime": "2019-09-11 20:42:29", "author": "@adn_twitts"}, "1172197655679774721": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "598", "datetime": "2019-09-12 17:17:54", "author": "@kargarisaac"}, "565779860849979392": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "46", "datetime": "2015-02-12 07:50:05", "author": "@jblestang"}, "1172339916941848576": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1", "datetime": "2019-09-13 02:43:11", "author": "@soo76699768"}, "565722583136755712": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "127", "datetime": "2015-02-12 04:02:29", "author": "@pj201"}, "567746419449405443": {"content_summary": "@googleresearch beat humans experts on #imagenet challenge http://t.co/FHMB8iX9l4 #MachineLearning,#bigdata, #deeplearning, #computervision", "followers": "1,601", "datetime": "2015-02-17 18:04:29", "author": "@AgoniGrammi"}, "567748128410181632": {"content_summary": "@googleresearch beat humans experts on #imagenet challenge http://t.co/FHMB8iX9l4 #MachineLearning,#bigdata, #deeplearning, #computervision", "followers": "6,883", "datetime": "2015-02-17 18:11:16", "author": "@1MalachiD"}, "1172390634239352832": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "7", "datetime": "2019-09-13 06:04:43", "author": "@bowenroom1"}, "774152949480075264": {"content_summary": "@hillbig \u6df1\u5c64\u5b66\u7fd2\u7b2c\uff11\u5237\u3092\u8aad\u3093\u3067\u307e\u3057\u3066\u3001137\u30da\u30fc\u30b8\u306e\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u5f93\u6765\u3068\u6bd4\u307917\u500d\u9ad8\u901f\u5316\u3068\u3042\u308a\u307e\u3059\u304c\u3001https://t.co/3BUlaXTqOG\u306e\u30a4\u30f3\u30c8\u30ed\u3067\u306f14\u500d\u3068\u306a\u3063\u3066\u3044\u3066\u3001\u307e\u305f\u53c2\u8003\u6587\u732e[12]\u304c\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u8ad6\u6587\u3058\u3083\u306a\u3044\uff1f\u306e\u3067\u3059\u304c\u3001\u6b63\u3057\u3044\u3067\u3057\u3087\u3046\u304b\uff1f", "followers": "145", "datetime": "2016-09-09 07:50:20", "author": "@tyfkda"}, "566998246632198144": {"content_summary": "Title: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covari\u2026 http://t.co/gu2CgTG5y5 http://t.co/84EWH0pCO8", "followers": "13", "datetime": "2015-02-15 16:31:30", "author": "@adamistpocket"}, "566001384785731585": {"content_summary": "Wow, the ImageNET classification battle between Google and Microsoft research is going fast and furious http://t.co/ae3ZAueTsG", "followers": "263", "datetime": "2015-02-12 22:30:20", "author": "@SQLInsane"}, "1223968317817475072": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "164,075", "datetime": "2020-02-02 13:56:01", "author": "@ceobillionaire"}, "1172148669610545152": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "303", "datetime": "2019-09-12 14:03:14", "author": "@subhobrata1"}, "1172192173627854848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,051", "datetime": "2019-09-12 16:56:07", "author": "@shlizee"}, "578212470881443840": {"content_summary": "A recent paper on DNNs which shows that a technique called Batch Normalization dramatically decreases training time. http://t.co/mBvQSP3tl6", "followers": "36", "datetime": "2015-03-18 15:12:50", "author": "@SebAguirre8"}, "565717391783755779": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "142", "datetime": "2015-02-12 03:41:51", "author": "@kuwaken0925"}, "567489166410145792": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - http://t.co/hWCxBJq8fK", "followers": "9,811", "datetime": "2015-02-17 01:02:15", "author": "@PeriscopeData"}, "566463162560430080": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "449", "datetime": "2015-02-14 05:05:16", "author": "@arrago2"}, "1173201313062014982": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,827", "datetime": "2019-09-15 11:46:04", "author": "@acidflask"}, "566520878574632960": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "33", "datetime": "2015-02-14 08:54:37", "author": "@0xAlexandros"}, "1171944985601744900": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "31", "datetime": "2019-09-12 00:33:52", "author": "@AnirudhIyer11"}, "1176222162060902400": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "414", "datetime": "2019-09-23 19:49:51", "author": "@NonLocalityGuy"}, "1172176334610563077": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "71", "datetime": "2019-09-12 15:53:10", "author": "@eigenVishal"}, "565911833366642689": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "774", "datetime": "2015-02-12 16:34:29", "author": "@hidetomasuoka"}, "1172494190686740481": {"content_summary": "RT software_tic :RT jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about training mechanics by studying this thread and the links it contains. https://t.co/ZCFTeyubNb", "followers": "4,168", "datetime": "2019-09-13 12:56:13", "author": "@dataprix"}, "676620665051414528": {"content_summary": "This is worth to read. #batchnormalization https://t.co/7IWnKaC1zz", "followers": "89", "datetime": "2015-12-15 04:31:52", "author": "@KingChungHo"}, "1172012141878099968": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "446", "datetime": "2019-09-12 05:00:44", "author": "@Brotherluii"}, "1172405012598677505": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "119", "datetime": "2019-09-13 07:01:51", "author": "@TusharJain_007"}, "1172854040662827009": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "7,210", "datetime": "2019-09-14 12:46:08", "author": "@fastml_extra"}, "566644946418298880": {"content_summary": "Breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/oJ6pBmoqXm #neuro #machinelearning", "followers": "105", "datetime": "2015-02-14 17:07:37", "author": "@Data88Geek"}, "1171914328544620544": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "371", "datetime": "2019-09-11 22:32:03", "author": "@aviopene"}, "1172192029599682560": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "91", "datetime": "2019-09-12 16:55:32", "author": "@0xhexhex"}, "1172086040020115456": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "10", "datetime": "2019-09-12 09:54:22", "author": "@seu_bolino"}, "1171984785901670400": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,413", "datetime": "2019-09-12 03:12:02", "author": "@RexDouglass"}, "1173856121422876672": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "602", "datetime": "2019-09-17 07:08:03", "author": "@I_eric_Y"}, "1034098507370573828": {"content_summary": "#100DaysOfMLCode day 13: Learned second paper about GoogLeNet --Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift(https://t.co/Xtj4hfDOkA). Doing this, I also learn the knowledge about whitening, pca and so on.", "followers": "8", "datetime": "2018-08-27 15:21:11", "author": "@lzyutopia"}, "935110323815710720": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/Xp6IW9aDgx", "followers": "376", "datetime": "2017-11-27 11:37:29", "author": "@learnlinksfeed"}, "1171874924929191936": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "299", "datetime": "2019-09-11 19:55:29", "author": "@westis96"}, "565782745121443840": {"content_summary": "New state of the art on image classification is 10x faster than before and more accurate than well-trained humans. http://t.co/Ni66gaAEFO", "followers": "24,640", "datetime": "2015-02-12 08:01:32", "author": "@bznotes"}, "1173557918043500544": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "19,440", "datetime": "2019-09-16 11:23:05", "author": "@Sam_Witteveen"}, "1171995683286478848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "143", "datetime": "2019-09-12 03:55:20", "author": "@raviprasadmkini"}, "1172158079435231232": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "216", "datetime": "2019-09-12 14:40:38", "author": "@tachillon"}, "624885631739670528": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "93", "datetime": "2015-07-25 10:15:19", "author": "@chauhraj"}, "1172162549044908032": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "66", "datetime": "2019-09-12 14:58:24", "author": "@diegovogeid"}, "1171877633677832193": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "12,040", "datetime": "2019-09-11 20:06:14", "author": "@citnaj"}, "1172133376683151362": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "615", "datetime": "2019-09-12 13:02:28", "author": "@KouroshMeshgi"}, "567933904130162688": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "226", "datetime": "2015-02-18 06:29:29", "author": "@Dharanesh_E"}, "1172143839869571072": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,650", "datetime": "2019-09-12 13:44:03", "author": "@ian_soboroff"}, "1171996239551959041": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,287", "datetime": "2019-09-12 03:57:32", "author": "@heghbalz"}, "565702663560777731": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "2,670", "datetime": "2015-02-12 02:43:19", "author": "@ayirpelle"}, "1172136408032653315": {"content_summary": "RT @yoavgo: (another!) great thread re batch norm. https://t.co/LP3E3vjJUa", "followers": "872", "datetime": "2019-09-12 13:14:31", "author": "@mrdrozdov"}, "565741736865968129": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/kQ06sOZpqP", "followers": "190", "datetime": "2015-02-12 05:18:35", "author": "@KenichToudoh"}, "566380071238131712": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] (Discussion on HN -... http://t.co/9uqJaR8MXd", "followers": "734", "datetime": "2015-02-13 23:35:06", "author": "@hnbot"}, "1172128468764712966": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "261", "datetime": "2019-09-12 12:42:58", "author": "@GuptaRajat033"}, "1172179334959980544": {"content_summary": "Great thread. Thanks!", "followers": "134", "datetime": "2019-09-12 16:05:06", "author": "@KMGPL"}, "1172025043636744192": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "68", "datetime": "2019-09-12 05:52:00", "author": "@sohom_vision"}, "1172159275126468608": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "226", "datetime": "2019-09-12 14:45:23", "author": "@cetusparibus"}, "1171874582304894977": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "538", "datetime": "2019-09-11 19:54:07", "author": "@stormtroper1721"}, "1172114121392889856": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "164,075", "datetime": "2019-09-12 11:45:58", "author": "@ceobillionaire"}, "1173453119096872960": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "1,024", "datetime": "2019-09-16 04:26:39", "author": "@czanalytics"}, "1171991234224766976": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "60", "datetime": "2019-09-12 03:37:39", "author": "@vettukal"}, "1172081916905959424": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "16", "datetime": "2019-09-12 09:37:59", "author": "@p_kot1"}, "1172040548653264896": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "35", "datetime": "2019-09-12 06:53:36", "author": "@sumanthmeenank2"}, "886268743659257856": {"content_summary": "@AllTom batch normalization cures all ills .. seriously, has to be tried. https://t.co/ltYAKNDSSU", "followers": "178", "datetime": "2017-07-15 16:58:29", "author": "@TomD_Hann"}, "1172247997251559425": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "949", "datetime": "2019-09-12 20:37:56", "author": "@m_deff"}, "819748555204268034": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/TjWiVI8OaV", "followers": "1,502", "datetime": "2017-01-13 03:31:00", "author": "@patrickmineault"}, "565700604941840384": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "305", "datetime": "2015-02-12 02:35:09", "author": "@jmohamedzahoor"}, "1172428107942260741": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,248", "datetime": "2019-09-13 08:33:38", "author": "@daisuzu"}, "1171880011248349185": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "6,225", "datetime": "2019-09-11 20:15:41", "author": "@alienelf"}, "1172350375392727040": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "12", "datetime": "2019-09-13 03:24:45", "author": "@katnoria1"}, "566401398770630656": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/Z6fqIKdnHu", "followers": "110", "datetime": "2015-02-14 00:59:51", "author": "@IMsmikeycain"}, "1173471820319744006": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "105", "datetime": "2019-09-16 05:40:58", "author": "@T0bias_Brandt"}, "1223578714300088322": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "131", "datetime": "2020-02-01 12:07:53", "author": "@hasangoni"}, "1172396299620012033": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "615", "datetime": "2019-09-13 06:27:14", "author": "@mattmcd"}, "565745278381072387": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "1,667", "datetime": "2015-02-12 05:32:40", "author": "@Scaled_Wurm"}, "886268745060163585": {"content_summary": "RT @TomD_Hann: @AllTom batch normalization cures all ills .. seriously, has to be tried. https://t.co/ltYAKNDSSU", "followers": "46", "datetime": "2017-07-15 16:58:29", "author": "@KittyPryde9"}, "1124484574019346432": {"content_summary": "TIL that some ML researchers had an idea (batch normalization) that worked, then tacked on a post-hoc mathematical analysis to claim that it reduced \"internal covariate shift\". The technique works, but not for that reason. https://t.co/tBXkbOUrNU https://t", "followers": "777", "datetime": "2019-05-04 01:23:09", "author": "@hackerfriendly"}, "1171878373594808320": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,547", "datetime": "2019-09-11 20:09:11", "author": "@TheGregYang"}, "566397490065600512": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] (http://t.co/mjprfd2ktd) http://t.co/n0PsEDRihO", "followers": "24", "datetime": "2015-02-14 00:44:19", "author": "@whatseatingbeau"}, "1172259853148000258": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,630", "datetime": "2019-09-12 21:25:03", "author": "@ChrisChoy208"}, "1171882299824078848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "662", "datetime": "2019-09-11 20:24:47", "author": "@sam_havens"}, "566386342373122049": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: http://t.co/LWHVgOACbl", "followers": "5", "datetime": "2015-02-14 00:00:01", "author": "@HnotTheBot"}, "1171930028046737409": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "165", "datetime": "2019-09-11 23:34:26", "author": "@tech_optimist"}, "565696811944656896": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "161", "datetime": "2015-02-12 02:20:04", "author": "@farhanhubble"}, "1172301119835402242": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "726", "datetime": "2019-09-13 00:09:01", "author": "@wenmingye"}, "565716440054235140": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "41,033", "datetime": "2015-02-12 03:38:04", "author": "@t3kcit"}, "1171941470288392192": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "7,790", "datetime": "2019-09-12 00:19:54", "author": "@cjmaddison"}, "1172014548934483969": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,456", "datetime": "2019-09-12 05:10:18", "author": "@Gabriel_Oguna"}, "1172169364570664960": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "27", "datetime": "2019-09-12 15:25:29", "author": "@mahesh21aug"}, "566419452820869121": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "6,486", "datetime": "2015-02-14 02:11:35", "author": "@jingbay"}, "565779999647866881": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "1,686", "datetime": "2015-02-12 07:50:38", "author": "@Soukhinov"}, "1171964703322583041": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "63", "datetime": "2019-09-12 01:52:13", "author": "@kuanchen22"}, "1171965081640259585": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "7", "datetime": "2019-09-12 01:53:44", "author": "@kobai19"}, "1172302586420051968": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "912", "datetime": "2019-09-13 00:14:51", "author": "@ChikaObuah"}, "566163708238643200": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "2,709", "datetime": "2015-02-13 09:15:21", "author": "@adnothing"}, "565835779948027904": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "1,720", "datetime": "2015-02-12 11:32:17", "author": "@mooopan"}, "1172180680962392064": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "154", "datetime": "2019-09-12 16:10:27", "author": "@DCasBol"}, "1171880345630851074": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "49", "datetime": "2019-09-11 20:17:01", "author": "@paulomannjr"}, "566392292710551552": {"content_summary": "http://t.co/kGW3GFidgj", "followers": "21", "datetime": "2015-02-14 00:23:40", "author": "@TheUGNews"}, "1171884822643585025": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "317", "datetime": "2019-09-11 20:34:48", "author": "@phi_nate"}, "565720743938654208": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "227", "datetime": "2015-02-12 03:55:10", "author": "@J_P_Raymond"}, "568533953742655489": {"content_summary": "Batch Normalization paper reports that with it, dropout can be removed (!), CNN trained 10x faster for ImageNet SOTA. http://t.co/hh9f5W68hM", "followers": "588", "datetime": "2015-02-19 22:13:52", "author": "@Hyperion_HQ"}, "1171980227079065600": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "25", "datetime": "2019-09-12 02:53:55", "author": "@sgodil"}, "1171903190201241600": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "7", "datetime": "2019-09-11 21:47:48", "author": "@IMuquis"}, "1172466042238619648": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "48", "datetime": "2019-09-13 11:04:22", "author": "@mzadrogaPL"}, "565740605251129344": {"content_summary": "\u3010\u306f\u3066\u30d6\u65b0\u7740IT\u3011 [1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/Cq3NPhmthZ", "followers": "1,207", "datetime": "2015-02-12 05:14:05", "author": "@hilbert_d"}, "1172050637497888768": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "70", "datetime": "2019-09-12 07:33:42", "author": "@sumitsethy"}, "1171949391818760198": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,651", "datetime": "2019-09-12 00:51:23", "author": "@adjiboussodieng"}, "1172206938332921856": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "1,103", "datetime": "2019-09-12 17:54:47", "author": "@scottedwards200"}, "565720692759756800": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "1,678", "datetime": "2015-02-12 03:54:58", "author": "@jackwmson"}, "1171952244243271682": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "165", "datetime": "2019-09-12 01:02:43", "author": "@dbparedes"}, "565706059613216771": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "1,711", "datetime": "2015-02-12 02:56:49", "author": "@Keiku"}, "984644239706415104": {"content_summary": "https://t.co/wRhHPWYGxn", "followers": "101", "datetime": "2018-04-13 04:07:35", "author": "@nenecchi_ex"}, "1172506125775921154": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "25", "datetime": "2019-09-13 13:43:39", "author": "@jg__dl"}, "1203030245974130688": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "50", "datetime": "2019-12-06 19:15:36", "author": "@kushalchauhan98"}, "624865297074360320": {"content_summary": "'Kenreisman/machine-learning' Top: [1502.03167] Batch Normalization: Accelerati\u2026 http://t.co/eaDRxa9rSR, see more http://t.co/VAU8peuLvN", "followers": "950", "datetime": "2015-07-25 08:54:30", "author": "@anitayorker"}, "1172219745879384064": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "826", "datetime": "2019-09-12 18:45:40", "author": "@chiheb_tr"}, "566500459599634433": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] (http://t.co/SWhAcmXO0f) http://t.co/waAK4CQdLG", "followers": "161", "datetime": "2015-02-14 07:33:29", "author": "@talentxco"}, "565845463903907841": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "19", "datetime": "2015-02-12 12:10:46", "author": "@nilegeisinger"}, "1133732656766578689": {"content_summary": "2\u56de\u304f\u3089\u3044\u3057\u304b\u5358\u8a9e\u3050\u3050\u3089\u305a\u8aad\u3081\u305f\u3002\u30de\u30b8\u5b09\u3057\u3044\u3002/[1502.03167v3] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/KBVSE5meCn", "followers": "125", "datetime": "2019-05-29 13:51:43", "author": "@Enutoesu"}, "1172032057385594882": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "309", "datetime": "2019-09-12 06:19:52", "author": "@Daniel_J_Im"}, "1173160047284670467": {"content_summary": "Huh; was just reading about this", "followers": "257", "datetime": "2019-09-15 09:02:06", "author": "@asif_rehan"}, "1223337635470544899": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "89", "datetime": "2020-01-31 20:09:55", "author": "@XoTTaBiChe"}, "1172942903208898560": {"content_summary": "\ud83d\udc47\ud83c\udffdclear thread on the theoretical intuition why BN works in practice and empirical evidence backing this up\ud83d\udc47\ud83c\udffd tl;Dr: yes it is what we can call limiting the \"inner covariate shift\", restricting the eingenvalues of the Hessian", "followers": "679", "datetime": "2019-09-14 18:39:15", "author": "@tetraduzione"}, "1023687869368545280": {"content_summary": "nice", "followers": "392", "datetime": "2018-07-29 21:53:01", "author": "@gauravjain49"}, "1172141689961680896": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "28", "datetime": "2019-09-12 13:35:30", "author": "@_extratrees_"}, "1171915921000026112": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "93", "datetime": "2019-09-11 22:38:23", "author": "@rantlab"}, "1172181750556102657": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "16", "datetime": "2019-09-12 16:14:42", "author": "@GarethFunkaLea"}, "1173159585324109824": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "588", "datetime": "2019-09-15 09:00:16", "author": "@flavioclesio"}, "1034098509245571072": {"content_summary": "RT @lzyutopia: #100DaysOfMLCode day 13: Learned second paper about GoogLeNet --Batch Normalization: Accelerating Deep Network Training by R\u2026", "followers": "5,362", "datetime": "2018-08-27 15:21:11", "author": "@100DaysOfMLCode"}, "1171992823479955459": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "560", "datetime": "2019-09-12 03:43:58", "author": "@TimSweeney83"}, "566419404414386177": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/gTM8JQoBNU", "followers": "67", "datetime": "2015-02-14 02:11:24", "author": "@Rumnha"}, "1172463896361193473": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "17", "datetime": "2019-09-13 10:55:50", "author": "@JoelChen95"}, "565891009813311488": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "437", "datetime": "2015-02-12 15:11:45", "author": "@grousselle"}, "1172390994840449024": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "107", "datetime": "2019-09-13 06:06:09", "author": "@SnurakBill"}, "1173593888898080768": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "786", "datetime": "2019-09-16 13:46:02", "author": "@bessimaestro"}, "565950280634621952": {"content_summary": "Wow, the ImageNET classification battle between Google and Microsoft research is going fast and furious http://t.co/ae3ZAueTsG", "followers": "673", "datetime": "2015-02-12 19:07:16", "author": "@GnGruj"}, "565946623830667265": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "661", "datetime": "2015-02-12 18:52:44", "author": "@syang"}, "1172297883372023813": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "46", "datetime": "2019-09-12 23:56:10", "author": "@yumash3"}, "565773865822191616": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "753", "datetime": "2015-02-12 07:26:15", "author": "@suneelmarthi"}, "1172295679781158917": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "28", "datetime": "2019-09-12 23:47:24", "author": "@JawaeChan"}, "1172298520767787013": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "456", "datetime": "2019-09-12 23:58:42", "author": "@PerthMLGroup"}, "565817882919989249": {"content_summary": "Fierce race for ImageNet, now Google gives us 4.82% - Baidu 5.98, Microsoft 4.9, and Google 4.82. http://t.co/0BlSHOFZFD #deeplearning", "followers": "74", "datetime": "2015-02-12 10:21:10", "author": "@annodomini80"}, "1172125361037041664": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "19", "datetime": "2019-09-12 12:30:37", "author": "@arroyadr"}, "1171962247888830464": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "655", "datetime": "2019-09-12 01:42:28", "author": "@iampujan20"}, "1209309144420954114": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "216", "datetime": "2019-12-24 03:05:42", "author": "@udaykovur"}, "1172519844803010560": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "8", "datetime": "2019-09-13 14:38:10", "author": "@boo_o1"}, "1173454997972873217": {"content_summary": "Bookmarking. Paper in a tweetstorm", "followers": "1,501", "datetime": "2019-09-16 04:34:07", "author": "@DrZeeshanZia"}, "1171885603933315072": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "201", "datetime": "2019-09-11 20:37:55", "author": "@vlatsis"}, "1171878801451757568": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,137", "datetime": "2019-09-11 20:10:53", "author": "@fabtar"}, "1171901706491162624": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "25", "datetime": "2019-09-11 21:41:54", "author": "@aiwarlord"}, "1171878952979378176": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "100", "datetime": "2019-09-11 20:11:29", "author": "@jimmycomfort"}, "565742381266243584": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "993", "datetime": "2015-02-12 05:21:09", "author": "@taniokah"}, "1171897271396552704": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "2,813", "datetime": "2019-09-11 21:24:16", "author": "@CSProfKGD"}, "1172035699186339840": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,353", "datetime": "2019-09-12 06:34:20", "author": "@gottapatchemall"}, "1171927753370849281": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "6", "datetime": "2019-09-11 23:25:24", "author": "@deVinay7"}, "1172750332436389888": {"content_summary": "Interesting thread summarising how batchnorm works", "followers": "1,068", "datetime": "2019-09-14 05:54:02", "author": "@emrobSci"}, "1172020339141857280": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "388", "datetime": "2019-09-12 05:33:18", "author": "@adropboxspace"}, "1172129654393556992": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "7,966", "datetime": "2019-09-12 12:47:41", "author": "@bhutanisanyam1"}, "565699980477104128": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "198", "datetime": "2015-02-12 02:32:40", "author": "@omar_javd"}, "1180935593926127616": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "96", "datetime": "2019-10-06 19:59:20", "author": "@Swaroopkml"}, "565906082552631296": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "243", "datetime": "2015-02-12 16:11:38", "author": "@neurosp1ke"}, "569689117837164544": {"content_summary": "Batch Normalization: Accelerating DeepNet Training by Reducing Internal Covariate Shift http://t.co/9UDIeZ6Fir #machinelearning Promising!!", "followers": "2,031", "datetime": "2015-02-23 02:44:04", "author": "@federicolois"}, "1172292840811978752": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "63", "datetime": "2019-09-12 23:36:08", "author": "@lgr3d"}, "1172013682227679234": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "32", "datetime": "2019-09-12 05:06:51", "author": "@Amplituhedr0n"}, "565722595119882240": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "935", "datetime": "2015-02-12 04:02:31", "author": "@stardazed0"}, "565780620228706304": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "78", "datetime": "2015-02-12 07:53:06", "author": "@mattvonrohr"}, "566392307146952704": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/kUnOBwyGOq", "followers": "255", "datetime": "2015-02-14 00:23:43", "author": "@MasterCMG"}, "1172281124212682752": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "121", "datetime": "2019-09-12 22:49:34", "author": "@leoleylevi"}, "565917438571577344": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (pdf) | http://t.co/eOpN0lMIAY", "followers": "3,515", "datetime": "2015-02-12 16:56:46", "author": "@agibsonccc"}, "1171977171243622401": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "28", "datetime": "2019-09-12 02:41:46", "author": "@an_interstice"}, "1172160624207392769": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "620", "datetime": "2019-09-12 14:50:45", "author": "@philtor"}, "1171973968808611841": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "135", "datetime": "2019-09-12 02:29:03", "author": "@_gngdb"}, "1101783864517767168": {"content_summary": "Batch Normalization is other esotheric technic in Deep Learning that has no theoretic background but works pretty well. https://t.co/SQlu7Skyt8 https://t.co/LupnBMyYBH #deeplearning", "followers": "43", "datetime": "2019-03-02 09:58:38", "author": "@devoslaru"}, "566386923699453952": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/RdwEDSpwjq", "followers": "205,636", "datetime": "2015-02-14 00:02:20", "author": "@newsycombinator"}, "1172104824927768576": {"content_summary": "Wondering about batch norm? Read this amazing thread.", "followers": "1,807", "datetime": "2019-09-12 11:09:01", "author": "@fdellaert"}, "941538836718354432": {"content_summary": "\u81ea\u6162\u3058\u3083\u306a\u3044\u304c\u3001\u3053\u3093\u306a\u306e\u8aad\u3093\u3067\u3082\u305c\u3093\u305c\u3093\u5206\u304b\u3093\u306a\u3044\u305c\u3002 https://t.co/EDntMwTrNK \u304c\u3093\u3070\u3063\u3066\u8aad\u3080\u3051\u3069\u3055\u3002", "followers": "834", "datetime": "2017-12-15 05:22:06", "author": "@mune44a"}, "938243685162000387": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/wcXpQpzN5n", "followers": "660", "datetime": "2017-12-06 03:08:20", "author": "@kristiyanto_"}, "566199900527153152": {"content_summary": "\u041e\u0433\u043e, \u043e\u0433\u043e. \u041e\u0431\u0443\u0447\u0430\u0435\u043c ImageNet \u0432 10 \u0440\u0430\u0437 \u0431\u044b\u0441\u0442\u0440\u0435\u0435. \u0422\u0430\u043a\u0438\u043c\u0438 \u0442\u0435\u043c\u043f\u0430\u043c\u0438 \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435\u043c \u0433\u043e\u0434\u0443 \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c ImageNet \u043d\u0430 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u0438\u0447\u043a\u0430\u043c) http://t.co/5YcF4D5aaf", "followers": "132", "datetime": "2015-02-13 11:39:10", "author": "@ekrofto"}, "565720602720608257": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "11,717", "datetime": "2015-02-12 03:54:36", "author": "@yutakashino"}, "566509005087211520": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "582", "datetime": "2015-02-14 08:07:26", "author": "@kaithomsen"}, "801468095839617024": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Paper https://t.co/Se5bRWmptM", "followers": "1,058", "datetime": "2016-11-23 16:50:59", "author": "@BigsnarfDude"}, "1171974104611606528": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "205", "datetime": "2019-09-12 02:29:35", "author": "@a_random_obsrvr"}, "1172065892458487808": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "103", "datetime": "2019-09-12 08:34:19", "author": "@urosn"}, "1171874070536904707": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "95", "datetime": "2019-09-11 19:52:05", "author": "@KuroseCSEE"}, "1171949686309232642": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,063", "datetime": "2019-09-12 00:52:33", "author": "@miguelalonsojr"}, "1172068798947569665": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "50", "datetime": "2019-09-12 08:45:52", "author": "@sennraf"}, "565928687225163777": {"content_summary": "Deep neural networks just got a lot faster: http://t.co/dnL1BkoI37", "followers": "673", "datetime": "2015-02-12 17:41:28", "author": "@anthonygarvan"}, "842967013983227904": {"content_summary": "Good paper about the importance of batch normalization: https://t.co/uEP1Lxj3Zg #deeplearning #neuralnetworks", "followers": "30", "datetime": "2017-03-18 05:12:52", "author": "@DavidCurrie32"}, "1172192054618705921": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "91", "datetime": "2019-09-12 16:55:38", "author": "@0xhexhex"}, "668513329414021120": {"content_summary": "RT @alexjc: Catching up on research: Batch Normalization is \"drop everything else and do it now\" insane. https://t.co/WwyS8P02DB https://t.\u2026", "followers": "812", "datetime": "2015-11-22 19:36:12", "author": "@alpinegizmo"}, "565711984780648449": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "802", "datetime": "2015-02-12 03:20:22", "author": "@ohnabe"}, "566452033784266752": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "2,670", "datetime": "2015-02-14 04:21:03", "author": "@ayirpelle"}, "1172316651636678662": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "4,670", "datetime": "2019-09-13 01:10:44", "author": "@xsteenbrugge"}, "1171925340567924736": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "222", "datetime": "2019-09-11 23:15:49", "author": "@chalerm_kul"}, "566392314994507776": {"content_summary": "Hacker: Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/kmhGkrXK1v", "followers": "414", "datetime": "2015-02-14 00:23:45", "author": "@RaivooNET"}, "1172051429499916288": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "236", "datetime": "2019-09-12 07:36:51", "author": "@_merovingienne_"}, "566508840586575872": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "427", "datetime": "2015-02-14 08:06:47", "author": "@mkalina"}, "566592394205097984": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "9,451", "datetime": "2015-02-14 13:38:48", "author": "@ARTEKLAB"}, "710544494282850304": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/xAM1clRxto", "followers": "52", "datetime": "2016-03-17 19:13:03", "author": "@hakanardo"}, "565982335560994817": {"content_summary": "New state of the art on image classification is 10x faster than before and more accurate than well-trained humans. http://t.co/Ni66gaAEFO", "followers": "464", "datetime": "2015-02-12 21:14:38", "author": "@JohnKuelper"}, "1172419072123736064": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "376", "datetime": "2019-09-13 07:57:43", "author": "@alessandroleite"}, "1171994644290211840": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "0", "datetime": "2019-09-12 03:51:12", "author": "@aobai222"}, "624829542809505792": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "47", "datetime": "2015-07-25 06:32:26", "author": "@phillypoopskins"}, "894163849116516352": {"content_summary": "Sunday for @deeplearning batch normalization https://t.co/RHdhUfA1Oc", "followers": "235", "datetime": "2017-08-06 11:50:49", "author": "@xavysp"}, "566217624582094848": {"content_summary": "Paper on improving the efficiency of deep learning networks by batched normalization #ml http://t.co/hvhuxNYZva", "followers": "6,021", "datetime": "2015-02-13 12:49:36", "author": "@carlcarrie"}, "1171888969530785792": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "113", "datetime": "2019-09-11 20:51:17", "author": "@resvera"}, "566846257352544256": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "825", "datetime": "2015-02-15 06:27:33", "author": "@morioka"}, "565719336489590788": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "12,147", "datetime": "2015-02-12 03:49:35", "author": "@theophaneweber"}, "1172245400519987200": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,519", "datetime": "2019-09-12 20:27:37", "author": "@EdHenry_"}, "1172120761756045313": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "231", "datetime": "2019-09-12 12:12:21", "author": "@nikhilbalaji"}, "1171874689188106240": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "168", "datetime": "2019-09-11 19:54:32", "author": "@ShumonLahiri"}, "566421927737057281": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "203", "datetime": "2015-02-14 02:21:25", "author": "@rogerplevy"}, "1171981954108936192": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "65", "datetime": "2019-09-12 03:00:46", "author": "@chenzhekl"}, "1172047612754743301": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "189", "datetime": "2019-09-12 07:21:41", "author": "@AndreuSancho"}, "1171875774200188928": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "34", "datetime": "2019-09-11 19:58:51", "author": "@DeepMatrixCloud"}, "1172854313686851585": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "338", "datetime": "2019-09-14 12:47:13", "author": "@__snehal__"}, "1171881386057306113": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "26", "datetime": "2019-09-11 20:21:09", "author": "@mmichalmadej"}, "721419648479039488": {"content_summary": "TIL Batch Normalization for Deep Learning https://t.co/lB5P3rxoqw", "followers": "1,058", "datetime": "2016-04-16 19:27:02", "author": "@BigsnarfDude"}, "1172280985511190528": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "121", "datetime": "2019-09-12 22:49:01", "author": "@leoleylevi"}, "1171875379457474562": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2,420", "datetime": "2019-09-11 19:57:17", "author": "@dcpage3"}, "1171968172964102144": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "4,740", "datetime": "2019-09-12 02:06:01", "author": "@rzembo"}, "1172008899475410944": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "29", "datetime": "2019-09-12 04:47:51", "author": "@sourajitCS"}, "566418175710199808": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf]: http://t.co/ORWoCc5S2l Comments: https://t.co/IGtWKmELip", "followers": "856", "datetime": "2015-02-14 02:06:31", "author": "@gallegoxx"}, "566387502550769667": {"content_summary": "#beta Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/Pj6Ahzjco7 #Movin", "followers": "43,453", "datetime": "2015-02-14 00:04:38", "author": "@alva_albarce"}, "1172026508845703168": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "226", "datetime": "2019-09-12 05:57:49", "author": "@malicannoyan"}, "565807188871221248": {"content_summary": "V pond\u011bl\u00ed um\u011bl Microsoft rozpozn\u00e1vat obr\u00e1zky l\u00e9pe ne\u017e \u010dlov\u011bk, dnes to oznamuje i Google: http://t.co/eoOIAchgb4 (4,8%, lid\u00e9 maj\u00ed 5,1%)", "followers": "27,134", "datetime": "2015-02-12 09:38:40", "author": "@tyinternety"}, "1172181735683100672": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "844", "datetime": "2019-09-12 16:14:38", "author": "@PiotrCzapla"}, "1172158325380800514": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "65", "datetime": "2019-09-12 14:41:37", "author": "@dave_co_dev"}, "746477362737483777": {"content_summary": "RT @onepaperperday: \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\": https://t.co/1aiLCaCwUG\u2026", "followers": "649", "datetime": "2016-06-24 22:57:26", "author": "@JoshARhoads"}, "1171999453319811072": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "72", "datetime": "2019-09-12 04:10:19", "author": "@gabeibagon"}, "1171886822739316747": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "46", "datetime": "2019-09-11 20:42:45", "author": "@davidbarbera9"}, "1172491007306469376": {"content_summary": "RT jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about training mechanics by studying this thread and the links it contains. https://t.co/mTZgsYzTm3", "followers": "419", "datetime": "2019-09-13 12:43:34", "author": "@datovisual"}, "566401397403299841": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/ODrz2rXKtk #hackernews", "followers": "1,649", "datetime": "2015-02-14 00:59:51", "author": "@jasondotstar"}, "705474306990411776": {"content_summary": "Mini-batch\u5358\u4f4d\u3067\u306enormalization\u306b\u3088\u3063\u3066internal covariance shift\u3092\u6291\u3048\u3001\u5b66\u7fd2\u30ec\u30fc\u30c8\u3092\u4e0a\u3052\u3066\u3082\u540c\u7b49\u306a\u7cbe\u5ea6\u3092\u9054\u6210\u3067\u304d\u308b\u3002\u3088\u3063\u3066\u3001\u5b66\u7fd2\u901f\u5ea6\u304c\u5411\u4e0a\u3002\u52a0\u3048\u3066\u3001\u6b63\u5247\u5316\u52b9\u679c\u3082\u4f75\u305b\u6301\u3064\u3002t.tkd https://t.co/gEjEuCBPs0", "followers": "109", "datetime": "2016-03-03 19:25:56", "author": "@hamadalab_jc"}, "1171949707758833664": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "671", "datetime": "2019-09-12 00:52:38", "author": "@erwtokritos"}, "1018604788362039296": {"content_summary": "RT @alienelf: #100DaysOfMLCode: Day #3 and Day #4: - Fighting with cloud providers to get a GPU - A refresher on CNN basics - A deep dive i\u2026", "followers": "5,362", "datetime": "2018-07-15 21:14:40", "author": "@100DaysOfMLCode"}, "566457909601894400": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "1,094", "datetime": "2015-02-14 04:44:24", "author": "@InsanityBit"}, "1172184909240328192": {"content_summary": "What\u2019s Batch Norm? Check out this great thread! \ud83e\udd29", "followers": "77", "datetime": "2019-09-12 16:27:15", "author": "@Michallys"}, "1172128267089907712": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "261", "datetime": "2019-09-12 12:42:10", "author": "@GuptaRajat033"}, "1172194094493253633": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,591", "datetime": "2019-09-12 17:03:45", "author": "@bigeagle_xd"}, "1172069920282165248": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "71", "datetime": "2019-09-12 08:50:19", "author": "@v_trokhymenko"}, "1171897096082993152": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,061", "datetime": "2019-09-11 21:23:35", "author": "@mvaldenegro"}, "1172045237952401408": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "418", "datetime": "2019-09-12 07:12:14", "author": "@dantkz"}, "651416375605481472": {"content_summary": "RT @karpathy: 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see \u2026", "followers": "473", "datetime": "2015-10-06 15:19:01", "author": "@MShahriariNia"}, "1174747182827757568": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "58", "datetime": "2019-09-19 18:08:48", "author": "@_WizDom13_"}, "1172096795725500416": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "93", "datetime": "2019-09-12 10:37:07", "author": "@Gjergji_"}, "1172169932571893761": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2", "datetime": "2019-09-12 15:27:44", "author": "@VerbovenSam"}, "1172157516903571459": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "74", "datetime": "2019-09-12 14:38:24", "author": "@bhavickat"}, "1172052475404111872": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "48", "datetime": "2019-09-12 07:41:00", "author": "@mpsampat"}, "1172079603868942336": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "100", "datetime": "2019-09-12 09:28:48", "author": "@mdasaduluofa"}, "566392303405658113": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/mhH4MejRND", "followers": "853", "datetime": "2015-02-14 00:23:42", "author": "@vojinurosevic"}, "566492795074924544": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/iukOK7CIrV (http://t.co/iCSKHIQTb6)", "followers": "8,709", "datetime": "2015-02-14 07:03:01", "author": "@newsyc100"}, "1172793917584097281": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "46", "datetime": "2019-09-14 08:47:14", "author": "@actualmberens"}, "1172128790987722752": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "120", "datetime": "2019-09-12 12:44:15", "author": "@sengkim123"}, "566386624246722560": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "4,265", "datetime": "2015-02-14 00:01:08", "author": "@jhamby"}, "1185541399036350464": {"content_summary": "RT @_brohrer_: @dcpage3 offers keen insights into how batch norm works. I particularly resonated with his meta-conclusions: 1) When studyin\u2026", "followers": "1,134", "datetime": "2019-10-19 13:01:10", "author": "@arunprakashml"}, "565773925582266368": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "1,276", "datetime": "2015-02-12 07:26:30", "author": "@tesatory"}, "1171963657149919232": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "180", "datetime": "2019-09-12 01:48:04", "author": "@SanjeethVeigas"}, "1172157444061040640": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "1,338", "datetime": "2019-09-12 14:38:06", "author": "@VJHodge"}, "565759261779968000": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "28,418", "datetime": "2015-02-12 06:28:13", "author": "@ogrisel"}, "566385952084742144": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/oKUw5jJQX3", "followers": "448", "datetime": "2015-02-13 23:58:28", "author": "@Un1v3rs0Z3r0"}, "566380113034764289": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/GWb6G7zzbX http://t.co/HaUgssBNTl", "followers": "314", "datetime": "2015-02-13 23:35:16", "author": "@hnbreaking"}, "1172073538569875458": {"content_summary": "Nice discussion about Batch Norm...", "followers": "445", "datetime": "2019-09-12 09:04:42", "author": "@FedPernici"}, "1172067248627957760": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "93", "datetime": "2019-09-12 08:39:42", "author": "@Xelfor"}, "566743954280824833": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "1,553", "datetime": "2015-02-14 23:41:02", "author": "@Eagerton"}, "568241447993892864": {"content_summary": "Save us from covariate shift! Ahhh! \"@newsycombinator: Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/pIoczz4JtN\"", "followers": "1,026", "datetime": "2015-02-19 02:51:33", "author": "@klepperson"}, "565915007120982017": {"content_summary": "@googleresearch http://t.co/ea554HiXpx bests @MSFTResearch http://t.co/2vuHYgcpz4 record breaking Imagenet results only hours later.", "followers": "1,443", "datetime": "2015-02-12 16:47:06", "author": "@marchamilton"}, "1172490599032926208": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "882", "datetime": "2019-09-13 12:41:57", "author": "@datasciencefan"}, "565917553726222336": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (pdf) | http://t.co/eOpN0lMIAY", "followers": "305", "datetime": "2015-02-12 16:57:13", "author": "@jmohamedzahoor"}, "1172157561941811200": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "65", "datetime": "2019-09-12 14:38:35", "author": "@jjayaram7"}, "1172184146203959296": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "87", "datetime": "2019-09-12 16:24:13", "author": "@pushpendra_7"}, "1171955177043218432": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "648", "datetime": "2019-09-12 01:14:22", "author": "@jvmancuso"}, "565783690836312064": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "3,449", "datetime": "2015-02-12 08:05:18", "author": "@reiver"}, "566077969245151232": {"content_summary": "GOOG +1 over MSFT! Input norm >Param init, 1/10x training steps #DeepLearning HT @annodomini80 http://t.co/ggnhDS4NlL http://t.co/2QARCdZdoQ", "followers": "2,581", "datetime": "2015-02-13 03:34:39", "author": "@pentagoniac"}, "565928550650236928": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "105", "datetime": "2015-02-12 17:40:55", "author": "@catzdong"}, "1172138533475233797": {"content_summary": "RT @yoavgo: (another!) great thread re batch norm. https://t.co/LP3E3vjJUa", "followers": "962", "datetime": "2019-09-12 13:22:58", "author": "@bgalbraith"}, "565725742450089984": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "353", "datetime": "2015-02-12 04:15:02", "author": "@Jeff88Ho"}, "1172017483395604480": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "252", "datetime": "2019-09-12 05:21:57", "author": "@_DLPBGJ80C04Z_"}, "1172123487957344256": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "136", "datetime": "2019-09-12 12:23:11", "author": "@leo_assaff"}, "1172078455695056896": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "177", "datetime": "2019-09-12 09:24:14", "author": "@KeithNyati"}, "565952297801641984": {"content_summary": "Google gets 4.8% on #ImageNet, 14x speedup, by normalizing inputs every mini-batch http://t.co/ovE4iY0hlP HT @karpathy", "followers": "179", "datetime": "2015-02-12 19:15:17", "author": "@yannick_martel"}, "1171993581688709120": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "162", "datetime": "2019-09-12 03:46:59", "author": "@skhanshadab87"}, "1171992784644915200": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "42", "datetime": "2019-09-12 03:43:49", "author": "@jonojace"}, "1175057318863613953": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "7", "datetime": "2019-09-20 14:41:10", "author": "@mewtwo_tr"}, "566392411723558912": {"content_summary": "@scribbler992000 Batch Normalization: Accelerating Deep Network Training [pdf]: Comments http://t.co/zXHm7Cv0iA", "followers": "669", "datetime": "2015-02-14 00:24:08", "author": "@scribbler992000"}, "1172063795138547714": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "484", "datetime": "2019-09-12 08:25:59", "author": "@timothy_lkh_"}, "1171900235540508673": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "291", "datetime": "2019-09-11 21:36:03", "author": "@Mickael_Chen"}, "1171934285483274240": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1", "datetime": "2019-09-11 23:51:21", "author": "@reksieapp"}, "565944705469657088": {"content_summary": "Wow, the ImageNET classification battle between Google and Microsoft research is going fast and furious http://t.co/ae3ZAueTsG", "followers": "7,418", "datetime": "2015-02-12 18:45:07", "author": "@rickasaurus"}, "566386445959450624": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human: http://t.co/4HMhn1E2r2 via @kouioapp", "followers": "221", "datetime": "2015-02-14 00:00:26", "author": "@ALinkedList"}, "566392417041932288": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/Gm6xRkZJgx", "followers": "102", "datetime": "2015-02-14 00:24:09", "author": "@mit01dev"}, "1171877024014766081": {"content_summary": "Great thread on #batchnorm - HT @jeremyphoward cc: @bbriniotis", "followers": "3,864", "datetime": "2019-09-11 20:03:49", "author": "@drsxr"}, "566378794634596352": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf]: http://t.co/ORWoCc5S2l Comments: https://t.co/IGtWKmELip", "followers": "17,702", "datetime": "2015-02-13 23:30:02", "author": "@HNTweets"}, "1172026126300061696": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "962", "datetime": "2019-09-12 05:56:18", "author": "@pjgrizel"}, "1171993630447325186": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "105", "datetime": "2019-09-12 03:47:10", "author": "@Coder26237717"}, "1177173367637651457": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "143", "datetime": "2019-09-26 10:49:36", "author": "@larosaandrea"}, "1172135894083624961": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "63", "datetime": "2019-09-12 13:12:29", "author": "@dpsmarques"}, "1172117565059018752": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "742", "datetime": "2019-09-12 11:59:39", "author": "@xpasky"}, "566379502985818112": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/xoFGogL17T", "followers": "318", "datetime": "2015-02-13 23:32:50", "author": "@pradeepbheron"}, "1171916596690018305": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,264", "datetime": "2019-09-11 22:41:04", "author": "@sroecker"}, "565952567365349376": {"content_summary": "Wow, the ImageNET classification battle between Google and Microsoft research is going fast and furious http://t.co/ae3ZAueTsG", "followers": "214", "datetime": "2015-02-12 19:16:21", "author": "@tajgur"}, "1172118547595546624": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "304", "datetime": "2019-09-12 12:03:33", "author": "@flrgsr"}, "1172028021500534784": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "131", "datetime": "2019-09-12 06:03:50", "author": "@konstantinos_fn"}, "565716462846095360": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "1,097", "datetime": "2015-02-12 03:38:09", "author": "@everpeace"}, "1171987128437411840": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "75", "datetime": "2019-09-12 03:21:20", "author": "@AndreySozykin"}, "1172216126748672001": {"content_summary": "Nice summary of batch norm, the motivation and history of related techniques.", "followers": "42", "datetime": "2019-09-12 18:31:17", "author": "@nsivacki"}, "1172386536916049922": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "7", "datetime": "2019-09-13 05:48:26", "author": "@hezhen13"}, "568525046202769409": {"content_summary": "Batch Normalization paper reports that with it, dropout can be removed (!), CNN trained 10x faster for ImageNet SOTA. http://t.co/hh9f5W68hM", "followers": "1,943", "datetime": "2015-02-19 21:38:28", "author": "@dnouri"}, "1172409268558913538": {"content_summary": "A fun deep dive on \u201cWhy Batch Norm?\u201d", "followers": "11,390", "datetime": "2019-09-13 07:18:46", "author": "@_brohrer_"}, "1172078943668715520": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "80", "datetime": "2019-09-12 09:26:11", "author": "@assoulix"}, "1171880682584457217": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,619", "datetime": "2019-09-11 20:18:21", "author": "@avitaloliver"}, "829287081214091264": {"content_summary": "#DLwPy Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift / \u201c1502.03167.\u2026\u201d https://t.co/uZR1aaG0gv", "followers": "4,825", "datetime": "2017-02-08 11:13:42", "author": "@prototechno"}, "1172293541957005312": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "825", "datetime": "2019-09-12 23:38:55", "author": "@morioka"}, "567004817953157120": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "47", "datetime": "2015-02-15 16:57:37", "author": "@giovanichiachia"}, "1172131626328961024": {"content_summary": "(another!) great thread re batch norm.", "followers": "13,782", "datetime": "2019-09-12 12:55:31", "author": "@yoavgo"}, "566193717586165760": {"content_summary": "\u041e\u0433\u043e, \u043e\u0433\u043e. \u041e\u0431\u0443\u0447\u0430\u0435\u043c ImageNet \u0432 10 \u0440\u0430\u0437 \u0431\u044b\u0441\u0442\u0440\u0435\u0435. \u0422\u0430\u043a\u0438\u043c\u0438 \u0442\u0435\u043c\u043f\u0430\u043c\u0438 \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435\u043c \u0433\u043e\u0434\u0443 \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c ImageNet \u043d\u0430 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u0438\u0447\u043a\u0430\u043c) http://t.co/5YcF4D5aaf", "followers": "363", "datetime": "2015-02-13 11:14:36", "author": "@aachigorin"}, "1172014184822976513": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "16,821", "datetime": "2019-09-12 05:08:51", "author": "@octonion"}, "1171985360726962176": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "17", "datetime": "2019-09-12 03:14:19", "author": "@shreyan_ch"}, "1171911151401230336": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "486", "datetime": "2019-09-11 22:19:26", "author": "@ChombaBupe"}, "1172301985988501504": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "726", "datetime": "2019-09-13 00:12:28", "author": "@wenmingye"}, "1023686842229481472": {"content_summary": "RS: Batch normalization: attempt to normalize inputs to speed up training. Normalizes pre-activation. Helps keep them in a non-saturating regime \u2192 helps with vanishing gradient problem. #JSM2018 https://t.co/raVaJFCuPw", "followers": "16,447", "datetime": "2018-07-29 21:48:57", "author": "@michaelhoffman"}, "566450603204042753": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "2,383", "datetime": "2015-02-14 04:15:22", "author": "@HeartPowered"}, "709687861541531649": {"content_summary": "@itchyankles @txustice In Deep Learning this is called Batch Normalization. 14x learning speed up https://t.co/Yr4R6j8HCv", "followers": "589", "datetime": "2016-03-15 10:29:06", "author": "@mjhirn"}, "1172530496481501185": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "77", "datetime": "2019-09-13 15:20:29", "author": "@jeanfrancois287"}, "1172133677725147137": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,766", "datetime": "2019-09-12 13:03:40", "author": "@jaialkdanel"}, "1172115021842845701": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "76", "datetime": "2019-09-12 11:49:32", "author": "@PavitSankar"}, "1173201672056668161": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "750", "datetime": "2019-09-15 11:47:30", "author": "@data__wizard"}, "566384423814594560": {"content_summary": "[Hacker News] Batch Normalization: Accelerating Deep Network Training [pdf]: https://t.co/xbcbIpEZC1", "followers": "995", "datetime": "2015-02-13 23:52:24", "author": "@betachatnews"}, "1171882936796401666": {"content_summary": "Everything you wanted to know about barchnorm but were too afraid to ask", "followers": "683", "datetime": "2019-09-11 20:27:19", "author": "@analyticsaurabh"}, "1172160245348499457": {"content_summary": "Very informative thread ....", "followers": "41", "datetime": "2019-09-12 14:49:14", "author": "@VenkyMsd"}, "566390370292879362": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/EhsbN2vGK4 (http://t.co/b1Zl7RE9gt)", "followers": "6,206", "datetime": "2015-02-14 00:16:01", "author": "@newsyc50"}, "1172605198906339329": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "11", "datetime": "2019-09-13 20:17:20", "author": "@wladrodriguez"}, "1172416082331619329": {"content_summary": "RT @fdellaert: Wondering about batch norm? Read this amazing thread. https://t.co/y5oXJNBXsq", "followers": "134", "datetime": "2019-09-13 07:45:51", "author": "@GiseopK"}, "1172007045819707397": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "591", "datetime": "2019-09-12 04:40:29", "author": "@rmaestrem"}, "1171949161064747008": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "67", "datetime": "2019-09-12 00:50:28", "author": "@dksdc"}, "1172009704324784128": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "87", "datetime": "2019-09-12 04:51:03", "author": "@mmoran0032"}, "776621751354687488": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate https://t.co/ePP09u7DV5", "followers": "2,283", "datetime": "2016-09-16 03:20:29", "author": "@AdaptToReality"}, "565908018781384705": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "26", "datetime": "2015-02-12 16:19:20", "author": "@random_samples"}, "1172278609031180289": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "399", "datetime": "2019-09-12 22:39:34", "author": "@kamilsindi"}, "566387509693673472": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/J9NqTmTE9F", "followers": "814", "datetime": "2015-02-14 00:04:39", "author": "@StartUpObserve"}, "1172032880526155777": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "1,103", "datetime": "2019-09-12 06:23:08", "author": "@permutans"}, "1018604890166349824": {"content_summary": "RT @alienelf: #100DaysOfMLCode: Day #3 and Day #4: - Fighting with cloud providers to get a GPU - A refresher on CNN basics - A deep dive i\u2026", "followers": "1,511", "datetime": "2018-07-15 21:15:05", "author": "@rogue_corq"}, "1018604786197975042": {"content_summary": "#100DaysOfMLCode: Day #3 and Day #4: - Fighting with cloud providers to get a GPU - A refresher on CNN basics - A deep dive into generator architectures (Upsampling etc) - Reading up on batch normalization https://t.co/wdvpL7sKgG Full journey here: https:", "followers": "6,225", "datetime": "2018-07-15 21:14:40", "author": "@alienelf"}, "1171880215934455808": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "224", "datetime": "2019-09-11 20:16:30", "author": "@shaneisley"}, "566389325475356672": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "23,054", "datetime": "2015-02-14 00:11:52", "author": "@kennwhite"}, "1172158318439059456": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "243", "datetime": "2019-09-12 14:41:35", "author": "@daigo_hirooka"}, "565702859367653376": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "192,230", "datetime": "2015-02-12 02:44:06", "author": "@fchollet"}, "1172313216543707137": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "13", "datetime": "2019-09-13 00:57:06", "author": "@Dionysus3261"}, "1171992764784902144": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "105", "datetime": "2019-09-12 03:43:44", "author": "@sheikmohdimran"}, "1171938804858408960": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "217", "datetime": "2019-09-12 00:09:19", "author": "@AssistedEvolve"}, "565886568356143104": {"content_summary": "Speedup CNN training by 14x by batch normalisation->reducing covariate shift. Will have to try it out.. http://t.co/WdrEC2G11e", "followers": "42", "datetime": "2015-02-12 14:54:06", "author": "@mobilepri"}, "1171990172537368576": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "12", "datetime": "2019-09-12 03:33:26", "author": "@bartoldson"}, "1172168093013106689": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,235", "datetime": "2019-09-12 15:20:25", "author": "@hadehdhayor"}, "1172216557910601728": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "211", "datetime": "2019-09-12 18:33:00", "author": "@vi66r"}, "1172156859047788544": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "99", "datetime": "2019-09-12 14:35:47", "author": "@treasured_write"}, "1172128986639622150": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "559", "datetime": "2019-09-12 12:45:02", "author": "@JayChance5"}, "1172207510314504192": {"content_summary": "Excellent!", "followers": "309", "datetime": "2019-09-12 17:57:03", "author": "@eraldoluis"}, "566387108483715072": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/RdwEDSpwjq", "followers": "218", "datetime": "2015-02-14 00:03:04", "author": "@elraro21"}, "626558571862339584": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "825", "datetime": "2015-07-30 01:02:59", "author": "@fly51fly"}, "1171906892689563658": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "29", "datetime": "2019-09-11 22:02:30", "author": "@daniel_mbiye"}, "1171879763843010560": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "37", "datetime": "2019-09-11 20:14:42", "author": "@saidwivedi"}, "566412033235513345": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/CtlQoitbsI", "followers": "14,830", "datetime": "2015-02-14 01:42:06", "author": "@gregowell"}, "1171929130234830848": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "36", "datetime": "2019-09-11 23:30:52", "author": "@vasan_ashwin"}, "1171995098273525762": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "885", "datetime": "2019-09-12 03:53:00", "author": "@RichmanRonald"}, "1172099211589906432": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "189", "datetime": "2019-09-12 10:46:43", "author": "@tnarihi"}, "565921284266012672": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "783", "datetime": "2015-02-12 17:12:03", "author": "@albietz"}, "996736555615047681": {"content_summary": "Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift https://t.co/YNOuFAwbRO", "followers": "18", "datetime": "2018-05-16 12:58:07", "author": "@ShinobuKinjo1"}, "1172130449491189761": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "50", "datetime": "2019-09-12 12:50:50", "author": "@isaac2lord"}, "1171881107215855616": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "22,788", "datetime": "2019-09-11 20:20:03", "author": "@DotCSV"}, "963824104980865025": {"content_summary": "https://t.co/14WcW8H34n I am amazed at how well batch normalization works. Models train faster and get better accuracy.", "followers": "3", "datetime": "2018-02-14 17:15:48", "author": "@escuccimarra"}, "1171990247011319813": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "767", "datetime": "2019-09-12 03:33:44", "author": "@_smileyball"}, "565956160734629888": {"content_summary": "Wow, the ImageNET classification battle between Google and Microsoft research is going fast and furious http://t.co/ae3ZAueTsG", "followers": "949", "datetime": "2015-02-12 19:30:38", "author": "@DrHallba"}, "1185528492533932032": {"content_summary": "@dcpage3 offers keen insights into how batch norm works. I particularly resonated with his meta-conclusions: 1) When studying neural networks, use simple examples. You can iterate faster and see what\u2019s going on more easily. 2) VISUALIZE. h/t @jeremyphowa", "followers": "11,390", "datetime": "2019-10-19 12:09:53", "author": "@_brohrer_"}, "1224388347789037570": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "56,127", "datetime": "2020-02-03 17:45:04", "author": "@IntelligenceTV"}, "567418340109721600": {"content_summary": "Using batch normalization to reduce internal covariate shift in deep neural nets & achieve a 4.8% ImageNet test error http://t.co/Nh3OqtIeyn", "followers": "31,108", "datetime": "2015-02-16 20:20:49", "author": "@benhamner"}, "1172773082676617216": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,193", "datetime": "2019-09-14 07:24:26", "author": "@frankolken"}, "566934463666348033": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "2,112", "datetime": "2015-02-15 12:18:03", "author": "@mitmul"}, "1172304774600855552": {"content_summary": "Most popular computer science paper of the day: \"Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift\" https://t.co/bWriGx1VYI https://t.co/pDOLNnAqw2", "followers": "280", "datetime": "2019-09-13 00:23:33", "author": "@HotCompScience"}, "566453094557712384": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "237", "datetime": "2015-02-14 04:25:16", "author": "@UCfor"}, "954508722289692674": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/R0J8ExeWx1", "followers": "149", "datetime": "2018-01-20 00:19:47", "author": "@parker_brydon"}, "1172059101288906754": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "113", "datetime": "2019-09-12 08:07:20", "author": "@vedax"}, "565833592149139456": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "31", "datetime": "2015-02-12 11:23:35", "author": "@ps989"}, "1209257357303136256": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "46", "datetime": "2019-12-23 23:39:55", "author": "@pczzy"}, "566383851287486464": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/3v8HhYiHB4 (http://t.co/wgsxgu8liC)", "followers": "15,959", "datetime": "2015-02-13 23:50:07", "author": "@newsyc20"}, "1171942201934454786": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "81", "datetime": "2019-09-12 00:22:49", "author": "@gtpash"}, "1172278285990014986": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "650", "datetime": "2019-09-12 22:38:17", "author": "@rwhitcomb"}, "1172029903690186752": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "306", "datetime": "2019-09-12 06:11:18", "author": "@juarelerrr"}, "1172046306795294720": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "681", "datetime": "2019-09-12 07:16:29", "author": "@pedrobizarro"}, "1172057542303727617": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by R https://t.co/eGy7kXJ1qI (Popularity:60.0) #Machine_Learning", "followers": "93", "datetime": "2019-09-12 08:01:08", "author": "@poqaa_ai"}, "1171990860398858240": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "63", "datetime": "2019-09-12 03:36:10", "author": "@saqibns"}, "566385513423073283": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/GJX7aQerOt", "followers": "140", "datetime": "2015-02-13 23:56:43", "author": "@myikegami_bot"}, "1172120085411971072": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "437", "datetime": "2019-09-12 12:09:39", "author": "@grousselle"}, "1171997906590547969": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "63", "datetime": "2019-09-12 04:04:10", "author": "@prasannaforai"}, "624863090757111808": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "825", "datetime": "2015-07-25 08:45:44", "author": "@fly51fly"}, "565891224583032833": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "1,493", "datetime": "2015-02-12 15:12:36", "author": "@mshron"}, "1171991426525102081": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "38", "datetime": "2019-09-12 03:38:25", "author": "@snairmeister"}, "1171877742792495104": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2,379", "datetime": "2019-09-11 20:06:40", "author": "@tedgreenwald"}, "566450250777231360": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "3,843", "datetime": "2015-02-14 04:13:58", "author": "@valdesjo77"}, "1171875413049692161": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "108", "datetime": "2019-09-11 19:57:25", "author": "@stocastico"}, "1172180876114874368": {"content_summary": "RT @ylecun: Nice tweethread on the effect of variable centering in deep nets (like batch norm). https://t.co/cDrTyVjAMd https://t.co/cDrTyV\u2026", "followers": "237", "datetime": "2019-09-12 16:11:13", "author": "@veydpz_public"}, "565705817991962626": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "4,631", "datetime": "2015-02-12 02:55:51", "author": "@beam2d"}, "1172189793255452673": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "91", "datetime": "2019-09-12 16:46:39", "author": "@0xhexhex"}, "1172497866058797058": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "32", "datetime": "2019-09-13 13:10:49", "author": "@hbwinther"}, "1124487635441979392": {"content_summary": "RT @hackerfriendly: TIL that some ML researchers had an idea (batch normalization) that worked, then tacked on a post-hoc mathematical anal\u2026", "followers": "1,063", "datetime": "2019-05-04 01:35:18", "author": "@DanielOCL"}, "851819296577789954": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/ZgvSjxHiG8 #hugePaper https://t.co/AasaVQzvku", "followers": "591", "datetime": "2017-04-11 15:28:40", "author": "@rmaestrem"}, "939957808748904448": {"content_summary": "@neuroecology Batch normalization's only 3 years old! https://t.co/rjik5GF3oG", "followers": "637", "datetime": "2017-12-10 20:39:39", "author": "@allmeasures"}, "1172133231442976769": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "2,067", "datetime": "2019-09-12 13:01:54", "author": "@EricSchles"}, "1171997492751282176": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "121", "datetime": "2019-09-12 04:02:31", "author": "@_lychrel"}, "1171879137969082370": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "288", "datetime": "2019-09-11 20:12:13", "author": "@akashpalrecha98"}, "1172045211175985152": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "36", "datetime": "2019-09-12 07:12:08", "author": "@thanhtu19392"}, "565778230859157504": {"content_summary": "[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift:\u2026 http://t.co/28mGhqKE7w [ml]", "followers": "200", "datetime": "2015-02-12 07:43:36", "author": "@overleo"}, "1171873417731006464": {"content_summary": "This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about training mechanics by studying this thread and the links it contains.", "followers": "99,904", "datetime": "2019-09-11 19:49:29", "author": "@jeremyphoward"}, "1172131872744206336": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "4,010", "datetime": "2019-09-12 12:56:30", "author": "@ballforest"}, "1172156861564563457": {"content_summary": "RT @ylecun: Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm). https://t.co/eBScx7v7qk", "followers": "2,280", "datetime": "2019-09-12 14:35:48", "author": "@AfroRoboticist"}, "1172258375901622272": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "736", "datetime": "2019-09-12 21:19:10", "author": "@unsorsodicorda"}, "565800138145558528": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "292", "datetime": "2015-02-12 09:10:39", "author": "@nsmetanin"}, "732811548100579328": {"content_summary": "This Batch Norm Back Propagation took too long to derive O_O Fun learning though https://t.co/umAkKVIrEV https://t.co/SCI1gYfSzN", "followers": "21", "datetime": "2016-05-18 05:54:22", "author": "@IAmAnkurAga"}, "576525830223953922": {"content_summary": "Keep up Deep Learning by recent works; http://t.co/XQUjBl6UIS http://t.co/92eSyvpUoh http://t.co/bxSaP3vUDs", "followers": "506", "datetime": "2015-03-13 23:30:43", "author": "@erogol"}, "565850402110062592": {"content_summary": "V pond\u011bl\u00ed um\u011bl Microsoft rozpozn\u00e1vat obr\u00e1zky l\u00e9pe ne\u017e \u010dlov\u011bk, dnes to oznamuje i Google: http://t.co/eoOIAchgb4 (4,8%, lid\u00e9 maj\u00ed 5,1%)", "followers": "0", "datetime": "2015-02-12 12:30:23", "author": "@makovecp"}, "1172090369674289153": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "57", "datetime": "2019-09-12 10:11:35", "author": "@Isinlor"}, "1172101127086727169": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1,231", "datetime": "2019-09-12 10:54:19", "author": "@jackiefloyd"}, "565827630524796928": {"content_summary": "Very interesting paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/pEkbscfEnT", "followers": "1,026", "datetime": "2015-02-12 10:59:54", "author": "@orestistsinalis"}, "1172167882576261126": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "39", "datetime": "2019-09-12 15:19:35", "author": "@stalhabukhari"}, "565709507771527168": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "3", "datetime": "2015-02-12 03:10:31", "author": "@bohdaisuke10"}, "566398476985311232": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "117", "datetime": "2015-02-14 00:48:14", "author": "@LeRoyFrost"}, "712941764206804992": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/fYlhNg9dxu", "followers": "896", "datetime": "2016-03-24 09:58:57", "author": "@gibson861"}, "1171867787259777025": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,083", "datetime": "2019-09-11 19:27:07", "author": "@rosstaylor90"}, "1072621056060547078": {"content_summary": "@Kesslkind Soll mir dabei helfen die Mathematischen Schritte von Batch Normalization (https://t.co/iTJ6P5jIOH) zu verstehen. Aber ich glaube ich lasse es f\u00fcr heute mit dieser \"simplen\" Erkl\u00e4rung und suche mir da lieber morgen ne andere :DD", "followers": "24", "datetime": "2018-12-11 22:36:02", "author": "@Tollpatsch93"}, "1171942959169900545": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "3,915", "datetime": "2019-09-12 00:25:49", "author": "@irenetrampoline"}, "1172095959452594176": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "179", "datetime": "2019-09-12 10:33:47", "author": "@yannick_martel"}, "842294699658424321": {"content_summary": "@madoibito80 \u3046\u30fc\u3093\u3001\u81ea\u5206\u3082\u305d\u3046\u3044\u3046\u7406\u89e3\u3060\u3063\u305f\u3051\u3069\u3001\u30c7\u30fc\u30bf\u6570\u3092\u5897\u3084\u3057\u3066\u6c4e\u5316\u3057\u3066\u308b\u21d2\u6b63\u5247\u5316\u3057\u3066\u308b\u304c\u8a00\u3048\u308b\u306e\u3088\u304f\u308f\u304b\u3089\u305a(https://t.co/cIhwoGaxGn)", "followers": "876", "datetime": "2017-03-16 08:41:20", "author": "@nanTosaka2"}, "1174289568050212864": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "220", "datetime": "2019-09-18 11:50:24", "author": "@ceshine_en"}, "1171980990505476096": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "154", "datetime": "2019-09-12 02:56:57", "author": "@ro_laguna_"}, "565725097051586561": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "7,333", "datetime": "2015-02-12 04:12:28", "author": "@Data88Smart"}, "1172157307133812743": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "283", "datetime": "2019-09-12 14:37:34", "author": "@njwfish"}, "1172268463118802945": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,187", "datetime": "2019-09-12 21:59:15", "author": "@ivanprado"}, "565874270166065153": {"content_summary": "Train ConvNets 14x faster with the same accuracy by Reducing Internal Covariate Shift http://t.co/XNPNhZPXMu #Pubs", "followers": "71", "datetime": "2015-02-12 14:05:14", "author": "@huiliguo"}, "1172158800054255616": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,932", "datetime": "2019-09-12 14:43:30", "author": "@dendisuhubdy"}, "565726860022398979": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "9,209", "datetime": "2015-02-12 04:19:28", "author": "@sla"}, "1172186635460665344": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "31", "datetime": "2019-09-12 16:34:06", "author": "@koenboeckx"}, "1127396465817427973": {"content_summary": "RT @shion_honda: Batch Normalization [Ioffe+, 2015, ICML] \u30df\u30cb\u30d0\u30c3\u30c1\u5b66\u7fd2\u306b\u304a\u3044\u3066\u5404\u5c64\u306e\u6d3b\u6027\u5316\u524d\u306b\u30d0\u30c3\u30c1\u65b9\u5411\u306e\u6b63\u898f\u5316\u3068\u30a2\u30d5\u30a3\u30f3\u5909\u63db\u3092\u304b\u3051\u308b\u3053\u3068\u3067\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u8efd\u6e1b\u3057\u3001\u3088\u308a\u9ad8\u3044\u5b66\u7fd2\u7387\u304c\u63a1\u7528\u53ef\u80fd\u306b\u3002\u6b63\u5247\u5316\u306e\u52b9\u679c\u3082\u78ba\u8a8d\u3067\u304d\u305f\u2026", "followers": "190", "datetime": "2019-05-12 02:13:58", "author": "@nskm_m"}, "566431141494681600": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "4,683", "datetime": "2015-02-14 02:58:02", "author": "@bradneuberg"}, "1127391739537289216": {"content_summary": "RT @shion_honda: Batch Normalization [Ioffe+, 2015, ICML] \u30df\u30cb\u30d0\u30c3\u30c1\u5b66\u7fd2\u306b\u304a\u3044\u3066\u5404\u5c64\u306e\u6d3b\u6027\u5316\u524d\u306b\u30d0\u30c3\u30c1\u65b9\u5411\u306e\u6b63\u898f\u5316\u3068\u30a2\u30d5\u30a3\u30f3\u5909\u63db\u3092\u304b\u3051\u308b\u3053\u3068\u3067\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u8efd\u6e1b\u3057\u3001\u3088\u308a\u9ad8\u3044\u5b66\u7fd2\u7387\u304c\u63a1\u7528\u53ef\u80fd\u306b\u3002\u6b63\u5247\u5316\u306e\u52b9\u679c\u3082\u78ba\u8a8d\u3067\u304d\u305f\u2026", "followers": "1,353", "datetime": "2019-05-12 01:55:11", "author": "@MathSorcerer"}, "1172203556197294081": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "119", "datetime": "2019-09-12 17:41:20", "author": "@gandhikanishk"}, "1171874249579073537": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "42", "datetime": "2019-09-11 19:52:48", "author": "@MishakinSergey"}, "1172149610921504769": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "255", "datetime": "2019-09-12 14:06:59", "author": "@TerencePlizga"}, "1172075935211958274": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "111", "datetime": "2019-09-12 09:14:13", "author": "@alsombra7"}, "1172011332012531712": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "36", "datetime": "2019-09-12 04:57:31", "author": "@SamirJabari2"}, "566382313270161409": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/RZbm4CfXUB (cmts http://t.co/GBaMlBdqNb)", "followers": "2,320", "datetime": "2015-02-13 23:44:01", "author": "@Megachrist"}, "1172375763980447745": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "36", "datetime": "2019-09-13 05:05:38", "author": "@boredsrk"}, "1172156724696014848": {"content_summary": "Nice thread on the effects of variable normalization in neural nets (e.g. with batch norm).", "followers": "194,979", "datetime": "2019-09-12 14:35:15", "author": "@ylecun"}, "1171927867808047104": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "1,022", "datetime": "2019-09-11 23:25:51", "author": "@kzykmyzw"}, "747408490193367040": {"content_summary": "RT @milesboard #DeepLearning: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/86L2ufKRQT #WAN_STR", "followers": "659", "datetime": "2016-06-27 12:37:24", "author": "@wspresso"}, "566413206671728640": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/kstVT5IY47", "followers": "1,644", "datetime": "2015-02-14 01:46:46", "author": "@PaulJoslin"}, "1171965542489411585": {"content_summary": "Cool thread on the intuition behind why BatchNorm works so well in practice! \ud83e\udd37\ud83c\udffc\u200d\u2640\ufe0f", "followers": "6,012", "datetime": "2019-09-12 01:55:34", "author": "@lavanyaai"}, "1171999908162752512": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "8,425", "datetime": "2019-09-12 04:12:07", "author": "@debasishg"}, "687963691375935492": {"content_summary": "\u305f\u3076\u3093batch normalization https://t.co/1vJ5iyOFrW #justtechtalk", "followers": "683", "datetime": "2016-01-15 11:45:00", "author": "@marujiruo"}, "1172105764980297730": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "518", "datetime": "2019-09-12 11:12:45", "author": "@p_hbar_k"}, "568398053603991552": {"content_summary": "Using batch normalization to reduce internal covariate shift in deep neural nets & achieve a 4.8% ImageNet test error http://t.co/Nh3OqtIeyn", "followers": "696", "datetime": "2015-02-19 13:13:50", "author": "@pjf2cfj"}, "566380499397246976": {"content_summary": "Googles breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/I3Nk4d9bnj", "followers": "497", "datetime": "2015-02-13 23:36:48", "author": "@devnewsbot"}, "1171892511301722112": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "48", "datetime": "2019-09-11 21:05:22", "author": "@lizoratech"}, "667821485440929792": {"content_summary": "Catching up on research: Batch Normalization is \"drop everything else and do it now\" insane. https://t.co/WwyS8P02DB https://t.co/I2vLHb6pL5", "followers": "25,260", "datetime": "2015-11-20 21:47:04", "author": "@alexjc"}, "1171978569138016256": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "22", "datetime": "2019-09-12 02:47:19", "author": "@nandan_cse"}, "566644349405249537": {"content_summary": "Breakthrough paper shows 10*faster neural nets, and beats a human [pdf] http://t.co/oJ6pBmoqXm #neuro #machinelearning", "followers": "415", "datetime": "2015-02-14 17:05:15", "author": "@davypeterbraun"}, "1171979465725005825": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2,620", "datetime": "2019-09-12 02:50:53", "author": "@MAXIM_MLA"}, "1172408082770477056": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "37", "datetime": "2019-09-13 07:14:03", "author": "@KevinBenSmith"}, "565751056458850304": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "278", "datetime": "2015-02-12 05:55:37", "author": "@aittalam"}, "565885553921761280": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "14,852", "datetime": "2015-02-12 14:50:04", "author": "@twiecki"}, "1171948003462209536": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "25", "datetime": "2019-09-12 00:45:52", "author": "@ASreesaila"}, "1172259621525786624": {"content_summary": "\u306a\u305cBatch Normalization\u304c\u3046\u307e\u304f\u884c\u304f\u306e\u304b\u3001\u306e\u8aac\u660e\u3002", "followers": "65", "datetime": "2019-09-12 21:24:07", "author": "@Motono_Mokuami"}, "1171986736563408902": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "53", "datetime": "2019-09-12 03:19:47", "author": "@rajivjk"}, "1172076327634997249": {"content_summary": "RT @RogerGrosse: Excellent overview of the (widely misunderstood) mechanism hypothesized in the original batch norm paper, as well as recen\u2026", "followers": "119", "datetime": "2019-09-12 09:15:47", "author": "@Mufei_Li"}, "1115739477366824960": {"content_summary": "minibatch \u306e\u5e73\u5747\u3068\u5206\u6563\u304b\u3089xi-\u03bc\u03b2/\u221a\u03c3\u03b2^2+\u03b5\u3092\u9069\u7528\u3057\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u306b\u3088\u308b\u5206\u5e03\u306e\u5909\u5316\u30fc\u5185\u90e8\u5171\u5206\u6563\u30b7\u30d5\u30c8\u30fc\u3092\u8efd\u6e1b,\u3088\u308a\u5927\u304d\u306a\u5b66\u7fd2\u7387\u3068\u5b66\u7fd2\u9ad8\u901f\u5316,\u30d1\u30e9\u30e1\u30fc\u30bf\u521d\u671f\u8a2d\u5b9a\u3078\u306e\u30ca\u30a4\u30fc\u30d6\u3055\u3092\u53ef\u80fd\u306b\u3057\u305f.2015\u5e74. -\u300cBatch Normalization: Accelerating Deep Network Training ..\u300d https://t.co/8lAxfgrXYo", "followers": "774", "datetime": "2019-04-09 22:13:15", "author": "@K00TSUKA"}, "566521798192549889": {"content_summary": "Both of Microsoft and Google's new AI vision systems exceed the capabilities of humans: http://t.co/1PDYDPjrcU http://t.co/Lo0aTHC6qy", "followers": "82,187", "datetime": "2015-02-14 08:58:16", "author": "@esesci"}, "1171875717988200451": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "18", "datetime": "2019-09-11 19:58:38", "author": "@marioalm74"}, "1181595715442626560": {"content_summary": "RT @abursuc: Nice summary of the knowledge and understanding we have over BatchNorm from both recent and classic perspectives at the time o\u2026", "followers": "303", "datetime": "2019-10-08 15:42:26", "author": "@subhobrata1"}, "1172133164161986560": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "578", "datetime": "2019-09-12 13:01:38", "author": "@byteshaz"}, "624976051106660353": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "21,221", "datetime": "2015-07-25 16:14:36", "author": "@DataSciNews"}, "1171977005392224256": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "1,371", "datetime": "2019-09-12 02:41:07", "author": "@chaitali_debp"}, "1172052999427874816": {"content_summary": "RT @threadreaderapp: @soldni Hola there is your unroll: Thread by @dcpage3: \"The paper that introduced Batch Norm https://t.co/ybxv8LzEMy c\u2026", "followers": "48", "datetime": "2019-09-12 07:43:05", "author": "@mpsampat"}, "1127723000239771651": {"content_summary": "RT @shion_honda: Batch Normalization [Ioffe+, 2015, ICML] \u30df\u30cb\u30d0\u30c3\u30c1\u5b66\u7fd2\u306b\u304a\u3044\u3066\u5404\u5c64\u306e\u6d3b\u6027\u5316\u524d\u306b\u30d0\u30c3\u30c1\u65b9\u5411\u306e\u6b63\u898f\u5316\u3068\u30a2\u30d5\u30a3\u30f3\u5909\u63db\u3092\u304b\u3051\u308b\u3053\u3068\u3067\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u8efd\u6e1b\u3057\u3001\u3088\u308a\u9ad8\u3044\u5b66\u7fd2\u7387\u304c\u63a1\u7528\u53ef\u80fd\u306b\u3002\u6b63\u5247\u5316\u306e\u52b9\u679c\u3082\u78ba\u8a8d\u3067\u304d\u305f\u2026", "followers": "825", "datetime": "2019-05-12 23:51:29", "author": "@morioka"}, "1172165591379128320": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "167", "datetime": "2019-09-12 15:10:29", "author": "@fferousi"}, "886356018401132544": {"content_summary": "RT @TomD_Hann: @AllTom batch normalization cures all ills .. seriously, has to be tried. https://t.co/ltYAKNDSSU", "followers": "649", "datetime": "2017-07-15 22:45:17", "author": "@AllTom"}, "565685568483635203": {"content_summary": "A new approach to training gives a big boost to deep vision networks - http://t.co/0TD2Y4DvQh - 4.9% is 0.96 @Karpathy's, to coin a new unit", "followers": "19,132", "datetime": "2015-02-12 01:35:24", "author": "@petewarden"}, "1171984488957530112": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "48", "datetime": "2019-09-12 03:10:51", "author": "@chbalajitilak"}, "565729052225703938": {"content_summary": "MT @karpathy 4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/kFEJjpYWGS #bigdata #machinelearning", "followers": "105", "datetime": "2015-02-12 04:28:11", "author": "@Data88Geek"}, "565932585558507520": {"content_summary": "4.8% test error on ImageNet with \"batch normalization\", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see 0% in ~1 month.", "followers": "5,245", "datetime": "2015-02-12 17:56:57", "author": "@ParisMLgroup"}, "1172065211483922432": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "75", "datetime": "2019-09-12 08:31:36", "author": "@MozejkoMarcin"}, "666536102308671488": {"content_summary": "\u30d0\u30c3\u30c1\u6b63\u5247\u5316\u3053\u308c\u304b https://t.co/pY4TGXRObd", "followers": "1,655", "datetime": "2015-11-17 08:39:25", "author": "@olanleed"}, "1172383140851011584": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "134", "datetime": "2019-09-13 05:34:57", "author": "@Uzziesh"}, "575274621445898241": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "903", "datetime": "2015-03-10 12:38:52", "author": "@wk77"}, "1230912038496542724": {"content_summary": "https://t.co/l4L8VBOfpF", "followers": "628", "datetime": "2020-02-21 17:47:53", "author": "@OdoyoOrwa"}, "747408491829047296": {"content_summary": "RT @wspresso: RT @milesboard #DeepLearning: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://t.co/86L2ufKRQ\u2026", "followers": "2,544", "datetime": "2016-06-27 12:37:25", "author": "@sentiwire"}, "1172362670575476736": {"content_summary": "RT @fdellaert: Wondering about batch norm? Read this amazing thread. https://t.co/y5oXJNBXsq", "followers": "161", "datetime": "2019-09-13 04:13:36", "author": "@DaeyunShin"}, "1172128478726098944": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "2", "datetime": "2019-09-12 12:43:01", "author": "@Keygem0"}, "1172405186653900800": {"content_summary": "RT: DataSciNews: RT jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about training mechanics by studying this thread and the links it contains. https://t.co/ERoFqSN1kj", "followers": "3,314", "datetime": "2019-09-13 07:02:33", "author": "@AndySugs"}, "1171967861071302656": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "233", "datetime": "2019-09-12 02:04:46", "author": "@royam0820"}, "1223431318496636930": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "7", "datetime": "2020-02-01 02:22:11", "author": "@Bhargavapratap"}, "1172282823048216576": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "20", "datetime": "2019-09-12 22:56:19", "author": "@trisgelar"}, "1171918658425229312": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "179", "datetime": "2019-09-11 22:49:16", "author": "@ajnovice"}, "1172112396040773632": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3,416", "datetime": "2019-09-12 11:39:06", "author": "@alxndrkalinin"}, "565908764805439488": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "3,439", "datetime": "2015-02-12 16:22:18", "author": "@delta2323_"}, "1172113335304753152": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "3", "datetime": "2019-09-12 11:42:50", "author": "@Dr_JJL"}, "1172170043003559937": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "198", "datetime": "2019-09-12 15:28:10", "author": "@omar_javd"}, "1172048244605640705": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "511", "datetime": "2019-09-12 07:24:11", "author": "@soobrosa"}, "1158166208233070592": {"content_summary": "My best three: 1. The Distill Pub article on why Momentum really works: https://t.co/cvLS5cR0Vz 2. Batch Normalization paper is a gem: https://t.co/V5GZtKUYOG 3. Original LASSO paper from 1995. Loved how they explain the geometry of the LASSO: https://t", "followers": "2,911", "datetime": "2019-08-05 00:01:56", "author": "@WaleAkinfaderin"}, "566588685643448320": {"content_summary": "Batch Normalization: Accelerating Deep Network Training [pdf] http://t.co/RdwEDSpwjq", "followers": "501", "datetime": "2015-02-14 13:24:04", "author": "@Aluxian"}, "1171874618958909442": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "66", "datetime": "2019-09-11 19:54:16", "author": "@battle8500"}, "1171882410105036807": {"content_summary": "RT @dcpage3: The paper that introduced Batch Norm https://t.co/vkT0LioKHc combines clear intuition with compelling experiments (14x speedup\u2026", "followers": "682", "datetime": "2019-09-11 20:25:13", "author": "@pakitochus"}, "570192683207012353": {"content_summary": "\u4eca\u5ea6\u306fGoogle\u304cILSVRC2012\u5206\u985e\u30bf\u30b9\u30af\u306e\u30c6\u30b9\u30c8\u30a8\u30e9\u30fc\u30924.82%\u306b\u66f4\u65b0 http://t.co/d8rlUnH1WC \u5404\u5c64\u3054\u3068\u306b\u6d3b\u6027\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5358\u4f4d\u3067\u6b63\u898f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u3044\u308d\u3093\u306a\u6b63\u5247\u5316\u30c6\u30af\u30cb\u30c3\u30af\u304c\u3044\u3089\u306a\u304f\u306a\u308b\u4e0a\u306b\u5b66\u7fd2\u304c14\u500d\u901f\u304f\u306a\u308b", "followers": "244", "datetime": "2015-02-24 12:05:04", "author": "@s_ryosky"}, "626506990341201921": {"content_summary": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2", "followers": "42,874", "datetime": "2015-07-29 21:38:01", "author": "@DeepLearningHub"}, "1172174927014899712": {"content_summary": "RT @jeremyphoward: This is the best distillation of recent (and old!) research on batchnorm I've seen. There is so much to learn about tra\u2026", "followers": "79", "datetime": "2019-09-12 15:47:35", "author": "@MBroox"}, "565987349767540736": {"content_summary": "Microsoft & Google duke it out on #deeplearning using #NVIDIA #CUDA #GPUs on ImageNet https://t.co/sWWcLuEIm1 https://t.co/IHjy7naLig", "followers": "1,636", "datetime": "2015-02-12 21:34:34", "author": "@SumitGup"}}}