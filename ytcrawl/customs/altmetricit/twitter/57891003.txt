{"citation_id": "57891003", "tab": "twitter", "twitter": {"1113241574207397888": {"author": "@arxiv_pop", "followers": "692", "datetime": "2019-04-03 00:47:28", "content_summary": "2019/03/27 \u6295\u7a3f 3\u4f4d LG(Machine Learning) Generalized Off-Policy Actor-Critic https://t.co/zU9oFoAYVE 12 Tweets 41 Retweets 184 Favorites"}, "1111311072231940096": {"author": "@AvisekNaug", "followers": "11", "datetime": "2019-03-28 16:56:21", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111255214470754307": {"author": "@mooopan", "followers": "1,720", "datetime": "2019-03-28 13:14:23", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111441001787252736": {"author": "@icoxfog417", "followers": "11,458", "datetime": "2019-03-29 01:32:39", "content_summary": "Off-Policy Policy Gradient\u3067\u306f\u5b9f\u969b\u306e\u6226\u7565(target policy)\u3068\u306f\u5225\u306b\u63a2\u7d22\u7528\u306ebehavior policy\u3092\u7528\u3044\u3066\u66f4\u65b0\u3092\u884c\u3046\u304c\u3001\u5b9f\u969b\u306e\u6226\u7565\u304c\u76ee\u6307\u3057\u305f\u3044\u3068\u3053\u308d(\u671f\u5f85\u5024\u306e\u6700\u5927\u5316)\u3068\u63a2\u7d22\u304c\u76ee\u6307\u3057\u305f\u3044\u3068\u3053\u308d(\u4fa1\u5024\u306e\u6700\u5927\u5316)\u306f\u30d0\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\u305d\u3053\u3067\u3001\u4e21\u8005\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u8abf\u6574\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u5c0e\u5165\u3092\u63d0\u6848"}, "1184271392969244674": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-10-16 00:54:37", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v7 [cs.LG] UPDATED)\" #arXiv https://t.co/LbV0icPbHK"}, "1111133726669324288": {"author": "@StatsPapers", "followers": "5,454", "datetime": "2019-03-28 05:11:38", "content_summary": "Generalized Off-Policy Actor-Critic. https://t.co/Sp2yOVVEVX"}, "1188990417951514624": {"author": "@pm_girl", "followers": "2,162", "datetime": "2019-10-29 01:26:20", "content_summary": "#Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v8 [cs.LG] UPDATED) https://t.co/sHyOdcqiD5 #artificialintelligence #ai"}, "1111657690907004928": {"author": "@VeronicaChelu", "followers": "215", "datetime": "2019-03-29 15:53:41", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1173395999370416133": {"author": "@pm_girl", "followers": "2,162", "datetime": "2019-09-16 00:39:41", "content_summary": "#Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v5 [cs.LG] UPDATED) https://t.co/sHyOdcqiD5 #artificialintelligence #ai"}, "1111228440626237441": {"author": "@d_kangin", "followers": "117", "datetime": "2019-03-28 11:28:00", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111080825758588929": {"author": "@deep_rl", "followers": "862", "datetime": "2019-03-28 01:41:26", "content_summary": "Generalized Off-Policy Actor-Critic - Shangtong Zhang https://t.co/eWbkvVa3cj"}, "1170160962843725825": {"author": "@stl950116", "followers": "11", "datetime": "2019-09-07 02:24:48", "content_summary": "RT @whi_rl: \"Generalized Off-Policy Actor-Critic\" - @ShangtongZhang, @WendelinBoehmer, @shimon8282. Off-Policy Policy Gradient Theorem ha\u2026"}, "1141145914519764993": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-06-19 00:49:22", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v4 [cs.LG] UPDATED)\" #arXiv https://t.co/LbV0icPbHK"}, "1141146409439285251": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-06-19 00:51:20", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/2BqYxMqslA Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v4 [cs.LG] UPDATED)"}, "1111375007588868097": {"author": "@tttorrr", "followers": "57", "datetime": "2019-03-28 21:10:24", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111368491960098816": {"author": "@ceobillionaire", "followers": "164,097", "datetime": "2019-03-28 20:44:31", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1138609424854069248": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-06-12 00:50:15", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v3 [cs.LG] UPDATED)\" #arXiv https://t.co/LbV0icPbHK"}, "1111075503790800896": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-03-28 01:20:17", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/sWEvC64LSa Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG])"}, "1111082391924289536": {"author": "@BrundageBot", "followers": "3,878", "datetime": "2019-03-28 01:47:39", "content_summary": "Generalized Off-Policy Actor-Critic. Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson https://t.co/FHIkhy3N7o"}, "1131722863671689216": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-05-24 00:45:31", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/2BqYxMqslA Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v2 [cs.LG] UPDATED)"}, "1111396063347277825": {"author": "@viktor_m81", "followers": "113", "datetime": "2019-03-28 22:34:04", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111554237496999939": {"author": "@ak1010", "followers": "1,086", "datetime": "2019-03-29 09:02:36", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1173398179527766017": {"author": "@gastronomy", "followers": "1,388", "datetime": "2019-09-16 00:48:21", "content_summary": "[arXiv] Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v5 [cs.LG] UPDATED) --> We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learn"}, "1111475172031246336": {"author": "@jackiefloyd", "followers": "1,231", "datetime": "2019-03-29 03:48:25", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111352347123310592": {"author": "@Miles_Brundage", "followers": "25,578", "datetime": "2019-03-28 19:40:22", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111225299562385409": {"author": "@ShangtongZhang", "followers": "280", "datetime": "2019-03-28 11:15:31", "content_summary": "Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Try Generalized Off-Policy Policy Gradient Theorem (https://t.co/IhVr6YTsYn) instead, my latest work w/ Wendelin & @shimon8282. @whi_rl"}, "1111410640818573312": {"author": "@unsorsodicorda", "followers": "735", "datetime": "2019-03-28 23:32:00", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111257262364999685": {"author": "@AmalFeriani", "followers": "127", "datetime": "2019-03-28 13:22:32", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111360704408367105": {"author": "@rupammahmood", "followers": "503", "datetime": "2019-03-28 20:13:34", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111371215040471040": {"author": "@iamknighton", "followers": "424", "datetime": "2019-03-28 20:55:20", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111342772261449728": {"author": "@brandondamos", "followers": "7,550", "datetime": "2019-03-28 19:02:19", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111198868727885824": {"author": "@mlmemoirs", "followers": "1,247", "datetime": "2019-03-28 09:30:30", "content_summary": "#arXiv #machinelearning [cs.LG] Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG]) https://t.co/w3jpwWkF8J We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in t"}, "1111100098128293888": {"author": "@arxiv_cs_LG", "followers": "308", "datetime": "2019-03-28 02:58:01", "content_summary": "Generalized Off-Policy Actor-Critic. Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson https://t.co/rKgDmuKA2i"}, "1111236798171283456": {"author": "@_Alex_Borghi_", "followers": "163", "datetime": "2019-03-28 12:01:13", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1114262685837922305": {"author": "@future_of_AI", "followers": "2,276", "datetime": "2019-04-05 20:25:00", "content_summary": "Generalized Off-Policy Actor-Critic https://t.co/PizELjcvOo #AI #Research via @Miles_Brundage"}, "1111278502743928832": {"author": "@thapraveensingh", "followers": "21", "datetime": "2019-03-28 14:46:56", "content_summary": "RT @shimon8282: New work from our lab! https://t.co/vmsBpKm6Fu"}, "1111554165120077824": {"author": "@NandoDF", "followers": "76,883", "datetime": "2019-03-29 09:02:19", "content_summary": "RT @shimon8282: New work from our lab! https://t.co/vmsBpKm6Fu"}, "1138609677510553600": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-06-12 00:51:16", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/2BqYxMqslA Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v3 [cs.LG] UPDATED)"}, "1177922476262793217": {"author": "@ceobillionaire", "followers": "164,097", "datetime": "2019-09-28 12:26:17", "content_summary": "RT @whi_rl: \"Generalized Off-Policy Actor-Critic\" - @ShangtongZhang, @WendelinBoehmer, @shimon8282. Off-Policy Policy Gradient Theorem ha\u2026"}, "1111075293710741505": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-03-28 01:19:27", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG])\" #arXiv https://t.co/LbV0icPbHK"}, "1111288413485817857": {"author": "@andrey_kurenkov", "followers": "3,416", "datetime": "2019-03-28 15:26:19", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111692967234801665": {"author": "@HariSikchi", "followers": "80", "datetime": "2019-03-29 18:13:52", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111462770766168065": {"author": "@thapraveensingh", "followers": "21", "datetime": "2019-03-29 02:59:09", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111226678427172865": {"author": "@whi_rl", "followers": "3,215", "datetime": "2019-03-28 11:21:00", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111199821229117441": {"author": "@arXiv__ml", "followers": "1,708", "datetime": "2019-03-28 09:34:17", "content_summary": "#arXiv #machinelearning [cs.LG] Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG]) https://t.co/jaojk9ex70 We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in t"}, "1111242904528404480": {"author": "@amrmkayid", "followers": "285", "datetime": "2019-03-28 12:25:28", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111397873432748032": {"author": "@zacharylipton", "followers": "29,587", "datetime": "2019-03-28 22:41:16", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111464402228645889": {"author": "@THsama2", "followers": "110", "datetime": "2019-03-29 03:05:38", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1121725360092012544": {"author": "@Sapana_007", "followers": "47", "datetime": "2019-04-26 10:39:01", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111679786634502144": {"author": "@koulanurag", "followers": "67", "datetime": "2019-03-29 17:21:29", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111396534816391170": {"author": "@volkuleshov", "followers": "3,774", "datetime": "2019-03-28 22:35:57", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111169867019943936": {"author": "@gastronomy", "followers": "1,388", "datetime": "2019-03-28 07:35:15", "content_summary": "[arXiv] Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG]) --> We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL)"}, "1111074593660387334": {"author": "@StatMLPapers", "followers": "9,685", "datetime": "2019-03-28 01:16:40", "content_summary": "Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG]) https://t.co/AvddUsAykS"}, "1188989711840350209": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-10-29 01:23:32", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v8 [cs.LG] UPDATED)\" #arXiv https://t.co/LbV0icPbHK"}, "1111244083740229634": {"author": "@muhammadali111", "followers": "752", "datetime": "2019-03-28 12:30:10", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111283463628308483": {"author": "@KouroshMeshgi", "followers": "615", "datetime": "2019-03-28 15:06:39", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111113572774367234": {"author": "@MelroLeandro", "followers": "56", "datetime": "2019-03-28 03:51:33", "content_summary": "Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v1 [cs.LG]) https://t.co/Ofs0uhbnsP"}, "1111385584684032003": {"author": "@KSKSKSKS2", "followers": "216", "datetime": "2019-03-28 21:52:26", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111554798522982402": {"author": "@1960vd", "followers": "813", "datetime": "2019-03-29 09:04:50", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1169886065781460995": {"author": "@whi_rl", "followers": "3,215", "datetime": "2019-09-06 08:12:28", "content_summary": "\"Generalized Off-Policy Actor-Critic\" - @ShangtongZhang, @WendelinBoehmer, @shimon8282. Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Try Geoff-PAC instead! https://t.co/9dmWSKeZE1 h"}, "1111647828839796736": {"author": "@morioka", "followers": "822", "datetime": "2019-03-29 15:14:30", "content_summary": "RT @icoxfog417: Off-Policy Policy Gradient\u3067\u306f\u5b9f\u969b\u306e\u6226\u7565(target policy)\u3068\u306f\u5225\u306b\u63a2\u7d22\u7528\u306ebehavior policy\u3092\u7528\u3044\u3066\u66f4\u65b0\u3092\u884c\u3046\u304c\u3001\u5b9f\u969b\u306e\u6226\u7565\u304c\u76ee\u6307\u3057\u305f\u3044\u3068\u3053\u308d(\u671f\u5f85\u5024\u306e\u6700\u5927\u5316)\u3068\u63a2\u7d22\u304c\u76ee\u6307\u3057\u305f\u3044\u3068\u3053\u308d(\u4fa1\u5024\u306e\u6700\u5927\u2026"}, "1131818231776382978": {"author": "@pm_girl", "followers": "2,162", "datetime": "2019-05-24 07:04:29", "content_summary": "#Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v2 [cs.LG] UPDATED) https://t.co/CFCeeas2A1 #artificialintelligence #ai"}, "1111556766444093440": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-03-29 09:12:39", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111243340845068288": {"author": "@PatrickOmid", "followers": "716", "datetime": "2019-03-28 12:27:13", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111417178945781761": {"author": "@AssistedEvolve", "followers": "217", "datetime": "2019-03-28 23:57:59", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1203686137254072322": {"author": "@whi_rl", "followers": "3,215", "datetime": "2019-12-08 14:41:53", "content_summary": "\u201cGeneralized Off-Policy Actor-Critic\u201d \u2013 @ShangtongZhang, @WendelinBoehmer, @shimon8282 (Poster #188, Tue morning) Paper: https://t.co/9dmWSJXoMt Slides: https://t.co/euHQayOqwu https://t.co/7KtbnNQrXi"}, "1111228344979410944": {"author": "@shimon8282", "followers": "7,434", "datetime": "2019-03-28 11:27:37", "content_summary": "New work from our lab!"}, "1173399661513777153": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-09-16 00:54:14", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/sWEvC64LSa Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v5 [cs.LG] UPDATED)"}, "1188989471091347456": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-10-29 01:22:34", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9CB65 https://t.co/sWEvC6mmJI Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v8 [cs.LG] UPDATED)"}, "1184271054077923331": {"author": "@pm_girl", "followers": "2,162", "datetime": "2019-10-16 00:53:16", "content_summary": "#Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v7 [cs.LG] UPDATED) https://t.co/sHyOdcqiD5 #artificialintelligence #ai"}, "1111320247049416705": {"author": "@yisongyue", "followers": "7,758", "datetime": "2019-03-28 17:32:48", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111717653805182976": {"author": "@tokoton01", "followers": "26", "datetime": "2019-03-29 19:51:58", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111237341862117376": {"author": "@pranjaltandon2", "followers": "144", "datetime": "2019-03-28 12:03:22", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111231345320062976": {"author": "@sk413025", "followers": "37", "datetime": "2019-03-28 11:39:33", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111131334725038080": {"author": "@arxivml", "followers": "772", "datetime": "2019-03-28 05:02:08", "content_summary": "\"Generalized Off-Policy Actor-Critic\", Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson https://t.co/J9tKvq0GII"}, "1111369534311673858": {"author": "@JeanMarcJAzzi", "followers": "365", "datetime": "2019-03-28 20:48:39", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1111641053508046848": {"author": "@juarelerrr", "followers": "306", "datetime": "2019-03-29 14:47:35", "content_summary": "RT @ShangtongZhang: Off-Policy Policy Gradient Theorem has been widely used (e.g., DDPG, ACER, IMPALA) but optimizes a biased objective. Tr\u2026"}, "1131723311950520325": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-05-24 00:47:18", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v2 [cs.LG] UPDATED)\" #arXiv https://t.co/LbV0icPbHK"}, "1173396749119045633": {"author": "@helioRocha_", "followers": "626", "datetime": "2019-09-16 00:42:40", "content_summary": "\"Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v5 [cs.LG] UPDATED)\" #arXiv https://t.co/LbV0icPbHK"}, "1184270774858899458": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-10-16 00:52:10", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/sWEvC64LSa Generalized Off-Policy Actor-Critic. (arXiv:1903.11329v7 [cs.LG] UPDATED)"}}, "completed": "1", "queriedAt": "2020-06-03 02:47:16"}