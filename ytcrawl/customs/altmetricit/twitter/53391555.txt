{"citation_id": "53391555", "tab": "twitter", "twitter": {"1081703167887396864": {"author": "@nettwerkerin", "followers": "7,452", "datetime": "2019-01-06 00:05:06", "content_summary": "4) KI, Philosophie, Logik: https://t.co/FxkS08Lwup 5) Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function): https://t.co/3bql0OLrI6"}, "1081826811074543616": {"author": "@gsvasan", "followers": "388", "datetime": "2019-01-06 08:16:25", "content_summary": "\"We do not presently have a trustworthy framework for making decisions about the welfare or existence of people in the future\" Peter Eckersley https://t.co/tqO8asGaEW"}, "1086316250941157378": {"author": "@steinly0", "followers": "1,809", "datetime": "2019-01-18 17:35:51", "content_summary": "\"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)\" Peter Eckersley #TPfML2019 https://t.co/zMJlf63NUj"}, "1100830598480957441": {"author": "@danmcquillan", "followers": "4,726", "datetime": "2019-02-27 18:50:41", "content_summary": "@davies_will come across 'Parfitt\u2019s Mere Addition Paradox' etc? check out https://t.co/I80IvRWDHp - as i understand it, trashes any 'maximising happiness' metric but also (interestingly!) the notion of subcontracting social decisions to AI"}, "1100836821775384576": {"author": "@danmcquillan", "followers": "4,726", "datetime": "2019-02-27 19:15:25", "content_summary": "'Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)' https://t.co/I80IvRWDHp uses ethical 'impossibility theorems' to prove #AI shouldn't make high-stakes social decisions. [note: Totalitarian"}, "1080992328502861824": {"author": "@PJ_Muncaster", "followers": "1,978", "datetime": "2019-01-04 01:00:29", "content_summary": "Impossibility and Uncertainty Theorems in AI Value Alignment https://t.co/R4mdl2XtOY \"ML researchers should avoid using totally ordered objective functions or loss functions as optimization goals in high-stakes applications\" @AbernsteinCIFAR"}, "1089630704046993408": {"author": "@undersequoias", "followers": "1,861", "datetime": "2019-01-27 21:06:18", "content_summary": "@CGAPeterson In a roundabout way it was Eckersley's paper about moral uncertainty for AGI that got me wondering this. https://t.co/w061GedyKA"}, "1080641284568686592": {"author": "@helioRocha_", "followers": "631", "datetime": "2019-01-03 01:45:34", "content_summary": "\"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function). (arXiv:1901.00064v1 [https://t.co/jqZ5dwubSV])\" #arXiv https://t.co/90cCl4Hmtp"}, "1089669077277134850": {"author": "@CarlaZoeC", "followers": "60", "datetime": "2019-01-27 23:38:47", "content_summary": "Very interesting talk by Peter Eckersley @SafeAI_WS on impossibility theorems pointing towards the need for uncertainty in ML https://t.co/XdA0HIiNjF"}, "1103116440796884993": {"author": "@helioRocha_", "followers": "631", "datetime": "2019-03-06 02:13:49", "content_summary": "\"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function). (arXiv:1901.00064v3 [https://t.co/jqZ5dwubSV] UPDATED)\" #arXiv https://t.co/90cCl4Hmtp"}, "1128331269731958784": {"author": "@RobotRules", "followers": "371", "datetime": "2019-05-14 16:08:32", "content_summary": "Interesting paper from Peter Eckersley and @PartnershipAI on the importance of using uncertainty in solving the value alignment problem in #AI. Similar observations apply to human experience: total certainty can lead to zealotry and extremism for us too."}, "1113446180418138113": {"author": "@danmcquillan", "followers": "4,726", "datetime": "2019-04-03 14:20:30", "content_summary": "@jimmyjwu i liked your optimisation article a lot. fwiw this paper has an interesting angle on the idea that even _if_ the objective function was 'happiness' not profit, the results would still be 'repugnant' https://t.co/I80IvRWDHp"}, "1101054345355907072": {"author": "@danmcquillan", "followers": "4,726", "datetime": "2019-02-28 09:39:47", "content_summary": "RT @danmcquillan: 'Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)' https\u2026"}, "1208719895086669824": {"author": "@lil_Aphasia", "followers": "185", "datetime": "2019-12-22 12:04:14", "content_summary": "We do not presently have a trustworthy framework for making decisions about the welfare or existence of people in the future,\u201d @Przegaa @edwinbendyk @szymielewicz @JedrzejNiklas @bpaszcza https://t.co/16xlQih9u8 https://t.co/2dKPWq6SAG"}, "1082000091811057671": {"author": "@StJohnDeakins", "followers": "2,947", "datetime": "2019-01-06 19:44:59", "content_summary": "RT @mona_sloane: Peter Eckersley of @PartnershipAI: 'high-stakes systems should always exhibit uncertainty about the best action in some ca\u2026"}, "1168932226681245698": {"author": "@anderssandberg", "followers": "8,733", "datetime": "2019-09-03 17:02:15", "content_summary": "@jonathanstray I am reminded of Peter Eckersley's \"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)\" https://t.co/z7pH20fzr3 Building in simple and obvious assumptions can easily lead into cr"}, "1080641363769741314": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-01-03 01:45:53", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/ckCbMgCeUB Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function). (arXiv:1901.00064v1 [https://t.co/8kHVi9UcuF])"}, "1092962908298829824": {"author": "@helioRocha_", "followers": "631", "datetime": "2019-02-06 01:47:18", "content_summary": "\"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function). (arXiv:1901.00064v2 [https://t.co/jqZ5dwLMKt] UPDATED)\" #arXiv https://t.co/90cCl4YXkX"}, "1092963167821389825": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-02-06 01:48:20", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9CB65 https://t.co/ckCbMgkDw1 Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function). (arXiv:1901.00064v2 [https://t.co/8kHVi9CB65] UPDATED)"}, "1080806364002353152": {"author": "@arxivml", "followers": "786", "datetime": "2019-01-03 12:41:32", "content_summary": "\"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)\"\u2026 https://t.co/plpeexdFVD"}, "1081948172648370179": {"author": "@The__E_G_G", "followers": "443", "datetime": "2019-01-06 16:18:40", "content_summary": "RT @PJ_Muncaster: Impossibility and Uncertainty Theorems in AI Value Alignment https://t.co/R4mdl2XtOY \"ML researchers should avoid using\u2026"}, "1103116813343354880": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-03-06 02:15:17", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/4Yl8vCteWO Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function). (arXiv:1901.00064v3 [https://t.co/8kHVi9UcuF] UPDATED)"}, "1081947851989635073": {"author": "@The__E_G_G", "followers": "443", "datetime": "2019-01-06 16:17:24", "content_summary": "RT @danmcquillan: looks good - 'Impossibility and Uncertainty Theorems in AI Value Alignment' https://t.co/RARC5e6TWN (pdf) - why your AI\u2026"}, "1089680650351738881": {"author": "@SafeAI_WS", "followers": "522", "datetime": "2019-01-28 00:24:46", "content_summary": "RT @CarlaZoeC: Very interesting talk by Peter Eckersley @SafeAI_WS on impossibility theorems pointing towards the need for uncertainty in M\u2026"}, "1081950306366300161": {"author": "@The__E_G_G", "followers": "443", "datetime": "2019-01-06 16:27:09", "content_summary": "RT @nettwerkerin: 4) KI, Philosophie, Logik: https://t.co/FxkS08Lwup 5) Impossibility and Uncertainty Theorems in AI Value Alignment (or wh\u2026"}, "1081892619502927872": {"author": "@danmcquillan", "followers": "4,726", "datetime": "2019-01-06 12:37:55", "content_summary": "looks good - 'Impossibility and Uncertainty Theorems in AI Value Alignment' https://t.co/RARC5e6TWN (pdf) - why your AI loss function will always fail on fairness"}, "1081467436489236480": {"author": "@jackspringman", "followers": "189", "datetime": "2019-01-05 08:28:24", "content_summary": "RT @mona_sloane: Peter Eckersley of @PartnershipAI: 'high-stakes systems should always exhibit uncertainty about the best action in some ca\u2026"}, "1081322383221420034": {"author": "@mona_sloane", "followers": "3,234", "datetime": "2019-01-04 22:52:00", "content_summary": "Peter Eckersley of @PartnershipAI: 'high-stakes systems should always exhibit uncertainty about the best action in some cases' -- interesting paper arguing for, rather than against, uncertainty in #AI systems https://t.co/Sfe3DbRKAt"}, "1264869522126880773": {"author": "@dinosaurier", "followers": "16", "datetime": "2020-05-25 10:42:48", "content_summary": "Wer sich vielleicht bei einer verschwommenen Vorstellung beruhigt, ethisches Handeln lie\u00dfe sich nicht berechnen, der irrt - zB hier: https://t.co/Ni7JeVpKrl #ai #bigdata #iot https://t.co/zP0S1zY3SH"}}, "completed": "1", "queriedAt": "2020-06-02 23:55:54"}