{"citation_id": "76769854", "completed": "1", "queriedAt": "2020-05-14 12:29:37", "tab": "twitter", "twitter": {"1235691592377405441": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "7", "datetime": "2020-03-05 22:20:08", "author": "@cristian_nicp"}, "1235932913168011264": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,371", "datetime": "2020-03-06 14:19:03", "author": "@robanhk"}, "1234453091581353984": {"content_summary": "RT @Deep_In_Depth: Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/2IyO\u2026", "followers": "217", "datetime": "2020-03-02 12:18:46", "author": "@ShyBOT7"}, "1235737054576668677": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,025", "datetime": "2020-03-06 01:20:47", "author": "@desertnaut"}, "1235831283634208770": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "707", "datetime": "2020-03-06 07:35:13", "author": "@tamaki_730"}, "1235766425303814144": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "1,418", "datetime": "2020-03-06 03:17:29", "author": "@sguada"}, "1235832947288113153": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,304", "datetime": "2020-03-06 07:41:49", "author": "@pierre_guillou"}, "1235837034670874625": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,414", "datetime": "2020-03-06 07:58:04", "author": "@alxndrkalinin"}, "1235633636675850242": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5,690", "datetime": "2020-03-05 18:29:50", "author": "@walkingrandomly"}, "1235822991725850624": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "29", "datetime": "2020-03-06 07:02:16", "author": "@Roli_Khanna"}, "1236367812547010565": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "63", "datetime": "2020-03-07 19:07:11", "author": "@AshwaqHamad"}, "1235693369814167552": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "43", "datetime": "2020-03-05 22:27:12", "author": "@jeandut14000"}, "1235711604450144257": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "522", "datetime": "2020-03-05 23:39:39", "author": "@pijili"}, "1235628667461492736": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "1,699", "datetime": "2020-03-05 18:10:05", "author": "@TyphonBaalAmmon"}, "1235840394065108993": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "302", "datetime": "2020-03-06 08:11:25", "author": "@UlrichJunker"}, "1235803659595739137": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,467", "datetime": "2020-03-06 05:45:27", "author": "@aDB"}, "1236264308457140224": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "658", "datetime": "2020-03-07 12:15:54", "author": "@akivajp"}, "1237161610386345984": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "1,079", "datetime": "2020-03-09 23:41:27", "author": "@Spri_ta"}, "1235930283523727361": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "294", "datetime": "2020-03-06 14:08:36", "author": "@AlmanafSivi"}, "1235990880365899776": {"content_summary": "RT @zhuohan123: Check our latest work! We show that accelerate BERT and MT training & inference by _increasing_ model size and stopping ear\u2026", "followers": "17", "datetime": "2020-03-06 18:09:24", "author": "@ma_shuming"}, "1235727418221940737": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "164", "datetime": "2020-03-06 00:42:29", "author": "@ZachBessinger"}, "1235735749665992705": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "290", "datetime": "2020-03-06 01:15:36", "author": "@dannyehb"}, "1240744882421018634": {"content_summary": "[Haga un seguimiento con inferencia usando una versi\u00f3n comprimida del modelo m\u00e1s grande.] Entrene en grande, luego comprima: repensando M\u2026 https://t.co/QHZ2ZVHchy | Autor: @AISC_TO https://t.co/rfSeJ9jTQH", "followers": "11", "datetime": "2020-03-19 21:00:06", "author": "@UnitInorganic"}, "1235705926834950145": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "961", "datetime": "2020-03-05 23:17:05", "author": "@_numerico"}, "1235959210237546496": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "188", "datetime": "2020-03-06 16:03:33", "author": "@jeremysayz"}, "1235623838404079617": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5,199", "datetime": "2020-03-05 17:50:54", "author": "@manorlaboratory"}, "1236100142064902144": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "22,788", "datetime": "2020-03-07 01:23:34", "author": "@DotCSV"}, "1235882514868244480": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "115", "datetime": "2020-03-06 10:58:47", "author": "@shohdi2"}, "1236425461032529920": {"content_summary": "What!? Great stuff.", "followers": "124", "datetime": "2020-03-07 22:56:16", "author": "@blake_camp_1"}, "1235921978889887745": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "824", "datetime": "2020-03-06 13:35:36", "author": "@morioka"}, "1237296217643761664": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "442", "datetime": "2020-03-10 08:36:20", "author": "@AdrianB82"}, "1256398922236354561": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "738", "datetime": "2020-05-02 01:43:40", "author": "@MFuturetech"}, "1251201546404147201": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "0", "datetime": "2020-04-17 17:31:09", "author": "@Sam09lol"}, "1235628196625649665": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "871", "datetime": "2020-03-05 18:08:13", "author": "@mihail_eric"}, "1235878088732487680": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "473", "datetime": "2020-03-06 10:41:12", "author": "@sansuiso"}, "1236140281243852802": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "222", "datetime": "2020-03-07 04:03:04", "author": "@PetertheHe"}, "1235758273724215296": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "664", "datetime": "2020-03-06 02:45:06", "author": "@universeinanegg"}, "1235802217078083589": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "6,225", "datetime": "2020-03-06 05:39:43", "author": "@alienelf"}, "1240744884774023168": {"content_summary": "[Acompanhe a infer\u00eancia usando uma vers\u00e3o compactada do modelo maior.] Treine grande e depois comprima: repensando o\u2026 https://t.co/QHZ2ZVHchy | Autor: @AISC_TO https://t.co/rfSeJ9jTQH", "followers": "11", "datetime": "2020-03-19 21:00:07", "author": "@UnitInorganic"}, "1236241989856890880": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "76", "datetime": "2020-03-07 10:47:13", "author": "@haithamalrayyes"}, "1235687430218375171": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "20", "datetime": "2020-03-05 22:03:35", "author": "@Boristream"}, "1237103945752985600": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "608", "datetime": "2020-03-09 19:52:19", "author": "@thtrieu_"}, "1236398221448151041": {"content_summary": "RT @merge_s0rt: I remember once getting really pissed off at a widely respected humanities scholar characterizing CS in a PMLA article as p\u2026", "followers": "920", "datetime": "2020-03-07 21:08:01", "author": "@rybesh"}, "1243159084452347907": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 h\u2026", "followers": "463", "datetime": "2020-03-26 12:53:17", "author": "@8kazu3"}, "1237231567858262017": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "9", "datetime": "2020-03-10 04:19:27", "author": "@SlobodanVuceti4"}, "1236181250290917378": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "17,187", "datetime": "2020-03-07 06:45:51", "author": "@MargaretWallace"}, "1237353077809786880": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "111", "datetime": "2020-03-10 12:22:17", "author": "@AeneasC37529259"}, "1236426924190240768": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "87", "datetime": "2020-03-07 23:02:05", "author": "@JrKibs"}, "1235863217907195904": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "332", "datetime": "2020-03-06 09:42:07", "author": "@vsinghalus"}, "1236745291891707904": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "256", "datetime": "2020-03-08 20:07:09", "author": "@jcchinhui"}, "1235995964650991616": {"content_summary": "Nice turn back\ud83d\ude0a", "followers": "898", "datetime": "2020-03-06 18:29:36", "author": "@bayethiernodiop"}, "1235740401673228288": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "63", "datetime": "2020-03-06 01:34:05", "author": "@dave_co_dev"}, "1240348542163615746": {"content_summary": "RT @gideonmann: Bigger is faster?? What?? Mind blown \ud83e\udd2f https://t.co/vJMRnZLBPx", "followers": "1,449", "datetime": "2020-03-18 18:45:11", "author": "@mcinnovative"}, "1235653973966651393": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,225", "datetime": "2020-03-05 19:50:39", "author": "@Skiminok"}, "1235715986042556419": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "40", "datetime": "2020-03-05 23:57:04", "author": "@calebsowers7"}, "1235904249227358208": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "63", "datetime": "2020-03-06 12:25:09", "author": "@LokiDarkGod"}, "1236119403030286340": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "86", "datetime": "2020-03-07 02:40:06", "author": "@swetaagrawal20"}, "1235790707664830464": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "52", "datetime": "2020-03-06 04:53:59", "author": "@r_lepert"}, "1234126342033170433": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2CZ4j", "followers": "3,501", "datetime": "2020-03-01 14:40:23", "author": "@arxiv_cscl"}, "1235913328557514752": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "1,938", "datetime": "2020-03-06 13:01:14", "author": "@EldarSilver"}, "1235815034187419649": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "278", "datetime": "2020-03-06 06:30:39", "author": "@svaksha"}, "1235713709038694400": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "198", "datetime": "2020-03-05 23:48:01", "author": "@dahlemd"}, "1235759990993895424": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "642", "datetime": "2020-03-06 02:51:55", "author": "@aleatha"}, "1236835146243989505": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "74", "datetime": "2020-03-09 02:04:12", "author": "@barryanderson_g"}, "1235883478261985280": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "17", "datetime": "2020-03-06 11:02:37", "author": "@JoelChen95"}, "1236416299380748294": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "560", "datetime": "2020-03-07 22:19:51", "author": "@yuvalmarton"}, "1236366052264685568": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "81", "datetime": "2020-03-07 19:00:12", "author": "@kelmahrsi"}, "1236212440200818689": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "109", "datetime": "2020-03-07 08:49:48", "author": "@gbarthelet"}, "1236043114151698432": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "35", "datetime": "2020-03-06 21:36:57", "author": "@Ray_Navarro_R"}, "1235688646201794560": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "83", "datetime": "2020-03-05 22:08:25", "author": "@Sriramya_SAS"}, "1236243760608141319": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "22", "datetime": "2020-03-07 10:54:15", "author": "@AzisJons"}, "1233223467941744640": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers. Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez https://t.co/otGoE5RLlN", "followers": "3,882", "datetime": "2020-02-28 02:52:41", "author": "@BrundageBot"}, "1236098382646702080": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "219", "datetime": "2020-03-07 01:16:34", "author": "@DamonCrockett"}, "1237047888171032577": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "0", "datetime": "2020-03-09 16:09:34", "author": "@martin_bqw"}, "1237164572252835840": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "797", "datetime": "2020-03-09 23:53:14", "author": "@toliner_"}, "1235771421344792576": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "253", "datetime": "2020-03-06 03:37:21", "author": "@apastorlm"}, "1237162954128162816": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "925", "datetime": "2020-03-09 23:46:48", "author": "@Pooh3Mobi"}, "1236275516346372097": {"content_summary": "Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is that a good idea? Read more: https://t.co/05y79R0v7j & https://t.co/ysfKQEQEM9 Via @Eric_Wallace_ https://t.co/Vkkd1ILi7m", "followers": "7,592", "datetime": "2020-03-07 13:00:26", "author": "@davorjord"}, "1235663390145576960": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,168", "datetime": "2020-03-05 20:28:04", "author": "@CarsonKahn"}, "1233583028372430850": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2CZ4j", "followers": "3,501", "datetime": "2020-02-29 02:41:27", "author": "@arxiv_cscl"}, "1235733231166803969": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "589", "datetime": "2020-03-06 01:05:35", "author": "@DannyAmadoL"}, "1236395225209344000": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "922", "datetime": "2020-03-07 20:56:07", "author": "@Visionscaper"}, "1236151357633171456": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "129", "datetime": "2020-03-07 04:47:04", "author": "@sirotenko_m"}, "1235900221827186689": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "30", "datetime": "2020-03-06 12:09:09", "author": "@BayramUlya"}, "1235642370093223936": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "162", "datetime": "2020-03-05 19:04:32", "author": "@jmin__cho"}, "1235696386382888960": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "8", "datetime": "2020-03-05 22:39:11", "author": "@funnyketh"}, "1235682560874811393": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "100", "datetime": "2020-03-05 21:44:15", "author": "@Erick404"}, "1235847528475549699": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "0", "datetime": "2020-03-06 08:39:46", "author": "@MA_BRTN"}, "1235870924588507137": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "127", "datetime": "2020-03-06 10:12:44", "author": "@shimopino"}, "1235644176269807625": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "331", "datetime": "2020-03-05 19:11:43", "author": "@sai_prasanna"}, "1235628383792320512": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "35", "datetime": "2020-03-05 18:08:58", "author": "@tianshi5940"}, "1235638444740599808": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "142", "datetime": "2020-03-05 18:48:56", "author": "@katiestasaski"}, "1236293480663654401": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5,802", "datetime": "2020-03-07 14:11:49", "author": "@jm_alexia"}, "1258986133062447105": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "1,731", "datetime": "2020-05-09 05:04:19", "author": "@Mr_BaaGii"}, "1235844738990796800": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "18", "datetime": "2020-03-06 08:28:41", "author": "@JuanLDominguez_"}, "1236112298889998336": {"content_summary": "Train Large & Compress Heavily! Check our recent work on accelerating BERT/MT training/inference by _increasing_ model size and stopping early! Blog: https://t.co/YSJIPZaVry Paper: https://t.co/cdOdGVYjcF w/ @zhuohan123,@Eric_Wallace_,@nlpkevinl,Kurt", "followers": "144", "datetime": "2020-03-07 02:11:52", "author": "@shengs1123"}, "1235701759789711360": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "67", "datetime": "2020-03-05 23:00:32", "author": "@gbcolborne"}, "1237514533456904192": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "1,151", "datetime": "2020-03-10 23:03:51", "author": "@jd_mashiro"}, "1235624098736287744": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4,526", "datetime": "2020-03-05 17:51:56", "author": "@CShorten30"}, "1235747849771474944": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "29", "datetime": "2020-03-06 02:03:41", "author": "@chenlailin"}, "1256200569678675970": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "5", "datetime": "2020-05-01 12:35:29", "author": "@CaLeSS0"}, "1235844166271127552": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "188", "datetime": "2020-03-06 08:26:24", "author": "@RictusDegrenov"}, "1233205406794682369": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2CZ4j", "followers": "3,501", "datetime": "2020-02-28 01:40:55", "author": "@arxiv_cscl"}, "1235619888531746817": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "156", "datetime": "2020-03-05 17:35:12", "author": "@apmoore94"}, "1235769698308042752": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "109", "datetime": "2020-03-06 03:30:30", "author": "@_Pranjal"}, "1236226987926196225": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "12", "datetime": "2020-03-07 09:47:36", "author": "@newellsystems"}, "1235673680224677889": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "18,442", "datetime": "2020-03-05 21:08:57", "author": "@Thom_Wolf"}, "1237444678217109504": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "61", "datetime": "2020-03-10 18:26:16", "author": "@Anthrobo"}, "1235820089317797888": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "158", "datetime": "2020-03-06 06:50:44", "author": "@diegombar"}, "1235687504663252993": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "874", "datetime": "2020-03-05 22:03:53", "author": "@mrdrozdov"}, "1235785950908698625": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "310", "datetime": "2020-03-06 04:35:05", "author": "@mental_flux"}, "1236516397502459905": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "80", "datetime": "2020-03-08 04:57:37", "author": "@BobbyAlter"}, "1236362423155728385": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5,383", "datetime": "2020-03-07 18:45:46", "author": "@Alfons_Valencia"}, "1235651820946468865": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "608", "datetime": "2020-03-05 19:42:06", "author": "@sanjaykamath"}, "1236106349425111040": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "65", "datetime": "2020-03-07 01:48:14", "author": "@AndreievichRose"}, "1233779063728287744": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2CZ4j", "followers": "3,501", "datetime": "2020-02-29 15:40:25", "author": "@arxiv_cscl"}, "1235971025877286913": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "952", "datetime": "2020-03-06 16:50:30", "author": "@tmaila"}, "1235783502370570240": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "615", "datetime": "2020-03-06 04:25:21", "author": "@KouroshMeshgi"}, "1235827699521712129": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "20", "datetime": "2020-03-06 07:20:58", "author": "@av_chidanand55"}, "1237046520945532930": {"content_summary": "Impressive work by @zhuohan123 et al.:", "followers": "12,363", "datetime": "2020-03-09 16:04:08", "author": "@sleepinyourhat"}, "1235934637030957056": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "57", "datetime": "2020-03-06 14:25:54", "author": "@iam_aashusingh"}, "1258926531465760768": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "13", "datetime": "2020-05-09 01:07:29", "author": "@Bharath09095865"}, "1235888778616569857": {"content_summary": "RT @nlpkevinl: how big should you make your model for fast training & inference of Transformers? we accelerate BERT and MT training & infer\u2026", "followers": "615", "datetime": "2020-03-06 11:23:41", "author": "@KouroshMeshgi"}, "1236413247156359168": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,141", "datetime": "2020-03-07 22:07:44", "author": "@JulioGonzalo1"}, "1235751237141708800": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "346", "datetime": "2020-03-06 02:17:08", "author": "@lhenault"}, "1235660113450422272": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "163", "datetime": "2020-03-05 20:15:03", "author": "@HouhouinK"}, "1236181018064777216": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,665", "datetime": "2020-03-07 06:44:56", "author": "@eigenhector"}, "1235875067638280192": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "315", "datetime": "2020-03-06 10:29:12", "author": "@BramVanroy"}, "1235868385038053378": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "190", "datetime": "2020-03-06 10:02:38", "author": "@Ayoub95"}, "1243913419884904448": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,671", "datetime": "2020-03-28 14:50:44", "author": "@ayirpelle"}, "1235726149914726400": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "65", "datetime": "2020-03-06 00:37:27", "author": "@kli_nlpr"}, "1235767952697327616": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "843", "datetime": "2020-03-06 03:23:34", "author": "@Shagaiyo"}, "1235674837579755521": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "159", "datetime": "2020-03-05 21:13:33", "author": "@EtGagnonPolSci"}, "1235855976500084737": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "740", "datetime": "2020-03-06 09:13:20", "author": "@maltanar"}, "1235707901328711681": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "151", "datetime": "2020-03-05 23:24:56", "author": "@MlResearcher1"}, "1235757624899104770": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "97", "datetime": "2020-03-06 02:42:31", "author": "@vin_mathur"}, "1236298312468201473": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "201", "datetime": "2020-03-07 14:31:01", "author": "@sirakzg"}, "1235649583675392002": {"content_summary": "Super unintuitive to me, but good to know.", "followers": "683", "datetime": "2020-03-05 19:33:12", "author": "@analyticsaurabh"}, "1235752275748020226": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "322", "datetime": "2020-03-06 02:21:16", "author": "@countrsignal"}, "1235628340649680896": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "141", "datetime": "2020-03-05 18:08:47", "author": "@waydegilliam"}, "1235666737896529920": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "7,590", "datetime": "2020-03-05 20:41:22", "author": "@graphific"}, "1235899195170541568": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "476", "datetime": "2020-03-06 12:05:04", "author": "@yasuokajihei"}, "1235674076703793153": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "122", "datetime": "2020-03-05 21:10:32", "author": "@BerndDoser"}, "1237176082257739776": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5,606", "datetime": "2020-03-10 00:38:58", "author": "@__MLT__"}, "1235864456178982912": {"content_summary": "Great study of training efficiency at large scale + nice results on compression for inference!", "followers": "2,420", "datetime": "2020-03-06 09:47:02", "author": "@dcpage3"}, "1235760678092165120": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "11", "datetime": "2020-03-06 02:54:39", "author": "@ShouvikSarkar3"}, "1237212620676984833": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "554", "datetime": "2020-03-10 03:04:09", "author": "@FBWM8888"}, "1235968016543813635": {"content_summary": "@NexWebSites @OpenAI https://t.co/G4doYPYlsZ https://t.co/nkoj0EdGzk https://t.co/HJJtf4Judu https://t.co/SJhrDKW6ub https://t.co/nkoj0EdGzk https://t.co/mxuJnqZxoC https://t.co/VTB4WadpHx https://t.co/6yP1UnvVHl", "followers": "20,564", "datetime": "2020-03-06 16:38:32", "author": "@gwern"}, "1235744814974423040": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "609", "datetime": "2020-03-06 01:51:37", "author": "@kurianbenoy2"}, "1235673641112793088": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "1,106", "datetime": "2020-03-05 21:08:48", "author": "@permutans"}, "1236505926779031552": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "38", "datetime": "2020-03-08 04:16:00", "author": "@experiencor"}, "1233431780197187584": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2UzVR", "followers": "3,501", "datetime": "2020-02-28 16:40:27", "author": "@arxiv_cscl"}, "1235629969201451008": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "86", "datetime": "2020-03-05 18:15:16", "author": "@tiagomluis"}, "1237801067984965632": {"content_summary": "RT @madelain777: Que top ser\u00eda que leyeses esto en #100HorasDeML @DotCSV https://t.co/WqB0HwDcMH", "followers": "52", "datetime": "2020-03-11 18:02:26", "author": "@_Hicarus"}, "1236242973144363008": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "466", "datetime": "2020-03-07 10:51:07", "author": "@aberasategi"}, "1243123386475106304": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 h\u2026", "followers": "543", "datetime": "2020-03-26 10:31:26", "author": "@wsuzume"}, "1235840782713536513": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "10", "datetime": "2020-03-06 08:12:58", "author": "@yarphs"}, "1235712868265295872": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,663", "datetime": "2020-03-05 23:44:40", "author": "@sigitpurnomo"}, "1235617342337343489": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "99,939", "datetime": "2020-03-05 17:25:05", "author": "@jeremyphoward"}, "1237054926569832453": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4,636", "datetime": "2020-03-09 16:37:32", "author": "@mohitban47"}, "1243155775897227264": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 h\u2026", "followers": "45", "datetime": "2020-03-26 12:40:08", "author": "@nullbytep"}, "1235954294207025153": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "295", "datetime": "2020-03-06 15:44:01", "author": "@dusenberrymw"}, "1235954694725537799": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4,804", "datetime": "2020-03-06 15:45:36", "author": "@cto_maverick"}, "1235628719038959620": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "818", "datetime": "2020-03-05 18:10:18", "author": "@deepgradient"}, "1236275734982860800": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "16", "datetime": "2020-03-07 13:01:18", "author": "@agoaddict"}, "1236361940928065536": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "90,659", "datetime": "2020-03-07 18:43:51", "author": "@stanfordnlp"}, "1235837190707400704": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "365", "datetime": "2020-03-06 07:58:41", "author": "@jeanmarcalkazzi"}, "1235824619560382464": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "164", "datetime": "2020-03-06 07:08:44", "author": "@davamix"}, "1235647148689154049": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "574", "datetime": "2020-03-05 19:23:32", "author": "@BloodyBadgerino"}, "1235832394717880320": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,542", "datetime": "2020-03-06 07:39:38", "author": "@tarfandy"}, "1236363627197849602": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,108", "datetime": "2020-03-07 18:50:33", "author": "@ryanjgallag"}, "1235853942833074176": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "109", "datetime": "2020-03-06 09:05:15", "author": "@ishgirwan"}, "1235667032667901952": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "366", "datetime": "2020-03-05 20:42:32", "author": "@iugoaoj"}, "1235836574337646593": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "417", "datetime": "2020-03-06 07:56:14", "author": "@TobiasSchlauch"}, "1235709896932225024": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "329", "datetime": "2020-03-05 23:32:52", "author": "@UndefBehavior"}, "1235620416506359808": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "366", "datetime": "2020-03-05 17:37:18", "author": "@gabriel_ilharco"}, "1235675962961989638": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "44", "datetime": "2020-03-05 21:18:01", "author": "@FJ_82"}, "1243198949931941889": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 h\u2026", "followers": "2,176", "datetime": "2020-03-26 15:31:41", "author": "@wednesdaymuse"}, "1235937927965835264": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "480", "datetime": "2020-03-06 14:38:59", "author": "@polyplexors"}, "1235653135164518400": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,551", "datetime": "2020-03-05 19:47:19", "author": "@sandeshr"}, "1235790250103848960": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "205", "datetime": "2020-03-06 04:52:10", "author": "@a_random_obsrvr"}, "1236102739568070656": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "904", "datetime": "2020-03-07 01:33:53", "author": "@Mentalo"}, "1235802074048155650": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,116", "datetime": "2020-03-06 05:39:09", "author": "@sameer_"}, "1235738544984838145": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "21", "datetime": "2020-03-06 01:26:42", "author": "@algo_diver"}, "1235859965505486848": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "", "datetime": "2020-03-06 09:29:11", "author": "@AaaLee"}, "1236092725063106560": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "17,768", "datetime": "2020-03-07 00:54:05", "author": "@ericjang11"}, "1235630152559865856": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "40", "datetime": "2020-03-05 18:15:59", "author": "@DChuchem"}, "1235888975270690816": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "40", "datetime": "2020-03-06 11:24:28", "author": "@asharma_95"}, "1235830356856549376": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "359", "datetime": "2020-03-06 07:31:32", "author": "@RomainClaret"}, "1235629013583814657": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "123", "datetime": "2020-03-05 18:11:28", "author": "@VickyVigia"}, "1235803083885572099": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "872", "datetime": "2020-03-06 05:43:09", "author": "@henrygarner"}, "1235617239966965760": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,578", "datetime": "2020-03-05 17:24:41", "author": "@BerkeleyNLP"}, "1256199430795145223": {"content_summary": "Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is that a good idea? Read more: https://t.co/05y79R0v7j & https://t.co/ysfKQEQEM9 Via @Eric_Wallace_ https://t.co/NfM6o9zjpF https", "followers": "7,592", "datetime": "2020-05-01 12:30:58", "author": "@davorjord"}, "1236561728114221058": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "52", "datetime": "2020-03-08 07:57:44", "author": "@hakanardo"}, "1236040676619493383": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "615", "datetime": "2020-03-06 21:27:16", "author": "@mattmcd"}, "1235807461682089984": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "103", "datetime": "2020-03-06 06:00:33", "author": "@avmoldovan"}, "1235741475524100096": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "17", "datetime": "2020-03-06 01:38:21", "author": "@liucaoming"}, "1235812110451400704": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "274", "datetime": "2020-03-06 06:19:02", "author": "@herrwieger"}, "1235631943146000385": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,188", "datetime": "2020-03-05 18:23:06", "author": "@fnielsen"}, "1233393502739763202": {"content_summary": "RT @arxiv_cscl: Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2C\u2026", "followers": "7", "datetime": "2020-02-28 14:08:21", "author": "@yyq90"}, "1235620101967073281": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4,577", "datetime": "2020-03-05 17:36:03", "author": "@prem_k"}, "1235632548069466114": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,019", "datetime": "2020-03-05 18:25:31", "author": "@pythiccoder"}, "1235997710882426880": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "34", "datetime": "2020-03-06 18:36:32", "author": "@therealaseifert"}, "1235646876101283842": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "459", "datetime": "2020-03-05 19:22:27", "author": "@PauRueQ"}, "1235693303351439360": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "1,934", "datetime": "2020-03-05 22:26:56", "author": "@mandubian"}, "1235915277461684224": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "317", "datetime": "2020-03-06 13:08:58", "author": "@kunalbht"}, "1235716371230502912": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "15", "datetime": "2020-03-05 23:58:36", "author": "@AbdullahMdKhan"}, "1235937740887166978": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1", "datetime": "2020-03-06 14:38:14", "author": "@caocscar"}, "1236310230256046080": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "70", "datetime": "2020-03-07 15:18:23", "author": "@waqas_sone_143"}, "1240889418229014533": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "0", "datetime": "2020-03-20 06:34:26", "author": "@theannshan"}, "1236265293040631810": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,673", "datetime": "2020-03-07 12:19:49", "author": "@mosko_mule"}, "1253499137129095168": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "155", "datetime": "2020-04-24 01:40:57", "author": "@Berry_Manly"}, "1235743088905158656": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "146", "datetime": "2020-03-06 01:44:46", "author": "@slava_h"}, "1237165861795848194": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "153", "datetime": "2020-03-09 23:58:21", "author": "@zakki"}, "1236598202687795201": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "10", "datetime": "2020-03-08 10:22:41", "author": "@AndreyZ_Z"}, "1235657455222349824": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,070", "datetime": "2020-03-05 20:04:29", "author": "@EricSchles"}, "1235802204461662221": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "339", "datetime": "2020-03-06 05:39:40", "author": "@IUILab"}, "1235842434099077120": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "48", "datetime": "2020-03-06 08:19:31", "author": "@mpsampat"}, "1236215519080046592": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "209", "datetime": "2020-03-07 09:02:02", "author": "@ragerri"}, "1235672292107194368": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "255", "datetime": "2020-03-05 21:03:26", "author": "@josipK"}, "1256203543834484736": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "179", "datetime": "2020-05-01 12:47:18", "author": "@PriyeshPatelUK"}, "1235635646959673344": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "594", "datetime": "2020-03-05 18:37:49", "author": "@flavioclesio"}, "1235675963234500613": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "118", "datetime": "2020-03-05 21:18:02", "author": "@iiacobacNLP"}, "1233386723389902848": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2CZ4j", "followers": "3,501", "datetime": "2020-02-28 13:41:24", "author": "@arxiv_cscl"}, "1236291466919424001": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "1,395", "datetime": "2020-03-07 14:03:49", "author": "@mttsubu"}, "1235644343181959168": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "25", "datetime": "2020-03-05 19:12:23", "author": "@ASreesaila"}, "1235968845644320768": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "772", "datetime": "2020-03-06 16:41:50", "author": "@UdayaPatnaik"}, "1235973711024406528": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "19", "datetime": "2020-03-06 17:01:10", "author": "@vimarsh_c"}, "1235637976916316161": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "104", "datetime": "2020-03-05 18:47:05", "author": "@vibhavagarwal5"}, "1236225832835788800": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "509", "datetime": "2020-03-07 09:43:01", "author": "@RGuitar96"}, "1235712441318871045": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,827", "datetime": "2020-03-05 23:42:59", "author": "@acidflask"}, "1235634901119991808": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "223", "datetime": "2020-03-05 18:34:52", "author": "@posadajd"}, "1237160706148950017": {"content_summary": "Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u30a6\u30f3\u30b5\u30a4\u30ba\u3059\u308b\u306e\u304c\u6700\u9069\u3068\u3057\u3066\u3044\u308b\u3002", "followers": "11,476", "datetime": "2020-03-09 23:37:52", "author": "@icoxfog417"}, "1235687686158946304": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "457", "datetime": "2020-03-05 22:04:37", "author": "@bluejin55"}, "1235628898299281408": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "818", "datetime": "2020-03-05 18:11:00", "author": "@deepgradient"}, "1235803233198669824": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "184", "datetime": "2020-03-06 05:43:45", "author": "@drugilsberg"}, "1235780732745850882": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "458", "datetime": "2020-03-06 04:14:21", "author": "@sajidshahbs"}, "1238502722048888832": {"content_summary": "DL model training paradigms \ud83e\uddd0: a. Train small model, stop when converges b. Train large model, stop early", "followers": "375", "datetime": "2020-03-13 16:30:33", "author": "@remykarem"}, "1235790837008625665": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "520", "datetime": "2020-03-06 04:54:30", "author": "@affige_yang"}, "1235792415040167938": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "93", "datetime": "2020-03-06 05:00:46", "author": "@chauhraj"}, "1243121599689240576": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL When computational resources are limited, it is suggested that it is better to train and compress a\u2026", "followers": "554", "datetime": "2020-03-26 10:24:20", "author": "@FBWM8888"}, "1236320443486871552": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "46", "datetime": "2020-03-07 15:58:58", "author": "@roryjmaierwies1"}, "1237766812483358720": {"content_summary": "RT @madelain777: Que top ser\u00eda que leyeses esto en #100HorasDeML @DotCSV https://t.co/x19iXm1ok5 https://t.co/WqB0HwDcMH", "followers": "22,788", "datetime": "2020-03-11 15:46:19", "author": "@DotCSV"}, "1235715913049128960": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "259", "datetime": "2020-03-05 23:56:46", "author": "@brdskggs"}, "1235773463782096896": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "131", "datetime": "2020-03-06 03:45:27", "author": "@yujia_bao"}, "1235688651549552640": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "73", "datetime": "2020-03-05 22:08:27", "author": "@rdlfSchneider"}, "1244336350259744769": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "7,592", "datetime": "2020-03-29 18:51:19", "author": "@davorjord"}, "1243152565799243776": {"content_summary": "Transformer\u3092\u5b66\u7fd2\u3055\u305b\u308b\u969b\u3001\u5c0f\u3055\u306a\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u56de\u6570\u3092\u5897\u3084\u3059\u3088\u308a\u3082\u5927\u304d\u306a\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3055\u305b\u305f\u307b\u3046\u304c\u53ce\u675f\u3092\u901f\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u5b9f\u9a13\u7684\u306b\u793a\u3057\u305f\u3002\u307e\u305f\u5927\u304d\u306a\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u306f\u91cf\u5b50\u5316\u3084\u679d\u5207\u308a\u306b\u5bfe\u3057\u3066\u3082\u3088\u308a\u30ed\u30d0\u30b9\u30c8\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002 https://t.co/OYBxafS7bV https://t.co/BrLBEZWyTv", "followers": "127", "datetime": "2020-03-26 12:27:23", "author": "@shimopino"}, "1236638977391824896": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "52", "datetime": "2020-03-08 13:04:42", "author": "@hakanardo"}, "1236573966204108800": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "21", "datetime": "2020-03-08 08:46:22", "author": "@ZafrirOfir"}, "1235938595363581958": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "374", "datetime": "2020-03-06 14:41:38", "author": "@AhmadMustafaAn1"}, "1235684557577314304": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "6,012", "datetime": "2020-03-05 21:52:11", "author": "@lavanyaai"}, "1236598403703779329": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "31", "datetime": "2020-03-08 10:23:28", "author": "@vectorAdCaelum"}, "1236427594033233920": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "76", "datetime": "2020-03-07 23:04:44", "author": "@BurakSayici"}, "1237938340923469829": {"content_summary": "What", "followers": "331", "datetime": "2020-03-12 03:07:54", "author": "@Rohitpatil5"}, "1237212633285013505": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "554", "datetime": "2020-03-10 03:04:12", "author": "@FBWM8888"}, "1235820301025292288": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "80", "datetime": "2020-03-06 06:51:34", "author": "@assoulix"}, "1235780170184880134": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "84", "datetime": "2020-03-06 04:12:06", "author": "@rahulmeetu"}, "1236364360785842176": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,028", "datetime": "2020-03-07 18:53:28", "author": "@c_schwemmer"}, "1236027352846565378": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4,255", "datetime": "2020-03-06 20:34:19", "author": "@weballergy"}, "1235792344152027141": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "189", "datetime": "2020-03-06 05:00:29", "author": "@venkyvarddineni"}, "1237167748419117056": {"content_summary": "RT @sleepinyourhat: Impressive work by @zhuohan123 et al.:", "followers": "106", "datetime": "2020-03-10 00:05:51", "author": "@morafaie"}, "1236248970067763203": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "36", "datetime": "2020-03-07 11:14:57", "author": "@Bihua_chen"}, "1235946358957170688": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,450", "datetime": "2020-03-06 15:12:29", "author": "@piacere_ex"}, "1235979819743141888": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "169", "datetime": "2020-03-06 17:25:27", "author": "@2010T2020"}, "1235636060866125824": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,206", "datetime": "2020-03-05 18:39:28", "author": "@JeffdotLayton"}, "1235689995119157248": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "25", "datetime": "2020-03-05 22:13:47", "author": "@NLP_Grayming"}, "1235740096294313984": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "31,251", "datetime": "2020-03-06 01:32:52", "author": "@berkeley_ai"}, "1236513878038601728": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "129", "datetime": "2020-03-08 04:47:36", "author": "@naveenpnd"}, "1235618296705122305": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "65", "datetime": "2020-03-05 17:28:53", "author": "@Dhruvrnaik"}, "1235621304541130752": {"content_summary": "how big should you make your model for fast training & inference of Transformers? we accelerate BERT and MT training & inference by _increasing_ model size and stopping early https://t.co/d6wCDYEUy5 w/ @zhuohan123, @Eric_Wallace_, @shengs1123, Kurt", "followers": "255", "datetime": "2020-03-05 17:40:50", "author": "@nlpkevinl"}, "1235616760595791872": {"content_summary": "Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However, you should actually *increase* model size to speed up training and inference for transformers. Why? [1/6] \ud83d\udc47 https://t.co/GcjytCEmox", "followers": "2,242", "datetime": "2020-03-05 17:22:47", "author": "@Eric_Wallace_"}, "1235626229350334465": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1", "datetime": "2020-03-05 18:00:24", "author": "@udemonist"}, "1235879965196861442": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "441", "datetime": "2020-03-06 10:48:39", "author": "@shivam13verma"}, "1235616768934076416": {"content_summary": "See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers\" By @zhuohan123, @Eric_Wallace_, @nlpkevinl, @shengs1123, Kurt Keutzer, Dan Klein, @mejoeyg Blog https://t.co/GcjytCEmox Paper ht", "followers": "2,242", "datetime": "2020-03-05 17:22:49", "author": "@Eric_Wallace_"}, "1235637300077326338": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "2,671", "datetime": "2020-03-05 18:44:24", "author": "@ayirpelle"}, "1236767194001768449": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "180", "datetime": "2020-03-08 21:34:11", "author": "@sriharshams"}, "1237047035397918722": {"content_summary": "RT @sleepinyourhat: Impressive work by @zhuohan123 et al.: https://t.co/Wvqex54X4z", "followers": "2,436", "datetime": "2020-03-09 16:06:11", "author": "@BayesForDays"}, "1235726585887625217": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "266", "datetime": "2020-03-06 00:39:11", "author": "@neeraj_wagh"}, "1235941317424775168": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "868", "datetime": "2020-03-06 14:52:27", "author": "@ikuyamada"}, "1235796525499871235": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "206", "datetime": "2020-03-06 05:17:06", "author": "@bnjasim"}, "1238472080477151232": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "409", "datetime": "2020-03-13 14:28:48", "author": "@jainnitk"}, "1235905336072007680": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "302", "datetime": "2020-03-06 12:29:28", "author": "@subhobrata1"}, "1235751440917786624": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "161", "datetime": "2020-03-06 02:17:57", "author": "@Sohilnewa"}, "1236904765373767681": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,107", "datetime": "2020-03-09 06:40:51", "author": "@aremcx"}, "1236017228862107649": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "92", "datetime": "2020-03-06 19:54:06", "author": "@SentimOfficial"}, "1235819444057735169": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "40", "datetime": "2020-03-06 06:48:10", "author": "@ipro52"}, "1237781855610470400": {"content_summary": "RT @SourabhSKatoch: Let me forward this to my previous boss. He used to call me crazy for saying this \ud83d\ude05", "followers": "386", "datetime": "2020-03-11 16:46:05", "author": "@EdelGM"}, "1235887939118301184": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "163", "datetime": "2020-03-06 11:20:21", "author": "@ChurchillMic"}, "1235661945858121729": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "16,817", "datetime": "2020-03-05 20:22:20", "author": "@octonion"}, "1235634646647357440": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,096", "datetime": "2020-03-05 18:33:51", "author": "@ivan_bezdomny"}, "1237164544922775554": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "203", "datetime": "2020-03-09 23:53:07", "author": "@wayama_ryousuke"}, "1237305641540022272": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "175", "datetime": "2020-03-10 09:13:47", "author": "@SquirrelYellow"}, "1236085852469526530": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,151", "datetime": "2020-03-07 00:26:47", "author": "@jd_mashiro"}, "1233960410371514368": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2CZ4j", "followers": "3,501", "datetime": "2020-03-01 03:41:02", "author": "@arxiv_cscl"}, "1236953755892899841": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3", "datetime": "2020-03-09 09:55:31", "author": "@hyunw_kim_"}, "1235646557955002369": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "397", "datetime": "2020-03-05 19:21:11", "author": "@KordoniN"}, "1235654080724439040": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "534", "datetime": "2020-03-05 19:51:04", "author": "@inemshan"}, "1240744887202525198": {"content_summary": "[\u4f7f\u7528\u8f83\u5927\u6a21\u578b\u7684\u538b\u7f29\u7248\u672c\u8fdb\u884c\u63a8\u7406\u3002]\u8bad\u7ec3\u5927\u7136\u540e\u538b\u7f29\uff1a\u91cd\u65b0\u601d\u8003M\u2026https://t.co/QHZ2ZVHchy |\u4f5c\u8005\uff1a@AISC_TO https://t.co/rfSeJ9jTQH", "followers": "11", "datetime": "2020-03-19 21:00:07", "author": "@UnitInorganic"}, "1258911604797771782": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "7,592", "datetime": "2020-05-09 00:08:10", "author": "@davorjord"}, "1240906464543178752": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,639", "datetime": "2020-03-20 07:42:10", "author": "@MikeMongo"}, "1236969661582856193": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2", "datetime": "2020-03-09 10:58:43", "author": "@purple246789"}, "1236094601108180992": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5", "datetime": "2020-03-07 01:01:33", "author": "@MCEChiu"}, "1235664461035220992": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "69", "datetime": "2020-03-05 20:32:19", "author": "@v_trokhymenko"}, "1235644350219997184": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "172", "datetime": "2020-03-05 19:12:24", "author": "@ggdupont"}, "1239678455115120643": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "96", "datetime": "2020-03-16 22:22:30", "author": "@Delfox29"}, "1235694527450689536": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,102", "datetime": "2020-03-05 22:31:48", "author": "@kastnerkyle"}, "1235983755212279810": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "25", "datetime": "2020-03-06 17:41:05", "author": "@solankirahul411"}, "1235964283348873223": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "5", "datetime": "2020-03-06 16:23:42", "author": "@Koundinya33"}, "1235628529263554562": {"content_summary": "\"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a big theoretical question. we clearly don't understand the role of model sizes very well yet.", "followers": "13,790", "datetime": "2020-03-05 18:09:32", "author": "@yoavgo"}, "1235620170799824896": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "144", "datetime": "2020-03-05 17:36:20", "author": "@shengs1123"}, "1235824258963476490": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "283", "datetime": "2020-03-06 07:07:18", "author": "@kowalskithomas"}, "1235697617461526528": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "95", "datetime": "2020-03-05 22:44:04", "author": "@rojas_f_adrian"}, "1235853994083225601": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "432", "datetime": "2020-03-06 09:05:27", "author": "@bhargavbardipur"}, "1235907934288326658": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "15", "datetime": "2020-03-06 12:39:48", "author": "@gdsttian"}, "1235629691492372480": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "78", "datetime": "2020-03-05 18:14:09", "author": "@richliaw"}, "1235621077222473728": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "538", "datetime": "2020-03-05 17:39:56", "author": "@stormtroper1721"}, "1235829810703679488": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "118", "datetime": "2020-03-06 07:29:22", "author": "@ting_s_"}, "1235873477824897025": {"content_summary": "Big models always win? The finetuning converges faster and then you can \"heavy\"-compress them.", "followers": "128", "datetime": "2020-03-06 10:22:53", "author": "@ekshakhs"}, "1235687076986748928": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "44", "datetime": "2020-03-05 22:02:11", "author": "@SpaceCyborgBoy"}, "1235737929097584640": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "962", "datetime": "2020-03-06 01:24:15", "author": "@bgalbraith"}, "1235620314689794048": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "144", "datetime": "2020-03-05 17:36:54", "author": "@shengs1123"}, "1235859420543725578": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "32", "datetime": "2020-03-06 09:27:01", "author": "@mickbo32"}, "1236208857162092544": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "13", "datetime": "2020-03-07 08:35:33", "author": "@gw03"}, "1236095855079243776": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "614", "datetime": "2020-03-07 01:06:32", "author": "@ivrik"}, "1237049594376122368": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "1,215", "datetime": "2020-03-09 16:16:21", "author": "@subzerochi"}, "1235618627442753536": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "89", "datetime": "2020-03-05 17:30:12", "author": "@mgrankin"}, "1235722326437740544": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "123", "datetime": "2020-03-06 00:22:15", "author": "@iantimmis"}, "1235800427251154944": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "159", "datetime": "2020-03-06 05:32:36", "author": "@abelpardolopez"}, "1236824799659184128": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "414", "datetime": "2020-03-09 01:23:05", "author": "@amrithajayanti"}, "1235704069362266114": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4,891", "datetime": "2020-03-05 23:09:43", "author": "@IgorCarron"}, "1236155679406051328": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "14", "datetime": "2020-03-07 05:04:15", "author": "@elevenyeast"}, "1236278243818237952": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2", "datetime": "2020-03-07 13:11:16", "author": "@jeongukjae"}, "1236181323976519681": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "17,187", "datetime": "2020-03-07 06:46:09", "author": "@MargaretWallace"}, "1235633858936074240": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "24", "datetime": "2020-03-05 18:30:43", "author": "@KantadoAaveChe"}, "1235895860338102272": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "36", "datetime": "2020-03-06 11:51:49", "author": "@kuz44ma69"}, "1236313110476206081": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "113", "datetime": "2020-03-07 15:29:49", "author": "@JensTuyls"}, "1235842040534949888": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,284", "datetime": "2020-03-06 08:17:57", "author": "@A_K_Nain"}, "1235634357941006337": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "85", "datetime": "2020-03-05 18:32:42", "author": "@FlorianDreher"}, "1236256276105101313": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "111", "datetime": "2020-03-07 11:43:59", "author": "@LSTMeow"}, "1235679543408685056": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "589", "datetime": "2020-03-05 21:32:15", "author": "@m_guerini"}, "1243116288433483779": {"content_summary": "https://t.co/Zxe16faFNL When computational resources are limited, it is suggested that it is better to train and compress a large model than to train/infer with a small model. The larger models converge faster and have less of a drop in accuracy when compr", "followers": "1,309", "datetime": "2020-03-26 10:03:13", "author": "@AkiraTOSEI"}, "1235829143595397122": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "140", "datetime": "2020-03-06 07:26:43", "author": "@romain_riviere"}, "1236367416017309696": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "26", "datetime": "2020-03-07 19:05:37", "author": "@ReedRoof"}, "1236093630722355203": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,898", "datetime": "2020-03-07 00:57:41", "author": "@nsaphra"}, "1235770497587167236": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "253", "datetime": "2020-03-06 03:33:40", "author": "@apastorlm"}, "1235688190159306753": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "427", "datetime": "2020-03-05 22:06:37", "author": "@jp_axs4ll"}, "1237188404858818571": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,046", "datetime": "2020-03-10 01:27:56", "author": "@ysaito8015"}, "1235673233359286273": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "4", "datetime": "2020-03-05 21:07:11", "author": "@pqbsbk"}, "1235684707364245505": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,249", "datetime": "2020-03-05 21:52:46", "author": "@weights_biases"}, "1233621870894673920": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of... https://t.co/Yov6YPVIrD https://t.co/L2EJKw1emq", "followers": "12,736", "datetime": "2020-02-29 05:15:48", "author": "@arxiv_org"}, "1235636833570009088": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,081", "datetime": "2020-03-05 18:42:32", "author": "@roeeaharoni"}, "1235672017493344256": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "104", "datetime": "2020-03-05 21:02:21", "author": "@Sumit__ML"}, "1235661468709908480": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "216", "datetime": "2020-03-05 20:20:26", "author": "@udaykovur"}, "1236099929766010880": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "317", "datetime": "2020-03-07 01:22:43", "author": "@rgherrmann"}, "1235639883453960193": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "15", "datetime": "2020-03-05 18:54:39", "author": "@prashil_t"}, "1235810692256509952": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "148", "datetime": "2020-03-06 06:13:23", "author": "@IsackOdero"}, "1235623653213171718": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "66", "datetime": "2020-03-05 17:50:10", "author": "@diegovogeid"}, "1236359225166557184": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "23", "datetime": "2020-03-07 18:33:04", "author": "@Cheng_Ching_Wen"}, "1237164100850835457": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "640", "datetime": "2020-03-09 23:51:21", "author": "@copasta_"}, "1235617620067573760": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "161", "datetime": "2020-03-05 17:26:11", "author": "@Sid____"}, "1234156802255638528": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/Abk5N2UzVR", "followers": "3,501", "datetime": "2020-03-01 16:41:25", "author": "@arxiv_cscl"}, "1235964547933978625": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "93", "datetime": "2020-03-06 16:24:45", "author": "@0xhexhex"}, "1235873760378384387": {"content_summary": "Let me forward this to my previous boss. He used to call me crazy for saying this \ud83d\ude05", "followers": "1,731", "datetime": "2020-03-06 10:24:00", "author": "@SourabhSKatoch"}, "1243121530327851009": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 h\u2026", "followers": "554", "datetime": "2020-03-26 10:24:03", "author": "@FBWM8888"}, "1235630775355109377": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,627", "datetime": "2020-03-05 18:18:28", "author": "@chris_brockett"}, "1235686681329664002": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "187", "datetime": "2020-03-05 22:00:37", "author": "@LouisvanBeurden"}, "1235819028532191233": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "19", "datetime": "2020-03-06 06:46:31", "author": "@soumi_das0407"}, "1236096048503930880": {"content_summary": "Most popular computer science paper of the day: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers\" https://t.co/UvVrFTguxC https://t.co/uBthpewr2T", "followers": "281", "datetime": "2020-03-07 01:07:18", "author": "@HotCompScience"}, "1235637313624887296": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,671", "datetime": "2020-03-05 18:44:27", "author": "@ayirpelle"}, "1235626038828371969": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "160", "datetime": "2020-03-05 17:59:39", "author": "@tuvuumass"}, "1235857414529773568": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "118", "datetime": "2020-03-06 09:19:03", "author": "@datamase"}, "1235821535958405125": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "153", "datetime": "2020-03-06 06:56:29", "author": "@jolibrain"}, "1235728520598491136": {"content_summary": "Bigger is faster?? What?? Mind blown \ud83e\udd2f", "followers": "2,607", "datetime": "2020-03-06 00:46:52", "author": "@gideonmann"}, "1243116228006178816": {"content_summary": "https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 https://t.co/Da0swGiJ0O", "followers": "1,309", "datetime": "2020-03-26 10:02:59", "author": "@AkiraTOSEI"}, "1235795145712226305": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "17", "datetime": "2020-03-06 05:11:37", "author": "@JoelChen95"}, "1235617842319548417": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "553", "datetime": "2020-03-05 17:27:04", "author": "@niedakh"}, "1235944035996622848": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "576", "datetime": "2020-03-06 15:03:15", "author": "@elieah"}, "1235674636538572817": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,188", "datetime": "2020-03-05 21:12:45", "author": "@joeddav"}, "1235823958361964547": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "16", "datetime": "2020-03-06 07:06:06", "author": "@Tzeny25"}, "1235695118918828034": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,727", "datetime": "2020-03-05 22:34:09", "author": "@gdbassett"}, "1235674344019255296": {"content_summary": "Check our latest work! We show that accelerate BERT and MT training & inference by _increasing_ model size and stopping early! Blog: https://t.co/RXh8bj3YSe Paper: https://t.co/P8GAq8jl6c w/ @Eric_Wallace_, @shengs1123, @nlpkevinl, Kurt Keutzer, Dan K", "followers": "170", "datetime": "2020-03-05 21:11:35", "author": "@zhuohan123"}, "1236187569580388352": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,434", "datetime": "2020-03-07 07:10:58", "author": "@damianborth"}, "1235639762767220737": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "9", "datetime": "2020-03-05 18:54:11", "author": "@ohta098"}, "1235834676658327552": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "114", "datetime": "2020-03-06 07:48:42", "author": "@michelole"}, "1235628694753832962": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "371", "datetime": "2020-03-05 18:10:12", "author": "@andresmoreno_b"}, "1236381219539095554": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "378", "datetime": "2020-03-07 20:00:28", "author": "@brvilar"}, "1236123836854063104": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "158", "datetime": "2020-03-07 02:57:43", "author": "@rkimura47"}, "1235845888339148802": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "130", "datetime": "2020-03-06 08:33:15", "author": "@ai_levia"}, "1235765232708055040": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "335", "datetime": "2020-03-06 03:12:45", "author": "@kobi78"}, "1235964701936254978": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "342", "datetime": "2020-03-06 16:25:22", "author": "@avineshpvs"}, "1237216979540295681": {"content_summary": "RT @icoxfog417: Transformer\u306e\u30c0\u30a6\u30f3\u30b5\u30a4\u30b8\u30f3\u30b0\u306b\u9069\u3057\u305f\u6226\u7565\u3092\u8abf\u3079\u305f\u7814\u7a76\u3002\u901a\u5e38\u306f\u5c0f\u3055\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u591a\u304f\u5b9f\u969biteration\u306e\u56de\u8ee2\u901f\u5ea6\u306f\u9ad8\u3044\u304c\u3001\u5927\u304d\u3044\u30b5\u30a4\u30ba\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u53ce\u675f\u304c\u901f\u3044\u3068\u3044\u3046\u7d50\u679c\u3002\u5927\u578b\u306e\u30e2\u30c7\u30eb\u3067\u7d20\u65e9\u304f\u5b66\u7fd2\u3057\u3001\u305d\u306e\u5f8c\u30c0\u2026", "followers": "824", "datetime": "2020-03-10 03:21:28", "author": "@morioka"}, "1235833118570868743": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "48", "datetime": "2020-03-06 07:42:30", "author": "@mzadrogaPL"}, "1250571655979638791": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "52", "datetime": "2020-04-15 23:48:11", "author": "@HengjianJia"}, "1235740401711144960": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "63", "datetime": "2020-03-06 01:34:05", "author": "@dave_co_dev"}, "1235777485930926080": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "578", "datetime": "2020-03-06 04:01:26", "author": "@usagisan2020"}, "1236363301614829568": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "20", "datetime": "2020-03-07 18:49:16", "author": "@rakeshsiri4"}, "1235892754363961345": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "275", "datetime": "2020-03-06 11:39:29", "author": "@danieldekok"}, "1235795859058524160": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "230", "datetime": "2020-03-06 05:14:27", "author": "@shashank_bits"}, "1236014426786013184": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "245", "datetime": "2020-03-06 19:42:58", "author": "@aladinoster"}, "1236362634510962688": {"content_summary": "I remember once getting really pissed off at a widely respected humanities scholar characterizing CS in a PMLA article as primarily about \"accuracy.\" Any first-year CS major can tell you it's about maximizing efficiency. Here's one of countless examples il", "followers": "825", "datetime": "2020-03-07 18:46:37", "author": "@merge_s0rt"}, "1236106641939910658": {"content_summary": "More parameters => faster convergence! Another example of deep learning working contrary to accepted wisdom and intution gained from observing small models and data.", "followers": "36", "datetime": "2020-03-07 01:49:23", "author": "@KbalajiTweets"}, "1235656433573617666": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "38", "datetime": "2020-03-05 20:00:25", "author": "@aamirjarda"}, "1235751213691482114": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "63", "datetime": "2020-03-06 02:17:03", "author": "@kuanchen22"}, "1235663309715591168": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "397", "datetime": "2020-03-05 20:27:45", "author": "@psteinb_"}, "1235716914497720321": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "68", "datetime": "2020-03-06 00:00:45", "author": "@stephaneghozzi"}, "1235796773676830721": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "10,291", "datetime": "2020-03-06 05:18:05", "author": "@pacoid"}, "1235624290302615554": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "73", "datetime": "2020-03-05 17:52:42", "author": "@__dwrodri"}, "1235617514681442310": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "49", "datetime": "2020-03-05 17:25:46", "author": "@vytwso"}, "1235948501936324608": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "448", "datetime": "2020-03-06 15:21:00", "author": "@MarkCunn98"}, "1235725940518354944": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "270", "datetime": "2020-03-06 00:36:37", "author": "@kotti_sasikanth"}, "1239540052981682176": {"content_summary": "RT @davorjord: Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is t\u2026", "followers": "347", "datetime": "2020-03-16 13:12:32", "author": "@miguelusque"}, "1235850105111994369": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "13", "datetime": "2020-03-06 08:50:00", "author": "@inderaum"}, "1235824576233238528": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "31", "datetime": "2020-03-06 07:08:34", "author": "@KarimElgohary9"}, "1235664891077431296": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "85", "datetime": "2020-03-05 20:34:02", "author": "@_GilRocha"}, "1235696471158280192": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "796", "datetime": "2020-03-05 22:39:31", "author": "@saghiali051"}, "1238027642672009217": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "15", "datetime": "2020-03-12 09:02:46", "author": "@qirui_w"}, "1236416815716356101": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "560", "datetime": "2020-03-07 22:21:55", "author": "@yuvalmarton"}, "1237055977842569216": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "229", "datetime": "2020-03-09 16:41:43", "author": "@amitunix"}, "1235940894328545281": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "21", "datetime": "2020-03-06 14:50:46", "author": "@cryptonymous_9"}, "1235941808657575942": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3", "datetime": "2020-03-06 14:54:24", "author": "@XKCDilbert"}, "1237120444253319168": {"content_summary": "RT @sleepinyourhat: Impressive work by @zhuohan123 et al.:", "followers": "2,070", "datetime": "2020-03-09 20:57:53", "author": "@EricSchles"}, "1239538202106527746": {"content_summary": "Not everyone can afford to train huge neural networks. So, we typically \"reduce\" model size to train/test faster. But, is that a good idea? Read more: https://t.co/05y79R0v7j & https://t.co/ysfKQEQEM9 Via @Eric_Wallace_ https://t.co/NfM6o9zjpF", "followers": "7,592", "datetime": "2020-03-16 13:05:11", "author": "@davorjord"}, "1235880566089822208": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "45", "datetime": "2020-03-06 10:51:03", "author": "@jshin491"}, "1235879311799812098": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "746", "datetime": "2020-03-06 10:46:04", "author": "@sawungrana"}, "1235617589885169666": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "392", "datetime": "2020-03-05 17:26:04", "author": "@HanGuo97"}, "1235766843983433730": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "91", "datetime": "2020-03-06 03:19:09", "author": "@Ohnyx2"}, "1235869116998635521": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "291", "datetime": "2020-03-06 10:05:33", "author": "@_lpag"}, "1236036519539826688": {"content_summary": "Good read", "followers": "2,105", "datetime": "2020-03-06 21:10:45", "author": "@nicolastorzec"}, "1235706826060165120": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "564", "datetime": "2020-03-05 23:20:40", "author": "@kalpeshk2011"}, "1235907215959171073": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "119", "datetime": "2020-03-06 12:36:56", "author": "@goldgof"}, "1235926727261552641": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "45", "datetime": "2020-03-06 13:54:28", "author": "@artuskg"}, "1235918539422232577": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "13", "datetime": "2020-03-06 13:21:56", "author": "@Bharath09095865"}, "1236102246326243330": {"content_summary": "Que top ser\u00eda que leyeses esto en #100HorasDeML @DotCSV https://t.co/WqB0HwDcMH", "followers": "347", "datetime": "2020-03-07 01:31:55", "author": "@madelain777"}, "1236266319290331136": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,067", "datetime": "2020-03-07 12:23:53", "author": "@yo_ehara"}, "1235812069926006787": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "38", "datetime": "2020-03-06 06:18:52", "author": "@DongkuanXu"}, "1246795677088264192": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "32", "datetime": "2020-04-05 13:43:48", "author": "@JinyiYang6"}, "1236189553624408064": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "36", "datetime": "2020-03-07 07:18:51", "author": "@aurko_roy"}, "1235841313905373184": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "13", "datetime": "2020-03-06 08:15:04", "author": "@Ramakrishna_05"}, "1235779082127843329": {"content_summary": "https://t.co/EEPhzy1khl", "followers": "14", "datetime": "2020-03-06 04:07:47", "author": "@YSML"}, "1236113855786487808": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "302", "datetime": "2020-03-07 02:18:03", "author": "@societyoftrees"}, "1235826179820511232": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "140", "datetime": "2020-03-06 07:14:56", "author": "@DextWard"}, "1235758947967135744": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,165", "datetime": "2020-03-06 02:47:47", "author": "@KevinKaichuang"}, "1234448534344273920": {"content_summary": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/2IyOFAdB1Z #DeepLearning #NeuralNetworks #ArtificialIntelligence #MachineLearning #AGI #NeuroMorphic #NPU #DL #AI #ML #NLP #TensorFlow #Ke", "followers": "8,950", "datetime": "2020-03-02 12:00:40", "author": "@Deep_In_Depth"}, "1235839179407249411": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "802", "datetime": "2020-03-06 08:06:35", "author": "@ElhuyarIG"}, "1235768725237882880": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "820", "datetime": "2020-03-06 03:26:38", "author": "@marskar"}, "1235690422283849731": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "131", "datetime": "2020-03-05 22:15:29", "author": "@KrishaMehta2"}, "1236172865931825153": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "131", "datetime": "2020-03-07 06:12:32", "author": "@DemiourgosUA"}, "1241816547863269376": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "10,602", "datetime": "2020-03-22 19:58:31", "author": "@ume_kyd"}, "1237134000944275456": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "73", "datetime": "2020-03-09 21:51:45", "author": "@adn_twitts"}, "1235649904153972739": {"content_summary": "RT @yoavgo: \"larger models train better and compress better\". very interesting (and useful!) empirical observation. and also points to a b\u2026", "followers": "164,079", "datetime": "2020-03-05 19:34:29", "author": "@ceobillionaire"}, "1235709808256397314": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "194", "datetime": "2020-03-05 23:32:31", "author": "@LukeAOLeary"}, "1235922464892260352": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "22", "datetime": "2020-03-06 13:37:32", "author": "@manojmohan2017"}, "1235791177548550147": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "192", "datetime": "2020-03-06 04:55:51", "author": "@JaviFuentesAI"}, "1235693438634467330": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "3,753", "datetime": "2020-03-05 22:27:28", "author": "@mapologo"}, "1236116367201292288": {"content_summary": "Transformer \u3067\u306f\u30e2\u30c7\u30eb\u3092\u5927\u304d\u304f\u3057\u305f\u65b9\u304c\u5b66\u7fd2\u3092\u901f\u304f\u3067\u304d\u308b\u3068\u306e\u3053\u3068\u3002 \u30dd\u30a4\u30f3\u30c8\u306f early stopping \u3057\u3066\u3001\u30e2\u30c7\u30eb\u3092\u5727\u7e2e ( quantization \u3068 pruning ) \u3059\u308b\u3053\u3068\u3002 https://t.co/sa1NDhFsZi", "followers": "186", "datetime": "2020-03-07 02:28:02", "author": "@nkmry_"}, "1235900587855589378": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "18", "datetime": "2020-03-06 12:10:36", "author": "@sajan_gohil"}, "1243236528827990016": {"content_summary": "RT @AkiraTOSEI: https://t.co/Zxe16faFNL \u8a08\u7b97\u8cc7\u6e90\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2/\u63a8\u8ad6\u3055\u305b\u308b\u3088\u308a\u3082\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3055\u305b\u3066\u5727\u7e2e\u3055\u305b\u308b\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u63d0\u6848\u3002\u5927\u304d\u306a\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u53ce\u675f\u304c\u65e9\u304f\u3001\u5727\u7e2e\u3057\u3066\u3082\u7cbe\u5ea6\u306e\u843d\u3061\u8fbc\u307f\u304c\u5c11\u306a\u3044\u3068\u306e\u3053\u3068\u3002 h\u2026", "followers": "148", "datetime": "2020-03-26 18:01:01", "author": "@Tata_oac"}, "1235830074051461121": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "186", "datetime": "2020-03-06 07:30:24", "author": "@ShivamKotwalia"}, "1235709267593707523": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "41", "datetime": "2020-03-05 23:30:22", "author": "@9ncM9v2R"}, "1235651877171335168": {"content_summary": "@Yeungy1212", "followers": "61", "datetime": "2020-03-05 19:42:19", "author": "@babbleshack"}, "1236053950127693824": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "145", "datetime": "2020-03-06 22:20:01", "author": "@esvhd"}, "1235861615158804481": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "62", "datetime": "2020-03-06 09:35:44", "author": "@ronan_lehy"}, "1235810262109663234": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "69", "datetime": "2020-03-06 06:11:41", "author": "@trammyyy"}, "1235644475743100929": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "9", "datetime": "2020-03-05 19:12:54", "author": "@izzatszenbubble"}, "1235808102357667840": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "137", "datetime": "2020-03-06 06:03:06", "author": "@TinDan_"}, "1235839162843951105": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "7", "datetime": "2020-03-06 08:06:31", "author": "@B5b3Uk"}, "1235803640855556102": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "248", "datetime": "2020-03-06 05:45:22", "author": "@peterjansen_ai"}, "1235768916875653120": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,654", "datetime": "2020-03-06 03:27:23", "author": "@Joe_Xie"}, "1235873615997857792": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "35", "datetime": "2020-03-06 10:23:26", "author": "@null_force"}, "1236073533593223168": {"content_summary": "RT @dcpage3: Great study of training efficiency at large scale + nice results on compression for inference! https://t.co/v5IvIH5dJq", "followers": "366", "datetime": "2020-03-06 23:37:50", "author": "@iugoaoj"}, "1235870293282840577": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "509", "datetime": "2020-03-06 10:10:13", "author": "@bhagirathl"}, "1235642291403943943": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "319", "datetime": "2020-03-05 19:04:14", "author": "@lucidrains"}, "1235776169594753024": {"content_summary": "Really fun result, glad to see it's out!", "followers": "116", "datetime": "2020-03-06 03:56:13", "author": "@nicholas_bhat"}, "1235617944920567809": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "161", "datetime": "2020-03-05 17:27:29", "author": "@Sid____"}, "1236131284780490752": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "96", "datetime": "2020-03-07 03:27:19", "author": "@Delfox29"}, "1236100229679771649": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1,935", "datetime": "2020-03-07 01:23:55", "author": "@Hector_Pulido_"}, "1236929610031472640": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "91", "datetime": "2020-03-09 08:19:34", "author": "@_nmaguette"}, "1240701832000729089": {"content_summary": "[Follow up with inference using a compressed version of the larger model.] Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers https://t.co/sXeFPKuhkd (submitted by Suhas Pai) https://t.co/5kYnXw79V2", "followers": "907", "datetime": "2020-03-19 18:09:02", "author": "@AISC_TO"}, "1235883592686997504": {"content_summary": "RT @dcpage3: Great study of training efficiency at large scale + nice results on compression for inference! https://t.co/v5IvIH5dJq", "followers": "2,193", "datetime": "2020-03-06 11:03:04", "author": "@calabi_and_yau"}, "1236402580692983808": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "990", "datetime": "2020-03-07 21:25:21", "author": "@Carsonlam"}, "1235924356284309511": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "1", "datetime": "2020-03-06 13:45:03", "author": "@IqbalRoziq"}, "1235638951836176385": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "14", "datetime": "2020-03-05 18:50:57", "author": "@andrei_st_n"}, "1237050642566238209": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "234", "datetime": "2020-03-09 16:20:31", "author": "@rahuljha"}, "1235921122455179265": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "28,418", "datetime": "2020-03-06 13:32:12", "author": "@ogrisel"}, "1235662235235741701": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "367", "datetime": "2020-03-05 20:23:29", "author": "@mdigangiPA"}, "1236362698385952768": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "261", "datetime": "2020-03-07 18:46:52", "author": "@InkWild1"}, "1235674976910532608": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "771", "datetime": "2020-03-05 21:14:06", "author": "@aCraigPfeifer"}, "1236725783680282625": {"content_summary": "RT @Eric_Wallace_: See all of this in: \"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transfor\u2026", "followers": "39", "datetime": "2020-03-08 18:49:38", "author": "@asitkumarmishra"}, "1235738145280241664": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "27", "datetime": "2020-03-06 01:25:07", "author": "@mahesh21aug"}, "1236744335900553217": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "317", "datetime": "2020-03-08 20:03:21", "author": "@Samujjwal_Sam"}, "1236289281871818753": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "164,079", "datetime": "2020-03-07 13:55:08", "author": "@ceobillionaire"}, "1235623196411322368": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "185", "datetime": "2020-03-05 17:48:21", "author": "@CyrusMaher"}, "1236075324229562368": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "455", "datetime": "2020-03-06 23:44:57", "author": "@SingingData"}, "1235826576224182274": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "2,052", "datetime": "2020-03-06 07:16:30", "author": "@eytan"}, "1235915027606990848": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "156", "datetime": "2020-03-06 13:07:59", "author": "@madrid_ankit"}, "1235778907170881537": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "6", "datetime": "2020-03-06 04:07:05", "author": "@yugalJain1999"}, "1236106662160871425": {"content_summary": "RT @KbalajiTweets: More parameters => faster convergence! Another example of deep learning working contrary to accepted wisdom and intution\u2026", "followers": "811", "datetime": "2020-03-07 01:49:28", "author": "@BlkHwk0ps"}, "1236391282211033088": {"content_summary": "RT @Eric_Wallace_: Not everyone can afford to train huge neural models. So, we typically *reduce* model size to train/test faster. However\u2026", "followers": "80", "datetime": "2020-03-07 20:40:27", "author": "@Mr_WhiteHawk"}}}