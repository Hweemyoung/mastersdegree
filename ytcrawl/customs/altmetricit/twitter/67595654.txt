{"citation_id": "67595654", "completed": "1", "queriedAt": "2020-05-14 13:20:36", "tab": "twitter", "twitter": {"1181441023412768768": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "92", "datetime": "2019-10-08 05:27:44", "author": "@andre134679"}, "1234896602772508672": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2020-03-03 17:41:08", "author": "@arxiv_cscl"}, "1179774601678774272": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "615", "datetime": "2019-10-03 15:05:58", "author": "@KouroshMeshgi"}, "1179767658067218436": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "19,042", "datetime": "2019-10-03 14:38:23", "author": "@huggingface"}, "1179766961238138883": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "334", "datetime": "2019-10-03 14:35:37", "author": "@mcgenergy"}, "1179905151437590533": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "179,069", "datetime": "2019-10-03 23:44:44", "author": "@Montreal_AI"}, "1179773771172859905": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "19,019", "datetime": "2019-10-03 15:02:40", "author": "@rctatman"}, "1179842089435222016": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "576", "datetime": "2019-10-03 19:34:09", "author": "@RecitalAI"}, "1237872762376450050": {"content_summary": "\ud83d\udcc7Distillation script: https://t.co/5SMtkBiWOO \ud83d\udca1Paper on distillation: https://t.co/doOMPJZt4u", "followers": "574", "datetime": "2020-03-11 22:47:19", "author": "@_stefan_munich"}, "1179908072606707712": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "12,855", "datetime": "2019-10-03 23:56:20", "author": "@CasualEffects"}, "1180392177509306369": {"content_summary": "[#AI #NLP] DistilBERT : 40% smaller 60% faster than BERT => 97% of the performance on GLUE w. a triple loss signal", "followers": "49,324", "datetime": "2019-10-05 08:00:00", "author": "@ctricot"}, "1234757234183278592": {"content_summary": "https://t.co/bJ2Eqevqv9 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. (arXiv:1910.01108v4 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc", "followers": "4,199", "datetime": "2020-03-03 08:27:19", "author": "@arxiv_cs_cl"}, "1184841870293291009": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2019-10-17 14:41:29", "author": "@arxiv_cscl"}, "1179802052580372481": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "3,683", "datetime": "2019-10-03 16:55:03", "author": "@zimei_no_ri"}, "1256777029736947712": {"content_summary": "Change BERT model to DistilBERT https://t.co/3rAjLuvAm9 I want you to change BERT model with distilBERT : https://t.co/DCC06eSKnB Deliverables: ** New code. ** Detail Hyperparameters changes. ** Things that", "followers": "450", "datetime": "2020-05-03 02:46:08", "author": "@python_import"}, "1185054430619529216": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "39", "datetime": "2019-10-18 04:46:08", "author": "@Ujjwal_1999"}, "1179889938738761728": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "361", "datetime": "2019-10-03 22:44:17", "author": "@sazi27"}, "1204153032725975046": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "99", "datetime": "2019-12-09 21:37:09", "author": "@adssidhu86"}, "1179822256974958593": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "173", "datetime": "2019-10-03 18:15:20", "author": "@matt_corkum"}, "1179787375095926784": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "40", "datetime": "2019-10-03 15:56:44", "author": "@linbo_pythoner"}, "1179767057354760194": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "9,353", "datetime": "2019-10-03 14:36:00", "author": "@julien_c"}, "1183488833083584512": {"content_summary": "\"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\": https://t.co/OXEuz4AfU0 #ml #bert #2019", "followers": "2,132", "datetime": "2019-10-13 21:05:00", "author": "@onepaperperday"}, "1179571930518687746": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2019-10-03 01:40:38", "author": "@arxiv_cscl"}, "1221609015341473793": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2020-01-27 01:41:00", "author": "@arxiv_cscl"}, "1185200024247914496": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "35", "datetime": "2019-10-18 14:24:40", "author": "@boredsrk"}, "1196315067026939904": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "130", "datetime": "2019-11-18 06:31:53", "author": "@soooooo_soooooo"}, "1179888616165666816": {"content_summary": "RT @arxiv_cs_cl: https://t.co/bJ2Eqevqv9 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. (arXiv:1910.01108v1\u2026", "followers": "2,663", "datetime": "2019-10-03 22:39:02", "author": "@sigitpurnomo"}, "1181732634658918401": {"content_summary": "2019/10/02 \u6295\u7a3f 2\u4f4d CL(Computation and Language) DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/TbbNW6H4x4 4 Tweets 88 Retweets 352 Favorites", "followers": "692", "datetime": "2019-10-09 00:46:30", "author": "@arxiv_pop"}, "1179832702066925568": {"content_summary": "Was discussing this with one of our research group members yesterday.", "followers": "4,456", "datetime": "2019-10-03 18:56:51", "author": "@vukosi"}, "1179987959635959808": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "89", "datetime": "2019-10-04 05:13:47", "author": "@SebastianEnger"}, "1179894772560850945": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "342", "datetime": "2019-10-03 23:03:29", "author": "@dariocazzani"}, "1179856209828089866": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "611", "datetime": "2019-10-03 20:30:15", "author": "@smart_grid_fr"}, "1185065289626472448": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "615", "datetime": "2019-10-18 05:29:17", "author": "@KouroshMeshgi"}, "1185095370721972225": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "123", "datetime": "2019-10-18 07:28:49", "author": "@almoslmi"}, "1180200188511690752": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,058", "datetime": "2019-10-04 19:17:06", "author": "@GillesMoyse"}, "1185054593438244864": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "64", "datetime": "2019-10-18 04:46:46", "author": "@SatishJasthiJ"}, "1187162359359389698": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,030", "datetime": "2019-10-24 00:22:17", "author": "@cold_beard"}, "1179775565500485634": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "72", "datetime": "2019-10-03 15:09:48", "author": "@ErdanGenc"}, "1180075026160263170": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "46", "datetime": "2019-10-04 10:59:45", "author": "@pczzy"}, "1179801716499374080": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "164,106", "datetime": "2019-10-03 16:53:43", "author": "@ceobillionaire"}, "1180099887981121537": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,918", "datetime": "2019-10-04 12:38:33", "author": "@DocXavi"}, "1213822302514626560": {"content_summary": "\u300c\u73fe\u72b6\u306e\u3001\u76f4\u611f\u306b\u57fa\u3065\u304f\u30b7\u30b9\u30c6\u30e0\u304c\u3042\u308b\u3002\u3055\u3089\u306b\u719f\u616e\u3059\u308b\u30b7\u30b9\u30c6\u30e0\u304c\u3042\u3063\u3066\u3001\u5f8c\u8005\u306e\u307b\u3046\u304c\u9ad8\u3044\u7cbe\u5ea6\u3092\u51fa\u3059\u3068\u3059\u308b\u3002\u305d\u306e\u5834\u5408\u3001\u719f\u616e\u30b7\u30b9\u30c6\u30e0\u3092\u6559\u5e2b\u3068\u3057\u3066\u76f4\u611f\u30b7\u30b9\u30c6\u30e0\u3092\u8a13\u7df4\u3059\u308c\u3070\u6e08\u3080\u300d\u5927\u898f\u6a21\u76f4\u611f\u30b7\u30b9\u30c6\u30e0\u3092\u4f7f\u3063\u3066\u5c0f\u898f\u6a21\u76f4\u611f\u30b7\u30b9\u30c6\u30e0\u3092\u8a13\u7df4\uff08\u84b8\u7559\uff09\u3059\u308b\u7814\u7a76\uff08https://t.co/7OI476ASIe \u306a\u3069\uff09\u591a\u3044\u3067\u3059\u3057\u306d\u2026", "followers": "789", "datetime": "2020-01-05 13:59:23", "author": "@tsuchm"}, "1180066496699097090": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "844", "datetime": "2019-10-04 10:25:52", "author": "@PiotrCzapla"}, "1185051332282961922": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "199", "datetime": "2019-10-18 04:33:49", "author": "@a_random_obsrvr"}, "1179767867056758784": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "222", "datetime": "2019-10-03 14:39:13", "author": "@deepak_s_rawat"}, "1185134785200566273": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "327", "datetime": "2019-10-18 10:05:26", "author": "@victor_iyi"}, "1180129858543730688": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "651", "datetime": "2019-10-04 14:37:38", "author": "@iamsidd"}, "1179749410655240192": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "0", "datetime": "2019-10-03 13:25:52", "author": "@Sam09lol"}, "1180954629871292416": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "637", "datetime": "2019-10-06 21:14:59", "author": "@shaunak38"}, "1179907273092747264": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "66", "datetime": "2019-10-03 23:53:10", "author": "@battle8500"}, "1179990273729990659": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "19", "datetime": "2019-10-04 05:22:59", "author": "@MassBassLol"}, "1234670088441122820": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2Byj4Q", "followers": "3,491", "datetime": "2020-03-03 02:41:02", "author": "@arxiv_cscl"}, "1186167850790277122": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "62", "datetime": "2019-10-21 06:30:28", "author": "@shivam_skt_7"}, "1179871724306927620": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "972", "datetime": "2019-10-03 21:31:54", "author": "@NotFakeBrendan"}, "1233384340098297856": {"content_summary": "@guillefix original: https://t.co/50uMcUO9No distilbert: https://t.co/xqRiVDHY3c distilgpt2: https://t.co/KXE5bF6jlK", "followers": "269", "datetime": "2020-02-28 13:31:56", "author": "@NaxAlpha"}, "1179987798968823809": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "18", "datetime": "2019-10-04 05:13:09", "author": "@JoelChen95"}, "1185399013769650177": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "135", "datetime": "2019-10-19 03:35:23", "author": "@karanchahal96"}, "1179906856254230528": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "32", "datetime": "2019-10-03 23:51:30", "author": "@Amplituhedr0n"}, "1221805403245248512": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2020-01-27 14:41:22", "author": "@arxiv_cscl"}, "1179877949564231681": {"content_summary": "good job", "followers": "71", "datetime": "2019-10-03 21:56:38", "author": "@v_trokhymenko"}, "1185104538568495104": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "122", "datetime": "2019-10-18 08:05:14", "author": "@hasansaikatt"}, "1185228233316298752": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "60", "datetime": "2019-10-18 16:16:45", "author": "@prabhu_vrinda"}, "1221611123574759424": {"content_summary": "https://t.co/bJ2Eqevqv9 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. (arXiv:1910.01108v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc", "followers": "4,199", "datetime": "2020-01-27 01:49:22", "author": "@arxiv_cs_cl"}, "1185216329978040320": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "1,762", "datetime": "2019-10-18 15:29:27", "author": "@akdm_bot"}, "1180145003756175362": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "43", "datetime": "2019-10-04 15:37:49", "author": "@MishakinSergey"}, "1185260354575585281": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "91", "datetime": "2019-10-18 18:24:24", "author": "@prampey7"}, "1179767891606036480": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "98", "datetime": "2019-10-03 14:39:19", "author": "@justinkhup"}, "1181418605549764608": {"content_summary": "RT @ctricot: [#AI #NLP] DistilBERT : 40% smaller 60% faster than BERT => 97% of the performance on GLUE w. a triple loss signal https://t.c\u2026", "followers": "8,728", "datetime": "2019-10-08 03:58:39", "author": "@XZPwEDVhHH37i3C"}, "1185055376653324288": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "288", "datetime": "2019-10-18 04:49:53", "author": "@menshikh_iv"}, "1179946292169998336": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "5,596", "datetime": "2019-10-04 02:28:13", "author": "@__MLT__"}, "1187018620754202625": {"content_summary": "Language model sizes from distilBERT paper. DistilBERT has even lesser parameters than ELMo! @huggingface Please add ALBERT for comparison and maybe distilALBERT soon :D https://t.co/dyIILAIgCy #bert #distillation #distilbert #nlp #nlproc #transferlea", "followers": "199", "datetime": "2019-10-23 14:51:07", "author": "@a_random_obsrvr"}, "1180049342071218176": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "413", "datetime": "2019-10-04 09:17:42", "author": "@ThomasBoquet"}, "1179810117279211520": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "2,070", "datetime": "2019-10-03 17:27:06", "author": "@EricSchles"}, "1179801644315402240": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "822", "datetime": "2019-10-03 16:53:26", "author": "@deepgradient"}, "1179947031931936770": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "750", "datetime": "2019-10-04 02:31:09", "author": "@data__wizard"}, "1179754826080538624": {"content_summary": "RT @arxiv_cscl: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "1,153", "datetime": "2019-10-03 13:47:23", "author": "@jd_mashiro"}, "1234700341117104129": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2020-03-03 04:41:15", "author": "@arxiv_cscl"}, "1187155514318299138": {"content_summary": "RT @a_random_obsrvr: Language model sizes from distilBERT paper. DistilBERT has even lesser parameters than ELMo! @huggingface Please add\u2026", "followers": "43", "datetime": "2019-10-23 23:55:05", "author": "@MishakinSergey"}, "1179812971465367552": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "54", "datetime": "2019-10-03 17:38:26", "author": "@deligatedgeek"}, "1179909849699631106": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "30", "datetime": "2019-10-04 00:03:24", "author": "@gitlostmurali"}, "1185054475863506944": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "3,167", "datetime": "2019-10-18 04:46:18", "author": "@A_K_Nain"}, "1185075697037000704": {"content_summary": "Surely this is evidence that training dynamics are suboptimal and we should invest more in per-sample learning rates (e.g. better regularisation, adaptive or entropy proportional loss, bayesian methods) rather than training a massive sample inefficient mod", "followers": "604", "datetime": "2019-10-18 06:10:38", "author": "@jusjosgra"}, "1179858403126063107": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "100", "datetime": "2019-10-03 20:38:58", "author": "@KarimiRabeeh"}, "1179767897578717185": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "473", "datetime": "2019-10-03 14:39:20", "author": "@MShahriariNia"}, "1179788510611398656": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "178", "datetime": "2019-10-03 16:01:14", "author": "@WhyEnggWhy"}, "1180118360551493632": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "197", "datetime": "2019-10-04 13:51:57", "author": "@MatthewTeschke"}, "1182343541747355648": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,026", "datetime": "2019-10-10 17:14:01", "author": "@CcPrakay"}, "1185087117044736001": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "8", "datetime": "2019-10-18 06:56:01", "author": "@gkblazys"}, "1179837687374778370": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "64", "datetime": "2019-10-03 19:16:39", "author": "@ONKaraHSTS"}, "1179822899483553797": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "81", "datetime": "2019-10-03 18:17:53", "author": "@giulianobertoti"}, "1180044031285436418": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "528", "datetime": "2019-10-04 08:56:35", "author": "@debora_nozza"}, "1179794322784313344": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "733", "datetime": "2019-10-03 16:24:20", "author": "@chaitjo"}, "1185051640690364416": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "19", "datetime": "2019-10-18 04:35:02", "author": "@MassBassLol"}, "1179820556843020288": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "784", "datetime": "2019-10-03 18:08:35", "author": "@muktabh"}, "1179820038582341632": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,370", "datetime": "2019-10-03 18:06:31", "author": "@Pimp_Fada"}, "1179963301784014848": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "662", "datetime": "2019-10-04 03:35:48", "author": "@agatan_"}, "1179797570551304192": {"content_summary": "RT @arxiv_cscl: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "615", "datetime": "2019-10-03 16:37:15", "author": "@KouroshMeshgi"}, "1185149624832679936": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "1,063", "datetime": "2019-10-18 11:04:24", "author": "@miguelalonsojr"}, "1180749500895416320": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "62", "datetime": "2019-10-06 07:39:52", "author": "@shivam_skt_7"}, "1180157888205066240": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,023", "datetime": "2019-10-04 16:29:01", "author": "@desertnaut"}, "1187019980560392194": {"content_summary": "RT @a_random_obsrvr: Language model sizes from distilBERT paper. DistilBERT has even lesser parameters than ELMo! @huggingface Please add\u2026", "followers": "65", "datetime": "2019-10-23 14:56:31", "author": "@sapUnI_rishana"}, "1179789008378970113": {"content_summary": "@tomanthonyseo you missed an opp here \ud83d\ude0a", "followers": "40,177", "datetime": "2019-10-03 16:03:13", "author": "@iPullRank"}, "1179801503084744705": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "97", "datetime": "2019-10-03 16:52:52", "author": "@vin_mathur"}, "1179937176026435585": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "53", "datetime": "2019-10-04 01:51:59", "author": "@JirenJin"}, "1179787826185084938": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "5,745", "datetime": "2019-10-03 15:58:31", "author": "@hamletbatista"}, "1180113984101314560": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "305", "datetime": "2019-10-04 13:34:33", "author": "@subhobrata1"}, "1179771420483756036": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "66", "datetime": "2019-10-03 14:53:20", "author": "@battle8500"}, "1179825531208060928": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "135", "datetime": "2019-10-03 18:28:21", "author": "@Jabjab05628487"}, "1179684452831842304": {"content_summary": "\"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik , @julien_c and @Thom_Wolf Now on arXiv: https://t.co/doOMPJHScW https://t.co/9fC7ZyKxnb", "followers": "574", "datetime": "2019-10-03 09:07:45", "author": "@_stefan_munich"}, "1179778996881547264": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "271", "datetime": "2019-10-03 15:23:26", "author": "@LukaszJDebowski"}, "1179578554704629761": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf https://t.co/WtbjlFJE8C", "followers": "3,878", "datetime": "2019-10-03 02:06:57", "author": "@BrundageBot"}, "1179559285250019329": {"content_summary": "https://t.co/bJ2Eqevqv9 DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. (arXiv:1910.01108v1 [https://t.co/HW5RVw4UkE]) #NLProc", "followers": "4,199", "datetime": "2019-10-03 00:50:23", "author": "@arxiv_cs_cl"}, "1180059183355289600": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "350", "datetime": "2019-10-04 09:56:48", "author": "@vochicong"}, "1179871394990964737": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "104", "datetime": "2019-10-03 21:30:36", "author": "@pasanOnline"}, "1229945760319336448": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (1) Paper at NeurIPS 2019 (updated) https://t.co/4FlxMQddbr (2) Introducing DistilBERT (blog) https://t.co/b2GkcntRMv #bert #distillation", "followers": "595", "datetime": "2020-02-19 01:48:15", "author": "@pinoystartup"}, "1195573128472268800": {"content_summary": "The future of machine learning is not corpulent models with 100M+ params. Work like knowledge distillation, teacher-student paradigms, transfer learning, and meta-learning will be the next deep-learning revolution. Killer work by an NYC / Paris group! h", "followers": "11", "datetime": "2019-11-16 05:23:41", "author": "@bryan_naidenov"}, "1221640168886689793": {"content_summary": "RT @arxiv_cscl: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "11,920", "datetime": "2020-01-27 03:44:47", "author": "@Rosenchild"}, "1179770222825410562": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "7,905", "datetime": "2019-10-03 14:48:34", "author": "@bhutanisanyam1"}, "1203354464331161600": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "24", "datetime": "2019-12-07 16:43:56", "author": "@ncooper57"}, "1180105737705385984": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "217", "datetime": "2019-10-04 13:01:47", "author": "@AssistedEvolve"}, "1179917406715564032": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/sQdWODiFkA", "followers": "35", "datetime": "2019-10-04 00:33:26", "author": "@tmhk_ab"}, "1179788478462255104": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "301", "datetime": "2019-10-03 16:01:07", "author": "@tobias_sterbak"}, "1179793909670481920": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "36", "datetime": "2019-10-03 16:22:42", "author": "@aurko_roy"}, "1179769483369558016": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "178", "datetime": "2019-10-03 14:45:38", "author": "@WhyEnggWhy"}, "1185050730131902464": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "7,313", "datetime": "2019-10-18 04:31:25", "author": "@ClementDelangue"}, "1179770683443879940": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "588", "datetime": "2019-10-03 14:50:24", "author": "@usmanahmed189"}, "1179938916150976512": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "378", "datetime": "2019-10-04 01:58:54", "author": "@RyanAEMetz"}, "1234980071108005888": {"content_summary": "RT @arxiv_cscl: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2Byj4Q", "followers": "1,153", "datetime": "2020-03-03 23:12:48", "author": "@jd_mashiro"}, "1179768215788019712": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "0", "datetime": "2019-10-03 14:40:36", "author": "@Sam09lol"}, "1179767281603289090": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "2,105", "datetime": "2019-10-03 14:36:53", "author": "@LysandreJik"}, "1184645362939768832": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2019-10-17 01:40:38", "author": "@arxiv_cscl"}, "1187177666245300224": {"content_summary": "RT @a_random_obsrvr: Language model sizes from distilBERT paper. DistilBERT has even lesser parameters than ELMo! @huggingface Please add\u2026", "followers": "47", "datetime": "2019-10-24 01:23:06", "author": "@yuvaforu84"}, "1179766852148445185": {"content_summary": "Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of the performance on GLUE w. a triple loss signal \ud83d\udca5We also distilled GPT2 in an 82M params model \ud83d\udcd6https://t.co/hoIm7RAVAk Code&weig", "followers": "3,977", "datetime": "2019-10-03 14:35:11", "author": "@SanhEstPasMoi"}, "1185051124564258816": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "19,042", "datetime": "2019-10-18 04:32:59", "author": "@huggingface"}, "1179791988733857792": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "208", "datetime": "2019-10-03 16:15:04", "author": "@das_ankit04"}, "1185204413805121538": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "117", "datetime": "2019-10-18 14:42:06", "author": "@MrVik7555"}, "1180148299564642305": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "456", "datetime": "2019-10-04 15:50:55", "author": "@PerthMLGroup"}, "1179968341429014529": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "164", "datetime": "2019-10-04 03:55:50", "author": "@ZachBessinger"}, "1179773658585128960": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "119", "datetime": "2019-10-03 15:02:13", "author": "@ZhiboXiao"}, "1234972192275714048": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2Byj4Q", "followers": "3,491", "datetime": "2020-03-03 22:41:29", "author": "@arxiv_cscl"}, "1179916027099893760": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "3,007", "datetime": "2019-10-04 00:27:57", "author": "@syoyo"}, "1179783404331495425": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "6,390", "datetime": "2019-10-03 15:40:57", "author": "@mtyka"}, "1179839948096077824": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,049", "datetime": "2019-10-03 19:25:38", "author": "@royschwartz02"}, "1179919438428008448": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter - https://t.co/9D1COPFWPY https://t.co/xxXVEJdTVf", "followers": "195", "datetime": "2019-10-04 00:41:30", "author": "@hereticreader"}, "1179905062899814400": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "164", "datetime": "2019-10-03 23:44:23", "author": "@devoidikk"}, "1179784156894486528": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "63", "datetime": "2019-10-03 15:43:56", "author": "@kvikas572"}, "1185139009665654784": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "336", "datetime": "2019-10-18 10:22:13", "author": "@DerekChia"}, "1180102270643527680": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "289", "datetime": "2019-10-04 12:48:01", "author": "@dannyehb"}, "1180037855826984960": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "125", "datetime": "2019-10-04 08:32:03", "author": "@nickdaleburns"}, "1179816398417256454": {"content_summary": "\"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\", Victor Sanh, Lysandre Debut, Julie\u2026 https://t.co/059aqbKYp7", "followers": "770", "datetime": "2019-10-03 17:52:03", "author": "@arxivml"}, "1185099472763797504": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "27", "datetime": "2019-10-18 07:45:07", "author": "@mahesh21aug"}, "1179826250279456768": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,047", "datetime": "2019-10-03 18:31:12", "author": "@loretoparisi"}, "1179948726187630594": {"content_summary": "RT @arxiv_cscl: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "292", "datetime": "2019-10-04 02:37:53", "author": "@Shujian_Liu"}, "1184947060052811777": {"content_summary": "Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their complex, cumbersome counterparts. #NLProc #DeepLearning #DataScience https://t.co/QOcTlqwIqU https://t.co/OEhmRNmD5h @huggingface https:", "followers": "93", "datetime": "2019-10-17 21:39:29", "author": "@dariusz_kajtoch"}, "1179770165451542531": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,287", "datetime": "2019-10-03 14:48:21", "author": "@remilouf"}, "1179877859759816705": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "1,705", "datetime": "2019-10-03 21:56:17", "author": "@Yuki_arase"}, "1179819511547080705": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "18,442", "datetime": "2019-10-03 18:04:26", "author": "@Thom_Wolf"}, "1179767149352620033": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "25,578", "datetime": "2019-10-03 14:36:22", "author": "@Miles_Brundage"}, "1184115186929557504": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "5", "datetime": "2019-10-15 14:33:55", "author": "@Melison48436887"}, "1180346433804865536": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "297", "datetime": "2019-10-05 04:58:14", "author": "@jmcimula"}, "1179753262301532165": {"content_summary": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://t.co/LtLS2BPUtq", "followers": "3,491", "datetime": "2019-10-03 13:41:11", "author": "@arxiv_cscl"}, "1179767213303177216": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "7,313", "datetime": "2019-10-03 14:36:37", "author": "@ClementDelangue"}, "1179798624252780544": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "615", "datetime": "2019-10-03 16:41:26", "author": "@KouroshMeshgi"}, "1180452415134486528": {"content_summary": "RT @ctricot: [#AI #NLP] DistilBERT : 40% smaller 60% faster than BERT => 97% of the performance on GLUE w. a triple loss signal https://t.c\u2026", "followers": "607", "datetime": "2019-10-05 11:59:22", "author": "@ConseilScribe"}, "1179801225287602176": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "822", "datetime": "2019-10-03 16:51:46", "author": "@deepgradient"}, "1180507385439367170": {"content_summary": "RT @ctricot: [#AI #NLP] DistilBERT : 40% smaller 60% faster than BERT => 97% of the performance on GLUE w. a triple loss signal https://t.c\u2026", "followers": "1,173", "datetime": "2019-10-05 15:37:48", "author": "@enrirosasdiaz"}, "1179786973059407873": {"content_summary": "RT @_stefan_munich: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" \ud83e\udd17 from @SanhEstPasMoi , @LysandreJik ,\u2026", "followers": "19,042", "datetime": "2019-10-03 15:55:08", "author": "@huggingface"}, "1185207578881679360": {"content_summary": "RT @dariusz_kajtoch: Distillation can produce smaller, faster and cheaper models that have comparable generalization performance than their\u2026", "followers": "68", "datetime": "2019-10-18 14:54:41", "author": "@diegovogeid"}, "1179937528209477633": {"content_summary": "RT @SanhEstPasMoi: Excited to see our DistilBERT paper accepted at NeurIPS 2019 ECM^2 wkshp! 40% smaller 60% faster than BERT => 97% of th\u2026", "followers": "265", "datetime": "2019-10-04 01:53:23", "author": "@Swetava"}}}