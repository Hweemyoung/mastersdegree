{"citation_id": "22322516", "completed": "1", "queriedAt": "2020-05-14 14:13:06", "tab": "twitter", "twitter": {"890044662811041792": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "244", "datetime": "2017-07-26 03:02:38", "author": "@s_ryosky"}, "954912924853731328": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "1,192", "datetime": "2018-01-21 03:05:57", "author": "@dfabbri"}, "898538463389155328": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "3,104", "datetime": "2017-08-18 13:33:58", "author": "@eturner303"}, "890590758327140352": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "574", "datetime": "2017-07-27 15:12:37", "author": "@51Takahashi"}, "898602127475585024": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,390", "datetime": "2017-08-18 17:46:57", "author": "@GoAbiAryan"}, "955072036409311232": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "145", "datetime": "2018-01-21 13:38:12", "author": "@esvhd"}, "898601258830897152": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "186", "datetime": "2017-08-18 17:43:29", "author": "@vamsi_kurama"}, "898608598325972993": {"content_summary": "En az parametreleri sinir a\u011fleri https://t.co/MnnwSvKOnI", "followers": "200", "datetime": "2017-08-18 18:12:39", "author": "@AHAbdulHafez"}, "898794157094846464": {"content_summary": "Neural network designed neural networks acheiving SOTA results. https://t.co/I33GAkB8Gh", "followers": "159", "datetime": "2017-08-19 06:30:00", "author": "@7VoltCrayon"}, "898898585181462528": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,298", "datetime": "2017-08-19 13:24:58", "author": "@transhumanbob"}, "893740762722185217": {"content_summary": "https://t.co/mHt6sSdxwJ", "followers": "781", "datetime": "2017-08-05 07:49:37", "author": "@TokyoWebmining"}, "1040994853146963968": {"content_summary": "RT @pacocat: \u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F https://t.co/vEvDM\u2026", "followers": "822", "datetime": "2018-09-15 16:04:48", "author": "@morioka"}, "899646735319826434": {"content_summary": "RT @ml_review: Learning Transferable Architectures for Scalable Image Recognition ImageNet SOTA: 0.8% more and 9bln FLOPS less https://t.\u2026", "followers": "1,439", "datetime": "2017-08-21 14:57:50", "author": "@naohisashoji"}, "1026153564123656193": {"content_summary": "\u0415\u0449\u0435 \u0432 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u0433\u043e\u0434\u0443 \u0437\u0430\u043f\u0438\u043b\u0438\u043b\u0438 NAS \u0441\u0435\u0442\u044c https://t.co/kQuFbfSGGu \u0430 \u0442\u0435\u043f\u0435\u0440\u044c \u0435\u0449\u0435 \u0438 \u0432 \u0441\u0435\u0440\u0432\u0438\u0441 \u043e\u0431\u0435\u0440\u043d\u0443\u043b\u0438. \u0422\u0435\u043f\u0435\u0440\u044c \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e \u0442\u044e\u043d\u0438\u0442\u044c \u0441\u0432\u043e\u044e \u0441\u0432\u0435\u0440\u0445\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c(\u0438\u0437-\u0437\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0441\u0435\u0442\u0435\u0439 \u044d\u0442\u043e \u0432\u0441\u0435 \u043e\u0447\u0435\u043d\u044c \u0434\u043e\u043b\u0433\u043e \u0438 \u043d\u0435 \u043e\u0441\u043e\u0431\u043e \u0432\u0435\u0441\u0435\u043b\u043e) \u0430 \u044d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0434\u0435\u0442 \u0441\u0430\u043c\u043e.", "followers": "680", "datetime": "2018-08-05 17:10:49", "author": "@Flcn"}, "937708453891633159": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-12-04 15:41:31", "author": "@arxiv_cscv"}, "890921699629084672": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "433", "datetime": "2017-07-28 13:07:40", "author": "@a_maumau_"}, "984229916106518528": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2018-04-12 00:41:12", "author": "@arxiv_cscv"}, "917068536949858304": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "8", "datetime": "2017-10-08 16:45:52", "author": "@WisdomHistory"}, "1187434170718420992": {"content_summary": "RT @imenurok: ImageNet\u306b\u304a\u3051\u308b\u4e3b\u8981\u306aNetwork\u306eTest Accuracy\u3068parameter\u6570\u306e\u6bd4\u8f03\u3002VGG16(\u53f3\u4e0b)\u306f\u610f\u5916\u3068parameter\u6570\u304c\u591a\u3044\u3002 (\u5143\u753b\u50cf:Learning Transferable Architectures for\u2026", "followers": "1,333", "datetime": "2019-10-24 18:22:22", "author": "@alfredplpl"}, "979844561840173056": {"content_summary": "@Spezzer gave a fantastic presentation on Neural Architecture Search, Auto ML refs: https://t.co/rb96SHFkSL https://t.co/wGnLNbPw7H https://t.co/7xpQ2SpHOK https://t.co/E3Kh6jy0Vm TensorFlow Dev Summit 2018 https://t.co/hceCwrrlOf #TFDevSummit", "followers": "320", "datetime": "2018-03-30 22:15:22", "author": "@microsurgeonbot"}, "899309248454242305": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "656", "datetime": "2017-08-20 16:36:47", "author": "@wjscheirer"}, "931481419292876800": {"content_summary": "@aiskoaskosd How about this one: https://t.co/wU04M2MZJy?", "followers": "53", "datetime": "2017-11-17 11:17:31", "author": "@JirenJin"}, "898562509212311552": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,450", "datetime": "2017-08-18 15:09:31", "author": "@CaimingXiong"}, "957945658547326976": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "2,837", "datetime": "2018-01-29 11:56:57", "author": "@rquintino"}, "984229668533555200": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v4 [https://t.co/blkL6h3gJ1] UPDATED) https://t.co/kNtO7vUA1Z", "followers": "9,685", "datetime": "2018-04-12 00:40:13", "author": "@StatMLPapers"}, "907259434266533888": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "4,054", "datetime": "2017-09-11 15:07:59", "author": "@chezou"}, "898562073801740291": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,145", "datetime": "2017-08-18 15:07:47", "author": "@iskander"}, "898659443579252736": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "", "datetime": "2017-08-18 21:34:42", "author": "@AaaLee"}, "890821194840145920": {"content_summary": "RT @TerryUm_ML: \"Learning Transferable Architectures for Scalable Image Recognition\": reduce the amount of architecture engineering https:/\u2026", "followers": "1,286", "datetime": "2017-07-28 06:28:18", "author": "@heghbalz"}, "907043140535586816": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,724", "datetime": "2017-09-11 00:48:31", "author": "@whisponchan"}, "898720174399864832": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "4,433", "datetime": "2017-08-19 01:36:01", "author": "@bencrox"}, "955161717067034624": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "63", "datetime": "2018-01-21 19:34:34", "author": "@saqibns"}, "898601035458953216": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "266,844", "datetime": "2017-08-18 17:42:36", "author": "@karpathy"}, "899200859053084673": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "317", "datetime": "2017-08-20 09:26:05", "author": "@thk_tw"}, "898747702782984192": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,091", "datetime": "2017-08-19 03:25:24", "author": "@alexis_b_cook"}, "890904992437092353": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,451", "datetime": "2017-07-28 12:01:17", "author": "@mulgray"}, "898520110499069952": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "401", "datetime": "2017-08-18 12:21:02", "author": "@SgtLennon"}, "898522308897198080": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "75", "datetime": "2017-08-18 12:29:46", "author": "@adityachivu"}, "890016465008513024": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "111", "datetime": "2017-07-26 01:10:35", "author": "@priy2201"}, "927334767124561920": {"content_summary": "\u3067\u3001\u3053\u306eNAS\u304c\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u9045\u304b\u3063\u305f\u306e\u3067\u3001search space\u3092\u9069\u5207\u306b\u751f\u6210\u3059\u308b\u3053\u3068\u3067\u5b66\u7fd2\u304c\u901f\u304f\u306a\u308b\u3088\u3068\u3044\u3046\u8ad6\u6587\u3002 https://t.co/XXODucSdf5", "followers": "1,052", "datetime": "2017-11-06 00:40:12", "author": "@yukinagae"}, "898550378152833027": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "276", "datetime": "2017-08-18 14:21:19", "author": "@kadarakos"}, "955107275617714176": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "1,193", "datetime": "2018-01-21 15:58:14", "author": "@findingjimoh"}, "898693378430214144": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "406", "datetime": "2017-08-18 23:49:32", "author": "@mattpetersen_ai"}, "899313931512160256": {"content_summary": "@karpathy A guess would be that he's referring to https://t.co/6TW5xgeHu5", "followers": "33", "datetime": "2017-08-20 16:55:24", "author": "@silasmorkgard"}, "1040969165924839424": {"content_summary": "RT @pacocat: \u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F https://t.co/vEvDM\u2026", "followers": "499", "datetime": "2018-09-15 14:22:44", "author": "@nagakagachi"}, "899209693805125632": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWXMda #Machinelearning #machinevision", "followers": "153", "datetime": "2017-08-20 10:01:12", "author": "@Caitlynmy39"}, "935732285776818176": {"content_summary": "RT @s_0samu: \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3082\u5b66\u7fd2\u3055\u305b\u308b\u8a66\u307f\u3002\u8208\u5473\u6df1\u3044\u3002 Learning Transferable Architectures for Scalable Image Recognition https://t.co/c2c59gev36 #tfug", "followers": "1,041", "datetime": "2017-11-29 04:48:56", "author": "@ysaito8015"}, "898537190602543104": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "54", "datetime": "2017-08-18 13:28:54", "author": "@Jo14Robert"}, "907052331878932480": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "63", "datetime": "2017-09-11 01:25:02", "author": "@fjnyan__"}, "938167149474013186": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "2,869", "datetime": "2017-12-05 22:04:13", "author": "@Tarpon_red2"}, "898446369605664769": {"content_summary": "Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t.co/6a0GqfcpCY https://t.co/R8M51neFW0", "followers": "81,357", "datetime": "2017-08-18 07:28:01", "author": "@hardmaru"}, "898704270731563008": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "444", "datetime": "2017-08-19 00:32:49", "author": "@outrepool"}, "898501196218081280": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "586", "datetime": "2017-08-18 11:05:53", "author": "@chaoticneural"}, "898634252874428416": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "824", "datetime": "2017-08-18 19:54:36", "author": "@_Simon_K"}, "900959890524454912": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "1,418", "datetime": "2017-08-25 05:55:51", "author": "@sguada"}, "898576306085838848": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,927", "datetime": "2017-08-18 16:04:20", "author": "@evolvingstuff"}, "938171049484500992": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "157", "datetime": "2017-12-05 22:19:43", "author": "@NCC1701R"}, "890845729622638592": {"content_summary": "RT @TerryUm_ML: \"Learning Transferable Architectures for Scalable Image Recognition\": reduce the amount of architecture engineering https:/\u2026", "followers": "62", "datetime": "2017-07-28 08:05:47", "author": "@mrchypark_"}, "955261013409984512": {"content_summary": "RT @starimpact1983: the result is very exciting! https://t.co/RTdIPfwt0T", "followers": "26", "datetime": "2018-01-22 02:09:08", "author": "@wangzhangup"}, "898602758265229313": {"content_summary": "There seems to be a sort of \u201coccam\u2019s razor\u201d-alike in machine learning: systems with the fewest human-coded assumptions are winning. https://t.co/sO8HqXgsS2", "followers": "27,803", "datetime": "2017-08-18 17:49:27", "author": "@andy_matuschak"}, "899003766346833921": {"content_summary": "RT @alanmnichol: NOW are we finally not feature engineering ? Or is there more finicky meta-hyperparameter tweaking going on https://t.co/8\u2026", "followers": "475", "datetime": "2017-08-19 20:22:55", "author": "@jmsl"}, "890060192003522560": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "89", "datetime": "2017-07-26 04:04:20", "author": "@cole_gulino"}, "907305386041942016": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "6", "datetime": "2017-09-11 18:10:35", "author": "@scott_fanatic"}, "898453467739471873": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "16,489", "datetime": "2017-08-18 07:56:13", "author": "@tkasasagi"}, "932645518865440768": {"content_summary": "RT @KalanityL: Major improvement for image #classification (and #detection as well) - Oct 2017 - by @GoogleBrain #DeepLearning #NeuralNetwo\u2026", "followers": "577", "datetime": "2017-11-20 16:23:14", "author": "@BFSbY"}, "907262890087931904": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "353", "datetime": "2017-09-11 15:21:43", "author": "@K3nt0W"}, "899307085191663616": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "28", "datetime": "2017-08-20 16:28:12", "author": "@darthvogler"}, "889999379582435328": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition. https://t.co/yGYLv4kx8l https://t.co/RBwUtOKl6d", "followers": "12,719", "datetime": "2017-07-26 00:02:42", "author": "@arxiv_org"}, "895149744921731073": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/10Btr7R9Yr", "followers": "2,895", "datetime": "2017-08-09 05:08:24", "author": "@doppenhe"}, "937496967063760896": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-12-04 01:41:09", "author": "@arxiv_cscv"}, "898628763159744512": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "95", "datetime": "2017-08-18 19:32:47", "author": "@bealish_lecun"}, "898636688045604867": {"content_summary": "Amazing...#AutoML of CNN architecture but not at the scale of problems usually tackled with scikit_learn. This is ImageNet scale! https://t.co/z3sYW6H0BS", "followers": "735", "datetime": "2017-08-18 20:04:16", "author": "@unsorsodicorda"}, "898631956123877376": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "208", "datetime": "2017-08-18 19:45:28", "author": "@sb2nov"}, "899420177820827648": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "456", "datetime": "2017-08-20 23:57:35", "author": "@PerthMLGroup"}, "900947432774672384": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "4,531", "datetime": "2017-08-25 05:06:21", "author": "@ChrSzegedy"}, "954872684437008384": {"content_summary": "state-of-the-art https://t.co/EnSvgdIIln", "followers": "192", "datetime": "2018-01-21 00:26:03", "author": "@ajyphr"}, "955113110314061824": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "604", "datetime": "2018-01-21 16:21:25", "author": "@rharang"}, "898645275132084224": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "46", "datetime": "2017-08-18 20:38:24", "author": "@vishnups91"}, "898792788518555649": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "72", "datetime": "2017-08-19 06:24:34", "author": "@the_fake_sk"}, "998874664096055296": {"content_summary": "\u5358\u7d14\u306a\u6570\u5024\u5217\u6319\u6bd4\u8f03\u306a\u306e\u3067\u3001\u3042\u307e\u308a\u6b63\u5f53\u6027\u304c\u306a\u3044\u3002\u306a\u304a\u3001AmoebaNet\u3068AlphaX\u306e\u4e0b2\u3064\u306f\u4ed6\u3068\u65b9\u5f0f\u304c\u3061\u3087\u3063\u3068\u9055\u3046(\u306f\u305a)\u3002 NASNet https://t.co/uvBpzrEGc5 PNASNet https://t.co/NxjGm8U49g ENASNet https://t.co/9Yp9sol4bR AmoebaNet https://t.co/nXQacnQbso AlphaX https://t.co/Q1SJJFCADj", "followers": "2,041", "datetime": "2018-05-22 10:34:12", "author": "@dosei_sanga"}, "899610502074175488": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "68", "datetime": "2017-08-21 12:33:52", "author": "@hussam_ashab"}, "954860725738135553": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "2,160", "datetime": "2018-01-20 23:38:32", "author": "@alanyttian"}, "907090540197376000": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "2,869", "datetime": "2017-09-11 03:56:52", "author": "@Tarpon_red2"}, "938177567433007104": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "268", "datetime": "2017-12-05 22:45:37", "author": "@KoutaR129"}, "899369311956942848": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "21", "datetime": "2017-08-20 20:35:28", "author": "@Skonumuri"}, "899604370215403521": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,048", "datetime": "2017-08-21 12:09:30", "author": "@swirlOsquirrel"}, "898607821587177473": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "756", "datetime": "2017-08-18 18:09:34", "author": "@lu_a_jalla"}, "893740151876198401": {"content_summary": "\u5148\u307b\u3069\u7d39\u4ecb\u3057\u3066\u3044\u305f\u3082\u306e\u3053\u3061\u3089\u3067\u3059\u3002 \"Learning Transferable Architectures for Scalable Image Recognition\" https://t.co/ZP1QdVLK08 #TokyoWebmining", "followers": "5,851", "datetime": "2017-08-05 07:47:11", "author": "@hamadakoichi"}, "898712354321448960": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWGaOA #Machinelearning #machinevision", "followers": "88", "datetime": "2017-08-19 01:04:57", "author": "@RebeccaGreen54"}, "955337778312699904": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "46", "datetime": "2018-01-22 07:14:10", "author": "@fcbstef"}, "898634684401307648": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "75", "datetime": "2017-08-18 19:56:19", "author": "@JyothirAditya5"}, "890982354541395968": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "89", "datetime": "2017-07-28 17:08:41", "author": "@dsanno"}, "984229277951496194": {"content_summary": "\"Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v4 [https://t.co/CjfMZ25B7Z] UPDATED)\" #arXiv https://t.co/E4Lfmuukyj", "followers": "626", "datetime": "2018-04-12 00:38:40", "author": "@helioRocha_"}, "907193093597704197": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "244", "datetime": "2017-09-11 10:44:23", "author": "@s_ryosky"}, "952272810973515776": {"content_summary": "State-of-the-art results for image #classification and #detection - Dec 2017 - by @GoogleBrain #DeepLearning #NeuralNetwork #AutoML https://t.co/2w3x196r1M Learning Transferable Architectures for Scalable Image Recognition", "followers": "202", "datetime": "2018-01-13 20:15:05", "author": "@rohanbafna"}, "898508944611717126": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,055", "datetime": "2017-08-18 11:36:40", "author": "@blattnerma"}, "898656167848628224": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "701", "datetime": "2017-08-18 21:21:41", "author": "@Omarito2412"}, "907267066192908289": {"content_summary": "Machine learning optimization of machine learning architecture https://t.co/MCeY9qgYm2", "followers": "492", "datetime": "2017-09-11 15:38:19", "author": "@PythonLoop"}, "907603606584541186": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "157", "datetime": "2017-09-12 13:55:36", "author": "@NCC1701R"}, "1122793780019630080": {"content_summary": "Developing neural network classification models often requires significant architecture engineering. In this paper, the authors present a method to learn the model architectures directly on the dataset of interest. https://t.co/pmssacM9J5 #deeplearning #", "followers": "6", "datetime": "2019-04-29 09:24:32", "author": "@neuralmachines"}, "955167012426211328": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "49", "datetime": "2018-01-21 19:55:36", "author": "@RenaudBougues"}, "899416327307149312": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "485", "datetime": "2017-08-20 23:42:17", "author": "@eve_yk"}, "910430550116069382": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "367", "datetime": "2017-09-20 09:08:52", "author": "@acgt01"}, "1187590363084550144": {"content_summary": "RT @imenurok: ImageNet\u306b\u304a\u3051\u308b\u4e3b\u8981\u306aNetwork\u306eTest Accuracy\u3068parameter\u6570\u306e\u6bd4\u8f03\u3002VGG16(\u53f3\u4e0b)\u306f\u610f\u5916\u3068parameter\u6570\u304c\u591a\u3044\u3002 (\u5143\u753b\u50cf:Learning Transferable Architectures for\u2026", "followers": "150", "datetime": "2019-10-25 04:43:01", "author": "@hitsgub"}, "926727360556273665": {"content_summary": "RT @rgem52: NASNet AutoML for ImageNet and COCO https://t.co/lrzFdJfNHD https://t.co/IPGJzOgf88 https://t.co/pyL7ns3tKA https://t.co/UAs\u2026", "followers": "2,070", "datetime": "2017-11-04 08:26:35", "author": "@Pvalsfr"}, "898893820271534082": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "97", "datetime": "2017-08-19 13:06:02", "author": "@Pomecloud"}, "898545140792537089": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "491", "datetime": "2017-08-18 14:00:30", "author": "@jelmerwolterink"}, "954877272246272000": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "217", "datetime": "2018-01-21 00:44:17", "author": "@AssistedEvolve"}, "898592366314930176": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "225", "datetime": "2017-08-18 17:08:09", "author": "@sagarpath"}, "1187521586674298880": {"content_summary": "RT @imenurok: ImageNet\u306b\u304a\u3051\u308b\u4e3b\u8981\u306aNetwork\u306eTest Accuracy\u3068parameter\u6570\u306e\u6bd4\u8f03\u3002VGG16(\u53f3\u4e0b)\u306f\u610f\u5916\u3068parameter\u6570\u304c\u591a\u3044\u3002 (\u5143\u753b\u50cf:Learning Transferable Architectures for\u2026", "followers": "461", "datetime": "2019-10-25 00:09:44", "author": "@motokimura1"}, "955390318391767042": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "133", "datetime": "2018-01-22 10:42:56", "author": "@MirkoTorrisi10"}, "898779844267229184": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "219", "datetime": "2017-08-19 05:33:08", "author": "@DamonCrockett"}, "899157436610748416": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "29", "datetime": "2017-08-20 06:33:33", "author": "@chenlailin"}, "898773134421049345": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "107", "datetime": "2017-08-19 05:06:28", "author": "@sdemyanov"}, "889986807781314560": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/4fZ7C58mq7", "followers": "11,717", "datetime": "2017-07-25 23:12:44", "author": "@yutakashino"}, "889827848005062662": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-07-25 12:41:05", "author": "@arxiv_cscv"}, "898713165772447745": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWGaOA #Machinelearning #machinevision", "followers": "7,977", "datetime": "2017-08-19 01:08:10", "author": "@calcaware"}, "898655625630146561": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "283", "datetime": "2017-08-18 21:19:32", "author": "@TrulyUli"}, "898777323859726337": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/FLUgLsDMs6 #Machinelearning #machinevision MT @v_vashishta", "followers": "2,542", "datetime": "2017-08-19 05:23:07", "author": "@amitabhrjha"}, "899003517771415552": {"content_summary": "RT @alanmnichol: NOW are we finally not feature engineering ? Or is there more finicky meta-hyperparameter tweaking going on https://t.co/8\u2026", "followers": "4,894", "datetime": "2017-08-19 20:21:55", "author": "@IgorCarron"}, "890363658844479489": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/0RqkjRNUdW", "followers": "861", "datetime": "2017-07-27 00:10:13", "author": "@ryf_feed"}, "955133262984335361": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "52", "datetime": "2018-01-21 17:41:30", "author": "@tahirwaseer"}, "905005623380725760": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,751", "datetime": "2017-09-05 09:52:09", "author": "@Slychief"}, "898629225204207616": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "303", "datetime": "2017-08-18 19:34:37", "author": "@Qihong_Lu"}, "898592836802576384": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "52", "datetime": "2017-08-18 17:10:01", "author": "@PredictivaA"}, "1004559545841020933": {"content_summary": "@nikete then I did two harder tests: NASNet and NASNet-Mobile. these two architectures were not designed by humans like the others and so are very hard generalization targets. The mobile version in particular is finicky and has a lower 74% top-1 accuracy.", "followers": "7,550", "datetime": "2018-06-07 03:03:54", "author": "@dribnet"}, "898607086472318976": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "14,647", "datetime": "2017-08-18 18:06:39", "author": "@iScienceLuvr"}, "899320860846288898": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "116", "datetime": "2017-08-20 17:22:56", "author": "@m1r4ge_ror"}, "898723249114173442": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "7,074", "datetime": "2017-08-19 01:48:14", "author": "@nsthorat"}, "898605686510190592": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "113", "datetime": "2017-08-18 18:01:05", "author": "@Miraut"}, "898730951433306112": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "322", "datetime": "2017-08-19 02:18:51", "author": "@indrango"}, "951463338990448643": {"content_summary": "@Gharr_home This kind of articles are not reliable. I want to see the original paper like this https://t.co/fW7jpwLX13", "followers": "3,280", "datetime": "2018-01-11 14:38:32", "author": "@sonicair"}, "898972982688620544": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "98", "datetime": "2017-08-19 18:20:35", "author": "@agrawalamey12"}, "899663471544942592": {"content_summary": "RT @deaneckles: Robots will even put the machine learning researchers out of jobs. ht @alex_peys https://t.co/lwKlLZNqd5", "followers": "416", "datetime": "2017-08-21 16:04:21", "author": "@jxnlco"}, "898604535735779328": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "33", "datetime": "2017-08-18 17:56:31", "author": "@rahnjonathan"}, "926723374868582400": {"content_summary": "RT @rgem52: NASNet AutoML for ImageNet and COCO https://t.co/lrzFdJfNHD https://t.co/IPGJzOgf88 https://t.co/pyL7ns3tKA https://t.co/UAs\u2026", "followers": "10,129", "datetime": "2017-11-04 08:10:44", "author": "@zippylab"}, "955215424832012289": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "24", "datetime": "2018-01-21 23:07:58", "author": "@deep_retina"}, "890730836223344640": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "124", "datetime": "2017-07-28 00:29:14", "author": "@ragi256"}, "898613091516129280": {"content_summary": "Reduce the amount of architecture engineering by using Neural Architecture Search to learn an architectural building\u2026https://t.co/YfuoecU6i2", "followers": "1,030", "datetime": "2017-08-18 18:30:31", "author": "@mahesh_narayan"}, "898611079411978240": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "113", "datetime": "2017-08-18 18:22:31", "author": "@fsign"}, "932645519037304832": {"content_summary": "RT @KalanityL: Major improvement for image #classification (and #detection as well) - Oct 2017 - by @GoogleBrain #DeepLearning #NeuralNetwo\u2026", "followers": "762", "datetime": "2017-11-20 16:23:14", "author": "@dspshin"}, "898602199902691329": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "508", "datetime": "2017-08-18 17:47:14", "author": "@sksq96"}, "905669835945906176": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "15,794", "datetime": "2017-09-07 05:51:30", "author": "@nordicinst"}, "1098301438869102594": {"content_summary": "@StuartReid1929 @RichmanRonald @Aerobotics_SA Interesting. I am looking at Reinforcement Approaches to AutoML. You might like https://t.co/EIJijDDm1i, they learned a Neural Network Architecture on Cifar10 and applied it to ImageNet with great results. :)", "followers": "442", "datetime": "2019-02-20 19:20:43", "author": "@KaliTessera"}, "898683401384771584": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "825", "datetime": "2017-08-18 23:09:54", "author": "@fly51fly"}, "998875882751062016": {"content_summary": "RT @dosei_sanga: \u5358\u7d14\u306a\u6570\u5024\u5217\u6319\u6bd4\u8f03\u306a\u306e\u3067\u3001\u3042\u307e\u308a\u6b63\u5f53\u6027\u304c\u306a\u3044\u3002\u306a\u304a\u3001AmoebaNet\u3068AlphaX\u306e\u4e0b2\u3064\u306f\u4ed6\u3068\u65b9\u5f0f\u304c\u3061\u3087\u3063\u3068\u9055\u3046(\u306f\u305a)\u3002 NASNet https://t.co/uvBpzrEGc5 PNASNet https://t.co/NxjG\u2026", "followers": "602", "datetime": "2018-05-22 10:39:03", "author": "@Swall0wTech"}, "954923196066574337": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "166", "datetime": "2018-01-21 03:46:46", "author": "@niketanpansare"}, "898603166094172160": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "207", "datetime": "2017-08-18 17:51:04", "author": "@sathya04"}, "955001727534665728": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "139", "datetime": "2018-01-21 08:58:49", "author": "@mmeierer"}, "891111742285127681": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "633", "datetime": "2017-07-29 01:42:50", "author": "@rkakamilan"}, "926697314340786176": {"content_summary": "NASNet AutoML for ImageNet and COCO https://t.co/lrzFdJfNHD https://t.co/IPGJzOgf88 https://t.co/pyL7ns3tKA https://t.co/UAsqueR4JM", "followers": "159", "datetime": "2017-11-04 06:27:11", "author": "@rgem52"}, "890562846341316609": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,308", "datetime": "2017-07-27 13:21:43", "author": "@crcrpar"}, "898712816227500034": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "317", "datetime": "2017-08-19 01:06:47", "author": "@MathWei"}, "890564369322397696": {"content_summary": "\u300c\u4eba\u5de5\u77e5\u80fd\u304c\u4eba\u5de5\u77e5\u80fd\u3092\u8a2d\u8a08\u3057\u305f\u3068\u3053\u308d\uff0c\u4eba\u9593\u304c\u8a2d\u8a08\u3059\u308b\u3088\u308a\u3059\u3050\u308c\u305f\u3082\u306e\u304c\u51fa\u6765\u305f\u300d\uff08\u3068\u304b\u8a00\u3046\u3068\u3046\u3051\u305d\u3046\uff09/450GPU\u3067Neural Architecture Search\u3092\u3057\u3066ImageNet\u3067SOTA\uff0e\u5c0f\u3055\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3082\uff0e https://t.co/2Igvi0D3E0 https://t.co/ASbqyAzazS", "followers": "2,653", "datetime": "2017-07-27 13:27:46", "author": "@mosko_mule"}, "898628478995529728": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "182", "datetime": "2017-08-18 19:31:39", "author": "@DrYogiLBear"}, "889931825245814784": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "1,286", "datetime": "2017-07-25 19:34:15", "author": "@heghbalz"}, "898622952995827712": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,619", "datetime": "2017-08-18 19:09:42", "author": "@MCLeopard"}, "1084402836304089088": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/8xNTGWZb55", "followers": "1,441", "datetime": "2019-01-13 10:52:38", "author": "@panggi"}, "907070762091081728": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "12,763", "datetime": "2017-09-11 02:38:16", "author": "@jaguring1"}, "950957590141284352": {"content_summary": "\u3084\u3063\u3068\u6642\u9593\u304c\u3067\u304d\u59cb\u3081\u305f\u3002\u4eca\u65e5\u304b\u3089\u672c\u8170\u5165\u308c\u3066automl\u3092\u8abf\u67fb\u3057\u59cb\u3081\u307e\u3075\u3002\u307e\u305a\u306f\u9045\u308c\u3070\u305b\u306a\u304c\u3089\u3053\u308c\u8aad\u3093\u3069\u304b\u306a\u306a\u3002 Learning Transferable Architectures for Scalable Image Recognition https://t.co/8UwDb5xo52", "followers": "160", "datetime": "2018-01-10 05:08:52", "author": "@few_friends"}, "954906352496726016": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "424", "datetime": "2018-01-21 02:39:50", "author": "@niloygupta"}, "898858007915712512": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWGaOA #Machinelearning #machinevision", "followers": "130", "datetime": "2017-08-19 10:43:43", "author": "@satyamere"}, "899305355171086336": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,709", "datetime": "2017-08-20 16:21:19", "author": "@adnothing"}, "955014870424064000": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "212", "datetime": "2018-01-21 09:51:03", "author": "@awaldock"}, "898804352659226624": {"content_summary": "Auto generated CNN architecture outperforms STOA https://t.co/VaL9IppYUY", "followers": "120", "datetime": "2017-08-19 07:10:31", "author": "@smelaatifi"}, "938457620213506048": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,333", "datetime": "2017-12-06 17:18:27", "author": "@alfredplpl"}, "932544065232498689": {"content_summary": "RT @KalanityL: Major improvement for image #classification (and #detection as well) - Oct 2017 - by @GoogleBrain #DeepLearning #NeuralNetwo\u2026", "followers": "577", "datetime": "2017-11-20 09:40:05", "author": "@BFSbY"}, "955212820295159808": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "24", "datetime": "2018-01-21 22:57:38", "author": "@deep_retina"}, "890039963756011520": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "98", "datetime": "2017-07-26 02:43:58", "author": "@treasured_write"}, "923348714084818945": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-10-26 00:41:03", "author": "@arxiv_cscv"}, "899593051936628737": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "179,069", "datetime": "2017-08-21 11:24:31", "author": "@Montreal_AI"}, "938249304841400320": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "380", "datetime": "2017-12-06 03:30:40", "author": "@harujoh"}, "890556713291489280": {"content_summary": "450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3vb9H", "followers": "1,673", "datetime": "2017-07-27 12:57:20", "author": "@shinya7y"}, "899939061007355904": {"content_summary": "RT @ml_review: Learning Transferable Architectures for Scalable Image Recognition ImageNet SOTA: 0.8% more and 9bln FLOPS less https://t.\u2026", "followers": "197", "datetime": "2017-08-22 10:19:26", "author": "@edubergeek"}, "901059686480400384": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "88", "datetime": "2017-08-25 12:32:24", "author": "@hugeiezzy"}, "898603983119372288": {"content_summary": "RT @alanmnichol: NOW are we finally not feature engineering ? Or is there more finicky meta-hyperparameter tweaking going on https://t.co/8\u2026", "followers": "9,353", "datetime": "2017-08-18 17:54:19", "author": "@julien_c"}, "898606707030360064": {"content_summary": "RT @andy_matuschak: There seems to be a sort of \u201coccam\u2019s razor\u201d-alike in machine learning: systems with the fewest human-coded assumptions\u2026", "followers": "907", "datetime": "2017-08-18 18:05:08", "author": "@graycrawford"}, "898720111883767808": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,898", "datetime": "2017-08-19 01:35:46", "author": "@Kimhaksoo228"}, "898826381768892416": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "193", "datetime": "2017-08-19 08:38:03", "author": "@alexhock"}, "938726455315382273": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "843", "datetime": "2017-12-07 11:06:42", "author": "@kazanagisora"}, "898594277994807296": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,600", "datetime": "2017-08-18 17:15:45", "author": "@AdamMarblestone"}, "1016036248178262021": {"content_summary": "I've read the article about transferable architectures for salable image recognition. Was very inspired until find out the performance on CIFAR-10. Their approach took 28 days with 500 GPU \ud83d\ude31. But they reach state-of-the-art accuracy! Link: https://t.co/Ysa", "followers": "63", "datetime": "2018-07-08 19:08:13", "author": "@DenisZuenko"}, "898711114019610625": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWGaOA #Machinelearning #machinevision", "followers": "30,496", "datetime": "2017-08-19 01:00:01", "author": "@v_vashishta"}, "899594049983832064": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "164,106", "datetime": "2017-08-21 11:28:29", "author": "@ceobillionaire"}, "935732181334417409": {"content_summary": "RT @m_hayashi: #tfug \u3053\u306e\u3042\u305f\u308a https://t.co/CCX10HfbPz https://t.co/XkUbESTGw1 https://t.co/ev4RSVQBZh", "followers": "1,041", "datetime": "2017-11-29 04:48:31", "author": "@ysaito8015"}, "898865028144746496": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "764", "datetime": "2017-08-19 11:11:37", "author": "@RandallJEllis"}, "1040969606205063169": {"content_summary": "RT @pacocat: \u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F https://t.co/vEvDM\u2026", "followers": "715", "datetime": "2018-09-15 14:24:29", "author": "@kazumamori_py"}, "898649888413569024": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition - https://t.co/9D1COPFWPY https://t.co/DHPPvclSYL", "followers": "195", "datetime": "2017-08-18 20:56:44", "author": "@hereticreader"}, "898511643973484544": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "731", "datetime": "2017-08-18 11:47:24", "author": "@dgueraco"}, "926310847558889472": {"content_summary": "@jeremyphoward this paper ? https://t.co/MlsjBijOP3", "followers": "287", "datetime": "2017-11-03 04:51:30", "author": "@nagaraj_arvind"}, "1040967154563112960": {"content_summary": "AutoML\u306e\u6c17\u6301\u3061\u3092\u77e5\u308a\u305f\u304f\u3066NAS / NASNet / ENAS\u3092\u8aad\u3093\u3060\u3051\u3069\u3053\u306e\u9818\u57df\u8208\u5473\u6df1\u3044\u3002CIFAR\u3084ImageNet\u4ee5\u5916\u306e\u500b\u5225\u30bf\u30b9\u30af\u306b\u3069\u308c\u3060\u3051\u8ee2\u7528\u3067\u304d\u308b\u306e\u304b\u6c17\u306b\u306a\u3063\u305f\u306e\u3068\u3001RL\u90e8\u5206\u3067\u63a2\u7d22\u65b9\u6cd5\u3084\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6539\u826f\u4f59\u5730\u304c\u307e\u3060\u3042\u308b\u306e\u304b\u3082\u3002 https://t.co/aCk4RUoV3M https://t.co/ifaOI6eUyg https://t.co/QSLOjTd8hY", "followers": "2,395", "datetime": "2018-09-15 14:14:44", "author": "@pacocat"}, "898804332216295426": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "11", "datetime": "2017-08-19 07:10:26", "author": "@DivyaChakshoo"}, "898712915737432064": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWGaOA #Machinelearning #machinevision", "followers": "4,944", "datetime": "2017-08-19 01:07:11", "author": "@shakamunyi"}, "907111093725683717": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,171", "datetime": "2017-09-11 05:18:32", "author": "@kibo35"}, "955163860129886208": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "1,216", "datetime": "2018-01-21 19:43:04", "author": "@devnag"}, "955342748420210688": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "4,390", "datetime": "2018-01-22 07:33:55", "author": "@GoAbiAryan"}, "899444387364470784": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "195", "datetime": "2017-08-21 01:33:47", "author": "@moodymwlee"}, "907254465232723968": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "2,060", "datetime": "2017-09-11 14:48:15", "author": "@yo_ehara"}, "964134324915249152": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "51", "datetime": "2018-02-15 13:48:30", "author": "@mljack0000"}, "900955499159445504": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "1,319", "datetime": "2017-08-25 05:38:24", "author": "@KrAbhinavGupta"}, "905004201100574720": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "7,590", "datetime": "2017-09-05 09:46:30", "author": "@graphific"}, "984234794069385217": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v4 [https://t.co/2Nuv3dYG1c] UPDATED) https://t.co/HUplYsqUXe Developing", "followers": "759", "datetime": "2018-04-12 01:00:35", "author": "@M157q_News_RSS"}, "898656885146095616": {"content_summary": "RT @deaneckles: Robots will even put the machine learning researchers out of jobs. ht @alex_peys https://t.co/lwKlLZNqd5", "followers": "217", "datetime": "2017-08-18 21:24:32", "author": "@robegs"}, "898453294217023488": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "3,515", "datetime": "2017-08-18 07:55:32", "author": "@agibsonccc"}, "898662146971639808": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "3,075", "datetime": "2017-08-18 21:45:26", "author": "@ivan_bezdomny"}, "954892478527819776": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "243", "datetime": "2018-01-21 01:44:42", "author": "@frett27"}, "1040968160961212417": {"content_summary": "\u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F", "followers": "2,395", "datetime": "2018-09-15 14:18:44", "author": "@pacocat"}, "898476467318927361": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "48", "datetime": "2017-08-18 09:27:37", "author": "@unknowlake"}, "938114048230547456": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "903", "datetime": "2017-12-05 18:33:13", "author": "@wk77"}, "899411215956320256": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "396", "datetime": "2017-08-20 23:21:58", "author": "@zuxfoucault"}, "889892411312267265": {"content_summary": "Starting to make neural architecture search useful in practice. Very cool. https://t.co/AENh8sikJi", "followers": "44", "datetime": "2017-07-25 16:57:38", "author": "@lars_hertel"}, "899016291016507393": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "83", "datetime": "2017-08-19 21:12:41", "author": "@sasadep"}, "898463241176727552": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "206", "datetime": "2017-08-18 08:35:03", "author": "@TigerScorpior"}, "954871044036182016": {"content_summary": "A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-approaches. https://t.co/bghTUW7gyu", "followers": "151,453", "datetime": "2018-01-21 00:19:32", "author": "@JeffDean"}, "938113524420640768": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "903", "datetime": "2017-12-05 18:31:08", "author": "@wk77"}, "890720165712744449": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "44", "datetime": "2017-07-27 23:46:50", "author": "@CharaBentoPapa"}, "954870344870871040": {"content_summary": "@gwern @kastnerkyle @KloudStrife VGG is also an anomaly if we look at this chart of classification performance vs compute (similar graph vs model size). Perhaps one can attempt to estimate an objective function for \"Neural Architecture Search\" to find the", "followers": "81,357", "datetime": "2018-01-21 00:16:45", "author": "@hardmaru"}, "1090263587925286918": {"content_summary": "RT @asas_mimi: \u3053\u306e\u8ad6\u6587\u9762\u767d\u3044\u3000DNN\u306e\u5171\u98df\u3044\u307f\u305f\u3044\uff57 Learning Transferable Architectures for Scalable Image Recognition https://t.co/EROIKAuHye", "followers": "1,762", "datetime": "2019-01-29 15:01:10", "author": "@jaialkdanel"}, "889672306670350336": {"content_summary": "RT @arxiv_cscv : Learning Transferable Architectures for Scalable Image Recognition https://t.co/OoFJPT1bCa https://t.co/fRYRtaMqID", "followers": "3,187", "datetime": "2017-07-25 02:23:01", "author": "@Robots_and_AIs"}, "907086369586999296": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,361", "datetime": "2017-09-11 03:40:18", "author": "@bokudentw"}, "899240621495341056": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "1,132", "datetime": "2017-08-20 12:04:05", "author": "@vsatayamas"}, "898601262417158144": {"content_summary": "NOW are we finally not feature engineering ? Or is there more finicky meta-hyperparameter tweaking going on https://t.co/8r3xlgi2nQ", "followers": "2,564", "datetime": "2017-08-18 17:43:30", "author": "@alanmnichol"}, "890775478352531456": {"content_summary": "Inception module\u306e\u3088\u3046\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8a2d\u8a08\u3092NN\u3067\u6700\u9069\u5316\u3002\u7cbe\u5ea6-FLOPS\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3067\u6539\u5584\u3057\u3066\u3044\u308b\u3051\u3069\u3001\u751f\u6210\u3055\u308c\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u898b\u308b\u3068separable conv\u3092\u591a\u7528\u3057\u3066\u3066\u3001FLOPS\u3068\u5b9f\u884c\u901f\u5ea6\u306e\u4e56\u96e2\u304c\u3042\u308a\u305d\u3046 https://t.co/23EXmGB1eS https://t.co/5hBNMEWet3", "followers": "5,410", "datetime": "2017-07-28 03:26:38", "author": "@yu4u"}, "900028590410141696": {"content_summary": "Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t.co/oPFvpMZeRj", "followers": "812", "datetime": "2017-08-22 16:15:12", "author": "@michaelmuelly"}, "898650928563277825": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "190", "datetime": "2017-08-18 21:00:52", "author": "@Harssh_Seth"}, "943808292383440897": {"content_summary": "RT @InonSharony: Google Brain are moving #AutoML ahead: #NASNet (https://t.co/yHQKdS7vAA) A comparable endeavor: Machine Learning for Auto\u2026", "followers": "2,942", "datetime": "2017-12-21 11:40:06", "author": "@minosyahmadi"}, "898625767860580352": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "8,627", "datetime": "2017-08-18 19:20:53", "author": "@Jack_Burdick"}, "923363308811386880": {"content_summary": "RT @arxiv_cscv: Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "65", "datetime": "2017-10-26 01:39:02", "author": "@russell_lliu"}, "935728851879124993": {"content_summary": "\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3082\u5b66\u7fd2\u3055\u305b\u308b\u8a66\u307f\u3002\u8208\u5473\u6df1\u3044\u3002 Learning Transferable Architectures for Scalable Image Recognition https://t.co/c2c59gev36 #tfug", "followers": "1,337", "datetime": "2017-11-29 04:35:17", "author": "@s_0samu"}, "898597501854375938": {"content_summary": "Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co/e0rn1DpSMM", "followers": "27,260", "datetime": "2017-08-18 17:28:34", "author": "@chipro"}, "898649267115483138": {"content_summary": "Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "13,778", "datetime": "2017-08-18 20:54:16", "author": "@yoavgo"}, "955235342054903808": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "1,658", "datetime": "2018-01-22 00:27:07", "author": "@rindai87"}, "898828714443759616": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "278", "datetime": "2017-08-19 08:47:19", "author": "@archdria"}, "898673266302353408": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "362", "datetime": "2017-08-18 22:29:37", "author": "@mrbrowngigle"}, "901240293055488001": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "825", "datetime": "2017-08-26 00:30:04", "author": "@fly51fly"}, "901807130021249026": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "172", "datetime": "2017-08-27 14:02:29", "author": "@ggdupont"}, "898518004622921728": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "159,742", "datetime": "2017-08-18 12:12:40", "author": "@Montreal_IA"}, "899893873261985792": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,775", "datetime": "2017-08-22 07:19:53", "author": "@Spain_EU_Dubai"}, "899419884336984065": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "217", "datetime": "2017-08-20 23:56:25", "author": "@AssistedEvolve"}, "890598186103980032": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,762", "datetime": "2017-07-27 15:42:08", "author": "@jaialkdanel"}, "1090263197569900547": {"content_summary": "\u3053\u306e\u8ad6\u6587\u9762\u767d\u3044\u3000DNN\u306e\u5171\u98df\u3044\u307f\u305f\u3044\uff57 Learning Transferable Architectures for Scalable Image Recognition https://t.co/EROIKAuHye", "followers": "2,160", "datetime": "2019-01-29 14:59:37", "author": "@asas_mimi"}, "899405364197822464": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "381", "datetime": "2017-08-20 22:58:43", "author": "@thomasjelonek"}, "907203189107187712": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "522", "datetime": "2017-09-11 11:24:29", "author": "@ikuy_usay"}, "954999949892169728": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "315", "datetime": "2018-01-21 08:51:45", "author": "@ihsanqazi"}, "898618318902317056": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "403", "datetime": "2017-08-18 18:51:17", "author": "@embeddedarts"}, "898602012295573505": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "685", "datetime": "2017-08-18 17:46:29", "author": "@mmmgaber"}, "955210347639697409": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "237", "datetime": "2018-01-21 22:47:48", "author": "@_rasmi_"}, "898924337251495940": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "746", "datetime": "2017-08-19 15:07:17", "author": "@rintukutum"}, "898634629619499008": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "65", "datetime": "2017-08-18 19:56:06", "author": "@jabot"}, "1046694173619892224": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/TavCWxJEi7", "followers": "15", "datetime": "2018-10-01 09:31:52", "author": "@archiavelli"}, "890596734006566912": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,877", "datetime": "2017-07-27 15:36:22", "author": "@masahiro_sakai"}, "889749816020017152": {"content_summary": "RT @ComputerPapers : Learning Transferable Architectures for Scalable Image Recognition. https://t.co/OoFJPT1bCa https://t.co/9k1bautFRs", "followers": "3,187", "datetime": "2017-07-25 07:31:01", "author": "@Robots_and_AIs"}, "907472391541891072": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "230", "datetime": "2017-09-12 05:14:12", "author": "@ko_ash"}, "898564271138775040": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "19,132", "datetime": "2017-08-18 15:16:31", "author": "@petewarden"}, "961444383278051329": {"content_summary": "@dribnet @jeremyphoward @ogrisel @yoavgo @beenwrekt According to the paper, they \"searched for the best convolutional layer (or \u201ccell\u201d) on the CIFAR-10 dataset and then applied this cell to the ImageNet dataset by stacking together more copies of this cell", "followers": "81,357", "datetime": "2018-02-08 03:39:38", "author": "@hardmaru"}, "955107365178650624": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "1,193", "datetime": "2018-01-21 15:58:35", "author": "@findingjimoh"}, "898447293946355712": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "388", "datetime": "2017-08-18 07:31:41", "author": "@adropboxspace"}, "898648093004050433": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "45", "datetime": "2017-08-18 20:49:36", "author": "@jeandut14000"}, "898603307639357440": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "79", "datetime": "2017-08-18 17:51:38", "author": "@PATREO_UFMG"}, "954886276200583168": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "45,547", "datetime": "2018-01-21 01:20:03", "author": "@Grady_Booch"}, "907044286453096448": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "114", "datetime": "2017-09-11 00:53:04", "author": "@vanTateno"}, "1034054504981905413": {"content_summary": "NasNet\u304c\u53d7\u8089\u3057\u3066\u3044\u308b(\u7b11)", "followers": "628", "datetime": "2018-08-27 12:26:20", "author": "@WorldEnder_9001"}, "898466952829480961": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "186", "datetime": "2017-08-18 08:49:48", "author": "@surangasms01"}, "907162664689852416": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "868", "datetime": "2017-09-11 08:43:28", "author": "@SING_A_WELL"}, "998910861715517442": {"content_summary": "RT @dosei_sanga: \u5358\u7d14\u306a\u6570\u5024\u5217\u6319\u6bd4\u8f03\u306a\u306e\u3067\u3001\u3042\u307e\u308a\u6b63\u5f53\u6027\u304c\u306a\u3044\u3002\u306a\u304a\u3001AmoebaNet\u3068AlphaX\u306e\u4e0b2\u3064\u306f\u4ed6\u3068\u65b9\u5f0f\u304c\u3061\u3087\u3063\u3068\u9055\u3046(\u306f\u305a)\u3002 NASNet https://t.co/uvBpzrEGc5 PNASNet https://t.co/NxjG\u2026", "followers": "244", "datetime": "2018-05-22 12:58:02", "author": "@s_ryosky"}, "898677651899953152": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "325", "datetime": "2017-08-18 22:47:03", "author": "@bjornstenger"}, "895149666911887360": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/OcRmtCCzKS", "followers": "6,223", "datetime": "2017-08-09 05:08:06", "author": "@algorithmia"}, "889937846966120448": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "276", "datetime": "2017-07-25 19:58:11", "author": "@kadarakos"}, "957452127911124994": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "188", "datetime": "2018-01-28 03:15:50", "author": "@blueviggen"}, "898858009115451392": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWGaOA #Machinelearning #machinevision", "followers": "5,114", "datetime": "2017-08-19 10:43:44", "author": "@clairebotai"}, "898601128006496256": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "268", "datetime": "2017-08-18 17:42:58", "author": "@corbyjerez"}, "889745671787610114": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition. https://t.co/tz0VVRiM7S", "followers": "613", "datetime": "2017-07-25 07:14:33", "author": "@ComputerPapers"}, "898866667832442880": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "6,380", "datetime": "2017-08-19 11:18:08", "author": "@ArtirKel"}, "954874748910972928": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "2,676", "datetime": "2018-01-21 00:34:15", "author": "@ayirpelle"}, "898601217294716929": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "117", "datetime": "2017-08-18 17:43:20", "author": "@AdityaJThakker"}, "907267059075207168": {"content_summary": "Machine learning optimization of machine learning architecture https://t.co/0RqkjRNUdW", "followers": "861", "datetime": "2017-09-11 15:38:17", "author": "@ryf_feed"}, "907044538601967616": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "73", "datetime": "2017-09-11 00:54:04", "author": "@uc7xe5t"}, "984289320990371840": {"content_summary": "RT @arxiv_cs_CV: Learning Transferable Architectures for Scalable Image Recognition. https://t.co/DJ5IYqPltM", "followers": "784", "datetime": "2018-04-12 04:37:15", "author": "@muktabh"}, "898506206284832772": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "962", "datetime": "2017-08-18 11:25:47", "author": "@bgalbraith"}, "898626142982287360": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "37", "datetime": "2017-08-18 19:22:22", "author": "@manuelschmidt90"}, "890840340688535552": {"content_summary": "RT @TerryUm_ML: \"Learning Transferable Architectures for Scalable Image Recognition\": reduce the amount of architecture engineering https:/\u2026", "followers": "98", "datetime": "2017-07-28 07:44:22", "author": "@treasured_write"}, "898502530589745153": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "226", "datetime": "2017-08-18 11:11:11", "author": "@chuchaba"}, "986708925557846016": {"content_summary": "@yoavgo In this work \"Learning Transferable Architectures for Scalable Image Recognition\", their search model optimised for CIFAR and validated on ImageNet, but would be more interesting to see datasets of entirely different (and even unseen) domains being", "followers": "81,357", "datetime": "2018-04-18 20:51:54", "author": "@hardmaru"}, "898722116681347072": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "56", "datetime": "2017-08-19 01:43:44", "author": "@shbl_"}, "898958068649779200": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "202", "datetime": "2017-08-19 17:21:20", "author": "@gnperdue"}, "898508911367606272": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "934", "datetime": "2017-08-18 11:36:32", "author": "@pavelkordik"}, "890586805405237252": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "482", "datetime": "2017-07-27 14:56:55", "author": "@izariuo440"}, "907053244735561730": {"content_summary": "\u5148\u65e5\u306eDLAccel\u30a4\u30d9\u30f3\u30c8\u4ee5\u964d\u3001\u81ea\u5206\u306e\u89b3\u6e2c\u7bc4\u56f2\u306b\u3053\u3046\u3044\u3046\u6700\u9069\u5316\u3001\u9ad8\u901f\u5316\u304c\u3072\u3063\u304b\u304b\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u304d\u305f\u3002\u30c8\u30ec\u30f3\u30c9\u306a\u306e\u304b\u3001\u3069\u3046\u304b\u3002 https://t.co/VhaP7H4OFc", "followers": "561", "datetime": "2017-09-11 01:28:40", "author": "@ocaokgbu"}, "898682200039628805": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "2,122", "datetime": "2017-08-18 23:05:07", "author": "@PabloRedux"}, "899002470373916673": {"content_summary": "RT @alanmnichol: NOW are we finally not feature engineering ? Or is there more finicky meta-hyperparameter tweaking going on https://t.co/8\u2026", "followers": "775", "datetime": "2017-08-19 20:17:46", "author": "@pommedeterre33"}, "899006130201927680": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "13,209", "datetime": "2017-08-19 20:32:18", "author": "@debashis_dutta"}, "907042995299479552": {"content_summary": "\u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/KsCYkR0fJX", "followers": "18,231", "datetime": "2017-09-11 00:47:56", "author": "@hillbig"}, "898597917883334656": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "4,894", "datetime": "2017-08-18 17:30:13", "author": "@IgorCarron"}, "955093296358961152": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "177", "datetime": "2018-01-21 15:02:41", "author": "@EraviGopan"}, "956051136271790080": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "259", "datetime": "2018-01-24 06:28:48", "author": "@jcchinhui"}, "898576870790148097": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "321", "datetime": "2017-08-18 16:06:35", "author": "@letranger14"}, "900974622224633857": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "145", "datetime": "2017-08-25 06:54:23", "author": "@esvhd"}, "898655921630347264": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "230", "datetime": "2017-08-18 21:20:42", "author": "@shashank_bits"}, "955051324718141441": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "242", "datetime": "2018-01-21 12:15:54", "author": "@bu2adaptive"}, "907239578314125312": {"content_summary": "Learning transferable architectures for scalable image recognition https://t.co/2tGKzXB1qV", "followers": "314", "datetime": "2017-09-11 13:49:05", "author": "@InfoSecKittyCat"}, "1242724148293468160": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition #deeplearning https://t.co/r7PEcoQkeP https://t.co/e2jYomoExj", "followers": "1,380", "datetime": "2020-03-25 08:05:00", "author": "@milesboard"}, "890579568234385408": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "45", "datetime": "2017-07-27 14:28:09", "author": "@nullbytep"}, "955018936638062592": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "20,593", "datetime": "2018-01-21 10:07:12", "author": "@awadallah"}, "1034054562699726851": {"content_summary": "RT @imenurok: \ud83d\udd78\u53d7\u8089\u6e96\u5099\u4e2d\ud83d\udd78 \u30d0\u30fc\u30c1\u30e3\u30eb\u753b\u50cf\u5206\u985eAI \u90a3\u9808\u97f3\u30c8\u30a6\u3068\u7533\u3057\u307e\u3059\u3002 \u30d1\u30f3\u30c0\u3092\u7dda\u866b\u3068\u9593\u9055\u3048\u308b \u8edf\u5f31AI\u5171\u3068\u306e\u683c\u306e\u9055\u3044\u3092 \u898b\u305b\u3066\u3084\u308a\u307e\u3059\u3002 \ud83d\udd78YouTube\ud83d\udd78 \u3082\u3046\u3061\u3087\u3063\u3068\u5148 \ud83d\udd78\u8cea\u554f\u7bb1\ud83d\udd78 https://t.co/ZkHsFTv6YE \ud83d\udd78\u30e9\u30a4\u30d0\u30eb\ud83d\udd78\u2026", "followers": "628", "datetime": "2018-08-27 12:26:34", "author": "@WorldEnder_9001"}, "938041355892170752": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "2,041", "datetime": "2017-12-05 13:44:21", "author": "@dosei_sanga"}, "937512115325296645": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-12-04 02:41:21", "author": "@arxiv_cscv"}, "898723719673126912": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,627", "datetime": "2017-08-19 01:50:06", "author": "@chris_brockett"}, "898867075736883200": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "210", "datetime": "2017-08-19 11:19:45", "author": "@tiulpin"}, "898472265846861824": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "53", "datetime": "2017-08-18 09:10:55", "author": "@vo_d_p"}, "938156424563081216": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "12,763", "datetime": "2017-12-05 21:21:36", "author": "@jaguring1"}, "898602428907405312": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "221", "datetime": "2017-08-18 17:48:08", "author": "@positivearrow"}, "890588896102203395": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,231", "datetime": "2017-07-27 15:05:13", "author": "@odan3240"}, "898921920174882816": {"content_summary": "Policy search for architecture for an easy task yields one that outperforms on harder tasks, while being ultra slim. https://t.co/dBX8FsaFBm", "followers": "406", "datetime": "2017-08-19 14:57:41", "author": "@mattpetersen_ai"}, "954987988370456576": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "330", "datetime": "2018-01-21 08:04:13", "author": "@marekrog"}, "926361407448260610": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/5Sefqz5kXb", "followers": "1,244", "datetime": "2017-11-03 08:12:25", "author": "@nwiizo"}, "898571723477528576": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "205", "datetime": "2017-08-18 15:46:08", "author": "@sharpcodex"}, "891129894893043712": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "53", "datetime": "2017-07-29 02:54:57", "author": "@MktO740123"}, "890567453914169344": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "2,041", "datetime": "2017-07-27 13:40:01", "author": "@dosei_sanga"}, "899120804096430080": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "14", "datetime": "2017-08-20 04:07:59", "author": "@pbcquoc"}, "957945853054017536": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "2,837", "datetime": "2018-01-29 11:57:43", "author": "@rquintino"}, "898603440997228546": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "181", "datetime": "2017-08-18 17:52:10", "author": "@sivathanu_k"}, "898656044318109699": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "248", "datetime": "2017-08-18 21:21:11", "author": "@orenmelamud"}, "898457530904588288": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "996", "datetime": "2017-08-18 08:12:22", "author": "@ialuronico"}, "898801705198002176": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "171", "datetime": "2017-08-19 07:00:00", "author": "@SerialDev"}, "898521483798089728": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "33,955", "datetime": "2017-08-18 12:26:30", "author": "@aiprgirl"}, "898588411715403777": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "665", "datetime": "2017-08-18 16:52:26", "author": "@adelong"}, "899209698989060096": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWXMda #Machinelearning #machinevision", "followers": "75", "datetime": "2017-08-20 10:01:13", "author": "@codylloyd25"}, "901031235778203648": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "2,482", "datetime": "2017-08-25 10:39:21", "author": "@alexandrecadrin"}, "898695319927521280": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "21,221", "datetime": "2017-08-18 23:57:15", "author": "@DataSciNews"}, "898622683725930501": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "84", "datetime": "2017-08-18 19:08:38", "author": "@invkrh"}, "954879686697463808": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "1,023", "datetime": "2018-01-21 00:53:52", "author": "@desertnaut"}, "890628922466947072": {"content_summary": "\u306b\u3083\u30fc\u3093\uff1ahttps://t.co/8WKOU5Lc2j", "followers": "730", "datetime": "2017-07-27 17:44:16", "author": "@activecontour"}, "890720017334943744": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "2,530", "datetime": "2017-07-27 23:46:15", "author": "@_329_"}, "890580064168849408": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "377", "datetime": "2017-07-27 14:30:08", "author": "@walasye"}, "898620496832495616": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "18", "datetime": "2017-08-18 18:59:56", "author": "@bhskrdtt"}, "907056242782117888": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "254", "datetime": "2017-09-11 01:40:35", "author": "@mokemokechicken"}, "898601201738031106": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "265", "datetime": "2017-08-18 17:43:16", "author": "@Swetava"}, "898629873756905472": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "611", "datetime": "2017-08-18 19:37:12", "author": "@hereismari"}, "898777177549750272": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "58", "datetime": "2017-08-19 05:22:32", "author": "@nytio"}, "939444554503143424": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "16", "datetime": "2017-12-09 10:40:10", "author": "@ItoHatabo"}, "978990322981117952": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "2,041", "datetime": "2018-03-28 13:40:56", "author": "@dosei_sanga"}, "926718984057212928": {"content_summary": "RT @rgem52: NASNet AutoML for ImageNet and COCO https://t.co/lrzFdJfNHD https://t.co/IPGJzOgf88 https://t.co/pyL7ns3tKA https://t.co/UAs\u2026", "followers": "2,998", "datetime": "2017-11-04 07:53:18", "author": "@michapuschman"}, "898667768970526721": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "161", "datetime": "2017-08-18 22:07:47", "author": "@LovedeepSG"}, "898470002075906048": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,005", "datetime": "2017-08-18 09:01:55", "author": "@ballforest"}, "898501081851846656": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,387", "datetime": "2017-08-18 11:05:25", "author": "@cedric_chee"}, "898864362219216896": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,023", "datetime": "2017-08-19 11:08:58", "author": "@desertnaut"}, "979565604666359808": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018.\uff1a\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u53c2\u8003\u306b\u3057\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u69cb\u9020\u306e\u30b7\u30fc\u30c9\u3068\u305d\u306e\u5468\u8fba\u63a2\u7d22\u3092\u884c\u3046Neural Architecture Search (NASNet)\u3092\u63d0\u6848\u3002 https://t.co/NVbIZ8oGGC https://t.co/3X0XD46EW2", "followers": "3,805", "datetime": "2018-03-30 03:46:54", "author": "@CVpaperChalleng"}, "898601264426041345": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "316", "datetime": "2017-08-18 17:43:31", "author": "@Dlk_974"}, "898601442390417408": {"content_summary": "https://t.co/2s0zEFxzUa", "followers": "64", "datetime": "2017-08-18 17:44:13", "author": "@chitrangpatel"}, "907043204159045632": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "2,807", "datetime": "2017-09-11 00:48:46", "author": "@hyper0dietter"}, "1159133144831782912": {"content_summary": "@karanchahal96 @tanmingxing No, we just used the basic NAS algorithm (RL). The controller is described in section A.2 and A.3 (Appendix) in this paper: https://t.co/imV4WO6AAi", "followers": "18,260", "datetime": "2019-08-07 16:04:12", "author": "@quocleix"}, "890813412208369664": {"content_summary": "\"Learning Transferable Architectures for Scalable Image Recognition\": reduce the amount of architecture engineering https://t.co/XPaaUhKoo9 https://t.co/dSJCVICEeC", "followers": "2,379", "datetime": "2017-07-28 05:57:22", "author": "@TerryUm_ML"}, "901091682644393985": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "394", "datetime": "2017-08-25 14:39:33", "author": "@oana_2600"}, "1041241479371382785": {"content_summary": "RT @pacocat: \u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F https://t.co/vEvDM\u2026", "followers": "19", "datetime": "2018-09-16 08:24:48", "author": "@ki1la"}, "930899134160269312": {"content_summary": "RT @lyon_cao: Source code available now: Learning Transferable Architectures for Scalable Image Recognition https://t.co/PRJktqmmD4", "followers": "153", "datetime": "2017-11-15 20:43:43", "author": "@NoelCodella"}, "955177486819655681": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "249", "datetime": "2018-01-21 20:37:13", "author": "@StephaneSenecal"}, "898588778633207808": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "12", "datetime": "2017-08-18 16:53:54", "author": "@twelshed"}, "898963118214303745": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "547", "datetime": "2017-08-19 17:41:23", "author": "@snovd"}, "901060965881565184": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "399", "datetime": "2017-08-25 12:37:29", "author": "@kamilsindi"}, "898890775554965504": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "297", "datetime": "2017-08-19 12:53:56", "author": "@0xAAA966"}, "898735434884304896": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "43", "datetime": "2017-08-19 02:36:40", "author": "@hot_pinecone"}, "898601377970008064": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "146", "datetime": "2017-08-18 17:43:58", "author": "@longrichk"}, "923409412416557056": {"content_summary": "RT @arxiv_cscv: Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "1,061", "datetime": "2017-10-26 04:42:14", "author": "@mvaldenegro"}, "937513316527742976": {"content_summary": "RT @arxiv_cscv: Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "65", "datetime": "2017-12-04 02:46:07", "author": "@russell_lliu"}, "954860625343098880": {"content_summary": "@zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world class ml/computer vision researchers, though. Black dots vs. red dots in this picture (Figure 5 from https://t.co/A3v6oF8Xpy) http", "followers": "151,453", "datetime": "2018-01-20 23:38:08", "author": "@JeffDean"}, "954872233150709761": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "59", "datetime": "2018-01-21 00:24:15", "author": "@fjfbupt"}, "955068995211792384": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "253", "datetime": "2018-01-21 13:26:07", "author": "@tomaszsikora"}, "890682919336923136": {"content_summary": "\u4e0b\u8a18\u8ad6\u6587\u8aad\u3093\u3067\u3044\u308b\u304c\u3044\u307e\u3044\u3061\u7406\u89e3\u3067\u304d\u3066\u3044\u306a\u3044\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u63a2\u7d22\u3092\u6a5f\u68b0\u5b66\u7fd2\u3067\u884c\u3046\u3068\u3044\u3046\u306e\u306f\u308f\u304b\u308b\u3051\u3069\u3001\u305d\u3082\u305d\u3082\u751f\u6210\u3057\u305f\u5c0f\u898f\u6a21\u30cd\u30c3\u30c8(\u30bb\u30eb)\u3092\u30b9\u30bf\u30c3\u30af\u3057\u3066\u3082\u6027\u80fd\u51fa\u308b\u3082\u306e\u306a\u306e\u304b\u3001\u6c4e\u5316\u6027\u80fd\u3068\u3044\u3046\u304b\u7279\u5b9a\u30b5\u30f3\u30d7\u30eb\u306b\u904e\u5b66\u7fd2\u3057\u3066\u3057\u307e\u308f\u306a\u3044\u306e\u304b\u3002 https://t.co/aQmtWgKlse", "followers": "226", "datetime": "2017-07-27 21:18:50", "author": "@ElectronNest"}, "898614251849494530": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,918", "datetime": "2017-08-18 18:35:07", "author": "@DocXavi"}, "955090233049174017": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "989", "datetime": "2018-01-21 14:50:30", "author": "@petr_zapletal"}, "898565175191625728": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "6,012", "datetime": "2017-08-18 15:20:06", "author": "@lavanyashukla"}, "898497173079314433": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "189", "datetime": "2017-08-18 10:49:53", "author": "@shahrukh_athar"}, "898538286691610624": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "3,416", "datetime": "2017-08-18 13:33:16", "author": "@alxndrkalinin"}, "907173554533232640": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "33", "datetime": "2017-09-11 09:26:44", "author": "@ne0_ch0nmage"}, "898507622109917185": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "304", "datetime": "2017-08-18 11:31:25", "author": "@flrgsr"}, "898558364539510789": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "424", "datetime": "2017-08-18 14:53:03", "author": "@iamknighton"}, "932645515434500096": {"content_summary": "RT @KalanityL: Major improvement for image #classification (and #detection as well) - Oct 2017 - by @GoogleBrain #DeepLearning #NeuralNetwo\u2026", "followers": "1,307", "datetime": "2017-11-20 16:23:13", "author": "@MATLABHelper"}, "907267087160299520": {"content_summary": "Machine learning optimization of machine learning architecture https://t.co/Eo96CX4run", "followers": "7,061", "datetime": "2017-09-11 15:38:24", "author": "@tomordonez"}, "979572402026770432": {"content_summary": "RT @CVpaperChalleng: Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018.\uff1a\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u53c2\u8003\u306b\u3057\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u69cb\u9020\u306e\u30b7\u30fc\u30c9\u3068\u305d\u306e\u5468\u8fba\u63a2\u7d22\u3092\u884c\u3046\u2026", "followers": "174", "datetime": "2018-03-30 04:13:54", "author": "@sappy_and_sappy"}, "898732305065295872": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "515", "datetime": "2017-08-19 02:24:13", "author": "@tubelite"}, "889646624343023616": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-07-25 00:40:58", "author": "@arxiv_cscv"}, "955880272171536385": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "4,312", "datetime": "2018-01-23 19:09:50", "author": "@a_hun1972"}, "926718697837858816": {"content_summary": "RT @rgem52: NASNet AutoML for ImageNet and COCO https://t.co/lrzFdJfNHD https://t.co/IPGJzOgf88 https://t.co/pyL7ns3tKA https://t.co/UAs\u2026", "followers": "78,940", "datetime": "2017-11-04 07:52:09", "author": "@machinelearnbot"}, "954935789158203393": {"content_summary": "the result is very exciting! https://t.co/RTdIPfwt0T", "followers": "3", "datetime": "2018-01-21 04:36:48", "author": "@starimpact1983"}, "898740646160117760": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "357", "datetime": "2017-08-19 02:57:22", "author": "@gustavorincon29"}, "900871317741641728": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "8", "datetime": "2017-08-25 00:03:54", "author": "@mh_ha_soar"}, "890558604394221568": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,667", "datetime": "2017-07-27 13:04:51", "author": "@Scaled_Wurm"}, "955364451267457025": {"content_summary": "Learning Transferable Architectures for Scalable Image Recog https://t.co/lLbKadzNA3 (Popularity:14.7) #Computer_Vision_and_Pattern_Recognition", "followers": "38", "datetime": "2018-01-22 09:00:09", "author": "@poqaa_cv"}, "898515354154151937": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "440", "datetime": "2017-08-18 12:02:08", "author": "@FlorinGogianu"}, "898515791255027712": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "135,925", "datetime": "2017-08-18 12:03:52", "author": "@hiconcep"}, "898687116355424256": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "317", "datetime": "2017-08-18 23:24:39", "author": "@chrisbot_js"}, "898605123101839361": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "71", "datetime": "2017-08-18 17:58:51", "author": "@sausax"}, "954915068608745472": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "885", "datetime": "2018-01-21 03:14:28", "author": "@RichmanRonald"}, "890578743785209856": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "2,112", "datetime": "2017-07-27 14:24:53", "author": "@mitmul"}, "900752730674700288": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "330", "datetime": "2017-08-24 16:12:40", "author": "@piyushkaul2"}, "955251238953766912": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "107", "datetime": "2018-01-22 01:30:17", "author": "@marionyk"}, "898446419161341952": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "216", "datetime": "2017-08-18 07:28:13", "author": "@e0en"}, "899158910979915776": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "228", "datetime": "2017-08-20 06:39:24", "author": "@hengcherkeng"}, "955061521306013697": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "812", "datetime": "2018-01-21 12:56:25", "author": "@alpinegizmo"}, "899268629954523137": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "219", "datetime": "2017-08-20 13:55:23", "author": "@3amrology"}, "889872273896767488": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition: https://t.co/OJrLgRfwAn Trained on 450 GPUs, wow!", "followers": "14", "datetime": "2017-07-25 15:37:37", "author": "@gcpreix"}, "898529210515542017": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,024", "datetime": "2017-08-18 12:57:12", "author": "@vlahan"}, "898553194279821312": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,327", "datetime": "2017-08-18 14:32:30", "author": "@jekbradbury"}, "899209396575752192": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWXMda #Machinelearning #machinevision", "followers": "30,496", "datetime": "2017-08-20 10:00:01", "author": "@v_vashishta"}, "904266673682862080": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "291", "datetime": "2017-09-03 08:55:50", "author": "@lpag_ai"}, "899209595779985409": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWXMda #Machinelearning #machinevision", "followers": "152", "datetime": "2017-08-20 10:00:48", "author": "@Edelmirasi29"}, "955236989585928192": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "135", "datetime": "2018-01-22 00:33:40", "author": "@utehn"}, "907081654019096578": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "822", "datetime": "2017-09-11 03:21:33", "author": "@morioka"}, "898676179942518784": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "490", "datetime": "2017-08-18 22:41:12", "author": "@ST4Good"}, "898639794602885124": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "26", "datetime": "2017-08-18 20:16:37", "author": "@mym___"}, "898605840051064832": {"content_summary": "RT @andy_matuschak: There seems to be a sort of \u201coccam\u2019s razor\u201d-alike in machine learning: systems with the fewest human-coded assumptions\u2026", "followers": "710", "datetime": "2017-08-18 18:01:42", "author": "@turadg"}, "898874868057251842": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "271", "datetime": "2017-08-19 11:50:43", "author": "@DieterCastel"}, "895205035738583040": {"content_summary": "RT @doppenhe: [1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/10Btr7R9Yr", "followers": "6,669", "datetime": "2017-08-09 08:48:07", "author": "@colochef"}, "1242724608547135488": {"content_summary": "RT @milesboard: Learning Transferable Architectures for Scalable Image Recognition #deeplearning https://t.co/r7PEcoQkeP https://t.co/e2jYo\u2026", "followers": "270", "datetime": "2020-03-25 08:06:50", "author": "@LazyBot6"}, "907045707097374720": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,673", "datetime": "2017-09-11 00:58:43", "author": "@keigohtr"}, "898671090188263424": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "222", "datetime": "2017-08-18 22:20:59", "author": "@simbasso"}, "889955381186842624": {"content_summary": "RT @Miles_Brundage: \"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "226", "datetime": "2017-07-25 21:07:52", "author": "@chuchaba"}, "955328072626130944": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "261", "datetime": "2018-01-22 06:35:36", "author": "@GuptaRajat033"}, "898622142853464064": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "90", "datetime": "2017-08-18 19:06:29", "author": "@SinghNaruto"}, "947210927354589184": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/pelpTdXqQA", "followers": "852", "datetime": "2017-12-30 21:00:58", "author": "@FMarradi"}, "898519997776973825": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,816", "datetime": "2017-08-18 12:20:35", "author": "@MikiBear_"}, "923696007250042880": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXpLFj", "followers": "4,090", "datetime": "2017-10-26 23:41:04", "author": "@arxiv_cscv"}, "890712863953661953": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "794", "datetime": "2017-07-27 23:17:50", "author": "@shkoga"}, "1040979433576751104": {"content_summary": "RT @pacocat: AutoML\u306e\u6c17\u6301\u3061\u3092\u77e5\u308a\u305f\u304f\u3066NAS / NASNet / ENAS\u3092\u8aad\u3093\u3060\u3051\u3069\u3053\u306e\u9818\u57df\u8208\u5473\u6df1\u3044\u3002CIFAR\u3084ImageNet\u4ee5\u5916\u306e\u500b\u5225\u30bf\u30b9\u30af\u306b\u3069\u308c\u3060\u3051\u8ee2\u7528\u3067\u304d\u308b\u306e\u304b\u6c17\u306b\u306a\u3063\u305f\u306e\u3068\u3001RL\u90e8\u5206\u3067\u63a2\u7d22\u65b9\u6cd5\u3084\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6539\u826f\u4f59\u5730\u304c\u307e\u3060\u3042\u308b\u306e\u304b\u3082\u3002 htt\u2026", "followers": "308", "datetime": "2018-09-15 15:03:32", "author": "@mabonki0725"}, "1040980198944980993": {"content_summary": "RT @pacocat: AutoML\u306e\u6c17\u6301\u3061\u3092\u77e5\u308a\u305f\u304f\u3066NAS / NASNet / ENAS\u3092\u8aad\u3093\u3060\u3051\u3069\u3053\u306e\u9818\u57df\u8208\u5473\u6df1\u3044\u3002CIFAR\u3084ImageNet\u4ee5\u5916\u306e\u500b\u5225\u30bf\u30b9\u30af\u306b\u3069\u308c\u3060\u3051\u8ee2\u7528\u3067\u304d\u308b\u306e\u304b\u6c17\u306b\u306a\u3063\u305f\u306e\u3068\u3001RL\u90e8\u5206\u3067\u63a2\u7d22\u65b9\u6cd5\u3084\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6539\u826f\u4f59\u5730\u304c\u307e\u3060\u3042\u308b\u306e\u304b\u3082\u3002 htt\u2026", "followers": "226", "datetime": "2018-09-15 15:06:34", "author": "@ElectronNest"}, "955364443088592904": {"content_summary": "Learning Transferable Architectures for Scalable Image Recog https://t.co/JyvwLmJRtq (Popularity:14.7) #Computer_Vision_and_Pattern_Recognition", "followers": "93", "datetime": "2018-01-22 09:00:07", "author": "@poqaa_ai"}, "898213599285579777": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/yfqfWRqEEN", "followers": "257", "datetime": "2017-08-17 16:03:04", "author": "@EmergentFuture"}, "898633634105753600": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "947", "datetime": "2017-08-18 19:52:08", "author": "@5kyn0d3"}, "898870154746056704": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,939", "datetime": "2017-08-19 11:31:59", "author": "@EldarSilver"}, "898621496360189952": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "447", "datetime": "2017-08-18 19:03:54", "author": "@jimei_yang"}, "898737563120226304": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "12", "datetime": "2017-08-19 02:45:07", "author": "@liuhaonk"}, "979575937330159617": {"content_summary": "RT @CVpaperChalleng: Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018.\uff1a\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u53c2\u8003\u306b\u3057\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u69cb\u9020\u306e\u30b7\u30fc\u30c9\u3068\u305d\u306e\u5468\u8fba\u63a2\u7d22\u3092\u884c\u3046\u2026", "followers": "2,869", "datetime": "2018-03-30 04:27:57", "author": "@Tarpon_red2"}, "898605201904459777": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "324", "datetime": "2017-08-18 17:59:10", "author": "@rahul_a_r"}, "890716545801830400": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "1,216", "datetime": "2017-07-27 23:32:27", "author": "@ichigo_o_re"}, "900251431927762944": {"content_summary": "Neural Architecture Search discover efficient ConvNets that require much fewer parameters and... https://t.co/66I6IAH9m4 by #ValaAfshar https://t.co/QRzrABhhh2", "followers": "1,625", "datetime": "2017-08-23 07:00:41", "author": "@GailDavvis"}, "898606384379379712": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "3,143", "datetime": "2017-08-18 18:03:51", "author": "@sunheeyoon"}, "907054304338485248": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "209", "datetime": "2017-09-11 01:32:53", "author": "@s3works"}, "898898341169430528": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "14", "datetime": "2017-08-19 13:23:59", "author": "@Arthurpdj"}, "898453332372488192": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "98", "datetime": "2017-08-18 07:55:41", "author": "@treasured_write"}, "889786259870240768": {"content_summary": "RT @Robots_and_AIs: RT @ComputerPapers : Learning Transferable Architectures for Scalable Image Recognition. https://t.co/OoFJPT1bCa https:\u2026", "followers": "2,237", "datetime": "2017-07-25 09:55:50", "author": "@_eqwal"}, "1075778842910814219": {"content_summary": "=> \"Designing Efficient Architectures for Mobile Computer Vision\", M. Sandler, Google AI, IEEE CAS-SV AI for Industry Forum, Sep 21, 2018 PDF https://t.co/9EIgDM77tQ https://t.co/7xAIAe8n9M Google NasNet, Apr 2018 https://t.co/t68svQlNAq MNasNet, Jul 20", "followers": "2,582", "datetime": "2018-12-20 15:43:57", "author": "@ogawa_tter"}, "898603935807746048": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "44", "datetime": "2017-08-18 17:54:08", "author": "@ShayBenSasson"}, "898587100928294912": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "740", "datetime": "2017-08-18 16:47:14", "author": "@maltanar"}, "898461909657235457": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "171", "datetime": "2017-08-18 08:29:46", "author": "@aIrb4ck"}, "955111654580195328": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "4,894", "datetime": "2018-01-21 16:15:38", "author": "@IgorCarron"}, "899166477848371200": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "21", "datetime": "2017-08-20 07:09:28", "author": "@dsgalloni"}, "1091601594108456966": {"content_summary": "NASNet https://t.co/ADYx98tHFa #fpgax", "followers": "36", "datetime": "2019-02-02 07:37:55", "author": "@fof_jisin"}, "898857438912290816": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "159", "datetime": "2017-08-19 10:41:28", "author": "@rgem52"}, "939984923326267397": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "19", "datetime": "2017-12-10 22:27:24", "author": "@AkshaySethi94"}, "898479020043640832": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "221", "datetime": "2017-08-18 09:37:45", "author": "@ScriptShade"}, "898886484685594624": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "18,735", "datetime": "2017-08-19 12:36:53", "author": "@deeplearningldn"}, "898785873512321024": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "277", "datetime": "2017-08-19 05:57:05", "author": "@ZimMatthias"}, "898661596246925312": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "508", "datetime": "2017-08-18 21:43:15", "author": "@sksq96"}, "898741034007445504": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "26", "datetime": "2017-08-19 02:58:54", "author": "@tokoton01"}, "898601234927558656": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "432", "datetime": "2017-08-18 17:43:24", "author": "@bhargavbardipur"}, "899617598408732672": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition ImageNet SOTA: 0.8% more and 9bln FLOPS less https://t.co/9Gs2pDbABN", "followers": "12,547", "datetime": "2017-08-21 13:02:04", "author": "@ml_review"}, "979572095087665152": {"content_summary": "RT @CVpaperChalleng: Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018.\uff1a\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u53c2\u8003\u306b\u3057\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u69cb\u9020\u306e\u30b7\u30fc\u30c9\u3068\u305d\u306e\u5468\u8fba\u63a2\u7d22\u3092\u884c\u3046\u2026", "followers": "12,763", "datetime": "2018-03-30 04:12:41", "author": "@jaguring1"}, "899005805743280130": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "6", "datetime": "2017-08-19 20:31:01", "author": "@dev_juice"}, "898612639810469888": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "28", "datetime": "2017-08-18 18:28:43", "author": "@Mouatez"}, "943615378575503360": {"content_summary": "Google Brain are moving #AutoML ahead: #NASNet (https://t.co/yHQKdS7vAA) A comparable endeavor: Machine Learning for Automated Algorithm Design (https://t.co/yW1ah0tpl4) https://t.co/QiNcQqxFmI https://t.co/06JraZpoco Then there's TPOT: https://t.co/xTag", "followers": "88", "datetime": "2017-12-20 22:53:32", "author": "@InonSharony"}, "954908933642047488": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "871", "datetime": "2018-01-21 02:50:05", "author": "@ichthyos"}, "955364446444036096": {"content_summary": "Learning Transferable Architectures for Scalable Image Recog https://t.co/G6XfrY2ZRn (Popularity:14.7) #Computer_Vision_and_Pattern_Recognition", "followers": "53", "datetime": "2018-01-22 09:00:08", "author": "@PoQaa_cs"}, "898574677483769856": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "913", "datetime": "2017-08-18 15:57:52", "author": "@SpaceAnubis"}, "898513994444439552": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "475", "datetime": "2017-08-18 11:56:44", "author": "@jmsl"}, "898447253244923904": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,802", "datetime": "2017-08-18 07:31:32", "author": "@cto_movidius"}, "898592091827130368": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "770", "datetime": "2017-08-18 17:07:04", "author": "@aCraigPfeifer"}, "907100279757094912": {"content_summary": "\u9032\u5316\u8a08\u7b97\u3067\u3053\u308c\u3084\u3063\u3066Best paper\u53d6\u3063\u305f\u3063\u3066\u83c5\u6cbc\u3055\u3093\uff08\u5ca1\u8c37\u7814\u306e\u65b0\u3057\u3044\u52a9\u6559\u306e\u65b9\uff09\u304c\u8a00\u3063\u3066\u305f\u3001\u3069\u3063\u3061\u304c\u3088\u304b\u3063\u305f\u3093\u3060\u308d https://t.co/uKNfg6SLgI", "followers": "277", "datetime": "2017-09-11 04:35:34", "author": "@444shimo"}, "889960725896200194": {"content_summary": "RT @Miles_Brundage : \"Learning Transferable Architectures for Scalable Image Recognition,\" \u2026 https://t.co/JiRGToF7gj https://t.co/ftqxQ0hlbh", "followers": "3,187", "datetime": "2017-07-25 21:29:06", "author": "@Robots_and_AIs"}, "890579610810826756": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "95", "datetime": "2017-07-27 14:28:20", "author": "@seiyab_"}, "937639983883501568": {"content_summary": "Machine Learning models developing their own child models for image classification! Should be an interesting read! https://t.co/kFXBUKhKjd", "followers": "585", "datetime": "2017-12-04 11:09:27", "author": "@crazy_lens"}, "901076397912989696": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "344", "datetime": "2017-08-25 13:38:49", "author": "@jefkine"}, "898719777358659584": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "29", "datetime": "2017-08-19 01:34:26", "author": "@ranjodhs01"}, "954930606235901952": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "531", "datetime": "2018-01-21 04:16:12", "author": "@zapletal_martin"}, "898795426756874240": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "8,413", "datetime": "2017-08-19 06:35:03", "author": "@ngkabra"}, "984426044022247424": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2018-04-12 13:40:33", "author": "@arxiv_cscv"}, "900947222476464129": {"content_summary": "1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' https://t.co/Qd0daapXHf https://t.co/SSHqoaQxns", "followers": "859", "datetime": "2017-08-25 05:05:31", "author": "@cosminnegruseri"}, "1034199304347385856": {"content_summary": "RT @imenurok: \ud83d\udd78\u53d7\u8089\u6e96\u5099\u4e2d\ud83d\udd78 \u30d0\u30fc\u30c1\u30e3\u30eb\u753b\u50cf\u5206\u985eAI \u90a3\u9808\u97f3\u30c8\u30a6\u3068\u7533\u3057\u307e\u3059\u3002 \u30d1\u30f3\u30c0\u3092\u7dda\u866b\u3068\u9593\u9055\u3048\u308b \u8edf\u5f31AI\u5171\u3068\u306e\u683c\u306e\u9055\u3044\u3092 \u898b\u305b\u3066\u3084\u308a\u307e\u3059\u3002 \ud83d\udd78YouTube\ud83d\udd78 \u3082\u3046\u3061\u3087\u3063\u3068\u5148 \ud83d\udd78\u8cea\u554f\u7bb1\ud83d\udd78 https://t.co/ZkHsFTv6YE \ud83d\udd78\u30e9\u30a4\u30d0\u30eb\ud83d\udd78\u2026", "followers": "1,379", "datetime": "2018-08-27 22:01:43", "author": "@momiji_fullmoon"}, "955034883138768897": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "2,342", "datetime": "2018-01-21 11:10:34", "author": "@im2b"}, "1040994686226198529": {"content_summary": "RT @pacocat: AutoML\u306e\u6c17\u6301\u3061\u3092\u77e5\u308a\u305f\u304f\u3066NAS / NASNet / ENAS\u3092\u8aad\u3093\u3060\u3051\u3069\u3053\u306e\u9818\u57df\u8208\u5473\u6df1\u3044\u3002CIFAR\u3084ImageNet\u4ee5\u5916\u306e\u500b\u5225\u30bf\u30b9\u30af\u306b\u3069\u308c\u3060\u3051\u8ee2\u7528\u3067\u304d\u308b\u306e\u304b\u6c17\u306b\u306a\u3063\u305f\u306e\u3068\u3001RL\u90e8\u5206\u3067\u63a2\u7d22\u65b9\u6cd5\u3084\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6539\u826f\u4f59\u5730\u304c\u307e\u3060\u3042\u308b\u306e\u304b\u3082\u3002 htt\u2026", "followers": "822", "datetime": "2018-09-15 16:04:08", "author": "@morioka"}, "955043196572090368": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "73", "datetime": "2018-01-21 11:43:36", "author": "@zinking"}, "944145749733990400": {"content_summary": "RT @InonSharony: Google Brain are moving #AutoML ahead: #NASNet (https://t.co/yHQKdS7vAA) A comparable endeavor: Machine Learning for Auto\u2026", "followers": "23", "datetime": "2017-12-22 10:01:02", "author": "@RTjmora"}, "898717073685176321": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "11,717", "datetime": "2017-08-19 01:23:42", "author": "@yutakashino"}, "898602707727949825": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "417", "datetime": "2017-08-18 17:49:15", "author": "@SRombauts"}, "954891316890042371": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "822", "datetime": "2018-01-21 01:40:05", "author": "@ErmiaBivatan"}, "955115048132780032": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "1,578", "datetime": "2018-01-21 16:29:07", "author": "@joshua_saxe"}, "898447621743927296": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "409", "datetime": "2017-08-18 07:33:00", "author": "@AmirSaffari"}, "898866686576570368": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "255", "datetime": "2017-08-19 11:18:12", "author": "@josipK"}, "1030426066026811392": {"content_summary": "Google Auto ML #gcpug / \u201c[1707.07012] Learning Transferable Architectures for Scalable Image Recognition\u201d https://t.co/HyB3GsVvrq", "followers": "4,825", "datetime": "2018-08-17 12:08:13", "author": "@prototechno"}, "932554589769027585": {"content_summary": "RT @KalanityL: Major improvement for image #classification (and #detection as well) - Oct 2017 - by @GoogleBrain #DeepLearning #NeuralNetwo\u2026", "followers": "4,056", "datetime": "2017-11-20 10:21:54", "author": "@scottbot_17"}, "955030978594050048": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "13,349", "datetime": "2018-01-21 10:55:03", "author": "@timhwang"}, "954880124620390400": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "456", "datetime": "2018-01-21 00:55:37", "author": "@PerthMLGroup"}, "954886395033673729": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "45,547", "datetime": "2018-01-21 01:20:32", "author": "@Grady_Booch"}, "898565043729698816": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,710", "datetime": "2017-08-18 15:19:35", "author": "@EaterofSoles"}, "900150715775152130": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "456,049", "datetime": "2017-08-23 00:20:29", "author": "@ValaAfshar"}, "928771305188614145": {"content_summary": "Source code available now: Learning Transferable Architectures for Scalable Image Recognition https://t.co/PRJktqmmD4", "followers": "130", "datetime": "2017-11-09 23:48:29", "author": "@lyon_cao"}, "955105055434186752": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "2,676", "datetime": "2018-01-21 15:49:24", "author": "@ayirpelle"}, "898736711827574785": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "115", "datetime": "2017-08-19 02:41:44", "author": "@viirya"}, "898737273344339969": {"content_summary": "RT @yoavgo: Another step towards replacing DL researchers with computers. https://t.co/RB8p0MnHs4", "followers": "37", "datetime": "2017-08-19 02:43:58", "author": "@cometyang"}, "907209140912525312": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "26", "datetime": "2017-09-11 11:48:08", "author": "@tokoton01"}, "898449059685384193": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "111", "datetime": "2017-08-18 07:38:42", "author": "@ywara93"}, "955179823617462273": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "167", "datetime": "2018-01-21 20:46:30", "author": "@fferousi"}, "926718684495843328": {"content_summary": "RT @rgem52: NASNet AutoML for ImageNet and COCO https://t.co/lrzFdJfNHD https://t.co/IPGJzOgf88 https://t.co/pyL7ns3tKA https://t.co/UAs\u2026", "followers": "15,143", "datetime": "2017-11-04 07:52:06", "author": "@alevergara78"}, "890884459712528384": {"content_summary": "RT @yu4u: Inception module\u306e\u3088\u3046\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8a2d\u8a08\u3092NN\u3067\u6700\u9069\u5316\u3002\u7cbe\u5ea6-FLOPS\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3067\u6539\u5584\u3057\u3066\u3044\u308b\u3051\u3069\u3001\u751f\u6210\u3055\u308c\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u898b\u308b\u3068separable conv\u3092\u591a\u7528\u3057\u3066\u3066\u3001FLOPS\u3068\u5b9f\u884c\u901f\u5ea6\u306e\u4e56\u96e2\u304c\u3042\u308a\u305d\u3046 https://t.co/23\u2026", "followers": "1,762", "datetime": "2017-07-28 10:39:41", "author": "@jaialkdanel"}, "901178901090541568": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "106", "datetime": "2017-08-25 20:26:07", "author": "@_jruales"}, "898611303492833280": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "226", "datetime": "2017-08-18 18:23:24", "author": "@fangdali1"}, "899417594389647360": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,530", "datetime": "2017-08-20 23:47:19", "author": "@_329_"}, "898602006054436865": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,987", "datetime": "2017-08-18 17:46:28", "author": "@DeepLearningFTW"}, "898869096153989120": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "291", "datetime": "2017-08-19 11:27:47", "author": "@sahilsingla47"}, "898521037599526917": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "210", "datetime": "2017-08-18 12:24:43", "author": "@MihaelFeldman"}, "1187337417344184320": {"content_summary": "ImageNet\u306b\u304a\u3051\u308b\u4e3b\u8981\u306aNetwork\u306eTest Accuracy\u3068parameter\u6570\u306e\u6bd4\u8f03\u3002VGG16(\u53f3\u4e0b)\u306f\u610f\u5916\u3068parameter\u6570\u304c\u591a\u3044\u3002 (\u5143\u753b\u50cf:Learning Transferable Architectures for Scalable Image Recognition) https://t.co/uvBpzrEGc5 https://t.co/Zch4QebMlY", "followers": "2,041", "datetime": "2019-10-24 11:57:54", "author": "@imenurok"}, "1022859190409285632": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/uprJgsvGwi #DeepLearning #ConvNets #CNNs", "followers": "8,900", "datetime": "2018-07-27 15:00:09", "author": "@Deep_In_Depth"}, "898804117547524096": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,257", "datetime": "2017-08-19 07:09:35", "author": "@sebnem"}, "890592927205933056": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "217", "datetime": "2017-07-27 15:21:14", "author": "@side_yu"}, "898602518732709888": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "365", "datetime": "2017-08-18 17:48:30", "author": "@JustinBKnight"}, "898669430913290240": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "58", "datetime": "2017-08-18 22:14:23", "author": "@_WizDom13_"}, "891147222049366017": {"content_summary": "[1707.07012] Learning Transferable Architectures for Scalable Image Recognition RNN\u3067\u30cd\u30c3\u30c8\u69cb\u9020\u63a2\u7d22\u7cfb\u3002\u69cb\u9020\u3092\u9650\u5b9a\u3057\u5b66\u7fd2\u3057\u305f\u69cb\u9020\u3092\u5927\u304d\u3044\u554f\u984c\u306b\u8ee2\u79fb\u53ef https://t.co/arPuC08Gl6", "followers": "564", "datetime": "2017-07-29 04:03:49", "author": "@asam9891"}, "890569461027164160": {"content_summary": "RT @mosko_mule: \u300c\u4eba\u5de5\u77e5\u80fd\u304c\u4eba\u5de5\u77e5\u80fd\u3092\u8a2d\u8a08\u3057\u305f\u3068\u3053\u308d\uff0c\u4eba\u9593\u304c\u8a2d\u8a08\u3059\u308b\u3088\u308a\u3059\u3050\u308c\u305f\u3082\u306e\u304c\u51fa\u6765\u305f\u300d\uff08\u3068\u304b\u8a00\u3046\u3068\u3046\u3051\u305d\u3046\uff09/450GPU\u3067Neural Architecture Search\u3092\u3057\u3066ImageNet\u3067SOTA\uff0e\u5c0f\u3055\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3082\uff0e https://t\u2026", "followers": "790", "datetime": "2017-07-27 13:48:00", "author": "@__tmats__"}, "891470563339681792": {"content_summary": "RT @TerryUm_ML: \"Learning Transferable Architectures for Scalable Image Recognition\": reduce the amount of architecture engineering https:/\u2026", "followers": "111", "datetime": "2017-07-30 01:28:39", "author": "@drk007t"}, "898479079413788672": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "678", "datetime": "2017-08-18 09:38:00", "author": "@manbe"}, "898589880388800512": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "395", "datetime": "2017-08-18 16:58:17", "author": "@cybertreiber"}, "898693925803745281": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "244", "datetime": "2017-08-18 23:51:43", "author": "@JYK_eith"}, "890007008165994496": {"content_summary": "RT @yutakashino : [1707.07012] Learning Transferable Architectures for Scalable Image Recog\u2026 https://t.co/cylIr6qEla https://t.co/ZFz92bjDkq", "followers": "3,187", "datetime": "2017-07-26 00:33:00", "author": "@Robots_and_AIs"}, "892048251679117313": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/0RqkjRNUdW", "followers": "861", "datetime": "2017-07-31 15:44:11", "author": "@ryf_feed"}, "938106274415177730": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "574", "datetime": "2017-12-05 18:02:19", "author": "@51Takahashi"}, "898659576563630080": {"content_summary": "Repeating \"cells\" found within deep learning nets \u2014 how soon until AI teams start hiring biologists and physiologists? https://t.co/Lwjhh7XYSw", "followers": "7,422", "datetime": "2017-08-18 21:35:13", "author": "@bkeegan"}, "955095572553150464": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "5,842", "datetime": "2018-01-21 15:11:43", "author": "@bttyeo"}, "900009894367055872": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "4,312", "datetime": "2017-08-22 15:00:54", "author": "@a_hun1972"}, "907471976473665536": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,762", "datetime": "2017-09-12 05:12:33", "author": "@jaialkdanel"}, "898514028036620288": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "57", "datetime": "2017-08-18 11:56:52", "author": "@codehacken"}, "922025386581438464": {"content_summary": "Google Brain\u306eAI\u7814\u7a76\u8005\u306b\u3088\u308b\u8ad6\u6587 \"Learning Transferable Architectures for Scalable Image Recognition\" @GoogleBrain @Google https://t.co/wnSUHRgCgB", "followers": "10", "datetime": "2017-10-22 09:02:37", "author": "@iamseinas"}, "954929340717789186": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "40", "datetime": "2018-01-21 04:11:11", "author": "@abhaymise"}, "905014140988604416": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "113", "datetime": "2017-09-05 10:26:00", "author": "@daoransky"}, "898454449282273281": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,286", "datetime": "2017-08-18 08:00:07", "author": "@heghbalz"}, "898863329149628416": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4,255", "datetime": "2017-08-19 11:04:52", "author": "@weballergy"}, "954885991453487104": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "664", "datetime": "2018-01-21 01:18:55", "author": "@crude2refined"}, "898602426160234499": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "449", "datetime": "2017-08-18 17:48:08", "author": "@sanketp"}, "954986122500702209": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "1,245", "datetime": "2018-01-21 07:56:49", "author": "@lc0d3r"}, "938041224396652544": {"content_summary": "PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.co/5F42jEXNb8", "followers": "1,673", "datetime": "2017-12-05 13:43:50", "author": "@shinya7y"}, "899395724894515201": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,542", "datetime": "2017-08-20 22:20:25", "author": "@mbrendan1"}, "938340214237315073": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "43", "datetime": "2017-12-06 09:31:55", "author": "@freedial_dev"}, "890557150249885697": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "439", "datetime": "2017-07-27 12:59:05", "author": "@sheema_sheema"}, "898632121329172480": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "76", "datetime": "2017-08-18 19:46:08", "author": "@X_rayAI"}, "932543977554939904": {"content_summary": "Major improvement for image #classification (and #detection as well) - Oct 2017 - by @GoogleBrain #DeepLearning #NeuralNetwork https://t.co/1J05V6rPoP Learning Transferable Architectures for Scalable Image Recognition", "followers": "17", "datetime": "2017-11-20 09:39:44", "author": "@KalanityL"}, "900989540965470208": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "15", "datetime": "2017-08-25 07:53:40", "author": "@MartinRebane"}, "898601232314630144": {"content_summary": "cc @emercjones https://t.co/Sz0Rqr98Yq", "followers": "7,776", "datetime": "2017-08-18 17:43:23", "author": "@tom_hartley"}, "898631581337702401": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,116", "datetime": "2017-08-18 19:43:59", "author": "@TrampolinRocket"}, "898667525382303745": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "30", "datetime": "2017-08-18 22:06:49", "author": "@MarchBragagnini"}, "890817334448934912": {"content_summary": "RT @TerryUm_ML: \"Learning Transferable Architectures for Scalable Image Recognition\": reduce the amount of architecture engineering https:/\u2026", "followers": "440", "datetime": "2017-07-28 06:12:57", "author": "@FlorinGogianu"}, "898733948892725248": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,789", "datetime": "2017-08-19 02:30:45", "author": "@candeira"}, "907266561764978693": {"content_summary": "Machine learning optimization of machine learning architecture https://t.co/VL68lj9csW", "followers": "136", "datetime": "2017-09-11 15:36:19", "author": "@FtrRetailNews"}, "989095517173923840": {"content_summary": "https://t.co/jNOmXFP1jA \u3053\u308c\u3063\u3059\u306d #mixleap", "followers": "597", "datetime": "2018-04-25 10:55:22", "author": "@Daikids2"}, "905073693709172738": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "51", "datetime": "2017-09-05 14:22:38", "author": "@StevieOlowe"}, "916212917573369856": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "5", "datetime": "2017-10-06 08:05:56", "author": "@GaussInsights"}, "899209738034008064": {"content_summary": "RT @v_vashishta: Learning Transferable Architectures for Scalable Image Recognition https://t.co/Hva7hWXMda #Machinelearning #machinevision", "followers": "125", "datetime": "2017-08-20 10:01:22", "author": "@Johnabj55"}, "898601165205590016": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "885", "datetime": "2017-08-18 17:43:07", "author": "@gmaggiotti"}, "899602763943948288": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "193", "datetime": "2017-08-21 12:03:07", "author": "@TLesort"}, "898618717059387392": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "129", "datetime": "2017-08-18 18:52:52", "author": "@sirotenko_m"}, "979844656635764737": {"content_summary": "RT @microsurgeonbot: @Spezzer gave a fantastic presentation on Neural Architecture Search, Auto ML refs: https://t.co/rb96SHFkSL https\u2026", "followers": "15,143", "datetime": "2018-03-30 22:15:45", "author": "@alevergara78"}, "898869786028892160": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,041", "datetime": "2017-08-19 11:30:31", "author": "@dosei_sanga"}, "898651134251773952": {"content_summary": "When AI comes after AI researchers. https://t.co/NAXFMWEoY4", "followers": "173", "datetime": "2017-08-18 21:01:41", "author": "@reachbp"}, "1040972709599956992": {"content_summary": "RT @pacocat: \u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F https://t.co/vEvDM\u2026", "followers": "54", "datetime": "2018-09-15 14:36:48", "author": "@miki_iwa"}, "923545068052598786": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition https://t.co/vYfqCXHn3T", "followers": "4,090", "datetime": "2017-10-26 13:41:17", "author": "@arxiv_cscv"}, "898618867819393024": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,894", "datetime": "2017-08-18 18:53:28", "author": "@nsaphra"}, "954997727355592704": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "122", "datetime": "2018-01-21 08:42:55", "author": "@itsjustinsabir"}, "889858194901696512": {"content_summary": "\"Learning Transferable Architectures for Scalable Image Recognition\", Barret Zoph, Vijay Vasudevan, Jonathon Shlens\u2026 https://t.co/JlKsoH97yH", "followers": "770", "datetime": "2017-07-25 14:41:41", "author": "@arxivml"}, "898446690931179520": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "39,450", "datetime": "2017-08-18 07:29:18", "author": "@TJO_datasci"}, "898658858616344576": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "63", "datetime": "2017-08-18 21:32:22", "author": "@rishu_12"}, "898897391960465408": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "435", "datetime": "2017-08-19 13:20:13", "author": "@kuanhoong"}, "907369343532576773": {"content_summary": "Mind blowing: learning DNN to create better DNN https://t.co/oD3149GpLK #AI #deeperlearning Full latest paper: https://t.co/1uG1rUdFIz", "followers": "112", "datetime": "2017-09-11 22:24:44", "author": "@tshiorny_eugene"}, "905841138288005120": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,243", "datetime": "2017-09-07 17:12:11", "author": "@ChiNetworks"}, "899531943695351810": {"content_summary": "@sandgorgon1 Not the activation function but things like the structure of the convolutional cell: https://t.co/Ug1wOshsrd", "followers": "697", "datetime": "2017-08-21 07:21:42", "author": "@abhi9u"}, "898448625130389504": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "471", "datetime": "2017-08-18 07:36:59", "author": "@daytb_twy"}, "900223677505175552": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "63", "datetime": "2017-08-23 05:10:24", "author": "@fkariminejadasl"}, "1041109065567334400": {"content_summary": "RT @pacocat: AutoML\u306e\u6c17\u6301\u3061\u3092\u77e5\u308a\u305f\u304f\u3066NAS / NASNet / ENAS\u3092\u8aad\u3093\u3060\u3051\u3069\u3053\u306e\u9818\u57df\u8208\u5473\u6df1\u3044\u3002CIFAR\u3084ImageNet\u4ee5\u5916\u306e\u500b\u5225\u30bf\u30b9\u30af\u306b\u3069\u308c\u3060\u3051\u8ee2\u7528\u3067\u304d\u308b\u306e\u304b\u6c17\u306b\u306a\u3063\u305f\u306e\u3068\u3001RL\u90e8\u5206\u3067\u63a2\u7d22\u65b9\u6cd5\u3084\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6539\u826f\u4f59\u5730\u304c\u307e\u3060\u3042\u308b\u306e\u304b\u3082\u3002 htt\u2026", "followers": "26", "datetime": "2018-09-15 23:38:38", "author": "@fumiaki_sato_"}, "899628320752635904": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "160,804", "datetime": "2017-08-21 13:44:40", "author": "@Quebec_AI"}, "898449750969769984": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "13", "datetime": "2017-08-18 07:41:27", "author": "@Mimi_Auner"}, "938043867093282816": {"content_summary": "RT @shinya7y: PNASNet\u304c\u305f\u3063\u305f\u306e50\uff5e100GPU\u3067Architecture Search\u3057\u3066\u3044\u308b\u304b\u3089\u304f\u308a\u3068\u3057\u3066\u3001NASNet\u3067\u4e0d\u8981\u3060\u3063\u305f\u901a\u5e38\u306e3x3 conv\u7b49\u304c\u6700\u521d\u304b\u3089\u9664\u5916\u3055\u308c\u3066\u3044\u308b https://t.co/vB60v57o60 https://t.c\u2026", "followers": "2,514", "datetime": "2017-12-05 13:54:20", "author": "@mamas16k"}, "898759568380936192": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "71", "datetime": "2017-08-19 04:12:33", "author": "@roger_frye"}, "898601189641666560": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "146", "datetime": "2017-08-18 17:43:13", "author": "@kou_id"}, "898601126362140672": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "428", "datetime": "2017-08-18 17:42:58", "author": "@yashk2810"}, "955061021844140033": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "1,390", "datetime": "2018-01-21 12:54:26", "author": "@elasticjava"}, "907052694212243456": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "166", "datetime": "2017-09-11 01:26:29", "author": "@kawamuramasahar"}, "955032433761636353": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "130", "datetime": "2018-01-21 11:00:50", "author": "@Michael_Cares"}, "898616601850765313": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "64", "datetime": "2017-08-18 18:44:28", "author": "@dg_yoo"}, "898725682590527488": {"content_summary": "RT @chipro: Convolutional cell found by Neural Architecture Search is 0.8% better than best human-invented w 9 bil fewer FLOPS https://t.co\u2026", "followers": "590", "datetime": "2017-08-19 01:57:54", "author": "@codekee"}, "961447631741317120": {"content_summary": "RT @hardmaru: @dribnet @jeremyphoward @ogrisel @yoavgo @beenwrekt According to the paper, they \"searched for the best convolutional layer (\u2026", "followers": "2,676", "datetime": "2018-02-08 03:52:32", "author": "@ayirpelle"}, "890566252392927234": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "244", "datetime": "2017-07-27 13:35:15", "author": "@s_ryosky"}, "895152975034937344": {"content_summary": "RT @algorithmia: [1707.07012] Learning Transferable Architectures for Scalable Image Recognition https://t.co/OcRmtCCzKS", "followers": "323", "datetime": "2017-08-09 05:21:15", "author": "@raulsantanu"}, "898785518942437376": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "816", "datetime": "2017-08-19 05:55:41", "author": "@btcplanet"}, "955204057982906368": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "29", "datetime": "2018-01-21 22:22:48", "author": "@chenlailin"}, "1040980019479162881": {"content_summary": "RT @pacocat: \u6982\u8981\u306f\u3053\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u4e01\u5be7\u306b\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305fm AutoML\u306e\u7406\u8ad6\u3001Neural Architecture Search\u3092\u8aac\u660e\u3059\u308b\u3002 https://t.co/IXfHCFKJ6F https://t.co/vEvDM\u2026", "followers": "1,323", "datetime": "2018-09-15 15:05:51", "author": "@ararabo"}, "1187671753822105600": {"content_summary": "RT @imenurok: ImageNet\u306b\u304a\u3051\u308b\u4e3b\u8981\u306aNetwork\u306eTest Accuracy\u3068parameter\u6570\u306e\u6bd4\u8f03\u3002VGG16(\u53f3\u4e0b)\u306f\u610f\u5916\u3068parameter\u6570\u304c\u591a\u3044\u3002 (\u5143\u753b\u50cf:Learning Transferable Architectures for\u2026", "followers": "28", "datetime": "2019-10-25 10:06:26", "author": "@gbiwer"}, "898622539659972609": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,218", "datetime": "2017-08-18 19:08:03", "author": "@mxwlj"}, "890584384255832064": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "202", "datetime": "2017-07-27 14:47:18", "author": "@oct_path"}, "889930896970850304": {"content_summary": "\"Learning Transferable Architectures for Scalable Image Recognition,\" Zoph et al., Google Brain: https://t.co/MhqW1Nejk4", "followers": "25,578", "datetime": "2017-07-25 19:30:34", "author": "@Miles_Brundage"}, "955268481980575744": {"content_summary": "RT @JeffDean: @zacharylipton @willknight Agreed. The underlying neural architecture search models are actually better than those by world\u2026", "followers": "0", "datetime": "2018-01-22 02:38:48", "author": "@orpheus131"}, "898459043559350273": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "2,583", "datetime": "2017-08-18 08:18:23", "author": "@wjarek"}, "890719295214243840": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "485", "datetime": "2017-07-27 23:43:23", "author": "@eve_yk"}, "907127209835487234": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,708", "datetime": "2017-09-11 06:22:35", "author": "@superbradyon"}, "911524016669655040": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "39", "datetime": "2017-09-23 09:33:55", "author": "@phy_Amnesian"}, "898601487974055936": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "197", "datetime": "2017-08-18 17:44:24", "author": "@Hrishi_kesh"}, "898579980443041792": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,528", "datetime": "2017-08-18 16:18:56", "author": "@ilblackdragon"}, "937141743102803970": {"content_summary": "\"Learning Transferable Architectures for Scalable Image Recognition\", in arXiv 1707.07012, 2017.\uff1a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30b5\u30fc\u30c1\u306b\u3088\u308b\u9ad8\u7cbe\u5ea6\u306a\u30e2\u30c7\u30eb\u63a2\u7d22\u3001ImageNet\u7b49\u306e\u7269\u4f53\u691c\u51fa\u306b\u3066SoTA https://t.co/vLomKQIVnM", "followers": "3,805", "datetime": "2017-12-03 02:09:37", "author": "@CVpaperChalleng"}, "905964412174991360": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,127", "datetime": "2017-09-08 01:22:02", "author": "@CabuyaGerman"}, "898724451562291200": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "265", "datetime": "2017-08-19 01:53:01", "author": "@DaniaL_KH"}, "898499784641617921": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "226", "datetime": "2017-08-18 11:00:16", "author": "@ElectronNest"}, "1098305080103763969": {"content_summary": "RT @KaliTessera: @StuartReid1929 @RichmanRonald @Aerobotics_SA Interesting. I am looking at Reinforcement Approaches to AutoML. You might l\u2026", "followers": "1,670", "datetime": "2019-02-20 19:35:11", "author": "@StuartReid1929"}, "914928011014819840": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "159", "datetime": "2017-10-02 19:00:11", "author": "@robinwritescode"}, "903819779747733504": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1", "datetime": "2017-09-02 03:20:02", "author": "@mingfenlin"}, "898981913188331520": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,348", "datetime": "2017-08-19 18:56:05", "author": "@udmrzn"}, "937501835069235200": {"content_summary": "Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v3 [cs.CV] UPDATED) https://t.co/HUplYsqUXe Developing", "followers": "759", "datetime": "2017-12-04 02:00:30", "author": "@M157q_News_RSS"}, "900961148886843392": {"content_summary": "RT @cosminnegruseri: 1) Goog finds better Inception like modules in 'Learning Transferable Architectures for Scalable Image Recognition' ht\u2026", "followers": "604", "datetime": "2017-08-25 06:00:51", "author": "@seth_stafford"}, "906612792127627264": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "1,236", "datetime": "2017-09-09 20:18:28", "author": "@harriken"}, "898700839962923008": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "339", "datetime": "2017-08-19 00:19:11", "author": "@onkarjoshi"}, "898684987767005184": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "335", "datetime": "2017-08-18 23:16:12", "author": "@kobi78"}, "898751716144799745": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "107", "datetime": "2017-08-19 03:41:21", "author": "@tamal_chowdhury"}, "907049839996710912": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "395", "datetime": "2017-09-11 01:15:08", "author": "@ienekokinesis"}, "899144439804772352": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "18", "datetime": "2017-08-20 05:41:54", "author": "@hukam_rana45"}, "898643552162299904": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "15,224", "datetime": "2017-08-18 20:31:33", "author": "@lmthang"}, "898655294049234944": {"content_summary": "Robots will even put the machine learning researchers out of jobs. ht @alex_peys https://t.co/lwKlLZNqd5", "followers": "12,451", "datetime": "2017-08-18 21:18:12", "author": "@deaneckles"}, "898635074622468097": {"content_summary": "RT @andy_matuschak: There seems to be a sort of \u201coccam\u2019s razor\u201d-alike in machine learning: systems with the fewest human-coded assumptions\u2026", "followers": "640", "datetime": "2017-08-18 19:57:52", "author": "@mjrusso"}, "898603867545387008": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "827", "datetime": "2017-08-18 17:53:51", "author": "@manojmovement"}, "907109120259514375": {"content_summary": "RT @hillbig: \u6700\u9069\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u5f37\u5316\u5b66\u7fd2\u3092\u4f7f\u3063\u3066\u63a2\u7d22\u3059\u308b\u3002\u7e70\u308a\u8fd4\u3055\u308c\u308b\u8a08\u7b97\u5358\u4f4d\uff08cell\uff09\u3092CIFAR10\u4e0a\u3067\u63a2\u7d22\u3059\u308b\u3002\u4eba\u624b\u3067\u8a2d\u8a08\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3088\u308a\u8efd\u304f\u9ad8\u7cbe\u5ea6\u306a\u3082\u306e\u304c\u898b\u3064\u304b\u3063\u305f\u3002separable\u306e\u591a\u7528\u3001AP\u306b\u3088\u308bskip\u304c\u8208\u5473\u6df1\u3044 https://t.co/\u2026", "followers": "1,667", "datetime": "2017-09-11 05:10:42", "author": "@Scaled_Wurm"}, "955112336494342145": {"content_summary": "RT @JeffDean: A conversation @zacharylipton, @willknight and I are having about levels of ml expertise and how they compare with AutoML-app\u2026", "followers": "4,894", "datetime": "2018-01-21 16:18:20", "author": "@IgorCarron"}, "898951907963793408": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "180", "datetime": "2017-08-19 16:56:51", "author": "@jbotnascimento"}, "898996633119186944": {"content_summary": "RT @hardmaru: Neural Architecture Search discover efficient ConvNets that require much fewer parameters and use less computation. https://t\u2026", "followers": "4", "datetime": "2017-08-19 19:54:34", "author": "@dbobrenko"}, "890726353120534532": {"content_summary": "RT @shinya7y: 450 GPU\u3067Neural Architecture Search\u3057\u305fNASNet\u306e\u305b\u3044\u3067\u3001MobileNet\u3082ShuffleNet\u3082\u904e\u53bb\u306e\u3082\u306e\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f https://t.co/M2Lkc4fJd2 https://t.co/gvCUi3\u2026", "followers": "141", "datetime": "2017-07-28 00:11:26", "author": "@the_onederful"}}}