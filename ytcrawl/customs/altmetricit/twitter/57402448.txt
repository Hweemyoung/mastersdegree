{"tab": "twitter", "queriedAt": "2020-05-21 20:27:30", "citation_id": "57402448", "twitter": {"1110127123342258176": {"datetime": "2019-03-25 10:31:46", "followers": "13,720", "author": "@Pardoe_AI", "content_summary": "RT @daniwi79: As mentioned in my talk Untangling Information for #MachineLearning and #Deeplearning Training for #DataScientists @nvidia @\u2026"}, "1111415426703974403": {"datetime": "2019-03-28 23:51:01", "followers": "122", "author": "@hasansaikatt", "content_summary": "RT @daniwi79: @JeffDean @GoogleAI @berkeley_ai Excellence in collaboration with @GoogleAI! Hope to see collaboration with #Africa and in pa\u2026"}, "1178370674181726214": {"datetime": "2019-09-29 18:07:16", "followers": "210", "author": "@daniwi79", "content_summary": "@vnfrombucharest @xtimv @mblondel_ml Subgradients and stochastic descent generally converges to balls around non-negative gradient projection points Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over functi"}, "1108542035919806467": {"datetime": "2019-03-21 01:33:11", "followers": "3,891", "author": "@BrundageBot", "content_summary": "Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minimizers in mini-batch sampled loss functions. Dominic Kafka and Daniel Wilke https://t.co/ZPV9gDVbFD"}, "1109084763476971521": {"datetime": "2019-03-22 13:29:48", "followers": "11", "author": "@Richard_d____", "content_summary": "RT @Schlymhaut: Our first venture into the arxiv. Check out a fresh take on finding optima in stochastic loss functions. @daniwi79 @Younghw\u2026"}, "1132026001364574208": {"datetime": "2019-05-24 20:50:05", "followers": "210", "author": "@daniwi79", "content_summary": "#AfricaDay2019 shout out to #ResearcherInAfrica @Schlymhaut for his dedication in realizing lots of clarity in #MachineLearning and #DeepLearning training - for a visual journey of these ideas checkout https://t.co/smgKBmZLTI @NRF_News @UPTuks @vukosi"}, "1132953111679709184": {"datetime": "2019-05-27 10:14:05", "followers": "23,140", "author": "@NRF_News", "content_summary": "RT @daniwi79: #AfricaDay2019 shout out to #ResearcherInAfrica @Schlymhaut for his dedication in realizing lots of clarity in #MachineLearni\u2026"}, "1108996776428736512": {"datetime": "2019-03-22 07:40:10", "followers": "27", "author": "@Schlymhaut", "content_summary": "RT @StatMLPapers: Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minimizers i\u2026"}, "1178368055660990465": {"datetime": "2019-09-29 17:56:52", "followers": "210", "author": "@daniwi79", "content_summary": "@xtimv @bremen79 Here are some more details when considering dynamic mini-batch sampled loss functions and the ball Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minimizers in mini-batch sampl"}, "1109296224270733312": {"datetime": "2019-03-23 03:30:04", "followers": "210", "author": "@daniwi79", "content_summary": "RT @Schlymhaut: Our first venture into the arxiv. Check out a fresh take on finding optima in stochastic loss functions. @daniwi79 @Younghw\u2026"}, "1110121764661723137": {"datetime": "2019-03-25 10:10:28", "followers": "210", "author": "@daniwi79", "content_summary": "As mentioned in my talk Untangling Information for #MachineLearning and #Deeplearning Training for #DataScientists @nvidia @NvidiaAI #GTC19 #GTC2019 our two papers: https://t.co/wXUrgj4Wy5 and https://t.co/DdG2nrK0sa #PyTorch and #TensorFlow code is comin"}, "1108534131649064961": {"datetime": "2019-03-21 01:01:47", "followers": "9,721", "author": "@StatMLPapers", "content_summary": "Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minimizers in mini-batch sampled loss functions. (arXiv:1903.08552v1 [https://t.co/zjV5HgYw5a]) https://t.co/YMFEoaT6uw"}, "1110124752759336960": {"datetime": "2019-03-25 10:22:20", "followers": "210", "author": "@daniwi79", "content_summary": "@JeffDean @GoogleAI @berkeley_ai Excellence in collaboration with @GoogleAI! Hope to see collaboration with #Africa and in particular #SouthAfrica growing with institutes like @UPTuks adding to the diversity of thought and understanding https://t.co/wXUrgj"}, "1131126462206152704": {"datetime": "2019-05-22 09:15:38", "followers": "210", "author": "@daniwi79", "content_summary": "@annaearwen You can also see a visual version of these ideas under https://t.co/smgKBmZLTI with a recent presentation in San Fransisco, USA following https://t.co/6ELQy9EN5n"}, "1149012402899492866": {"datetime": "2019-07-10 17:47:59", "followers": "210", "author": "@daniwi79", "content_summary": "@hardmaru Hope to see Gradient-Only Optimization & NN-GPP to form a small part of the next piece in the puzzle in the future, or being at least an alternative set of glasses to view the problem: https://t.co/DdG2nrK0sa https://t.co/wXUrgj4Wy5 https://t"}, "1108686117954453506": {"datetime": "2019-03-21 11:05:43", "followers": "5,454", "author": "@StatsPapers", "content_summary": "Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minimizers in mini-batch sampled loss functions. https://t.co/AeSBDRHXgX"}, "1108619600374259712": {"datetime": "2019-03-21 06:41:24", "followers": "780", "author": "@arxivml", "content_summary": "\"Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minim\u2026 https://t.co/q2stQtGO67"}, "1132037929994457093": {"datetime": "2019-05-24 21:37:29", "followers": "17,124", "author": "@Africa25Day", "content_summary": "RT @daniwi79: #AfricaDay2019 shout out to #ResearcherInAfrica @Schlymhaut for his dedication in realizing lots of clarity in #MachineLearni\u2026"}, "1108996724700467200": {"datetime": "2019-03-22 07:39:57", "followers": "27", "author": "@Schlymhaut", "content_summary": "Our first venture into the arxiv. Check out a fresh take on finding optima in stochastic loss functions. @daniwi79 @YounghwanChae"}, "1149018671450329088": {"datetime": "2019-07-10 18:12:53", "followers": "210", "author": "@daniwi79", "content_summary": "@roydanroy @philippec Thank you, I look forward to see how our recent work on Gradient-Only Optimization & NN-GPPs fits into this variational framework. Feel free to open a discussion if you have any ideas: https://t.co/DdG2nrK0sa https://t.co/wXUrgj4W"}, "1132953256211173377": {"datetime": "2019-05-27 10:14:40", "followers": "9,568", "author": "@machine_ml", "content_summary": "RT @NRF_News: RT @daniwi79: #AfricaDay2019 shout out to #ResearcherInAfrica @Schlymhaut for his dedication in realizing lots of clarity in #MachineLearning and #DeepLearning training - for a visual journey of these ideas checkout https://t.co/IvUYlGJxFP @N"}, "1109164634655854598": {"datetime": "2019-03-22 18:47:10", "followers": "318", "author": "@arxiv_cs_LG", "content_summary": "Visual interpretation of the robustness of Non-Negative Associative Gradient Projection Points over function minimizers in mini-batch sampled loss functions. Dominic Kafka and Daniel Wilke https://t.co/DnjqTrU42s"}}, "completed": "1"}