{"citation_id": "67776968", "completed": "1", "queriedAt": "2020-05-14 12:31:13", "tab": "twitter", "twitter": {"1229360110062927872": {"content_summary": "[1/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,605 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-17 11:01:05", "author": "@PapersTrending"}, "1228997639728959489": {"content_summary": "[1/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,583 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-16 11:00:45", "author": "@PapersTrending"}, "1227494976214335489": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "162", "datetime": "2020-02-12 07:29:42", "author": "@kurama554101"}, "1230809837317038080": {"content_summary": "[4/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,819 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-21 11:01:47", "author": "@PapersTrending"}, "1227448949780422656": {"content_summary": "RT @ankurhandos: ZeRO optimiser used in the recent deepspeech allows training really large models with clever memory optimisations https:/\u2026", "followers": "784", "datetime": "2020-02-12 04:26:49", "author": "@muktabh"}, "1230447505676161024": {"content_summary": "[5/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,789 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-20 11:02:00", "author": "@PapersTrending"}, "1227282983767248898": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "727", "datetime": "2020-02-11 17:27:19", "author": "@tsubame959"}, "1227796381709950976": {"content_summary": "@erikbryn @MSFTResearch @danielrock Definitely seems to be decreasing returns to model scale. The real story may actually be operationalizing DeepSpeed + ZeRO, laying down a path to 1TN params. https://t.co/UaHzJo7Gbh https://t.co/WQ5wWKdBk0", "followers": "473", "datetime": "2020-02-13 03:27:23", "author": "@zubinwadia"}, "1227383255776813056": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "773", "datetime": "2020-02-12 00:05:46", "author": "@kuritateppei"}, "1227558167891992576": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "949", "datetime": "2020-02-12 11:40:48", "author": "@hkawaguc"}, "1229722602219954182": {"content_summary": "[2/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,697 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-18 11:01:30", "author": "@PapersTrending"}, "1181061611450060800": {"content_summary": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models ... reduces memory and boosts model size by 4x compared with the state-of-art, scaling up to 100B parameters. https://t.co/oVQwFh8M7L", "followers": "317", "datetime": "2019-10-07 04:20:05", "author": "@arankomatsuzaki"}, "1227473736036683776": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "1,333", "datetime": "2020-02-12 06:05:18", "author": "@alfredplpl"}, "1246386402369175554": {"content_summary": "@smkalami \u0628\u0627 \u0634\u0645\u0627 \u0645\u0648\u0627\u0641\u0642\u0645\u060c \u0627\u0645\u0631\u0648\u0632 \u06cc\u06a9 \u0645\u0642\u0627\u0644\u0647 \u062c\u062f\u06cc\u062f \u062f\u0631 \u0627\u06cc\u0646 \u062d\u0648\u0632\u0647 \u062e\u0648\u0627\u0646\u062f\u0645: Training Large Neural Networks with Constant Memory using a New Execution Algorithm https://t.co/7yKd8rfUYh \u0648 \u0627\u06cc\u0646 \u0647\u0645 \u0645\u0642\u0627\u0644\u0647 \u0645\u0631\u062a\u0628\u0637 \u0628\u0627 \u067e\u0633\u062a \u0634\u0645\u0627: https://t.co/5ACdeRqecC", "followers": "98", "datetime": "2020-04-04 10:37:29", "author": "@Koo_ec"}, "1227910585276301312": {"content_summary": "[1/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,100 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-13 11:01:11", "author": "@PapersTrending"}, "1227381289398878208": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "167", "datetime": "2020-02-11 23:57:57", "author": "@samurairodeo"}, "1227383915028992001": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "2,312", "datetime": "2020-02-12 00:08:23", "author": "@dandymania_tw"}, "1227385149177778176": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "163", "datetime": "2020-02-12 00:13:17", "author": "@okajax_me"}, "1227269270582759424": {"content_summary": "RT @ankurhandos: ZeRO optimiser used in the recent deepspeech allows training really large models with clever memory optimisations https:/\u2026", "followers": "61", "datetime": "2020-02-11 16:32:50", "author": "@dave_co_dev"}, "1227274580676485120": {"content_summary": "RT @ankurhandos: ZeRO optimiser used in the recent deepspeech allows training really large models with clever memory optimisations https:/\u2026", "followers": "424", "datetime": "2020-02-11 16:53:56", "author": "@jp_axs4ll"}, "1233242750956589056": {"content_summary": "Roundup: Scaling laws for language models: https://t.co/ggxMlZDqMr Training a trillion parameter model: https://t.co/CaHbvhhSIh Improving GANs with compressed sensing: https://t.co/GuuhJkfkmS", "followers": "0", "datetime": "2020-02-28 04:09:19", "author": "@academic_spam"}, "1181023916879482881": {"content_summary": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He https://t.co/8TT7fR6xNm", "followers": "3,850", "datetime": "2019-10-07 01:50:18", "author": "@BrundageBot"}, "1181238146056511488": {"content_summary": "RT @arankomatsuzaki: ZeRO: Memory Optimization Towards Training A Trillion Parameter Models ... reduces memory and boosts model size by 4x\u2026", "followers": "424", "datetime": "2019-10-07 16:01:35", "author": "@iamknighton"}, "1227391009354473472": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "971", "datetime": "2020-02-12 00:36:35", "author": "@WUGner_matthew"}, "1227809768971984900": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "7,654", "datetime": "2020-02-13 04:20:35", "author": "@ynupc"}, "1227510203177787392": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "368", "datetime": "2020-02-12 08:30:13", "author": "@y_shindoh"}, "1227430380996722688": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "620", "datetime": "2020-02-12 03:13:02", "author": "@Eseshinpu"}, "1229289592261472257": {"content_summary": "\u8aad\u3093\u3060 / 1\u4ef6\u306e\u30b3\u30e1\u30f3\u30c8 https://t.co/EnxTddL2xV (1 user) https://t.co/LCwatZn16g", "followers": "162", "datetime": "2020-02-17 06:20:52", "author": "@argv_sat184"}, "1227388806451458048": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "1,920", "datetime": "2020-02-12 00:27:49", "author": "@dahatake"}, "1227248114676822016": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "1,117", "datetime": "2020-02-11 15:08:46", "author": "@minjinaffa"}, "1227411484453425154": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "8", "datetime": "2020-02-12 01:57:56", "author": "@sincreth"}, "1227381629942845440": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "1,670", "datetime": "2020-02-11 23:59:18", "author": "@Scaled_Wurm"}, "1226922383988154369": {"content_summary": "Currently reading about the Zero Redundancy Optimizer (ZeRO) from @MSFTResearch. Truly inspired work. Bonus that the manuscript is well-written and easy to follow. https://t.co/ddzpV070Lb", "followers": "609", "datetime": "2020-02-10 17:34:26", "author": "@thomasjo"}, "1227392781443895296": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "768", "datetime": "2020-02-12 00:43:37", "author": "@DUXROLL"}, "1231172275455582209": {"content_summary": "[7/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,854 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-22 11:01:59", "author": "@PapersTrending"}, "1228940480504709122": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "364", "datetime": "2020-02-16 07:13:37", "author": "@dl_from_scratch"}, "1227250024645480449": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "565", "datetime": "2020-02-11 15:16:21", "author": "@yuma_koizumi"}, "1227518875228491777": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "287", "datetime": "2020-02-12 09:04:40", "author": "@rose_miura"}, "1227526260601114625": {"content_summary": "\u0414\u043b\u044f \u0442\u0435\u0445, \u043a\u0442\u043e \u043f\u043e\u0442\u0440\u0430\u0442\u0438\u043b \u0432\u0441\u044e \u043f\u0430\u043c\u044f\u0442\u044c \u043d\u0430 \u0437\u0430\u043f\u0438\u0445\u0438\u0432\u0430\u043d\u0438\u0435 \u0432 \u043b\u0438\u0441\u0442\u044b https://t.co/UmBjz6w6jH", "followers": "1", "datetime": "2020-02-12 09:34:01", "author": "@dmevdok"}, "1227242638774087680": {"content_summary": "#ml #nlp Cool new nlp model from Microsoft hn: https://t.co/RD4MEjcfWx detailed blog post: https://t.co/rBsuwK6dAw DeepSpeed: https://t.co/Tg2tZwfsna ZeRO optimizer: https://t.co/xt7WKDONrn https://t.co/4oVuh6ISKv", "followers": "367", "datetime": "2020-02-11 14:47:00", "author": "@acgt01"}, "1226967547561549824": {"content_summary": "@Azure 2/ Here is the paper on ZeRO. https://t.co/0PiFdXYO3j", "followers": "220", "datetime": "2020-02-10 20:33:53", "author": "@zenlytix"}, "1227378380288323584": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "826", "datetime": "2020-02-11 23:46:24", "author": "@morioka"}, "1227262291172741120": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "633", "datetime": "2020-02-11 16:05:06", "author": "@ChagallBlau"}, "1231534687572692993": {"content_summary": "[10/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,865 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-23 11:02:04", "author": "@PapersTrending"}, "1228635295148576768": {"content_summary": "[1/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,497 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-15 11:00:55", "author": "@PapersTrending"}, "1227548143979499520": {"content_summary": "[1/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 875 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-12 11:00:58", "author": "@PapersTrending"}, "1227380971340587008": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "593", "datetime": "2020-02-11 23:56:41", "author": "@usagisan2020"}, "1181057174400901122": {"content_summary": "\"ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\", Samyam Rajbhandari, Jeff Rasley, Olatunji\u2026 https://t.co/mGuWDqbKRk", "followers": "769", "datetime": "2019-10-07 04:02:28", "author": "@arxivml"}, "1228640042232344578": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "903", "datetime": "2020-02-15 11:19:47", "author": "@wk77"}, "1181034598450352128": {"content_summary": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He https://t.co/YcH6v5cDkL", "followers": "307", "datetime": "2019-10-07 02:32:45", "author": "@arxiv_cs_LG"}, "1237203991152320512": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "0", "datetime": "2020-03-10 02:29:52", "author": "@kuuml1"}, "1227394626069286912": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "224", "datetime": "2020-02-12 00:50:57", "author": "@MarcyWorkOut"}, "1227247871545577473": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "3", "datetime": "2020-02-11 15:07:48", "author": "@luda_green"}, "1227247380606533632": {"content_summary": "Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.co/48LeU96GNj \u3092\u767a\u8868\uff0e\u5185\u90e8\u3067\u30c7\u30fc\u30bf/\u30e2\u30c7\u30eb\u4e26\u5217\u5316\u3092\u9ad8\u901f/\u52b9\u7387\u5316\u3059\u308b\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6 ZeRO https://t.co/93av2XL4Ik \u3092\u5229\u7528\uff0e https://t.co/W7cpB1yFM4", "followers": "2,439", "datetime": "2020-02-11 15:05:51", "author": "@kyoun"}, "1227448268457758722": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "252", "datetime": "2020-02-12 04:24:06", "author": "@HydryHydra"}, "1227437368862552064": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "208", "datetime": "2020-02-12 03:40:48", "author": "@HYO_GO"}, "1227191227126353920": {"content_summary": "Wha https://t.co/NhDbusuQTR Trillion parameter models. #mindblown #ai", "followers": "568", "datetime": "2020-02-11 11:22:43", "author": "@andrewgrosser"}, "1227268262649425922": {"content_summary": "ZeRO optimiser used in the recent deepspeech allows training really large models with clever memory optimisations https://t.co/T4vSbKsLg5 https://t.co/JKv6sUZKVQ", "followers": "5,042", "datetime": "2020-02-11 16:28:50", "author": "@ankurhandos"}, "1227346522758467584": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "464", "datetime": "2020-02-11 21:39:48", "author": "@8kazu3"}, "1181760671697924096": {"content_summary": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He https://t.co/YcH6v5cDkL", "followers": "307", "datetime": "2019-10-09 02:37:54", "author": "@arxiv_cs_LG"}, "1227363513074536448": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "1", "datetime": "2020-02-11 22:47:19", "author": "@haradatm"}, "1230085114010505216": {"content_summary": "[3/10] \ud83d\udcc8 - ZeRO: Memory Optimization Towards Training A Trillion Parameter Models - 1,721 \u2b50 - \ud83d\udcc4 https://t.co/TgK96PZaeb - \ud83d\udd17 https://t.co/XK8s2Tj1rv", "followers": "219", "datetime": "2020-02-19 11:01:59", "author": "@PapersTrending"}, "1227251663531999233": {"content_summary": "RT @kyoun: Microsoft\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u657017B\uff0878\u5c64/4256\u6b21\u5143Transformer\uff09\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb Turing-NLG https://t.co/29ujots6ek \u3068\uff0c\u305d\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3057\u305fPyTorch\u30e9\u30c3\u30d1\u30fc DeepSpeed https://t.\u2026", "followers": "1,154", "datetime": "2020-02-11 15:22:52", "author": "@jd_mashiro"}}}