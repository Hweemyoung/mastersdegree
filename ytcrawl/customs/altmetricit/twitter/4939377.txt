{"citation_id": "4939377", "completed": "1", "queriedAt": "2020-05-14 13:11:53", "tab": "twitter", "twitter": {"682450338792652800": {"content_summary": "https://t.co/XaCUXtTXL3 Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "followers": "150", "datetime": "2015-12-31 06:36:54", "author": "@y_yammt"}, "1029008035912531968": {"content_summary": "2 - We leverage attention's ability to model loosely ordered sequences, as shown in @colinraffel's https://t.co/2pEks4rltP to predict the number of primary vertices in an event from VP readouts. 4/10 https://t.co/GuUYZES6Cw", "followers": "529", "datetime": "2018-08-13 14:13:28", "author": "@dtsbourg"}, "684807976515813376": {"content_summary": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems https://t.co/LcpwC0Ne80", "followers": "2,927", "datetime": "2016-01-06 18:45:19", "author": "@evolvingstuff"}}}