{"citation_id": "22242488", "completed": "1", "queriedAt": "2020-05-14 13:48:35", "tab": "twitter", "twitter": {"1000268433924145152": {"content_summary": "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control https://t.co/dX8QRdLYGp (TRPO\u304b\u3089\u4e8c\u968e\u5fae\u5206\u3092\u4f7f\u3046\u3053\u3068\u7121\u304f\uff0c\u4fe1\u983c\u9818\u57df\u90e8\u5206\u306f\u30b5\u30ed\u30b2\u30fc\u30c8\u30e2\u30c7\u30eb\u3067\u8fd1\u4f3c[?]) Proximal Policy Optimization Algorithms https://t.co/SHqZta0RIQ #iclr18yomikai", "followers": "161", "datetime": "2018-05-26 06:52:33", "author": "@argv_sat184"}, "1246014852658860032": {"content_summary": "[1/10] \ud83d\udcc8 - Proximal Policy Optimization Algorithms - 956 \u2b50 - \ud83d\udcc4 https://t.co/Li2xVI8Ils - \ud83d\udd17 https://t.co/GXw4Q8T1eI", "followers": "222", "datetime": "2020-04-03 10:01:05", "author": "@PapersTrending"}, "1016687720901509120": {"content_summary": "RT @indutny: I should read papers more carefully. Just discovered that with PPO, the \u201creflection\u201d phase can be run in multiple epochs! htt\u2026", "followers": "2,619", "datetime": "2018-07-10 14:16:56", "author": "@Tsundu_Mak"}, "1246377484217724930": {"content_summary": "[3/10] \ud83d\udcc8 - Proximal Policy Optimization Algorithms - 1,015 \u2b50 - \ud83d\udcc4 https://t.co/Li2xVI8Ils - \ud83d\udd17 https://t.co/GXw4Q8T1eI", "followers": "222", "datetime": "2020-04-04 10:02:03", "author": "@PapersTrending"}, "942850001394429952": {"content_summary": "RT @Richvn: 2017's top-cited arXiv papers so far [cf https://t.co/EceP4VEXS9]: Q1: GANs! [deep learning] (https://t.co/QAG395ulo1) Q2: 2 bl\u2026", "followers": "2,070", "datetime": "2017-12-18 20:12:12", "author": "@Pvalsfr"}, "952085288133644288": {"content_summary": "RT @chibaf: Unity\u3067\u306emachine learning\u306b\u95a2\u3059\u308b\u8ad6\u6587 [1707.06347] Proximal Policy Optimization Algorithms https://t.co/qupwlq7ca9 We propose a new fam\u2026", "followers": "643", "datetime": "2018-01-13 07:49:56", "author": "@Space_kid_Jr"}, "1118582869561991168": {"content_summary": "@jonasschneider @OpenAI Thanks to @jackclarkSF for an important correction: Schulman's work on PPO was indeed done at OpenAI https://t.co/3G7Ac0LqqP I had confused that with his thesis at Berkeley (I should know this!)", "followers": "375", "datetime": "2019-04-17 18:31:53", "author": "@robinc"}, "902688366965035008": {"content_summary": "\u3069\u3046\u3067\u3082\u3044\u3044\u3053\u3068\u306a\u3093\u3060\u3051\u3069 https://t.co/i3PEPp2pm1 \u3053\u306e\u8ad6\u6587\u8aad\u3093\u3067\u305f\u3089 A3C \u306e synchronous \u7248\u306e\u3053\u3068\u3092 A2C \u3068\u7701\u7565\u3057\u3066\u3066\u3001\u3042\u3063! \u3068\u601d\u3063\u305f\u3002", "followers": "1,552", "datetime": "2017-08-30 00:24:12", "author": "@nagachika"}, "1018552737112494081": {"content_summary": "RT @indutny: I should read papers more carefully. Just discovered that with PPO, the \u201creflection\u201d phase can be run in multiple epochs! htt\u2026", "followers": "2,619", "datetime": "2018-07-15 17:47:50", "author": "@Tsundu_Mak"}, "1203215069930090496": {"content_summary": "Proximal Policy Optimization Algorithms paper - definition of \"KL\" operation?: In the original paper on Proximal Policy Optimization Algorithms https://t.co/YhJLyMqTQc in equation (4) the authors use an operation denoted by KL[]. Unfortunately, they\u2026 https", "followers": "1,186", "datetime": "2019-12-07 07:30:01", "author": "@CarlRioux"}, "1099628711710679041": {"content_summary": "https://t.co/pMxyfBeiVg TRPO\u3067\u306f\u3001\u65b9\u7b56\u306e\u5909\u5316\u91cf\u306b\u5bfe\u3057\u3066KL\u8ddd\u96e2\u306b\u6bd4\u4f8b\u3059\u308b\u7f70\u5247\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u5927\u304d\u306a\u5909\u5316\u3092\u5236\u9650\u3057\u3066\u3044\u305f\u304c\u3001\u5909\u5316\u91cf\u3092\u30af\u30ea\u30c3\u30d7\u3059\u308b\u3053\u3068\u3067\u305d\u308c\u3092\u9054\u6210\u3057\u305f\u3002\u3088\u308a\u69cb\u9020\u304c\u30b7\u30f3\u30d7\u30eb\u306b\u306a\u308b\u305f\u3081\u3001TRPO\u3067\u306f\u4f7f\u3048\u306a\u304b\u3063\u305fDropout\u7b49\u3082\u4f7f\u3048\u308b\u3088\u3046\u306b\u306a\u308b\u3002", "followers": "1,309", "datetime": "2019-02-24 11:14:49", "author": "@akihiro_akichan"}, "1247102356619169793": {"content_summary": "[5/10] \ud83d\udcc8 - Proximal Policy Optimization Algorithms - 1,042 \u2b50 - \ud83d\udcc4 https://t.co/Li2xVI8Ils - \ud83d\udd17 https://t.co/GXw4Q8T1eI", "followers": "222", "datetime": "2020-04-06 10:02:26", "author": "@PapersTrending"}, "989548264784519169": {"content_summary": "@NeuronForest @hardmaru Proximal Policy Optimization: https://t.co/rS15NlkSk3", "followers": "6,400", "datetime": "2018-04-26 16:54:25", "author": "@awjuliani"}, "1236339708822581248": {"content_summary": "Proximal policy modeling is kind of deep. There's an interesting implication with the surrogate model, that you can derive a trajectory of state-action pairs for an agent from a subtly different policy. More in the original paper here: https://t.co/fm0qxx", "followers": "528", "datetime": "2020-03-07 17:15:31", "author": "@aiexplorations"}, "888252566210985984": {"content_summary": "\"Proximal Policy Optimization Algorithms\", John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov https://t.co/hg7M9X8EiC", "followers": "773", "datetime": "2017-07-21 04:21:29", "author": "@arxivml"}, "942810125911363585": {"content_summary": "2017's top-cited arXiv papers so far [cf https://t.co/EceP4VEXS9]: Q1: GANs! [deep learning] (https://t.co/QAG395ulo1) Q2: 2 black holes (https://t.co/x6Ohv3ikbh) Q3: reinforcement learning (https://t.co/B9tsmD4xTp) Q4: Dark energy after grav-waves (https:", "followers": "6,621", "datetime": "2017-12-18 17:33:45", "author": "@Richvn"}, "888206720115384320": {"content_summary": "Proximal Policy Optimization Algorithms - John Schulman https://t.co/iDWom9GcQa", "followers": "858", "datetime": "2017-07-21 01:19:18", "author": "@deep_rl"}, "1196544057461354497": {"content_summary": "- Trust Region Policy Optimization https://t.co/2gCKNCxWtJ - Proximal Policy Optimization Algorithms https://t.co/Zl5oZLIb99 - Proximal Policy Optimization https://t.co/V047Cwd2uw - Deep Deterministic Policy Gradient https://t.co/GFn4XWGep3", "followers": "598", "datetime": "2019-11-18 21:41:48", "author": "@kargarisaac"}, "1044592838485454848": {"content_summary": "[1707.06347] Proximal Policy Optimization Algorithms https://t.co/DTYvp8j7s6", "followers": "855", "datetime": "2018-09-25 14:21:54", "author": "@FMarradi"}, "1255364549605314560": {"content_summary": "https://t.co/pEFc9xIINb Read this later", "followers": "0", "datetime": "2020-04-29 05:13:26", "author": "@chen_kaishang"}, "888229545370845187": {"content_summary": "Proximal Policy Optimization Algorithms. https://t.co/raz5KmsGYa", "followers": "613", "datetime": "2017-07-21 02:50:00", "author": "@ComputerPapers"}, "1245290273531473931": {"content_summary": "[2/10] \ud83d\udcc8 - Proximal Policy Optimization Algorithms - 584 \u2b50 - \ud83d\udcc4 https://t.co/Li2xVI8Ils - \ud83d\udd17 https://t.co/GXw4Q8T1eI", "followers": "222", "datetime": "2020-04-01 10:01:52", "author": "@PapersTrending"}, "1247195053887389698": {"content_summary": "@brianringley @keithmgould Proximal policy optimization https://t.co/kRo5dAdXWs and Soft actor critic are the two I'm most familiar with but I'm sure there's more https://t.co/GO3mYNZ1wr the problem is that tuning the reward functions for such a complex lo", "followers": "4,238", "datetime": "2020-04-06 16:10:47", "author": "@sterlingcrispin"}, "1206839325746290688": {"content_summary": "RT @CarlRioux: Proximal Policy Optimization Algorithms paper - definition of \"KL\" operation?: In the original paper on Proximal Policy Opti\u2026", "followers": "1,186", "datetime": "2019-12-17 07:31:31", "author": "@CarlRioux"}, "928173374437703681": {"content_summary": "https://t.co/oMpJQlvT4w", "followers": "29", "datetime": "2017-11-08 08:12:31", "author": "@ilovecamel_00"}, "983367224575610880": {"content_summary": "Proximal Policy Optimization Algorithms. https://t.co/BDukXCs3fS Looks like a reasonable and elegant solution to distribute DeepRL agents sampling.", "followers": "1,450", "datetime": "2018-04-09 15:33:11", "author": "@aloiscochard"}, "942810341012066305": {"content_summary": "RT @Richvn: 2017's top-cited arXiv papers so far [cf https://t.co/EceP4VEXS9]: Q1: GANs! [deep learning] (https://t.co/QAG395ulo1) Q2: 2 bl\u2026", "followers": "1,228", "datetime": "2017-12-18 17:34:36", "author": "@nrobinsongarcia"}, "1185118994262646785": {"content_summary": "@fjwolski I think I found a typo in your @OpenAI PPO paper at Arxiv (https://t.co/GQ8K2DBTiI) https://t.co/UMxF1ImOpc", "followers": "1", "datetime": "2019-10-18 09:02:41", "author": "@j_plocharczyk"}, "1247195780777205760": {"content_summary": "RT @sterlingcrispin: @brianringley @keithmgould Proximal policy optimization https://t.co/kRo5dAdXWs and Soft actor critic are the two I'm\u2026", "followers": "4,213", "datetime": "2020-04-06 16:13:40", "author": "@brianringley"}, "1245652555952054274": {"content_summary": "[2/10] \ud83d\udcc8 - Proximal Policy Optimization Algorithms - 867 \u2b50 - \ud83d\udcc4 https://t.co/Li2xVHR6WS - \ud83d\udd17 https://t.co/GXw4Q9aC6g", "followers": "222", "datetime": "2020-04-02 10:01:27", "author": "@PapersTrending"}, "1108716673702481920": {"content_summary": "@tdietterich @arxiv_org Huh, the ML community seems to be using arXiv as a place to post self-published work. There are some widely cited articles on arXiv that are not even peer-reviewed, for instance: https://t.co/H49Jyc5JFG. I thought it was all part", "followers": "63", "datetime": "2019-03-21 13:07:08", "author": "@JohnCooperAI"}, "996259671794110464": {"content_summary": "\u65b9\u7b56\u52fe\u914d\u6cd5\u306e\u6bd4\u8f03\u7684\u65b0\u3057\u3044\u624b\u6cd5PPO\u306e\u8ad6\u6587\uff0cactor-critic\u3067\u306f\u306a\u3044 Proximal Policy Optimization Algorithms https://t.co/6WINFvpfKR", "followers": "8", "datetime": "2018-05-15 05:23:10", "author": "@ya0201_"}, "1246739859462598657": {"content_summary": "[3/10] \ud83d\udcc8 - Proximal Policy Optimization Algorithms - 1,025 \u2b50 - \ud83d\udcc4 https://t.co/Li2xVI8Ils - \ud83d\udd17 https://t.co/GXw4Q8T1eI", "followers": "222", "datetime": "2020-04-05 10:02:00", "author": "@PapersTrending"}, "1014682803806658560": {"content_summary": "I should read papers more carefully. Just discovered that with PPO, the \u201creflection\u201d phase can be run in multiple epochs! https://t.co/1BAi2dfhfo #reinforcementlearning", "followers": "9,365", "datetime": "2018-07-05 01:30:06", "author": "@indutny"}, "944673229800706051": {"content_summary": "https://t.co/DTYvp8j7s6", "followers": "855", "datetime": "2017-12-23 20:57:03", "author": "@FMarradi"}, "951476851410653185": {"content_summary": "[1707.06347] Proximal Policy Optimization Algorithms https://t.co/DTYvp8j7s6", "followers": "855", "datetime": "2018-01-11 15:32:13", "author": "@FMarradi"}, "952089178161885186": {"content_summary": "RT @chibaf: Unity\u3067\u306emachine learning\u306b\u95a2\u3059\u308b\u8ad6\u6587 [1707.06347] Proximal Policy Optimization Algorithms https://t.co/qupwlq7ca9 We propose a new fam\u2026", "followers": "6,181", "datetime": "2018-01-13 08:05:23", "author": "@adhara_mathphys"}, "1192705638217875457": {"content_summary": "ABST100\u30e1\u30e2\uff1a96 PPO\uff082017\uff09\u2192https://t.co/DlaZwrzaeQ\u3000\uff0cTRPO\u306bA3C\u3092\u9069\u7528\u3057\u305f\u3082\u306e\uff0c\u300cwhich alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent\u300d", "followers": "63", "datetime": "2019-11-08 07:29:18", "author": "@Ronmemo1"}}}