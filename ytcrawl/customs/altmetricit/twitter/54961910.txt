{"queriedAt": "2020-06-03 15:10:36", "completed": "1", "citation_id": "54961910", "tab": "twitter", "twitter": {"1092934870232371200": {"author": "@pmianjy", "datetime": "2019-02-05 23:55:53", "content_summary": "RT @QuanquanGu: Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initial\u2026", "followers": "74"}, "1092839267858968576": {"author": "@QuanquanGu", "datetime": "2019-02-05 17:36:00", "content_summary": "Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initialization can learn over-parameterized DNNs to achieve arbitrarily small generalization error! Check out our new paper with @_YuanCao_:", "followers": "779"}, "1092859054324498437": {"author": "@_YuanCao_", "datetime": "2019-02-05 18:54:37", "content_summary": "RT @QuanquanGu: Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initial\u2026", "followers": "58"}, "1118507225528569856": {"author": "@alsombra7", "datetime": "2019-04-17 13:31:18", "content_summary": "RT @QuanquanGu: Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initial\u2026", "followers": "111"}, "1093258029552881664": {"author": "@unsorsodicorda", "datetime": "2019-02-06 21:20:00", "content_summary": "RT @QuanquanGu: Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initial\u2026", "followers": "740"}, "1092637060450660353": {"author": "@arxivml", "datetime": "2019-02-05 04:12:30", "content_summary": "\"A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks\", Yuan Cao, Quanqua\u2026 https://t.co/OCO6ah137z", "followers": "780"}, "1118508183092224001": {"author": "@tsauri_eecs", "datetime": "2019-04-17 13:35:06", "content_summary": "RT @QuanquanGu: Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initial\u2026", "followers": "159"}, "1118507084474126341": {"author": "@heghbalz", "datetime": "2019-04-17 13:30:44", "content_summary": "RT @QuanquanGu: Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initial\u2026", "followers": "1,306"}, "1092626912088776706": {"author": "@arxiv_cs_LG", "datetime": "2019-02-05 03:32:10", "content_summary": "A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks. Yuan Cao and Quanquan Gu https://t.co/VZbBc8VE24", "followers": "318"}, "1093018469904322560": {"author": "@unsorsodicorda", "datetime": "2019-02-06 05:28:05", "content_summary": "@roydanroy C'mon, make names (or titles) \ud83d\ude1c https://t.co/SE4KHUR8xN?", "followers": "740"}, "1092615853831532544": {"author": "@BrundageBot", "datetime": "2019-02-05 02:48:14", "content_summary": "A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks. Yuan Cao and Quanquan Gu https://t.co/QanV6gQDYS", "followers": "3,891"}, "1092614176751652864": {"author": "@deep_rl", "datetime": "2019-02-05 02:41:34", "content_summary": "A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks - Yuan Cao https://t.co/iV2TCNH4F0", "followers": "858"}}}