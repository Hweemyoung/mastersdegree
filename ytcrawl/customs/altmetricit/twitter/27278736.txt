{"citation_id": "27278736", "queriedAt": "2020-05-09 13:18:47", "completed": "0", "twitter": {"1046094525981237248": {"followers": "20", "content_summary": "@Tim_Dettmers Mixed precision needs tuning scaling factor S & skips/overflow N. With wrong values model diverges. Only with correct values you reach FP32 accuracy. Does 2080ti 1.65x speedup vanish if you may need to tune these? Paper https://t.co/XABQn", "author": "@Master_Yoda_1", "datetime": "2018-09-29 17:49:05"}, "1125355066464989185": {"followers": "375", "content_summary": "Automatic Mixed Precision training in @TensorFlow using @NvidiaAI\u2019s Tensor Cores. Simple idea \ud83d\udc4d\ud83c\udffc ^ float32 * float16 \u2207 gradient w weights Forward: w* = float16(w^) y* = model(w*, x*) Backward: w^ = w^ - \u03b1*\u2022\u2207* Paper: https://t.co/y10ijRkyGt Article: htt", "author": "@remykarem", "datetime": "2019-05-06 11:02:10"}, "1001610688219959296": {"followers": "833", "content_summary": "@jwangARK You probably know these papers, but I found them useful. Mixed precision training: https://t.co/QhipaZVKXN Nvidia Volta: https://t.co/PVECb5F2YY Google TPU: https://t.co/K0Fmbnro1K", "author": "@fabinger", "datetime": "2018-05-29 23:46:11"}, "1144023625969745922": {"followers": "184", "content_summary": "RT @remykarem: Automatic Mixed Precision training in @TensorFlow using @NvidiaAI\u2019s Tensor Cores. Simple idea \ud83d\udc4d\ud83c\udffc ^ float32 * float16 \u2207 grad\u2026", "author": "@SiavashSakhavi", "datetime": "2019-06-26 23:24:22"}}, "tab": "twitter"}