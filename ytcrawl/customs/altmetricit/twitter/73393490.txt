{"citation_id": "73393490", "completed": "1", "queriedAt": "2020-05-14 13:13:06", "tab": "twitter", "twitter": {"1211519725940772864": {"content_summary": "Axial Attention in Multidimensional Transformers. Efficient generalization of self-attention to perform SOTA image and video modeling. https://t.co/mV90UP2ROj https://t.co/vqqUUeIW8x", "followers": "310", "datetime": "2019-12-30 05:29:46", "author": "@arankomatsuzaki"}, "1211559684152471552": {"content_summary": "RT @arankomatsuzaki: Axial Attention in Multidimensional Transformers. Efficient generalization of self-attention to perform SOTA image and\u2026", "followers": "294", "datetime": "2019-12-30 08:08:33", "author": "@westis96"}, "1211658430861189120": {"content_summary": "Axial Attention in Multidimensional Transformers https://t.co/n8nSw7iG59", "followers": "4,000", "datetime": "2019-12-30 14:40:56", "author": "@arxiv_cscv"}, "1211495949933195265": {"content_summary": "Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NAEDKW", "followers": "7,479", "datetime": "2019-12-30 03:55:17", "author": "@ak92501"}, "1211498346893852673": {"content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "followers": "9", "datetime": "2019-12-30 04:04:49", "author": "@Luck2john"}, "1211754583116132352": {"content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "followers": "2,072", "datetime": "2019-12-30 21:03:00", "author": "@EricSchles"}, "1211477133517885440": {"content_summary": "Axial Attention in Multidimensional Transformers https://t.co/n8nSw7iG59", "followers": "4,000", "datetime": "2019-12-30 02:40:31", "author": "@arxiv_cscv"}, "1228631036310904832": {"content_summary": "@theshawwn You and Gwern should try Axial Attention https://t.co/sIFcctWqft", "followers": "319", "datetime": "2020-02-15 10:44:00", "author": "@lucidrains"}, "1211548634518560768": {"content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "followers": "17", "datetime": "2019-12-30 07:24:38", "author": "@MassBassLol"}, "1211598525844336640": {"content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "followers": "2,208", "datetime": "2019-12-30 10:42:53", "author": "@UAesthete"}, "1212082269730426880": {"content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "followers": "301", "datetime": "2019-12-31 18:45:07", "author": "@subhobrata1"}, "1211507325187444736": {"content_summary": "Axial Attention in Multidimensional Transformers https://t.co/n8nSw714Gz", "followers": "4,000", "datetime": "2019-12-30 04:40:29", "author": "@arxiv_cscv"}, "1211794423081779201": {"content_summary": "\"Axial Attention in Multidimensional Transformers\", Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans https://t.co/BDtnAIrQ1e", "followers": "765", "datetime": "2019-12-30 23:41:19", "author": "@arxivml"}, "1211637081941987329": {"content_summary": "RT @arankomatsuzaki: Axial Attention in Multidimensional Transformers. Efficient generalization of self-attention to perform SOTA image and\u2026", "followers": "285", "datetime": "2019-12-30 13:16:06", "author": "@dannyehb"}, "1211528351501803520": {"content_summary": "[R] Axial Attention in Multidimensional Transformers: submitted by /u/Aran_Komatsuzaki [visit reddit] [comments] https://t.co/ZSnz7DgrVu", "followers": "1,186", "datetime": "2019-12-30 06:04:02", "author": "@CarlRioux"}, "1211485517180628993": {"content_summary": "Axial Attention in Multidimensional Transformers \u591a\u6b21\u5143\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u8ef8\u65b9\u5411\u306e\u6ce8\u610f 2019-12-20T13:27:27+00:00 arXiv: https://t.co/Fw7DXEcm6k \u82f1/\u65e5\u30b5\u30de\u30ea\u2193 https://t.co/G5ockRrtSD", "followers": "126", "datetime": "2019-12-30 03:13:50", "author": "@arXiv_reaDer"}}}