{"citation_id": "21021647", "queriedAt": "2020-05-09 13:59:42", "completed": "0", "twitter": {"1206242595866923009": {"followers": "129", "content_summary": "Reward-function free reinforcement learning: replacing the reward function by human-in-the-loop supervision. No pre-existing dataset required since uncertainty-maximising examples are created on-the-fly by the agent. Paper: https://t.co/38ouDWTmBm Code: h", "author": "@AjarAilerons", "datetime": "2019-12-15 16:00:20"}, "1211324090973331456": {"followers": "232", "content_summary": "@Grimeandreason there actually is research in machine learning on that too: https://t.co/zMaK5UyT5s the issue i was describing is the issue of answering why we get certain quantifiable results in system, i.e. modeling that system, rather than answering que", "author": "@JeWe37", "datetime": "2019-12-29 16:32:23"}, "1238104890938884102": {"followers": "648", "content_summary": "@theshawwn that's awesome -- an open-source wireframe for preference learning/active learning with generality in mind would be a huge win, not least for https://t.co/aoUqtlPUjA or https://t.co/hoWD7dul6M", "author": "@jvmncs", "datetime": "2020-03-12 14:09:43"}, "1206245356721319940": {"followers": "871", "content_summary": "Interesting. Will need to dig in over few days vacation", "author": "@MonosovLab", "datetime": "2019-12-15 16:11:18"}}, "tab": "twitter"}