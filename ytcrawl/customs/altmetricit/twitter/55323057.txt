{"tab": "twitter", "completed": "1", "twitter": {"1095154364812062720": {"author": "@yapp1e", "followers": "49", "datetime": "2019-02-12 02:55:22", "content_summary": "A simple and efficient architecture for trainable activation functions. (arXiv:1902.03306v1 [cs.LG]) https://t.co/Mof2S5w4qx Learning automatically the best activation function for the task is an active topic in neural network research. At the moment, des"}, "1095168997425737728": {"author": "@treasured_write", "followers": "101", "datetime": "2019-02-12 03:53:30", "content_summary": "RT @BrundageBot: A simple and efficient architecture for trainable activation functions. Andrea Apicella, Francesco Isgr\u00f2, and Roberto Prev\u2026"}, "1095143557172789248": {"author": "@StatMLPapers", "followers": "9,756", "datetime": "2019-02-12 02:12:25", "content_summary": "A simple and efficient architecture for trainable activation functions. (arXiv:1902.03306v1 [cs.LG]) https://t.co/hs1xY2c1yI"}, "1095171136579559425": {"author": "@arxivml", "followers": "785", "datetime": "2019-02-12 04:02:00", "content_summary": "\"A simple and efficient architecture for trainable activation functions\", Andrea Apicella, Francesco Isgr\u00f2, Roberto\u2026 https://t.co/IOb3JTUw99"}, "1095152598854844416": {"author": "@BrundageBot", "followers": "3,908", "datetime": "2019-02-12 02:48:21", "content_summary": "A simple and efficient architecture for trainable activation functions. Andrea Apicella, Francesco Isgr\u00f2, and Roberto Prevete https://t.co/wionddLux4"}}, "citation_id": "55323057", "queriedAt": "2020-06-03 23:04:08"}