{"tab": "twitter", "twitter": {"1062872114061963264": {"author": "@vodkamomo", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-15 00:57:13", "followers": "323"}, "1051405819776880640": {"author": "@niszet0", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 09:34:16", "followers": "1,052"}, "1050963935497662464": {"author": "@nsd244", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:18:22", "followers": "2,084"}, "1050618891741720577": {"author": "@MikelForcada", "content_summary": "RT @beck_daniel: A new era of NLP has just begun: a dystopia where 1) English is the only language spoken, and 2) big companies ensure gate\u2026", "datetime": "2018-10-12 05:27:18", "followers": "1,503"}, "1052062142080286721": {"author": "@tdzungz", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-16 05:02:15", "followers": "160"}, "1050718807314972673": {"author": "@prototechno", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 12:04:19", "followers": "4,825"}, "1050561287959339008": {"author": "@darksideofcode", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/Z4OuoImWfb", "datetime": "2018-10-12 01:38:24", "followers": "88"}, "1051017597683040256": {"author": "@gabeibagon", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 07:51:36", "followers": "72"}, "1050846305789112320": {"author": "@romanbsd", "content_summary": "https://t.co/o3AiYmIA2a", "datetime": "2018-10-12 20:30:57", "followers": "74"}, "1058156787461812224": {"author": "@chenlailin", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 00:40:12", "followers": "29"}, "1050608305603657728": {"author": "@kaineko", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 04:45:14", "followers": "1,377"}, "1050823379702312960": {"author": "@pranavrajpurkar", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:59:51", "followers": "4,448"}, "1052147896777203712": {"author": "@k_oi", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-16 10:43:01", "followers": "719"}, "1050655780515373056": {"author": "@keratin7", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:53:52", "followers": "293"}, "1050938031983616000": {"author": "@tsatie", "content_summary": "\u4f55\u3084\u308d\u3053\u308c", "datetime": "2018-10-13 02:35:26", "followers": "704"}, "1051009128221396992": {"author": "@iamalstat", "content_summary": "Exciting", "datetime": "2018-10-13 07:17:57", "followers": "222"}, "1050784228680577024": {"author": "@jayeshmthakur", "content_summary": "RT @mihail_eric: WIth ELMo and BERT already out, I predict within the next 1-2 years we'll be able to do a @sesamestreet re-run using nothi\u2026", "datetime": "2018-10-12 16:24:17", "followers": "2,038"}, "1050761086948057088": {"author": "@JeanMarcJAzzi", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:52:19", "followers": "365"}, "1050713010128457728": {"author": "@Lemoncloak", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-12 11:41:17", "followers": "195"}, "1050712172370808833": {"author": "@sannikpatel", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:37:57", "followers": "222"}, "1050646451389263872": {"author": "@Yayabenben", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:16:48", "followers": "26"}, "1173537540009340928": {"author": "@PapersTrending", "content_summary": "[8/10] \ud83d\udcc8 - pytorch-transformers - 12,196 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/tvmmNM6XVV", "datetime": "2019-09-16 10:02:07", "followers": "222"}, "1050911973955768321": {"author": "@snake_tianchez", "content_summary": "RT @rown: Totally! But it\u2019s not the task that\u2019s solved, it\u2019s the dataset :) it remains to be seen whether collecting the data using the sam\u2026", "datetime": "2018-10-13 00:51:54", "followers": "172"}, "1050933899847585792": {"author": "@eve_yk", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:19:01", "followers": "485"}, "1069718444793323521": {"author": "@simon_mich", "content_summary": "RT @BrianRoemmele: Wonderful academic paper on #VoiceFirst AI I showed last month: \"BERT: Pre-training of Deep Bidirectional Transformers\u2026", "datetime": "2018-12-03 22:22:06", "followers": "1,958"}, "1050813170435219456": {"author": "@thinkmariya", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:19:17", "followers": "28,529"}, "1051364025861324800": {"author": "@ionandrou", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 06:48:11", "followers": "650"}, "1051009270462984192": {"author": "@rkakamilan", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 07:18:31", "followers": "637"}, "1128959055584563200": {"author": "@HirokatuKataoka", "content_summary": "\u5143\u8ad6\u6587\u3002 BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding https://t.co/ipdwglJRNS", "datetime": "2019-05-16 09:43:08", "followers": "3,636"}, "1050891414085591040": {"author": "@howardmeng", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 23:30:12", "followers": "44"}, "1051358495218360321": {"author": "@TheEcolss", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-14 06:26:13", "followers": "13"}, "1050917769020092416": {"author": "@fjfbupt", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-13 01:14:55", "followers": "59"}, "1050735445540257794": {"author": "@phu_pmh", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 13:10:26", "followers": "556"}, "1051618336994291712": {"author": "@mwmsnn", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 23:38:44", "followers": "109"}, "1050624165248622592": {"author": "@ciscaoladipo", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:48:15", "followers": "889"}, "1132486453592768512": {"author": "@steveplunkett", "content_summary": "Lol.. @theGypsy", "datetime": "2019-05-26 03:19:45", "followers": "7,721"}, "1050758368048992256": {"author": "@Melnus_", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 14:41:31", "followers": "411"}, "1058254065409245185": {"author": "@KouroshMeshgi", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 07:06:45", "followers": "616"}, "1050716751430897664": {"author": "@ZiyuYao", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:56:09", "followers": "13"}, "1050692942489214976": {"author": "@KouroshMeshgi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:21:33", "followers": "616"}, "1051138704683790337": {"author": "@GillesMoyse", "content_summary": "RT @cmarschner: The NLProc world is shaken up by the models ELMo and BERT \ud83d\ude02. https://t.co/1HcIOhIqeG", "datetime": "2018-10-13 15:52:51", "followers": "1,058"}, "1050628419300417538": {"author": "@fedefalco92", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:05:09", "followers": "270"}, "1051057204537765888": {"author": "@doombris", "content_summary": "RT @madrugad0: new SotA on many tasks from Google; bidirectional transformer language model, which is trained for 1M steps with 128k words/\u2026", "datetime": "2018-10-13 10:28:59", "followers": "8"}, "1051281635641982977": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: \u4e0a\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u30894\u30ab\u6708\u3002\u4eca\u5ea6\u306fGoogleAI\u304b\u3089\u885d\u6483\u7684\u306a\u624b\u6cd5\u304c\u767b\u5834\uff01\u518d\u3073\u5927\u91cf\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\uff08SoTA\uff09\u3092\u53e9\u304d\u51fa\u3059\u3002\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u793a\u3059\u30bf\u30b9\u30af\u3082\u3042\u308b\u3002BERT: Pre-training of Deep Bidirecti\u2026", "datetime": "2018-10-14 01:20:48", "followers": "2,861"}, "1051331315381850112": {"author": "@TSaodake", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 04:38:13", "followers": "19"}, "1050852022952648706": {"author": "@nicogontier", "content_summary": "https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Google. Unsupervised pretraining of language models is becoming a standard technique", "datetime": "2018-10-12 20:53:40", "followers": "203"}, "1050855578883563523": {"author": "@jparedesj", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 21:07:48", "followers": "29"}, "1050770863887273984": {"author": "@CherryRonao", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:31:10", "followers": "78"}, "1060843421101830144": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-09 10:35:55", "followers": "843"}, "1053475688093634561": {"author": "@haiattoC", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-20 02:39:11", "followers": "252"}, "1051079492058386438": {"author": "@Montreal_AI", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 11:57:33", "followers": "177,481"}, "1050974226344108033": {"author": "@tomtung", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 04:59:16", "followers": "401"}, "1059494221869019140": {"author": "@kmoriyama", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 17:14:41", "followers": "9,096"}, "1050940585182298112": {"author": "@oyanakama", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:45:35", "followers": "374"}, "1050579027331833856": {"author": "@amitunix", "content_summary": "https://t.co/gjbHjKF4Ja", "datetime": "2018-10-12 02:48:53", "followers": "229"}, "1059592619586658304": {"author": "@blessingyuki", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 23:45:41", "followers": "1,781"}, "1119404873135607808": {"author": "@akelleh", "content_summary": "RT @parker_brydon: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/3w7rlwsQz0 SOTA for many\u2026", "datetime": "2019-04-20 00:58:13", "followers": "1,770"}, "1050543868041555969": {"author": "@lmthang", "content_summary": "A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive compute is all you need. BERT from @GoogleAI: SOTA results on everything https://t.co/YQaY7baIQg. Results on SQuAD are just mind-", "datetime": "2018-10-12 00:29:10", "followers": "15,224"}, "1050824329833971714": {"author": "@At7788546", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 19:03:38", "followers": "32"}, "1050586086181756928": {"author": "@KRiver1", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/mojVIr1rLE Masked Language Model (MLM) \u3068\u3044\u3046\u65b0\u3057\u3044\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u3068Transformer\u306e\u7d44\u307f\u5408\u308f\u305b\u3002MLM\u3067\u306f\u5de6\u304b\u3089\u53f3\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u3067\u306f\u306a\u304f\u3001\u5165\u529b\u7cfb\u5217\u5168\u4f53\u3092\u5165\u308c\u3066\u4e00\u90e8\u3092\u7a74\u57cb\u3081\u3059\u308b\u5b66\u7fd2\u3092\u884c\u3046\u3002", "datetime": "2018-10-12 03:16:56", "followers": "1,871"}, "1114163652796358656": {"author": "@app_math_study", "content_summary": "\u305f\u307e\u305f\u307e\u3054\u8cea\u554f\u3044\u305f\u3060\u3044\u3066BERT\u3092\u8efd\u304f\u8aad\u3093\u3060\u3051\u3069\u9762\u767d\u305d\u3046\uff01\u3071\u3063\u3068\u898b\u305f\u611f\u3058\u306f\u8a00\u8a9e\u51e6\u7406\u7528\u306epre-train\u30e2\u30c7\u30eb\u306e\u7814\u7a76\u3063\u307d\u3044\u3002 \u306a\u3093\u304b\u306e\u4f01\u753b\u3067\u3053\u306e\u8fba\u306e\u8a00\u8a9e\u51e6\u7406\u7cfb\u30e1\u30a4\u30f3\u3067\u6271\u3046\u306e\u3082\u697d\u3057\u305d\u3046\u3002 https://t.co/EifSSJRVwN", "datetime": "2019-04-05 13:51:29", "followers": "171"}, "1050625653366611969": {"author": "@sara_hlt", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:54:10", "followers": "1,370"}, "1057836294829719553": {"author": "@muktabh", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 03:26:40", "followers": "781"}, "1051566328769699841": {"author": "@TsuguoMogami", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 20:12:04", "followers": "14"}, "1171312896254648321": {"author": "@Boristream", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026", "datetime": "2019-09-10 06:42:11", "followers": "20"}, "1051144033932730369": {"author": "@ilove2dgirl", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 16:14:01", "followers": "567"}, "1050647345006694400": {"author": "@albertrafols", "content_summary": "RT @beck_daniel: A new era of NLP has just begun: a dystopia where 1) English is the only language spoken, and 2) big companies ensure gate\u2026", "datetime": "2018-10-12 07:20:21", "followers": ""}, "1259867685141254150": {"author": "@AbramGeest", "content_summary": "The BERT paper: https://t.co/qCPqgWPLMy 4/", "datetime": "2020-05-11 15:27:18", "followers": "45"}, "1051159963005939713": {"author": "@alexissmirnov", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 17:17:19", "followers": "578"}, "1160682862808428545": {"author": "@Chillylin", "content_summary": "\u807d\u8d77\u4f86\u5c31\u50cf\u662f\u8001\u5e2b\u5085\u5728\u6253\u78e8\u96f6\u4ef6\u3002 \u2014\u2014\u2014\u2014 For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set) https://t.co/KqDtGrSji3", "datetime": "2019-08-11 22:42:13", "followers": "3,038"}, "1161387034390519809": {"author": "@Rosenchild", "content_summary": "RT @HubBucket: \u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Languag\u2026", "datetime": "2019-08-13 21:20:21", "followers": "11,933"}, "1051078036752236544": {"author": "@sappy_and_sappy", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 11:51:46", "followers": "175"}, "1051082185380253698": {"author": "@fumiaki_sato_", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 12:08:15", "followers": "26"}, "1097079490335801344": {"author": "@hausenjapan", "content_summary": "E\u30c6\u30ec\u3067\u3001BERT\u306e\u8a71\u984c\u304c\u51fa\u305f\u306e\u3067\u3001\u3042\u3089\u3086\u308b\u5e02\u6c11\u304cBERT\u95a2\u4fc2\u306e\u30b5\u30a4\u30c8\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u6642\u4ee3\u3082\u8fd1\u3044\u306e\u3067\u3042\u308d\u3046\u304b\u3002 \uff08\u3068\u308a\u3042\u3048\u305a\u3001arXiv\u306a\u3069\u306e\u8cc7\u6599\u304c\u3042\u308a\u307e\u3059\u3002\uff09 https://t.co/oNtO0TDmA9", "datetime": "2019-02-17 10:25:07", "followers": "451"}, "1075733311929155584": {"author": "@Ml101Freak", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-12-20 12:43:02", "followers": "39"}, "1087031833328603145": {"author": "@FletcherBello", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/tv2CO8KqWu #Bert #Pre-training #DeepBidirectionalTransformers", "datetime": "2019-01-20 16:59:19", "followers": "59"}, "1050951246809579520": {"author": "@ASreesaila", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 03:27:57", "followers": "25"}, "1051895897188130817": {"author": "@mattrero42", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 18:01:39", "followers": "39"}, "1050997580769816581": {"author": "@grep_news", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/edZK8kxEHr", "datetime": "2018-10-13 06:32:04", "followers": "45"}, "1057759504354357248": {"author": "@dynamis724", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-31 22:21:32", "followers": "1,651"}, "1053904052977844224": {"author": "@AndySugs", "content_summary": "RT: DataSciNews: RT seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidirectional LM pretraining w/ masking - Next-sentence prediction aux task - Bigger, more data It seems\u2026", "datetime": "2018-10-21 07:01:21", "followers": "3,314"}, "1050551353255317505": {"author": "@JatanaHQ", "content_summary": "[R] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/1l57VgQT17", "datetime": "2018-10-12 00:58:55", "followers": "1,248"}, "1050560829018566656": {"author": "@sjoerdapp", "content_summary": "\ud83d\udd1d #tech story: BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/h8tQmpDNqi \ud83e\udd16\ud83e\udd16\ud83e\udd16 #awesome #news #technology \ud83d\udd25 https://t.co/Fs7MmL5goH", "datetime": "2018-10-12 01:36:34", "followers": "1,964"}, "1050551959982231552": {"author": "@M157q_News_RSS", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.04805v1 [https://t.co/5hSkiR3zEk]) https://t.co/6REU8BUwuc We in", "datetime": "2018-10-12 01:01:20", "followers": "756"}, "1050617851839578114": {"author": "@yudiwbs", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:23:10", "followers": "1,430"}, "1059882831101325312": {"author": "@aytjq", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-06 18:58:53", "followers": "14"}, "1050788781660495872": {"author": "@fabtar", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 16:42:22", "followers": "3,137"}, "1050590367676956672": {"author": "@morioka", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:33:57", "followers": "826"}, "1050732022581354497": {"author": "@baykenney", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:56:50", "followers": "2,195"}, "1059807910597865472": {"author": "@hiroosa", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 14:01:10", "followers": "1,816"}, "1050645478906327045": {"author": "@mikechrzano", "content_summary": "RT @jonathanrraiman: Incredible achievement in NLP Pretraining from @GoogleAI with BERT. I assume translation will fall to this approach ne\u2026", "datetime": "2018-10-12 07:12:56", "followers": "87"}, "1050678281693876224": {"author": "@hypermush", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:23:17", "followers": "242"}, "1050559957790273537": {"author": "@Stasoni", "content_summary": "#RT @JatanaHQ: [R] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/h3hS1Yl3fH", "datetime": "2018-10-12 01:33:07", "followers": "1,447"}, "1050790455305072641": {"author": "@andyhickl", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 16:49:01", "followers": "5,958"}, "1050624085787439107": {"author": "@TyzenLee", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:47:56", "followers": "477"}, "1057704730929950721": {"author": "@zeus85815505", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-31 18:43:53", "followers": "15"}, "1050676808482344960": {"author": "@barneyp", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:17:26", "followers": "6,522"}, "1051402198834270208": {"author": "@koreyou_", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 09:19:52", "followers": "143"}, "1050760806466576388": {"author": "@lelayf", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 14:51:13", "followers": "573"}, "1064307729710276608": {"author": "@v_vashishta", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/1s4Xa6RE0e #nlproc #MachineLearning", "datetime": "2018-11-19 00:01:51", "followers": "30,496"}, "1050881476105924608": {"author": "@ayirpelle", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-12 22:50:42", "followers": "2,672"}, "1050943314009051137": {"author": "@kobi78", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:56:26", "followers": "335"}, "1051669828015017985": {"author": "@jaguring1", "content_summary": "RT @stealthinu: Transformer\u3092\u5927\u898f\u6a21\u306b\u3057\u3066\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5de5\u592b\u3057\u3066\u5b66\u7fd2\u3055\u305b\u308b\u3053\u3068\u3067\u3076\u3063\u3061\u304e\u308a\u306e\u6027\u80fd\u306b\u3002\u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3053\u308c\u3082\u3046\u4eba\u9593\u8d85\u3048\u3066\u308b\u306e\u3067\u306f\uff1f / \u201c[1810.04805] BERT: Pre-training of Deep Bi\u2026\u201d https:\u2026", "datetime": "2018-10-15 03:03:20", "followers": "12,765"}, "1050587988021792773": {"author": "@AltangChagnaa", "content_summary": "\u0425\u04e9\u04e9\u0445, \u0448\u0438\u043d\u044d \u044d\u0440\u0438\u043d \u044d\u0445\u043b\u044d\u0432 \u0433\u044d\u043d\u044d \u04af\u04af?", "datetime": "2018-10-12 03:24:29", "followers": "883"}, "1050543971074600960": {"author": "@arvind_io", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:29:35", "followers": "1,141"}, "1050628502762835971": {"author": "@alenushka", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 06:05:29", "followers": "594"}, "1050863465152307200": {"author": "@muchafel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 21:39:08", "followers": "65"}, "1050797425970036736": {"author": "@fastml_extra", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 17:16:43", "followers": "7,210"}, "1050544641811079168": {"author": "@BioNLProc", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:32:15", "followers": "361"}, "1152301536472707072": {"author": "@HubBucket", "content_summary": "RT @HubBucket: \u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Languag\u2026", "datetime": "2019-07-19 19:37:49", "followers": "5,315"}, "1050555677532393477": {"author": "@HrSaghir", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:16:06", "followers": "781"}, "1050689397841154048": {"author": "@AmirSaffari", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/bk8ZSpfUKk Great results on transfer learning for NLP tasks with strong results", "datetime": "2018-10-12 10:07:27", "followers": "409"}, "1050579283511373825": {"author": "@ayirpelle", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 02:49:54", "followers": "2,672"}, "1051279896884858880": {"author": "@trailinga", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-14 01:13:53", "followers": "6"}, "1059864751331627009": {"author": "@Matsuhiro", "content_summary": "RT @HayashiPeke: \u82f1\u8a9e\u306e\u6700\u5927\u306e\u5229\u70b9\u306f\u3001\u53d6\u5f97\u3067\u304d\u308b\u60c5\u5831\u304c\u7269\u51c4\u304f\u5897\u3048\u308b\u3053\u3068\u3060\u3068\u304a\u3082\u3046\u3002 \u8a71\u305b\u306a\u304f\u3066\u3082\u300c\u8aad\u3081\u308b\u300d\u3060\u3051\u3067\u65b0\u3057\u3044\u60c5\u5831\u304c\u624b\u306b\u5165\u308b\u3060\u3051\u3058\u3083\u306a\u304f\u3066\u3001\u540c\u3058\u3053\u3068\u3067\u3082\u3088\u308a\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u3057\u3066\u3044\u308bsource\u306b\u89e6\u308c\u3089\u308c\u305f\u308a\u3002 \u307b\u3093\u306e\u3001\u307b\u3093\u306e\u5c11\u3057\u305a\u3064\u3067\u3082\u3001\u8ad6\u6587\u306a\u3089ab\u2026", "datetime": "2018-11-06 17:47:02", "followers": "23,124"}, "1051219103581499393": {"author": "@__phanhoang__", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 21:12:19", "followers": "219"}, "1054428191710941184": {"author": "@JustinTimesUK", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-22 17:44:05", "followers": "608"}, "1050729900104134657": {"author": "@mgraffg", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:48:24", "followers": "188"}, "1051277561425092609": {"author": "@momiji_fullmoon", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 01:04:37", "followers": "1,380"}, "1050930419590287360": {"author": "@SythonUK", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:05:12", "followers": "691"}, "1113388157335146496": {"author": "@miyayou", "content_summary": "\u6628\u65e5\u306e\u30b9\u30af\u30a6\u30a7\u30a2\u30fb\u30a8\u30cb\u30c3\u30af\u30b9 \u793e\u5185\u30b2\u30fc\u30e0AI\u30bb\u30df\u30ca\u30fc\uff08\u7b2c\uff12\uff14\uff18\u56de\uff09\u3067\u306f\u3001Google \u306eBERT\u3092\u89e3\u8aac\u3057\u307e\u3057\u305f\u3002\u601d\u60f3\u7684\u306b\u3082\u3068\u3066\u3082\u9762\u767d\u304f\u3001\u30b2\u30fc\u30e0\u30c7\u30b6\u30a4\u30ca\u30fc\u304b\u3089\u30b5\u30fc\u30d0\u30fc\u6280\u8853\u8005\u307e\u3067\u305f\u304f\u3055\u3093\u306e\u65b9\u306b\u3054\u53c2\u52a0\u3044\u305f\u3060\u304d\u8b70\u8ad6\u3082\u767d\u71b1\u3057\u307e\u3057\u305f\u3002 https://t.co/AG8HhV6NlK https://t.co/AG8HhV6NlK", "datetime": "2019-04-03 10:29:57", "followers": "18,316"}, "1051475971478499328": {"author": "@CyborgTribe", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 14:13:01", "followers": "482"}, "1050635297858306050": {"author": "@budhaditya_deb", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 06:32:29", "followers": "11"}, "1055404728111124480": {"author": "@wakawaka_titi", "content_summary": "RT @nsaphra: Sad glances at academic compute resources https://t.co/dgOkoQREEH", "datetime": "2018-10-25 10:24:30", "followers": "280"}, "1050600182776135682": {"author": "@Swetava", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/7Gis6878fx", "datetime": "2018-10-12 04:12:57", "followers": "265"}, "1057760256766337025": {"author": "@came1223pg", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-31 22:24:32", "followers": "1,472"}, "1096203888397553665": {"author": "@julian3833", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-02-15 00:25:48", "followers": "254"}, "1050693748567420930": {"author": "@kazanagisora", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 10:24:45", "followers": "843"}, "1050724084248715264": {"author": "@essentialskill", "content_summary": "A linguistic phenomenon, BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks. https://t.co/IOkhcGhF0Q via @harvardnlp @mchang21 @kchonyc and @hiconcep #ArtificialIntell", "datetime": "2018-10-12 12:25:17", "followers": "5,041"}, "1118970000382005250": {"author": "@tecmvamec", "content_summary": "WolframResearch: New in the #WolframNetRepo: extract text features w/ a BERT model trained on BookCorpus & Wikipedia https://t.co/L2hK5BJnag Thanks Jacob Devlin, Ming-Wei Chang, Kenton Lee & Kristina Toutanova for creating the model https://t.co/K", "datetime": "2019-04-18 20:10:12", "followers": "13"}, "1050651650434551808": {"author": "@shurain", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:37:28", "followers": "327"}, "1058015870704869377": {"author": "@desertnaut", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 15:20:15", "followers": "1,025"}, "1050693406937235456": {"author": "@justinhedge", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:23:23", "followers": "583"}, "1050709504378179585": {"author": "@__snehal__", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:27:21", "followers": "338"}, "1050782231353020416": {"author": "@zaph0id", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 16:16:21", "followers": "136"}, "1050559334642380800": {"author": "@Miles_Brundage", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:30:38", "followers": "25,741"}, "1050613288176779264": {"author": "@ggdupont", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:05:02", "followers": "172"}, "1050604298671804417": {"author": "@viirya", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:29:18", "followers": "115"}, "1051676114962153472": {"author": "@wayasas", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-15 03:28:19", "followers": "48"}, "1050540425197572097": {"author": "@kchonyc", "content_summary": "the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova https://t.co/kKDzq3GF5W", "datetime": "2018-10-12 00:15:30", "followers": "23,866"}, "1068564992423804929": {"author": "@plevy", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-30 17:58:41", "followers": "26,824"}, "1050682078621184005": {"author": "@EldarSilver", "content_summary": "RT @albarjip: New state of the art results in a wide range of NLP tasks by using very deep bidirectional pretrained models https://t.co/g1J\u2026", "datetime": "2018-10-12 09:38:22", "followers": "1,942"}, "1051440838029365250": {"author": "@stealthinu", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 11:53:25", "followers": "1,055"}, "1193914436081328133": {"author": "@Starrr_1987", "content_summary": "@SamiraHemmati \u0628\u06af\u0648 \u062f\u0627\u0631\u0645 \u0627\u0632 \u0645\u062f\u0644 \u0628\u0631\u062a* \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u06a9\u0646\u0645 \u0628\u0631\u0627\u06cc language modeling \u0628\u0627 \u06cc\u0647 \u0645\u062a\u0646 \u0628\u0632\u0631\u06af 700 \u0645\u06af\u06cc *BERT https://t.co/KIcH7bR0Zs", "datetime": "2019-11-11 15:32:38", "followers": "560"}, "1051167632655835137": {"author": "@reyhaneh_es", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 17:47:48", "followers": "185"}, "1050803402752413696": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: SWAG\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u5e38\u8b58\u63a8\u8ad6\u30bf\u30b9\u30af\uff09\u306b\u304a\u3044\u3066\u3001BERT\u304c\u65e9\u3005\u306b\u4eba\u9593\u30ec\u30d9\u30eb\u306b\u5230\u9054\uff01\uff1f BERT\uff082018\u5e7410\u670811\u65e5\uff01\uff01\uff01\uff09 https://t.co/Kan2rWqMqu SWAG\uff082018\u5e748\u670816\u65e5\uff01\uff01\uff01\uff09 https://t.co/bSA\u2026", "datetime": "2018-10-12 17:40:28", "followers": "2,861"}, "1058126297338204162": {"author": "@peerside", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-01 22:39:02", "followers": "598"}, "1208343305957699584": {"author": "@273sn", "content_summary": "Attention is all you need \u306f transformer \u3067\u3001BERT \u306fhttps://t.co/PublsJbpf7 \u30678\u6708\u9803\u306b\u8aad\u3093\u3060\u304c\u3001\u6539\u3081\u3066\u8aad\u307f\u76f4\u3059\u3068\u5168\u304f\u8aad\u3081\u3066\u3044\u306a\u304b\u3063\u305f\u3002\u307e\u3041\u5f53\u7136\u306a\u3093\u3060\u3051\u3069\u3002\u3002 \u9055\u3044\u306ebidirectional\u3092\u5b9f\u88c5\u3057\u305f\u3044\u304c\u3001\u305d\u306e\u524d\u306bQ&A\u306b\u5bfe\u5fdc\u3059\u308b\u65b9\u304c\u5148\u304b\u3002classification\u3067\u306f\u306a\u3044\u3002\u8aad\u89e3\u3068\u306f\u3002\u62bd\u8c61\u3068\u5177\u4f53\u306e\u53cd\u5fa9\u6a2a\u8df3\u3073", "datetime": "2019-12-21 11:07:48", "followers": "656"}, "1051159812787060736": {"author": "@Ritmonegro", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 17:16:43", "followers": "1,860"}, "1050798303485538304": {"author": "@annabelle_nlp", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 17:20:13", "followers": "184"}, "1051382656787656704": {"author": "@panpu333", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 08:02:13", "followers": "62"}, "1089199748811186177": {"author": "@MLHispano", "content_summary": "El club de lectura de papers est\u00e1 m\u00e1s que en marcha \ud83d\udc9c Est\u00e1 semana ha tocado ... \ud83d\udd2c: https://t.co/7hketEs1Zp Os animamos a uniros a Slack para poder leerlo/comentarlo todos juntos \ud83d\udcaa", "datetime": "2019-01-26 16:33:51", "followers": "7,292"}, "1050946370524217344": {"author": "@jaguring1", "content_summary": "RT @tsatie: \u4f55\u3084\u308d\u3053\u308c https://t.co/SYtMsA1JvW", "datetime": "2018-10-13 03:08:35", "followers": "12,765"}, "1057754158961311744": {"author": "@ballforest", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-31 22:00:18", "followers": "4,024"}, "1050734971554394113": {"author": "@jaguring1", "content_summary": "\u2460 BERT https://t.co/HIplHpINZL", "datetime": "2018-10-12 13:08:33", "followers": "12,765"}, "1051714989537382400": {"author": "@Xiaolei33", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-15 06:02:48", "followers": "90"}, "1059751783063408640": {"author": "@santarou", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 10:18:08", "followers": "153"}, "1058226699664388096": {"author": "@artofbiology", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/WWkAcVP0H6", "datetime": "2018-11-02 05:18:00", "followers": "383"}, "1053962256482893825": {"author": "@kazanagisora", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-10-21 10:52:38", "followers": "843"}, "1058303922136182784": {"author": "@lacti", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 10:24:52", "followers": "153"}, "1051671892086538245": {"author": "@Tarpon_red2", "content_summary": "RT @stealthinu: Transformer\u3092\u5927\u898f\u6a21\u306b\u3057\u3066\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5de5\u592b\u3057\u3066\u5b66\u7fd2\u3055\u305b\u308b\u3053\u3068\u3067\u3076\u3063\u3061\u304e\u308a\u306e\u6027\u80fd\u306b\u3002\u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3053\u308c\u3082\u3046\u4eba\u9593\u8d85\u3048\u3066\u308b\u306e\u3067\u306f\uff1f / \u201c[1810.04805] BERT: Pre-training of Deep Bi\u2026\u201d https:\u2026", "datetime": "2018-10-15 03:11:32", "followers": "2,861"}, "1050614768069828609": {"author": "@weehyong", "content_summary": "Very nice work on BERT!", "datetime": "2018-10-12 05:10:54", "followers": "1,354"}, "1051804473515855872": {"author": "@298gama", "content_summary": "RT @jaguring1: SWAG\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u5e38\u8b58\u63a8\u8ad6\u30bf\u30b9\u30af\uff09\u306b\u304a\u3044\u3066\u3001BERT\u304c\u65e9\u3005\u306b\u4eba\u9593\u30ec\u30d9\u30eb\u306b\u5230\u9054\uff01\uff1f BERT\uff082018\u5e7410\u670811\u65e5\uff01\uff01\uff01\uff09 https://t.co/Kan2rWqMqu SWAG\uff082018\u5e748\u670816\u65e5\uff01\uff01\uff01\uff09 https://t.co/bSA\u2026", "datetime": "2018-10-15 11:58:22", "followers": "1,753"}, "1113391450631200769": {"author": "@Keuzer", "content_summary": "RT @miyayou: \u6628\u65e5\u306e\u30b9\u30af\u30a6\u30a7\u30a2\u30fb\u30a8\u30cb\u30c3\u30af\u30b9 \u793e\u5185\u30b2\u30fc\u30e0AI\u30bb\u30df\u30ca\u30fc\uff08\u7b2c\uff12\uff14\uff18\u56de\uff09\u3067\u306f\u3001Google \u306eBERT\u3092\u89e3\u8aac\u3057\u307e\u3057\u305f\u3002\u601d\u60f3\u7684\u306b\u3082\u3068\u3066\u3082\u9762\u767d\u304f\u3001\u30b2\u30fc\u30e0\u30c7\u30b6\u30a4\u30ca\u30fc\u304b\u3089\u30b5\u30fc\u30d0\u30fc\u6280\u8853\u8005\u307e\u3067\u305f\u304f\u3055\u3093\u306e\u65b9\u306b\u3054\u53c2\u52a0\u3044\u305f\u3060\u304d\u8b70\u8ad6\u3082\u767d\u71b1\u3057\u307e\u3057\u305f\u3002 https://t.co/\u2026", "datetime": "2019-04-03 10:43:02", "followers": "2,184"}, "1057885766116929536": {"author": "@DanielCanueto", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 06:43:15", "followers": "206"}, "1132694837864673281": {"author": "@jordanpfowler", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 17:07:48", "followers": "365"}, "1050775949640433666": {"author": "@joselcs", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:51:23", "followers": "5,057"}, "1133186424910618630": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2019-05-28 01:41:11", "followers": "3,500"}, "1050651502077865984": {"author": "@dawnieando", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:36:52", "followers": "19,900"}, "1058105332654841857": {"author": "@desertnaut", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 21:15:44", "followers": "1,025"}, "1056603655393157120": {"author": "@fjfbupt", "content_summary": "RT @vesko_st: The results from BERT are truly mind-blowing. BERT: Pre-training of Deep Bidirectional Transformers for Language Understandi\u2026", "datetime": "2018-10-28 17:48:36", "followers": "59"}, "1051348062147239939": {"author": "@tn0bu", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 05:44:45", "followers": "511"}, "1050797479153741824": {"author": "@jsdelfino", "content_summary": "Impressive...", "datetime": "2018-10-12 17:16:56", "followers": "54"}, "1050593362221232128": {"author": "@Undeedz", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:45:51", "followers": "163"}, "1186945762783043584": {"author": "@PapersTrending", "content_summary": "[10/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 132 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-10-23 10:01:36", "followers": "222"}, "1050882173644038144": {"author": "@JohnSigmon", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 22:53:29", "followers": "1,188"}, "1051361940117680134": {"author": "@tawatawara", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 06:39:54", "followers": "1,504"}, "1058012503429459968": {"author": "@barneyp", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 15:06:52", "followers": "6,522"}, "1058157825027788800": {"author": "@hereticreader", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - https://t.co/9D1COPFWPY https://t.co/vC4isR7Hlk", "datetime": "2018-11-02 00:44:19", "followers": "195"}, "1050639980601565184": {"author": "@chenlailin", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 06:51:05", "followers": "29"}, "1050750089424035840": {"author": "@s_w_d_", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 14:08:37", "followers": "1,039"}, "1050926667709874176": {"author": "@uc7xe5t", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 01:50:17", "followers": "73"}, "1050897956042407937": {"author": "@francovalentino", "content_summary": "RT @dawnieando: Meet BERT from Google AI @GoogleAI (great name although I am biased as it is the same name as my pomeranian) -> 'BERT: Pre-\u2026", "datetime": "2018-10-12 23:56:12", "followers": "354"}, "1051292137986371590": {"author": "@iskander", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-14 02:02:32", "followers": "2,172"}, "1050782425712914432": {"author": "@AlianHesam", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:17:07", "followers": "88"}, "1051108583599792128": {"author": "@enari", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 13:53:09", "followers": "240"}, "1050882774347976704": {"author": "@K_Ryuichirou", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 22:55:52", "followers": "782"}, "1050775974814527488": {"author": "@grrr_inu", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 15:51:29", "followers": "113"}, "1051699082845011968": {"author": "@jaguring1", "content_summary": "RT @jaguring1: \u4e0a\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u30894\u30ab\u6708\u3002\u4eca\u5ea6\u306fGoogleAI\u304b\u3089\u885d\u6483\u7684\u306a\u624b\u6cd5\u304c\u767b\u5834\uff01\u518d\u3073\u5927\u91cf\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\uff08SoTA\uff09\u3092\u53e9\u304d\u51fa\u3059\u3002\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u793a\u3059\u30bf\u30b9\u30af\u3082\u3042\u308b\u3002BERT: Pre-training of Deep Bidirecti\u2026", "datetime": "2018-10-15 04:59:35", "followers": "12,765"}, "1050579007958175745": {"author": "@eman1972", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 02:48:48", "followers": "7,324"}, "1099084137359646721": {"author": "@ozkirimli_elif", "content_summary": "RT @profillic: Google open sourced a new technique for NLP pre-training called BERT. With this release, anyone in the world can train their\u2026", "datetime": "2019-02-22 23:10:52", "followers": "906"}, "1050703696185700352": {"author": "@guillaumearhay", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:04:16", "followers": "220"}, "1050695017243627521": {"author": "@isaiahtaguibao", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:29:47", "followers": "113"}, "1115367484616585217": {"author": "@iPullRank", "content_summary": "Looks like BERT is what currently powers this type of extraction: https://t.co/wp69apjqZp", "datetime": "2019-04-08 21:35:05", "followers": "40,177"}, "1050760638123761665": {"author": "@summer4an", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 14:50:32", "followers": "95"}, "1058336263512711173": {"author": "@jbowayles", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-02 12:33:22", "followers": "221"}, "1051138014418792449": {"author": "@IgorCarron", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 15:50:06", "followers": "4,890"}, "1050651955587047426": {"author": "@faisal_putra", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:38:41", "followers": "273"}, "1050824484394098688": {"author": "@_josh_meyer_", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 19:04:15", "followers": "1,080"}, "1050775645754613760": {"author": "@marypcbuk", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:50:11", "followers": "11,293"}, "1082669469640675328": {"author": "@societyoftrees", "content_summary": "You, the savvy well-connected reader, already know BERT: a state of the art method for building contextual language models using the power of Transformer Net encoders https://t.co/pOdllzFMyi", "datetime": "2019-01-08 16:04:51", "followers": "302"}, "1051048576506314752": {"author": "@adjiboussodieng", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-13 09:54:42", "followers": "3,651"}, "1194195610959777792": {"author": "@falfaroaguila", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/7O04GIScnI", "datetime": "2019-11-12 10:09:55", "followers": "1,033"}, "1051819494413426688": {"author": "@chiefaiofficers", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 12:58:03", "followers": "239"}, "1050598990650064898": {"author": "@mla_lma", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:08:13", "followers": "19"}, "1051112487603589121": {"author": "@windx0303", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 14:08:40", "followers": "1,191"}, "1058103671123263490": {"author": "@sinatv52", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 21:09:08", "followers": "54"}, "1050929766012837889": {"author": "@willer_young", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 02:02:36", "followers": "27"}, "1050592778608996354": {"author": "@XandaSchofield", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 03:43:32", "followers": "3,942"}, "1050658769607127040": {"author": "@nsaphra", "content_summary": "Sad glances at academic compute resources", "datetime": "2018-10-12 08:05:45", "followers": "2,898"}, "1051052430488526848": {"author": "@y_shindoh", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 10:10:01", "followers": "368"}, "1050633939033837568": {"author": "@tobias_sterbak", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:27:05", "followers": "301"}, "1057837032599257088": {"author": "@MohdFitriAlif", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 03:29:36", "followers": "150"}, "1050714498858799105": {"author": "@mawsonguy", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:47:12", "followers": "2,439"}, "1050645646011555841": {"author": "@pranavrajpurkar", "content_summary": "Paper here: https://t.co/c78X5OLMtn", "datetime": "2018-10-12 07:13:36", "followers": "4,448"}, "1059642795755884544": {"author": "@geinfituesd01", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:05:04", "followers": "441"}, "1097878387236909056": {"author": "@RexDouglass", "content_summary": "TensorFlow code and pre-trained models for BERT https://t.co/R6H9Takjdf https://t.co/Zo2gze8bIL", "datetime": "2019-02-19 15:19:39", "followers": "1,410"}, "1205475091410292739": {"author": "@MordyOberstein", "content_summary": "RT @VorticonCmdr: @MordyOberstein I had to look it up. This doesn't say Google is using BERT for named entity recognition but in the origin\u2026", "datetime": "2019-12-13 13:10:33", "followers": "1,297"}, "1050778827494895616": {"author": "@jasleenkaur1291", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:02:49", "followers": "87"}, "1051498790530691073": {"author": "@experiencor", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 15:43:42", "followers": "38"}, "1059631793073152000": {"author": "@nunn43356", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 02:21:21", "followers": "8"}, "1050934440854151168": {"author": "@kyoun", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:21:10", "followers": "2,439"}, "1050706265008037890": {"author": "@myutwo150", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:14:29", "followers": "12"}, "1051377598725410816": {"author": "@shinya1900012", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:42:07", "followers": "8"}, "1050760301665226753": {"author": "@arnaudsors", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 14:49:12", "followers": "26"}, "1051516714947661824": {"author": "@rintukutum", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-14 16:54:55", "followers": "746"}, "1050650460309225472": {"author": "@_TobiasLee", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:32:44", "followers": "27"}, "1050861537794117632": {"author": "@reiver", "content_summary": "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova https://t.co/tifGqS9QK8 (machine learning, natural language processing (NLP))", "datetime": "2018-10-12 21:31:29", "followers": "3,449"}, "1060843364621287424": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-09 10:35:42", "followers": "843"}, "1057210775545221120": {"author": "@fnielsen", "content_summary": "@ulfaslak It's #BERT that claims superhuman performance on SQuAD dataset. https://t.co/Rjww1fFxzE", "datetime": "2018-10-30 10:01:05", "followers": "1,188"}, "1083621512236666886": {"author": "@HieuNgu76974629", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-01-11 07:07:55", "followers": "1"}, "1051703058411991040": {"author": "@dmitry_svetlich", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-15 05:15:23", "followers": "177"}, "1050617816502558720": {"author": "@dbparedes", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:23:01", "followers": "166"}, "1052023257044475904": {"author": "@yh_note", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-16 02:27:44", "followers": "16"}, "1064384315394482176": {"author": "@michaelsecom", "content_summary": "Read about #Bert - new #AI that's smart https://t.co/d5IVJ6eDXo", "datetime": "2018-11-19 05:06:10", "followers": "1,818"}, "1050876098932891648": {"author": "@a11byte", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-12 22:29:20", "followers": "496"}, "1059698043346243584": {"author": "@mega_takoyaki", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 06:44:36", "followers": "100"}, "1050721876752945152": {"author": "@FlavorDave92", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:16:31", "followers": "86"}, "1060058164140347392": {"author": "@Jay_AHR_", "content_summary": "RT @bigdata: Advancing the state-of-the-art for eleven #NLproc tasks \ud83d\udcd6 BERT (Bidirectional Encoder Representations from Transformers) from\u2026", "datetime": "2018-11-07 06:35:35", "followers": "1,187"}, "1083027565781037061": {"author": "@rctatman", "content_summary": "@fmailhot Oh, whoops! Here's the real link: https://t.co/OLfo5XMD4e", "datetime": "2019-01-09 15:47:48", "followers": "19,019"}, "1050717263517503490": {"author": "@ViktorM80532322", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:58:11", "followers": "115"}, "1065289170766032897": {"author": "@kdxu", "content_summary": "Google \u306e BERT \u306e\u8ad6\u6587 https://t.co/wraYjIU3li", "datetime": "2018-11-21 17:01:45", "followers": "1,513"}, "1050721635831959552": {"author": "@tunguz", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:15:34", "followers": "10,081"}, "1050547113510227968": {"author": "@vasan_ashwin", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:42:04", "followers": "36"}, "1064570411155894273": {"author": "@desertnaut", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-11-19 17:25:39", "followers": "1,025"}, "1072651918697250816": {"author": "@himanshu_ragtah", "content_summary": "RT @profillic: Google open sourced a new technique for NLP pre-training called BERT. With this release, anyone in the world can train their\u2026", "datetime": "2018-12-12 00:38:40", "followers": "507"}, "1051619442231148544": {"author": "@ktansai", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-14 23:43:07", "followers": "2,587"}, "1072727423727296512": {"author": "@stfate", "content_summary": "\u00bb [1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/xQWALStyni", "datetime": "2018-12-12 05:38:42", "followers": "1,772"}, "1118495708515053568": {"author": "@westis96", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-04-17 12:45:32", "followers": "299"}, "1051640667955318784": {"author": "@kaitoluciferzz", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 01:07:28", "followers": "51"}, "1050982945144561664": {"author": "@garygarywang", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:33:55", "followers": "156"}, "1058333158482104320": {"author": "@samim", "content_summary": "Code and pre-trained models for Google's BERT (\"Bidirectional Encoder Representations from Transformers\": https://t.co/ruCT1oEExa): https://t.co/xeeiIQ7MBk https://t.co/niW1PR6whU", "datetime": "2018-11-02 12:21:02", "followers": "19,607"}, "1050699312102068225": {"author": "@108pravi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:46:51", "followers": "154"}, "1050646184140787712": {"author": "@AyoubRmidi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:15:45", "followers": "37"}, "1120686991643828226": {"author": "@takaishikawa42", "content_summary": "1810.04805.pdf https://t.co/0cuvXxqXSD", "datetime": "2019-04-23 13:52:54", "followers": "594"}, "1050596453540753409": {"author": "@trigate099", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:58:08", "followers": "456"}, "1057797394178420736": {"author": "@alshedivat", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 00:52:06", "followers": "714"}, "1050582140499292161": {"author": "@machinelearnflx", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:01:15", "followers": "79,246"}, "1050703328932442113": {"author": "@thembani_p", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:02:49", "followers": "193"}, "1059974221793652736": {"author": "@akima", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 01:02:02", "followers": "560"}, "1058028752494608385": {"author": "@datitran", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 16:11:26", "followers": "2,102"}, "1095888077350825985": {"author": "@mkan_0141", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2019-02-14 03:30:52", "followers": "880"}, "1115914665228414976": {"author": "@XOR0sha2wine", "content_summary": "@argent_smith \u041e\u043d\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u043c\u0443\u043b\u044c\u0442\u0438\u044f\u0437\u044b\u0447\u043d\u044b\u0439 BERT \u0434\u043b\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e: https://t.co/hW3vpcreQr https://t.co/aL2R7cqjGR https://t.co/8ZmlLdlJqO \u0415\u0448\u0435 \u0435\u0441\u0442\u044c \u0437\u0430\u0444\u0430\u0439\u0442\u044e\u043d\u0435\u043d\u043d\u044b\u0439 multilingual BERT \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u043e\u0442 iPavlov, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u043b\u0438 \u043d\u0430 \u043d\u043e\u0432\u043e\u0441\u0442\u044f\u0445 \u0438 \u0412\u0438\u043a\u0438\u043f\u0435\u0434\u0438\u0438: https://t", "datetime": "2019-04-10 09:49:23", "followers": "3,455"}, "1050627021464719360": {"author": "@KouroshMeshgi", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:59:36", "followers": "616"}, "1050662553976619008": {"author": "@Maaarcocr", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 08:20:47", "followers": "231"}, "1050878427832684544": {"author": "@stjaco", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 22:38:36", "followers": "1,356"}, "1128631227844571136": {"author": "@kmd252525", "content_summary": "RT @_hideoyamada: BERT\u306f\u5727\u5012\u7684\u306a\u6027\u80fd\u3092\u51fa\u3057\u3066\u3044\u308b https://t.co/EBxJ3OmzoG #dl4practitioners", "datetime": "2019-05-15 12:00:28", "followers": "34"}, "1051491411269431297": {"author": "@AkiraAnalytics", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 15:14:22", "followers": "48"}, "1050740452385787904": {"author": "@jastner109", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:30:20", "followers": "194"}, "1050578801040609280": {"author": "@mundoplano", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:47:59", "followers": "358"}, "1051046756480299008": {"author": "@pesasape", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 09:47:28", "followers": "47"}, "1050586691143008256": {"author": "@Lovely_LoveLive", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:19:20", "followers": "129"}, "1059814033648443393": {"author": "@HayashiPeke", "content_summary": "\u82f1\u8a9e\u306e\u6700\u5927\u306e\u5229\u70b9\u306f\u3001\u53d6\u5f97\u3067\u304d\u308b\u60c5\u5831\u304c\u7269\u51c4\u304f\u5897\u3048\u308b\u3053\u3068\u3060\u3068\u304a\u3082\u3046\u3002 \u8a71\u305b\u306a\u304f\u3066\u3082\u300c\u8aad\u3081\u308b\u300d\u3060\u3051\u3067\u65b0\u3057\u3044\u60c5\u5831\u304c\u624b\u306b\u5165\u308b\u3060\u3051\u3058\u3083\u306a\u304f\u3066\u3001\u540c\u3058\u3053\u3068\u3067\u3082\u3088\u308a\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u3057\u3066\u3044\u308bsource\u306b\u89e6\u308c\u3089\u308c\u305f\u308a\u3002 \u307b\u3093\u306e\u3001\u307b\u3093\u306e\u5c11\u3057\u305a\u3064\u3067\u3082\u3001\u8ad6\u6587\u306a\u3089abstract\u306e\u6240\u3060\u3051\u3067\u3082\u8aad\u3093\u3067\u307f\u305f\u3089\u4e16\u754c\u304c\u5e83\u304c\u308b\u3002", "datetime": "2018-11-06 14:25:30", "followers": "41"}, "1058347784137568256": {"author": "@ava_punksmash", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-02 13:19:09", "followers": "18"}, "1131563243741122560": {"author": "@Taka_input", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2019-05-23 14:11:15", "followers": "175"}, "1054364161864622080": {"author": "@johannestknd", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-22 13:29:39", "followers": "133"}, "1050664111451729920": {"author": "@danielbloureiro", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:26:59", "followers": "201"}, "1051916119160717312": {"author": "@peterjliu", "content_summary": "After ELMo and BERT (https://t.co/Ah8vNM7OF0), how long until the next Sesame Street character is used as the name for an ML model?", "datetime": "2018-10-15 19:22:01", "followers": "287"}, "1050697865234763776": {"author": "@yashuseth", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:41:06", "followers": "196"}, "1061227318817910784": {"author": "@siingUB", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-10 12:01:24", "followers": "140"}, "1057792095690272769": {"author": "@__youki__", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 00:31:03", "followers": "502"}, "1050624776962596864": {"author": "@akinori_ito", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:50:41", "followers": "1,367"}, "1176489839543406592": {"author": "@HubBase1", "content_summary": "RT @HubBucket: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-24 13:33:30", "followers": "127"}, "1050921473794297857": {"author": "@Pan_MuTux", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 01:29:39", "followers": "7"}, "1050708554586443776": {"author": "@tejscript", "content_summary": "RT @jonathanrraiman: Incredible achievement in NLP Pretraining from @GoogleAI with BERT. I assume translation will fall to this approach ne\u2026", "datetime": "2018-10-12 11:23:35", "followers": "73"}, "1050830229554515969": {"author": "@ChikaObuah", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 19:27:04", "followers": "912"}, "1060171562278543360": {"author": "@python_nnn", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 14:06:12", "followers": "169"}, "1050939731247157248": {"author": "@trigate099", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:42:12", "followers": "456"}, "1050611273505730560": {"author": "@ijiessie", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:57:01", "followers": "49"}, "1057839057126875138": {"author": "@makiedan", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-11-01 03:37:39", "followers": "96"}, "1050693011703615489": {"author": "@dmax10001", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:21:49", "followers": "2"}, "1059659975084015618": {"author": "@2enk", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 04:13:20", "followers": "320"}, "1166026863787028482": {"author": "@TatanMorenoN", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding #NLU https://t.co/3EFjWLcpkg", "datetime": "2019-08-26 16:37:22", "followers": "208"}, "1050617127428739079": {"author": "@AarneTalman", "content_summary": "Interesting work by Google on bidirectional transformer-based language representations. Great improvements on multiple language understanding tasks https://t.co/umT0cbZIMB", "datetime": "2018-10-12 05:20:17", "followers": "335"}, "1054675181833084928": {"author": "@Guutara", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-23 10:05:32", "followers": "1,341"}, "1050706705909276672": {"author": "@halvarflake", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:16:14", "followers": "28,298"}, "1089935296987189248": {"author": "@aarcher510", "content_summary": "@maia_mcc @roeeaharoni The paper: https://t.co/j0d6ABXkP0 And the paper writing about the paper: https://t.co/hCsZcSrsQC", "datetime": "2019-01-28 17:16:39", "followers": "204"}, "1050615493902577664": {"author": "@triper1022", "content_summary": "\u305f\u307e\u306b \u4eba\u5de5\u77e5\u80fd\u306e\u7814\u7a76\u306b\u95a2\u3059\u308b\u30da\u30fc\u30d1\u30fc\u3092\u8aad\u307f\u307e\u3059", "datetime": "2018-10-12 05:13:47", "followers": "325"}, "1050733176228843525": {"author": "@pragmaticml", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:01:25", "followers": "1,217"}, "1052346956125634560": {"author": "@OmarUFlorez", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-16 23:54:00", "followers": "935"}, "1053469644999151616": {"author": "@DCivin", "content_summary": "https://t.co/SYUn95QDzA & https://t.co/PoJ1Cdi5Dl & https://t.co/V8f43DqXw4", "datetime": "2018-10-20 02:15:10", "followers": "612"}, "1052978389672517632": {"author": "@Vasca96", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-18 17:43:06", "followers": "85"}, "1205095730228211712": {"author": "@norpadon", "content_summary": "@kazdalevsky @0player @skazal_net \u0414\u0430 \u043d\u0435\u0442, \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0442 \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0443\u0436\u0435. BERT (https://t.co/iDSlodd9nw) \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u0435 \u0443 \u0433\u0443\u0433\u043b\u0430 \u0432 \u043f\u043e\u0438\u0441\u043a\u0435", "datetime": "2019-12-12 12:03:06", "followers": "357"}, "1059504214362120192": {"author": "@hafoc", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-05 17:54:23", "followers": "242"}, "1051120706518433795": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-13 14:41:19", "followers": "3,500"}, "1050638390972936193": {"author": "@TDehaene", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:44:46", "followers": "147"}, "1050609431296204800": {"author": "@sappy_and_sappy", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 04:49:42", "followers": "175"}, "1050551559673839616": {"author": "@LuisJatana", "content_summary": "RT JatanaHQ \"[R] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/wZ1jQS760r\"", "datetime": "2018-10-12 00:59:44", "followers": "23"}, "1051044624507265024": {"author": "@cmarschner", "content_summary": "The NLProc world is shaken up by the models ELMo and BERT \ud83d\ude02.", "datetime": "2018-10-13 09:39:00", "followers": "721"}, "1050669722369384448": {"author": "@elikiper", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 08:49:16", "followers": "228"}, "1050705864150126592": {"author": "@sam_havens", "content_summary": "This looks even better than ULM-FiT, which I haven\u2019t shut up about for the last few months... though I assume the name (BERT) is a reference to ELMo by @allen_ai. Retrainable language models are becoming my go-to. Easy to imagine them displacing pretraine", "datetime": "2018-10-12 11:12:53", "followers": "662"}, "1050625596319911939": {"author": "@salamander_jp", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-12 05:53:56", "followers": "133"}, "1052529887192350720": {"author": "@ImQuangPham", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-17 12:00:54", "followers": "4"}, "1061014904998518784": {"author": "@chibicode", "content_summary": "My favorite part of the BERT paper (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding) https://t.co/yLTafeqSVJ https://t.co/rOv9WkYKaj", "datetime": "2018-11-09 21:57:20", "followers": "15,980"}, "1089622096060313603": {"author": "@vivilearns2code", "content_summary": "ToDo List: - https://t.co/V6eKS2ZaZP", "datetime": "2019-01-27 20:32:06", "followers": "3"}, "1050894364199833601": {"author": "@jeremyjordan", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:41:55", "followers": "1,321"}, "1056114426359627776": {"author": "@nzkronus", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-27 09:24:35", "followers": "363"}, "1050616650007949312": {"author": "@YvesMarie_Seite", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:18:23", "followers": "144"}, "1059597986609618944": {"author": "@gsminek", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:07:01", "followers": "1,073"}, "1132539570103496704": {"author": "@PiousGeek", "content_summary": "Chai,,, dammit \ud83d\ude2b Man's been reading/doing LSI for years oo, SEO on rewind \ud83d\ude2d", "datetime": "2019-05-26 06:50:49", "followers": "8,704"}, "1050924404195102720": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-13 01:41:17", "followers": "3,500"}, "1051406541230825472": {"author": "@tsuchi0120", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 09:37:08", "followers": "253"}, "1206927157961740288": {"author": "@jaguring1", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Kan2rWqMqu", "datetime": "2019-12-17 13:20:32", "followers": "12,765"}, "1067386380605509632": {"author": "@ynupc", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-11-27 11:55:18", "followers": "7,671"}, "1059610193435082752": {"author": "@ksyundo", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:55:31", "followers": "1,186"}, "1050704637538447360": {"author": "@shrikantkashyap", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 11:08:01", "followers": "22"}, "1050720335350427648": {"author": "@amaru_muru", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 12:10:24", "followers": "206"}, "1050741368841269249": {"author": "@tiagoandresvaz", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:33:58", "followers": "486"}, "1051029454275928064": {"author": "@ciscaoladipo", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 08:38:43", "followers": "889"}, "1051054906151329792": {"author": "@ScriptShade", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 10:19:51", "followers": "221"}, "1051810782898413568": {"author": "@a4_nghm", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 12:23:26", "followers": "126"}, "1132815330563297280": {"author": "@tstolber", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 01:06:36", "followers": "360"}, "1050951930606809088": {"author": "@ki0PJyJXBV4wMYf", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 03:30:40", "followers": "66"}, "1050789162583056385": {"author": "@cartalop", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:43:53", "followers": "515"}, "1050568221600804864": {"author": "@Sam09lol", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:05:57", "followers": "0"}, "1051135449979187200": {"author": "@kobi78", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 15:39:55", "followers": "335"}, "1050929990269775872": {"author": "@tomo_makes", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:03:29", "followers": "1,054"}, "1050688941463097344": {"author": "@emilmont", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:05:39", "followers": "385"}, "1050770300013617152": {"author": "@Tanaygahlot", "content_summary": "RT @hn_frontpage: BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding L: https://t.co/14zbtYAA9s C: https://t.\u2026", "datetime": "2018-10-12 15:28:56", "followers": "251"}, "1050585285870796800": {"author": "@msaffarm", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:13:45", "followers": "453"}, "1057212039087587328": {"author": "@Singularity_43", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-10-30 10:06:06", "followers": "201"}, "1050553744935591936": {"author": "@berty38", "content_summary": "Aw man. This is going to make conversations really confusing.", "datetime": "2018-10-12 01:08:25", "followers": "1,154"}, "1050717643454537728": {"author": "@lena_uskova", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:59:42", "followers": "35"}, "1050686968231862273": {"author": "@mosicr", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:57:48", "followers": "115"}, "1050800337437757440": {"author": "@ak1010", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 17:28:18", "followers": "1,088"}, "1052038438113837056": {"author": "@big2teacher", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-16 03:28:04", "followers": "177"}, "1059590284214886401": {"author": "@FormSmooth", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 23:36:24", "followers": "21"}, "1050564123270905856": {"author": "@crude2refined", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:49:40", "followers": "664"}, "1051192656255053824": {"author": "@dharmeshkakadia", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 19:27:14", "followers": "922"}, "1171505207630413825": {"author": "@CchangJ", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026", "datetime": "2019-09-10 19:26:21", "followers": "2"}, "1050821603926241280": {"author": "@dondini", "content_summary": "So - I presume you've seen this already: https://t.co/fPdLI8sbPJ", "datetime": "2018-10-12 18:52:48", "followers": "196"}, "1050729968244678656": {"author": "@SepehrAkhavan", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 12:48:40", "followers": "281"}, "1050986730050879488": {"author": "@yuhang_dl", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:48:57", "followers": "5"}, "1050744010481000454": {"author": "@vykthur", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 13:44:28", "followers": "1,755"}, "1051048010304565249": {"author": "@alvations", "content_summary": "Hmmmm... \ud83e\udd14", "datetime": "2018-10-13 09:52:27", "followers": "1,006"}, "1051259122891481088": {"author": "@atsuc", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 23:51:21", "followers": "54"}, "1231929256583651328": {"author": "@zakki", "content_summary": "RT @icoxfog417: \u5b9f\u88c5\u898b\u305f\u9650\u308a\u307e\u3063\u305f\u304fNER\u3089\u3057\u3055\u3092\u611f\u3058\u306a\u304b\u3063\u305f\u304c\u3001BERT\u306e\u8ad6\u6587\u3092\u307f\u308b\u3068CRF\u3092\u4f7f\u308f\u305a\u5165\u529bToken\u306e\u6700\u521d\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u5206\u985e\u6a5f\u306b\u304b\u3051\u3066\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3088\u3046\u3060\u306a(5.3)\u3002\u307b\u307c\u5206\u985e\u306e\u5ef6\u9577\u3067\u5b9f\u88c5\u3067\u304d\u3066\u7cbe\u5ea6\u3082\u51fa\u308b\u306a\u3089\u3042\u308a\u304c\u305f\u3044\u3068\u3053\u308d\u3060\u3002 https://\u2026", "datetime": "2020-02-24 13:09:57", "followers": "153"}, "1050770786263347202": {"author": "@shigekzishihara", "content_summary": "RT @jaguring1: SWAG\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u5e38\u8b58\u63a8\u8ad6\u30bf\u30b9\u30af\uff09\u306b\u304a\u3044\u3066\u3001BERT\u304c\u65e9\u3005\u306b\u4eba\u9593\u30ec\u30d9\u30eb\u306b\u5230\u9054\uff01\uff1f BERT\uff082018\u5e7410\u670811\u65e5\uff01\uff01\uff01\uff09 https://t.co/Kan2rWqMqu SWAG\uff082018\u5e748\u670816\u65e5\uff01\uff01\uff01\uff09 https://t.co/bSA\u2026", "datetime": "2018-10-12 15:30:52", "followers": "5,146"}, "1051609140483653632": {"author": "@RND_png", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 23:02:11", "followers": "3,999"}, "1050666852232261634": {"author": "@talkdatatomee", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:37:52", "followers": "369"}, "1050933574759673857": {"author": "@syikootw", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:17:44", "followers": "330"}, "1051590947660677121": {"author": "@chokkanorg", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 21:49:54", "followers": "3,753"}, "1050772076477509638": {"author": "@ItsMeMaho", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:36:00", "followers": "9"}, "1050561194434551809": {"author": "@kuanchen22", "content_summary": "larger model, more data, more computing power, better performance, wonder when will we reach the limit...", "datetime": "2018-10-12 01:38:01", "followers": "63"}, "1051016193874657280": {"author": "@tovbinm", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 07:46:02", "followers": "531"}, "1051348268410494976": {"author": "@HundertSteine", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 05:45:34", "followers": "770"}, "1152294152383684610": {"author": "@HubBucket", "content_summary": "\u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Language Understanding at large scale Article: https://t.co/Uh46oeFvbo Paper: https://t.co/gu5abzjVB7 @Azure @HubBucket @HubDataScience", "datetime": "2019-07-19 19:08:29", "followers": "5,315"}, "1051152403645550593": {"author": "@jp_axs4ll", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 16:47:17", "followers": "426"}, "1050692974328406016": {"author": "@Swayson", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:21:40", "followers": "157"}, "1050757045861793793": {"author": "@yizhongwyz", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 14:36:16", "followers": "193"}, "1050694435200278528": {"author": "@seb_ruder", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidirectional LM pretraining w/ masking - Next-sentence prediction aux task - Bigger, more data It seems LM pretraining is here to stay.", "datetime": "2018-10-12 10:27:28", "followers": "44,898"}, "1187830555712380929": {"author": "@jessthebp", "content_summary": "and because it's important to cite your sources, TO LEARN MORE: https://t.co/NwYpgkDqx9 https://t.co/r18NlJAzPS https://t.co/zKAcWeUX44", "datetime": "2019-10-25 20:37:27", "followers": "546"}, "1083881696192131072": {"author": "@Joeliu2016", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2019-01-12 00:21:48", "followers": "48"}, "1050777193746436096": {"author": "@PabloDoval", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:56:20", "followers": "978"}, "1050730737064792065": {"author": "@vksbhandary", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 12:51:44", "followers": "282"}, "1051151253198295040": {"author": "@faaez", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 16:42:42", "followers": "342"}, "1050551455495667713": {"author": "@MohitIyyer", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:59:19", "followers": "2,675"}, "1051348390779400192": {"author": "@Tarpon_red2", "content_summary": "RT @TSaodake: BERT\u304c\u3059\u3054\u3044\u3089\u3057\u3044 https://t.co/h0mMPCboG9", "datetime": "2018-10-14 05:46:04", "followers": "2,861"}, "1187780405497417729": {"author": "@mikethebbop", "content_summary": "If you are interested in learning more about BERT: https://t.co/Al8r8NIbNu", "datetime": "2019-10-25 17:18:11", "followers": "7,903"}, "1052984655215964160": {"author": "@danbri", "content_summary": "second longest character sequence?", "datetime": "2018-10-18 18:07:59", "followers": "8,940"}, "1114257464076034048": {"author": "@nskm_m", "content_summary": "RT @app_math_study: \u305f\u307e\u305f\u307e\u3054\u8cea\u554f\u3044\u305f\u3060\u3044\u3066BERT\u3092\u8efd\u304f\u8aad\u3093\u3060\u3051\u3069\u9762\u767d\u305d\u3046\uff01\u3071\u3063\u3068\u898b\u305f\u611f\u3058\u306f\u8a00\u8a9e\u51e6\u7406\u7528\u306epre-train\u30e2\u30c7\u30eb\u306e\u7814\u7a76\u3063\u307d\u3044\u3002 \u306a\u3093\u304b\u306e\u4f01\u753b\u3067\u3053\u306e\u8fba\u306e\u8a00\u8a9e\u51e6\u7406\u7cfb\u30e1\u30a4\u30f3\u3067\u6271\u3046\u306e\u3082\u697d\u3057\u305d\u3046\u3002 https://t.co/EifSSJRVwN", "datetime": "2019-04-05 20:04:15", "followers": "190"}, "1132561425438846976": {"author": "@seokai", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 08:17:40", "followers": "4,231"}, "1050620037072007168": {"author": "@Singularity_43", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 05:31:51", "followers": "201"}, "1250498507577987072": {"author": "@sussenglish", "content_summary": "#BERT: State of the Art #NLP #Model, Explained The paper: https://t.co/Qxyie9Dj2k https://t.co/Oi80ysyrL3 https://t.co/AaC9IslMaP", "datetime": "2020-04-15 18:57:32", "followers": "625"}, "1050627054842957824": {"author": "@zhaochun_ren", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:59:44", "followers": "424"}, "1050589054092230658": {"author": "@penzant", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Yf9xAuTThJ >Training of BERT_LARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre-training took 4 days to complete.", "datetime": "2018-10-12 03:28:44", "followers": "576"}, "1050891318237380608": {"author": "@howardmeng", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 23:29:49", "followers": "44"}, "1050546692615856128": {"author": "@pijili", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:40:24", "followers": "521"}, "1051136361632288768": {"author": "@alexhock", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 15:43:32", "followers": "193"}, "1133438860946362368": {"author": "@StephenPiment", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/WBTDiCmbLF", "datetime": "2019-05-28 18:24:17", "followers": "6,128"}, "1171231932421722115": {"author": "@_Sharraf", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026", "datetime": "2019-09-10 01:20:27", "followers": "718"}, "1075368104236965889": {"author": "@math_elliptic", "content_summary": "@NegeLon \u3053\u306etw\u306e\u56db\u679a\u76ee\u306e\u753b\u50cf\u3068\u304b\u898b\u3066\u307f\u305f\uff1f https://t.co/ES33LDuCBY", "datetime": "2018-12-19 12:31:50", "followers": "1,176"}, "1234433879043428352": {"author": "@SpirosDenaxas", "content_summary": "I really liked this paper \"A Primer in BERTology: What we know about how BERT works\" by @annargrs & coauthors, gives a really useful overview of BERT accessible to non-NLP researchers like myself :) Paper: https://t.co/JgvHniqUOA original BERT paper: h", "datetime": "2020-03-02 11:02:26", "followers": "1,174"}, "1050893772827942913": {"author": "@ivan_bezdomny", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:39:34", "followers": "3,100"}, "1050575756609359872": {"author": "@313V", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/BARXQTNVEr", "datetime": "2018-10-12 02:35:53", "followers": "518"}, "1051380733246242817": {"author": "@morioka", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:54:35", "followers": "826"}, "1050714060159881216": {"author": "@montrealdotai", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:45:27", "followers": "33,955"}, "1132697676632526849": {"author": "@delzennejc", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 17:19:05", "followers": "185"}, "1050695110139072512": {"author": "@hey_kishore", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:30:09", "followers": "28"}, "1081932056093966337": {"author": "@yasinesref", "content_summary": "\"Beginning of a new era in NLP\" https://t.co/WS7jiBuN9e", "datetime": "2019-01-06 15:14:38", "followers": "189"}, "1059593065055313920": {"author": "@fuku0185", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 23:47:27", "followers": "2,020"}, "1058173640347271168": {"author": "@Adarshreddyash", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 01:47:10", "followers": "41"}, "1050923812429139969": {"author": "@togelius", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 01:38:56", "followers": "9,620"}, "1050933989710557184": {"author": "@harujoh", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:19:23", "followers": "380"}, "1176995896891969536": {"author": "@HubBaseDB", "content_summary": "RT @HubBaseDB: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-25 23:04:24", "followers": "127"}, "1064720721077829632": {"author": "@diegovogeid", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-20 03:22:56", "followers": "68"}, "1050937764550590464": {"author": "@bylloop", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 02:34:23", "followers": "17"}, "1057790462923177985": {"author": "@hardmaru", "content_summary": "Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representations that get state-of-the-art results on a bunch of NLP tasks. pdf: https://t.co/I91U8wPs2c code: https://t.co/kfPFXocGBL The release", "datetime": "2018-11-01 00:24:33", "followers": "81,650"}, "1058078860368068608": {"author": "@IntuitMachine", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 19:30:33", "followers": "4,812"}, "1154017352033427457": {"author": "@RobertoGEMartin", "content_summary": "#AI #NLP #BERT #OpenSource Microsoft open-sources scripts and notebooks to pre-train BERT natural language model with domain-specific texts https://t.co/EV0YVW3LLg paper: https://t.co/HVha4gFH8I code: https://t.co/dh0Wz2ifc5 https://t.co/avmDpkp4lY", "datetime": "2019-07-24 13:15:51", "followers": "160"}, "1050763396856107008": {"author": "@rsilveira79", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:01:30", "followers": "178"}, "1050580180589314048": {"author": "@MANIKANTROY", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 02:53:28", "followers": "186"}, "1051371058387804160": {"author": "@11shubh_laabh11", "content_summary": "RT @arxiv_cs_cl: https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.0480\u2026", "datetime": "2018-10-14 07:16:08", "followers": "66"}, "1051635862419562496": {"author": "@Maxwell_110", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 00:48:22", "followers": "3,187"}, "1050768247732224002": {"author": "@DDDzai", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:20:47", "followers": "0"}, "1050737467807141888": {"author": "@BHerrmann_INTL", "content_summary": "RT @fgilbane: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u201c\u2026 obtains new state-of-the-art results on e\u2026", "datetime": "2018-10-12 13:18:28", "followers": "1,864"}, "1051078217824722952": {"author": "@DanielCanueto", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 11:52:29", "followers": "206"}, "1050721255010263045": {"author": "@amaru_muru", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:14:03", "followers": "206"}, "1050755370023444481": {"author": "@_1651324671212", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:29:36", "followers": "908"}, "1050772823361253377": {"author": "@fukutax", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 15:38:58", "followers": "653"}, "1089693045065437184": {"author": "@gigimimimi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-01-28 01:14:02", "followers": "1,114"}, "1050757339068821505": {"author": "@thakurrajanand", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 14:37:26", "followers": "129"}, "1187791438550949888": {"author": "@leojiang145", "content_summary": "Welcome BERT \ud83d\udcf0 https://t.co/Hc9vA8GRg7", "datetime": "2019-10-25 18:02:01", "followers": "74"}, "1226964060064899073": {"author": "@IvanM08106184", "content_summary": "Google's new Bidirectional Encoder Representations from Transformers (BERT) helps understand the nuances and context of words in searches. Paper: https://t.co/hSWHYAw3P0 #cs800s20 #TIL", "datetime": "2020-02-10 20:20:02", "followers": "12"}, "1050856590373519360": {"author": "@GaltierThomas", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 21:11:49", "followers": "66"}, "1051439644901027845": {"author": "@AndreasHoepner", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 11:48:40", "followers": "1,363"}, "1050964783397097472": {"author": "@ailurus1991", "content_summary": "\u4e00\u89c9\u9192\u6765\u4e16\u754c\u90fd\u53d8\u4e86\uff0cBERT\u8fd9\u4e48\u5f3a\u7684\u5417\u3002\u3002\u3002", "datetime": "2018-10-13 04:21:45", "followers": "8,922"}, "1050557087481094144": {"author": "@muktabh", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 01:21:42", "followers": "781"}, "1051017298427961344": {"author": "@GillesMoyse", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 07:50:25", "followers": "1,058"}, "1055711536268894214": {"author": "@cpomeara", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-26 06:43:39", "followers": "90"}, "1117155700835274752": {"author": "@HugoPump", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-04-13 20:00:49", "followers": "257"}, "1059547601790353408": {"author": "@A_Ym", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 20:46:48", "followers": "408"}, "1051113716106719232": {"author": "@chrix2", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 14:13:33", "followers": "448"}, "1050780119290261504": {"author": "@MarcoZorzi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:07:57", "followers": "289"}, "1050785958814277632": {"author": "@ArthurCamara", "content_summary": "RT @teagermylk: Or maybe we need better human performance metrics. SWAG human evaluation is only on 100 samples, so the human accuracy of @\u2026", "datetime": "2018-10-12 16:31:09", "followers": "589"}, "1050734030910558208": {"author": "@khandelia1000", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 13:04:49", "followers": "31"}, "1050936332648820736": {"author": "@bamboo4031", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 02:28:41", "followers": "235"}, "1050797476540755977": {"author": "@_willfalcon", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 17:16:55", "followers": "1,574"}, "1050730371749425152": {"author": "@pywirrarika", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 12:50:16", "followers": "331"}, "1064521450231554048": {"author": "@Adarshreddyash", "content_summary": "https://t.co/4IBkLe4pxF", "datetime": "2018-11-19 14:11:06", "followers": "41"}, "1089373193519935491": {"author": "@NoyoMeDicen", "content_summary": "Espero al de esta semana si poder entenderlo", "datetime": "2019-01-27 04:03:03", "followers": "439"}, "1050941807222317057": {"author": "@hellorahulk", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 02:50:27", "followers": "264"}, "1051169942639431680": {"author": "@ErickMuzart", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 17:56:58", "followers": "108"}, "1057758663551549440": {"author": "@td_lomf", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-31 22:18:12", "followers": "186"}, "1059606609867956224": {"author": "@strayflotsam", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:41:16", "followers": "482"}, "1051622148635127809": {"author": "@Sionn1224", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-14 23:53:53", "followers": "491"}, "1059636967774666752": {"author": "@eryngi", "content_summary": "\u3053\u3046\u3044\u3046\u306e\u306b\u3064\u3044\u3066\u308b\u30a4\u30e9\u30b9\u30c8\u3063\u3066\u30a2\u30cb\u30e1\u3063\u307d\u3044\u3053\u3068\u304c\u591a\u3044\u306d\u3002\u305d\u3057\u3066\u5973\u6027\u306e\u3053\u3068\u304c\u591a\u3044\uff1f", "datetime": "2018-11-06 02:41:54", "followers": "836"}, "1050750110496247808": {"author": "@tochikuji", "content_summary": "\u30d3\u30c3\u30c8\u5217\u306e\u8aa4\u308a\u7387\u6e2c\u308c\u305d\u3046 https://t.co/cIO6XSD4XT BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "datetime": "2018-10-12 14:08:42", "followers": "830"}, "1051539329611251712": {"author": "@fuchi_800", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 18:24:47", "followers": "148"}, "1059800819682357248": {"author": "@ChenShan1988", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-06 13:33:00", "followers": "30"}, "1050730485746491393": {"author": "@stealth_hex", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 12:50:44", "followers": "19"}, "1050994303974170624": {"author": "@marsemo", "content_summary": "RT @marcfede: Thanks for sharing this! https://t.co/2csID0xWQp", "datetime": "2018-10-13 06:19:03", "followers": "516"}, "1050814344110493697": {"author": "@beduffy1", "content_summary": "The Language Model pre-train train keeps on chugging along. And finally we beat human F1 performance on SQuAD!!!! I was following that leaderboard for yonks! Now onto harder QA datasets like: Qangaroo, NarrativeQA, CLINIQA, FigureQA, SQuAD 2.0,NewsQA, Wik", "datetime": "2018-10-12 18:23:57", "followers": "350"}, "1050649535909748737": {"author": "@sbmaruf", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 07:29:04", "followers": "21"}, "1050592377595879425": {"author": "@Allen_A_N", "content_summary": "Best new SQuAD score :)", "datetime": "2018-10-12 03:41:56", "followers": "447"}, "1050712521592705025": {"author": "@drjohncliu", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 11:39:21", "followers": "510"}, "1050566934247424001": {"author": "@BlyNotes", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:00:50", "followers": "192"}, "1050635340900233218": {"author": "@marcfede", "content_summary": "Thanks for sharing this!", "datetime": "2018-10-12 06:32:39", "followers": "508"}, "1059603018079825920": {"author": "@hayano", "content_summary": "\uff08Bidirectional Encoder Representations from Transformers \u3059\u3054\u3044\u306e\u306d\u2026 https://t.co/KGpyFUvu3v \uff09", "datetime": "2018-11-06 00:27:00", "followers": "128,831"}, "1119009234283372545": {"author": "@iMaSije", "content_summary": "RT @WolframResearch: New in the #WolframNetRepo: extract text features w/ a BERT model trained on BookCorpus & Wikipedia https://t.co/kxDgM\u2026", "datetime": "2019-04-18 22:46:06", "followers": "186"}, "1051159740825460736": {"author": "@flowing", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-13 17:16:26", "followers": "236"}, "1050560444753162242": {"author": "@hacker_news_hir", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding : https://t.co/QDFO3yRL6N Comments: https://t.co/ZteQA7ILX6", "datetime": "2018-10-12 01:35:03", "followers": "347"}, "1085926442557267970": {"author": "@nkmtoo6", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2019-01-17 15:46:54", "followers": "209"}, "1050574616341565441": {"author": "@bharadwajymg", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:31:21", "followers": "19"}, "1051115025358868481": {"author": "@rgreenjr", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 14:18:45", "followers": "208"}, "1051307918149136384": {"author": "@sugi3_34", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 03:05:14", "followers": "1,340"}, "1050825406650101760": {"author": "@daigo_hirooka", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 19:07:54", "followers": "243"}, "1219060861735751682": {"author": "@HotCompScience", "content_summary": "Most popular computer science paper of the day: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" https://t.co/LqBsDIA3mF https://t.co/veMHNz1hbi", "datetime": "2020-01-20 00:55:33", "followers": "281"}, "1050729819615313921": {"author": "@overleo", "content_summary": "https://t.co/zRmAgaEN1x: https://t.co/zRmAgaEN1x https://t.co/Z69JUBWRQ2 [ml]", "datetime": "2018-10-12 12:48:05", "followers": "200"}, "1050734243343679488": {"author": "@maqrollmanuel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:05:39", "followers": "160"}, "1053894580284715008": {"author": "@jaguring1", "content_summary": "\u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-10-21 06:23:43", "followers": "12,765"}, "1051069148854345728": {"author": "@Trtd6Trtd", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 11:16:27", "followers": "155"}, "1051682482737709057": {"author": "@derekchen14", "content_summary": "RT @rown: Totally! But it\u2019s not the task that\u2019s solved, it\u2019s the dataset :) it remains to be seen whether collecting the data using the sam\u2026", "datetime": "2018-10-15 03:53:37", "followers": "249"}, "1050590433464672256": {"author": "@conjugate_box", "content_summary": "RT @penzant: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Yf9xAuTThJ >Training of BERT_LAR\u2026", "datetime": "2018-10-12 03:34:13", "followers": "473"}, "1050643194457640961": {"author": "@jbkavungal", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/3gf34nDkqE #techvu", "datetime": "2018-10-12 07:03:52", "followers": "1,247"}, "1213956503373258752": {"author": "@Alfons_Valencia", "content_summary": "RT @wzuidema: [13] J. Devlin, M. Chang, K. Lee and K. Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Underst\u2026", "datetime": "2020-01-05 22:52:39", "followers": "5,383"}, "1059641138657677313": {"author": "@anoushnajarian", "content_summary": "Reading a cool #NLP paper on #BERT with the lights out! https://t.co/ir442r01c3 #shelovesmatlab https://t.co/VaYI1IntVh", "datetime": "2018-11-06 02:58:29", "followers": "1,905"}, "1050880892296720385": {"author": "@CharlesAydin", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 22:48:23", "followers": "128"}, "1051263867114774529": {"author": "@ayirpelle", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-14 00:10:12", "followers": "2,672"}, "1135434662346670080": {"author": "@raffcoach", "content_summary": "RT @wordmetrics: This is an excellent explanation of BERT. https://t.co/2Arb3EFxX4 Original Google AI paper here: https://t.co/yhp9vkKNRR\u2026", "datetime": "2019-06-03 06:34:53", "followers": "163"}, "1050585557657505792": {"author": "@KentaroOgawa", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:14:50", "followers": "887"}, "1059808455706402817": {"author": "@gijigae", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-06 14:03:20", "followers": "19,021"}, "1050898520306343936": {"author": "@annabelle_nlp", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:58:26", "followers": "184"}, "1050587857134346240": {"author": "@sigitpurnomo", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 03:23:58", "followers": "2,663"}, "1066360334246506496": {"author": "@shion_honda", "content_summary": "Bidirectional Transformers [Devlin+, 2018] \u5927\u578b\u5316\u3057\u305f\u53cc\u65b9\u5411Transformer(BERT)\u3092\u30de\u30b9\u30af\u4ed8\u304d\u8a00\u8a9e\u30e2\u30c7\u30eb\u3068\u5f8c\u7d9a\u6587\u4e88\u6e2c\u3068\u3044\u30462\u3064\u306e\u30bf\u30b9\u30af\u3067\u4e8b\u524d\u5b66\u7fd2\u3055\u305b\u305f\u5f8c\u3001\u30bf\u30b9\u30af\u3054\u3068\u306bfine-tune\u3059\u308b\u3053\u3068\u3067\u300111\u500b\u306e\u30bf\u30b9\u30af\u3067SOTA\u3092\u9054\u6210\u3057\u305f\u3002\u7279\u306bSQuAD\u3067\u306f\u4eba\u9593\u3092\u8d85\u3048\u305f\u3002 https://t.co/Xvh0pFqBdS #NowReading https://t.co/A6tGvthrrO", "datetime": "2018-11-24 15:58:10", "followers": "2,481"}, "1050587152436748290": {"author": "@PGajjewar", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:21:10", "followers": "33"}, "1050630469014843394": {"author": "@elasticjava", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:13:18", "followers": "1,390"}, "1059365795505958912": {"author": "@Nooo0o0Ooo", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-11-05 08:44:22", "followers": "279"}, "1142069783866642432": {"author": "@bill_slawski", "content_summary": "@jpaulhendricks There seems to be a lot of fondness for BERT at Google. No telling how much it is being used on Google index of the Web, but that is worth thinking about: https://t.co/uhlZp3bwjK", "datetime": "2019-06-21 14:00:29", "followers": "51,099"}, "1132790665425936384": {"author": "@PeterNikolow", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 23:28:35", "followers": "4,758"}, "1259093414982742016": {"author": "@3XCL4M4T10N", "content_summary": "Transformer\u3092\u7528\u3044\u305f\u8a00\u8a9e\u51e6\u7406\u30e2\u30c7\u30eb\u306e\u63d0\u6848\uff0e \u53cc\u65b9\u5411\u304b\u3089\u306e\u5b66\u7fd2\u3092\u53ef\u80fd\u306b\u3059\u308b2\u3064\u306e\u4e8b\u524d\u5b66\u7fd2\u30bf\u30b9\u30af(MLM, NSP)\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3042\u3089\u3086\u308bNLP\u30bf\u30b9\u30af\u306b\u304a\u3051\u308bstate-of-the-art\u306a\u7d50\u679c\u3092\u6b8b\u3057\u305f\uff0e https://t.co/wdx6VsgpZT", "datetime": "2020-05-09 12:10:37", "followers": "4"}, "1051490221483732992": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 15:09:39", "followers": "843"}, "1050721869492412416": {"author": "@ArchieIndian", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 12:16:29", "followers": "723"}, "1098672055652880384": {"author": "@jacqueslethuaut", "content_summary": "3 months ago, @GoogleAI released a paper on a new #language representation model. It aims to create a deep bidirectional representations of both contexts https://t.co/VnzN1cOte3", "datetime": "2019-02-21 19:53:25", "followers": "60"}, "1050577851886575616": {"author": "@SigP226", "content_summary": "#stanfordnlp RT lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive compute is all you need. BERT from GoogleAI: SOTA results on everything https://t.co/iiUnOTaEPY. Resul\u2026", "datetime": "2018-10-12 02:44:13", "followers": "257"}, "1050546248917217281": {"author": "@dfm794", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 00:38:38", "followers": "162"}, "1050809372367093760": {"author": "@gurvindahiya", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 18:04:12", "followers": "116"}, "1142768622672261120": {"author": "@sussenglish", "content_summary": "Full paper https://t.co/f9sTVgfV5g", "datetime": "2019-06-23 12:17:25", "followers": "625"}, "1051214884724195328": {"author": "@GattoniLuca", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 20:55:33", "followers": "9"}, "1051010912906137606": {"author": "@Peach_Brother", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 07:25:03", "followers": "66"}, "1052343732417134592": {"author": "@mainyaa", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-16 23:41:12", "followers": "1,784"}, "1203085340036210688": {"author": "@EinsteinsAttic", "content_summary": "Fascinating peek behind the scenes at @FullFact. After the @Microsoft #Bob & #Tay melt-downs, who knew @Google #BERT* would turn out to be so useful? -- *https://t.co/MieYP7fBjN", "datetime": "2019-12-06 22:54:31", "followers": "2,055"}, "1051617276753895425": {"author": "@takahi_i", "content_summary": "\ud83d\udc40 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Lir9Y0ykwl", "datetime": "2018-10-14 23:34:31", "followers": "1,259"}, "1132713390105923584": {"author": "@delpapelalaweb", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 18:21:31", "followers": "1,232"}, "1057801343988101121": {"author": "@BioNLProc", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 01:07:48", "followers": "361"}, "1113814600930209792": {"author": "@vykthur", "content_summary": "Reading the BERT paper in detail - strikes me how much effort (a whole figure and several sections) is put into distinguishing their work from related papers (Open AI GPT, ELMO etc). Anything less and the paper will be rejected by reviewer 2 \ud83d\ude1e. #academia h", "datetime": "2019-04-04 14:44:29", "followers": "1,755"}, "1059788537107640320": {"author": "@masaki_iwahara", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 12:44:11", "followers": "37"}, "1050837537198104576": {"author": "@lemborio", "content_summary": "RT @arxiv_cscl: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyrbYpu", "datetime": "2018-10-12 19:56:07", "followers": "283"}, "1072999468755238912": {"author": "@TusharJain_007", "content_summary": "RT @alienelf: Just used BERT pre-trained models on a problem we were initially using Skip-thoughts for and have been fighting with for year\u2026", "datetime": "2018-12-12 23:39:43", "followers": "122"}, "1050904874399936512": {"author": "@versae", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 00:23:41", "followers": "615"}, "1105942043732393989": {"author": "@denis_arnaud", "content_summary": "https://t.co/mZZGfAh2Nc https://t.co/KYDKtBTqzh", "datetime": "2019-03-13 21:21:45", "followers": "680"}, "1051163596737007618": {"author": "@lao_ni", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-13 17:31:45", "followers": "57"}, "1050751793569792002": {"author": "@Sam09lol", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:15:24", "followers": "0"}, "1050678973267492864": {"author": "@AntoineCollas", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:26:02", "followers": "42"}, "1192253360826699776": {"author": "@freakonometrics", "content_summary": "\"Computers Are Learning to Read\u2014But They're Still Not So Smart\" https://t.co/qyV95tj8ne with GLUE https://t.co/2Qqv1Bd9ri, Word2Vec https://t.co/kVZ5VxJCjr, BERT https://t.co/0HEKX3UJff (a lot of BERT, actually)", "datetime": "2019-11-07 01:32:06", "followers": "26,980"}, "1051021836945776640": {"author": "@brianwithaneye", "content_summary": "Amazing, @googleai has blown ahead of human performance on the Stanford question answering task.", "datetime": "2018-10-13 08:08:27", "followers": "10"}, "1050930303022161920": {"author": "@imaginebit", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:04:44", "followers": "123"}, "1051501061997903877": {"author": "@yo_ehara", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 15:52:43", "followers": "2,065"}, "1058160477816016898": {"author": "@PGajjewar", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 00:54:52", "followers": "33"}, "1051617633357819905": {"author": "@K_Ryuichirou", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:35:56", "followers": "782"}, "1050668747881570305": {"author": "@sfenliva", "content_summary": "RT @dawnieando: Meet BERT from Google AI @GoogleAI (great name although I am biased as it is the same name as my pomeranian) -> 'BERT: Pre-\u2026", "datetime": "2018-10-12 08:45:24", "followers": "576"}, "1050898887324528641": {"author": "@ReedRoof", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:59:54", "followers": "28"}, "1051375912418066432": {"author": "@sughimsi", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 07:35:25", "followers": "232"}, "1050704902517854208": {"author": "@fgilbane", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u201c\u2026 obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4%\u201d h/t @alexismadrigal https://t.co/7Gn2qHD2Px", "datetime": "2018-10-12 11:09:04", "followers": "1,574"}, "1050813680416382976": {"author": "@petercahill", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:21:19", "followers": "628"}, "1050768625882214400": {"author": "@sanjanalreddy", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:22:17", "followers": "165"}, "1057867054198415360": {"author": "@blueviggen", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 05:28:54", "followers": "188"}, "1051010126486753282": {"author": "@kyoun", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\uff09\u3084OpenAI GPT\uff08\u5358\u65b9\u5411Transformer\uff09\u3068\u7570\u306a\u308a\uff0c\u53cc\u65b9\u5411Transformer\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u5927\u898f\u6a21\u306b\u4e8b\u524d\u5b66\u7fd2\uff0e BERT\u306b\u51fa\u529b\u5c64\u30921\u5c64\u8ffd\u52a0\u3059\u308b\u3060\u3051\u3067\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067SOTA\uff0e https://t.co/MC97LleFVB", "datetime": "2018-10-13 07:21:55", "followers": "2,439"}, "1050975632497217536": {"author": "@Ahmad0Darwiche", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:04:51", "followers": "237"}, "1050740354448646144": {"author": "@ArchieIndian", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:29:56", "followers": "723"}, "1050610917086453761": {"author": "@unsaifi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:55:36", "followers": "151"}, "1050628114269667328": {"author": "@cmkumar87", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:03:56", "followers": "217"}, "1058014640431542273": {"author": "@bsaeta", "content_summary": "I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are very promising, and underpin the latest SotA NLP such as BERT https://t.co/5noohy4oam and Mesh @TensorFlow https://t.co/qXRjeiyqQ4", "datetime": "2018-11-01 15:15:21", "followers": "2,285"}, "1051591311646547968": {"author": "@phdax", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 21:51:20", "followers": "12"}, "1050752700885999617": {"author": "@unsorsodicorda", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 14:19:00", "followers": "738"}, "1051301956503916544": {"author": "@shunk031", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 02:41:33", "followers": "2,050"}, "1050722240462028801": {"author": "@manaalfar", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:17:58", "followers": "2,522"}, "1050925004471201792": {"author": "@ScriptShade", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 01:43:40", "followers": "221"}, "1051789262855659520": {"author": "@Raquel1934", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 10:57:56", "followers": "110"}, "1050644639294750721": {"author": "@eDezhic", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:09:36", "followers": "62"}, "1053287799665901569": {"author": "@numPerfecto28", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-19 14:12:35", "followers": "32"}, "1051429259292508160": {"author": "@Tarpon_red2", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 11:07:24", "followers": "2,861"}, "1050787870640926723": {"author": "@sannikpatel", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:38:45", "followers": "222"}, "1050582267943428097": {"author": "@Datascience__", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:01:46", "followers": "18,188"}, "1050734076242595840": {"author": "@joshtronic", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding - https://t.co/8eKBlySwFi", "datetime": "2018-10-12 13:05:00", "followers": "2,003"}, "1050556901598158848": {"author": "@ChikaObuah", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:20:58", "followers": "912"}, "1083882123021377536": {"author": "@Joeliu2016", "content_summary": "RT @AUEBNLPGroup: Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0\u2026", "datetime": "2019-01-12 00:23:30", "followers": "48"}, "1050673726746808320": {"author": "@mzeid4real", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:05:11", "followers": "82"}, "1051005613562634241": {"author": "@Shujian_Liu", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 07:03:59", "followers": "292"}, "1050964713356423173": {"author": "@risounomoewaifu", "content_summary": "RT @hikarinonakano7: \u6771\u5927\u306e\u677e\u5c3e\u8c4a\u51c6\u6559\u6388\u306b\u3088\u308c\u3070\u3001\u8a00\u8a9e\u7406\u89e3\u306f\u30012014\u5e74\u3067\u306e\u4e88\u60f3\u3060\u3068\u30012030\u5e74\u9803\u306b\u9054\u6210\u3059\u308b\u3068\u304a\u3063\u3057\u3083\u3089\u308c\u3066\u3044\u307e\u3057\u305f\u3088\u306d\u3002\u660e\u3089\u304b\u306b\u958b\u767a\u30b9\u30d4\u30fc\u30c9\u304c\u4e0a\u304c\u3063\u3066\u3044\u307e\u3059\u306d\uff01\u3059\u3054\u3044\u3067\u3059\u306d\uff01 https://t.co/xCFPR4qhk1", "datetime": "2018-10-13 04:21:28", "followers": "517"}, "1050963331400445952": {"author": "@a_hasimoto", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:15:58", "followers": "688"}, "1052188606364864512": {"author": "@paulkaefer", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-16 13:24:47", "followers": "522"}, "1051113328519630848": {"author": "@iawaisrauf", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 14:12:00", "followers": "163"}, "1131082347162554368": {"author": "@NVSData", "content_summary": "RT @cosimo_fedeli: #DataScience Pick of the week: Pre-training of deep bidirectional transformers for language understanding https://t.co/S\u2026", "datetime": "2019-05-22 06:20:20", "followers": "3,528"}, "1051116145963495425": {"author": "@Shen19589425", "content_summary": "RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J", "datetime": "2018-10-13 14:23:12", "followers": "51"}, "1135706561022758912": {"author": "@MuhMoussa", "content_summary": "RT @wordmetrics: This is an excellent explanation of BERT. https://t.co/2Arb3EFxX4 Original Google AI paper here: https://t.co/yhp9vkKNRR\u2026", "datetime": "2019-06-04 00:35:19", "followers": "2,274"}, "1132547361119584256": {"author": "@seounited", "content_summary": "#awesomeness in einer Formel #seo", "datetime": "2019-05-26 07:21:47", "followers": "18,096"}, "1050699231403462657": {"author": "@aneesha", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 10:46:32", "followers": "1,586"}, "1051030059669164033": {"author": "@MikelForcada", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 08:41:08", "followers": "1,503"}, "1050805575645315076": {"author": "@_lpag", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 17:49:06", "followers": "291"}, "1050798423019057152": {"author": "@JavierLorenzoN", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 17:20:41", "followers": "66"}, "1051334498753818624": {"author": "@nskm_m", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 04:50:52", "followers": "190"}, "1051138279184117760": {"author": "@roshan_87", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 15:51:09", "followers": "90"}, "1176511943206723585": {"author": "@HubBase1", "content_summary": "#BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transformers for Language Understanding #MachineLearning #DeepLearning @ARXIV_ORG \ud83d\udda5\ufe0fhttps://t.co/YcPpMJePHC @HubBucket @HubAnalytics2 @", "datetime": "2019-09-24 15:01:20", "followers": "127"}, "1188268644163178497": {"author": "@bill_slawski", "content_summary": "RT @dawnieando: @Suzzicks @BritneyMuller @AlexisKSanders My dog was here first. My Bert is 6 years old. https://t.co/QKoiNtV3Jy", "datetime": "2019-10-27 01:38:16", "followers": "51,099"}, "1050995681001914368": {"author": "@GiorgioPatrini", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:24:31", "followers": "1,942"}, "1050931370615164928": {"author": "@girichukkapalli", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 02:08:58", "followers": "159"}, "1051053088046366721": {"author": "@rkakamilan", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 10:12:38", "followers": "637"}, "1201923132589826048": {"author": "@raufsamestone", "content_summary": "Google BERT g\u00fcncellemesinden \u00f6nce: https://t.co/idE96aysOA #BERT #TensorFlow #google #ai #tensor2tensor", "datetime": "2019-12-03 17:56:20", "followers": "33"}, "1059650641977430016": {"author": "@Ney6666", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:36:15", "followers": "91"}, "1053962301529706496": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-21 10:52:49", "followers": "843"}, "1051375630728613888": {"author": "@nakanohito_piyo", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:34:18", "followers": "803"}, "1050940989798379520": {"author": "@honya06k14", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:47:12", "followers": "587"}, "1050750556216680449": {"author": "@philomate", "content_summary": "RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J", "datetime": "2018-10-12 14:10:29", "followers": "331"}, "1051035352734097410": {"author": "@sdemyanov", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 09:02:10", "followers": "107"}, "1050566408369782785": {"author": "@kalpeshk2011", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 01:58:44", "followers": "564"}, "1051102155862499331": {"author": "@AssistedEvolve", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 13:27:37", "followers": "218"}, "1072245497103048705": {"author": "@froginthevalley", "content_summary": "BERT, BERT, BERT is the word. Or really the words to finish your sentence. \"Pre-training of Deep Bidirectional Transformers for Language Understanding\" https://t.co/g47cJZP0af", "datetime": "2018-12-10 21:43:42", "followers": "11,300"}, "1082826959112101891": {"author": "@Julius_Frost", "content_summary": "Reading the @GoogleAI BERT paper (https://t.co/BWOW7aHu62) and I was wondering what a Transformer was that appears in the paper. This blog explains it pretty well The Transformer \u2013 Attention is all you need. https://t.co/FybwF8iJb9", "datetime": "2019-01-09 02:30:39", "followers": "94"}, "1058008588856086529": {"author": "@AssistedEvolve", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 14:51:19", "followers": "218"}, "1051079972532736000": {"author": "@liorshkiller", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 11:59:28", "followers": "76"}, "1051088067782746119": {"author": "@facells", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 12:31:38", "followers": "203"}, "1050846361388756993": {"author": "@mandercorn", "content_summary": "While this research is on machine learning techniques for natural language processing, there's some interesting sentence-level ideas and databases in here. https://t.co/g0s89T4NWE", "datetime": "2018-10-12 20:31:10", "followers": "2,181"}, "1050948254718779392": {"author": "@Melnus_", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 03:16:04", "followers": "411"}, "1050772984481251333": {"author": "@wsiya", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 15:39:36", "followers": "685"}, "1050922948763287552": {"author": "@miguelalonsojr", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 01:35:30", "followers": "1,063"}, "1050961279559454720": {"author": "@toru_inoue", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:07:49", "followers": "2,097"}, "1057850575008583682": {"author": "@jaguring1", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-01 04:23:25", "followers": "12,765"}, "1050577405704687616": {"author": "@rown", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 02:42:26", "followers": "1,670"}, "1068241361160278016": {"author": "@LouisvanBeurden", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-29 20:32:42", "followers": "187"}, "1132823938927452160": {"author": "@chrisboulanger", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 01:40:48", "followers": "1,158"}, "1050545040743747584": {"author": "@MarkNeumannnn", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:33:50", "followers": "2,204"}, "1094150826573082624": {"author": "@alexrevuelta", "content_summary": "RT @strategist922: Pytorch implementation of Google AI's 2018 BERT, with simple annotation Ref. BERT 2018 BERT: Pre-training of Deep Bidire\u2026", "datetime": "2019-02-09 08:27:40", "followers": "268"}, "1050561137098379264": {"author": "@rejectionking", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:37:48", "followers": "2,251"}, "1050804293605953542": {"author": "@ugocupcic", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 17:44:01", "followers": "599"}, "1050577858643550208": {"author": "@sleepinyourhat", "content_summary": "RT @liviobs: @sleepinyourhat @solomatovk https://t.co/ecxV4CBuSS", "datetime": "2018-10-12 02:44:14", "followers": "12,363"}, "1064347845669634048": {"author": "@mjayliebs", "content_summary": "BERT looks really interesting https://t.co/CKyB8CUxLo Bidirectional Encoder Representations from Transformers. designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. Google Open Sourc", "datetime": "2018-11-19 02:41:15", "followers": "10,224"}, "1050631549509496839": {"author": "@tuvuumass", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 06:17:35", "followers": "160"}, "1050861781264957440": {"author": "@jaguring1", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 21:32:27", "followers": "12,765"}, "1176438655835000832": {"author": "@ShivamChandhok2", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026", "datetime": "2019-09-24 10:10:07", "followers": "8"}, "1050930608048721920": {"author": "@ksasao", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:05:56", "followers": "8,989"}, "1050917414815318016": {"author": "@Pan_MuTux", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-13 01:13:31", "followers": "7"}, "1059617278465953792": {"author": "@kiyoi_nohara", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:23:40", "followers": "184"}, "1050563572084879361": {"author": "@thtrieu_", "content_summary": "exciting times", "datetime": "2018-10-12 01:47:28", "followers": "608"}, "1132543091133444096": {"author": "@AadilArif", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 07:04:49", "followers": "892"}, "1050928095861989376": {"author": "@ryo_masumura", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 01:55:58", "followers": "425"}, "1058879881776451585": {"author": "@surfieT", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-04 00:33:31", "followers": "663"}, "1051636371775938561": {"author": "@hrs1985", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 00:50:24", "followers": "1,194"}, "1051126528342253569": {"author": "@tuxnguyen", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 15:04:28", "followers": "57"}, "1215038453596864512": {"author": "@superlegend_zyf", "content_summary": "https://t.co/txXyuncYDb BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Good paper to read", "datetime": "2020-01-08 22:31:56", "followers": "64"}, "1050561309316530179": {"author": "@kakemeister", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:38:29", "followers": "298"}, "1050776253710491648": {"author": "@martin_wicke", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:52:36", "followers": "4,884"}, "1051187673329782784": {"author": "@_WizDom13_", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 19:07:26", "followers": "55"}, "1050728334441832448": {"author": "@olgaiv39", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:42:11", "followers": "90"}, "1050928787456589825": {"author": "@suripenchan", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 01:58:42", "followers": "607"}, "1051010618898112512": {"author": "@y_yammt", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 07:23:53", "followers": "150"}, "1050722725008797696": {"author": "@akinori_ito", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 12:19:53", "followers": "1,367"}, "1050576702449963009": {"author": "@mchang21", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 02:39:39", "followers": "132"}, "1053895014399455234": {"author": "@trigate099", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-10-21 06:25:26", "followers": "456"}, "1203143056926674946": {"author": "@PoseysThumbs", "content_summary": "Weekly #WIR #WIL #WIB What I'm Reading: Secrets of Sand Hill Road What I'm Learning: Google BERT (https://t.co/ECcwYJLYLN) What I'm Building: AI-Driven Financial Insights #AI #Startups #Malgo", "datetime": "2019-12-07 02:43:52", "followers": "295"}, "1050612352943448068": {"author": "@10_1100011_01", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:01:19", "followers": "48"}, "1059604092085911552": {"author": "@izm", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:31:16", "followers": "8,388"}, "1051246598255067137": {"author": "@makoFALAR1229", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 23:01:34", "followers": "1,091"}, "1051145479034032133": {"author": "@aktarcar", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-13 16:19:46", "followers": "122"}, "1051014324091785216": {"author": "@joermungandr", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 07:38:36", "followers": "1,077"}, "1051113153399021568": {"author": "@GilgameshNusku", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 14:11:19", "followers": "283"}, "1057760883710091264": {"author": "@tori_tukune29", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-31 22:27:01", "followers": "859"}, "1074950192959553536": {"author": "@apopescubelis", "content_summary": "RT @arxiv_cs_cl: https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.0480\u2026", "datetime": "2018-12-18 08:51:12", "followers": "158"}, "1050667374406328321": {"author": "@samhardyhey", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:39:57", "followers": "62"}, "1050582785214148608": {"author": "@_HoF__", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:03:49", "followers": "411"}, "1050964051088486400": {"author": "@tdualdir", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:18:50", "followers": "11,078"}, "1051697471045296128": {"author": "@MktO740123", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 04:53:11", "followers": "53"}, "1051625358632468480": {"author": "@Scaled_Wurm", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 00:06:38", "followers": "1,667"}, "1050564421913796608": {"author": "@dtsbourg", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:50:51", "followers": "529"}, "1207979718466777088": {"author": "@PapersTrending", "content_summary": "[9/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 45 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-12-20 11:03:02", "followers": "222"}, "1051050388705677312": {"author": "@MeneDev", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 10:01:54", "followers": "431"}, "1176357175012286464": {"author": "@Rosenchild", "content_summary": "RT @Rosenchild: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transf\u2026", "datetime": "2019-09-24 04:46:20", "followers": "11,933"}, "1050592045562122240": {"author": "@MarkNeumannnn", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 03:40:37", "followers": "2,204"}, "1050910284393533443": {"author": "@eimei0080", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 00:45:11", "followers": "686"}, "1072774535080857600": {"author": "@tpk_in", "content_summary": "RT @profillic: Google open sourced a new technique for NLP pre-training called BERT. With this release, anyone in the world can train their\u2026", "datetime": "2018-12-12 08:45:55", "followers": "60"}, "1073521016498880512": {"author": "@Inf1_Si", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-12-14 10:12:10", "followers": "25"}, "1050715572445560833": {"author": "@dawnieando", "content_summary": "Picture of Bert incoming", "datetime": "2018-10-12 11:51:28", "followers": "19,900"}, "1060542749710024705": {"author": "@IkuoMizuuchi", "content_summary": "\u4eca\u65e5\u306f\u3001\u4eca\u3092\u3068\u304d\u3081\u304f\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30f3\u30cb\u30f3\u30b0\u5354\u4f1a\u306e\u7406\u4e8b\u306b\u3001BERT\u306e\u8a71\u3068\u304b\u3057\u3066\u3001\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u306e\u30d6\u30ed\u30b0\u3092\u7d39\u4ecb\u3059\u308b\u306a\u3069\u3057\u305f\u3002 \u307b\u3093\u3068\u30b9\u30d4\u30fc\u30c9\u611f\u304c\u3042\u308b\u306a\u3042\u3068\u611f\u3058\u3066\u305f\u306e\u3067\u2025", "datetime": "2018-11-08 14:41:10", "followers": "1,082"}, "1050583654915657728": {"author": "@yuxili99", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:07:16", "followers": "430"}, "1163752990760742912": {"author": "@PapersTrending", "content_summary": "[8/10] \ud83d\udcc8 - pytorch-transformers - 10,951 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/SBdhq9Mhj0", "datetime": "2019-08-20 10:01:49", "followers": "222"}, "1051427805454430209": {"author": "@sappy_and_sappy", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 11:01:38", "followers": "175"}, "1050882259358760960": {"author": "@rajarshi_nitc", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 22:53:49", "followers": "481"}, "1059799184272850946": {"author": "@letssaga3", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 13:26:30", "followers": "712"}, "1050672347412189191": {"author": "@k1ckedinthehead", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:59:42", "followers": "797"}, "1058269197875249152": {"author": "@octadero", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 08:06:53", "followers": "12"}, "1050562784373686272": {"author": "@dipanjand", "content_summary": "New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 01:44:20", "followers": "1,646"}, "1059649965943775233": {"author": "@tohki", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:33:33", "followers": "286"}, "1051639993020469248": {"author": "@wakaba130", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 01:04:47", "followers": "422"}, "1052767411609460737": {"author": "@fraseHQ", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-18 03:44:45", "followers": "27"}, "1057988092768980992": {"author": "@mattmcd", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 13:29:52", "followers": "615"}, "1051830934805200896": {"author": "@sabimaru0701", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 13:43:31", "followers": "19"}, "1051076136434847745": {"author": "@damianborth", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 11:44:13", "followers": "1,434"}, "1190557804500836355": {"author": "@pwatsonwailes", "content_summary": "@dawnieando @imdanielcooper @dr_pete @DanLeibson @bill_slawski @BritneyMuller Table from the original BERT paper at https://t.co/pHUTDm01Mv", "datetime": "2019-11-02 09:14:34", "followers": "1,948"}, "1050829624429535232": {"author": "@cbsvision", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:24:40", "followers": "24"}, "1087028128852180993": {"author": "@Montreal_AI", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-01-20 16:44:36", "followers": "177,481"}, "1050571171383693313": {"author": "@LakshmanBoddoju", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:17:40", "followers": "96"}, "1057929094858444801": {"author": "@DCasBol", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 09:35:26", "followers": "154"}, "1114795865271492608": {"author": "@ttakenouchi3", "content_summary": "https://t.co/7iiIJI12Cg \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067BERT(Bidirectional Encoder Representations from Transformers)\u304c\u6ce8\u76ee\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u53cc\u65b9\u5411\u6027\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u4f7f\u7528\u3057Masked Language Model\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u4f7f\u7528\u3002 \u3044\u304f\u3064\u304b\u306e\u8a00\u8449\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u30de\u30b9\u30af\u3057\u3001\u30de\u30b9\u30af\u3055\u308c\u305f\u8a00\u8449\u3092\u4e88\u6e2c\u3057\u3066\u8868\u73fe\u3092\u5b66\u7fd2\u3057\u307e\u3059\u3002", "datetime": "2019-04-07 07:43:40", "followers": "17"}, "1050575102704680960": {"author": "@hereticreader", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - https://t.co/9D1COPFWPY https://t.co/vC4isR7Hlk", "datetime": "2018-10-12 02:33:17", "followers": "195"}, "1141494351630987264": {"author": "@semiinvariant", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2019-06-19 23:53:56", "followers": "72"}, "1052783457464659968": {"author": "@strategist922", "content_summary": "Pytorch implementation of Google AI's 2018 BERT, with simple annotation Ref. BERT 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Paper URL : https://t.co/THyeMvFPHA https://t.co/s2NW87HpnS", "datetime": "2018-10-18 04:48:30", "followers": "650"}, "1102159478504415232": {"author": "@jun40vn", "content_summary": "RT @nyaos524: \u3042\u3068\u6539\u3081\u3066BERT(https://t.co/z6RjoIHIFE)\u3068GPT-2(https://t.co/pG8CW8Q4KF)\u306e\u539f\u7a3f\u8aad\u3093\u3060\u3051\u3069\u3001\u3059\u3054\u3044\u3063\u3059\u306d\u3002\u306a\u3093\u304b\u3082\u3046\u4ed6\u306e\u8272\u3005\u306a\u30bf\u30b9\u30af\u3067\u5b66\u7fd2\u3057\u305f\u3067\u304b\u3044\u30e2\u30c7\u30eb\u3092\u4f7f\u3044\u56de\u3059\u3079\u304d\u3068\u3044\u3046\u98a8\u6f6e\u304c\u51fa\u6765\u4e0a\u304c\u3063\u3066\u3057\u2026", "datetime": "2019-03-03 10:51:11", "followers": "98"}, "1050764724898426882": {"author": "@jithsjoy", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:06:47", "followers": "76"}, "1050582197139206144": {"author": "@ComputerPapers", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://t.co/Vc9QwW82gN", "datetime": "2018-10-12 03:01:29", "followers": "613"}, "1050547419316813827": {"author": "@shivam13verma", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:43:17", "followers": "441"}, "1070987245103009792": {"author": "@ulliwaltinger", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-12-07 10:23:51", "followers": "701"}, "1050545335368478720": {"author": "@earnmyturns", "content_summary": "Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 00:35:00", "followers": "16,454"}, "1089918107705958402": {"author": "@Tn23008624", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-01-28 16:08:21", "followers": "30"}, "1059842835904512000": {"author": "@akakit", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 16:19:57", "followers": "1,878"}, "1051470582238208000": {"author": "@ruka_funaki", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 13:51:36", "followers": "1,304"}, "1051385818957238273": {"author": "@4_d", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 08:14:47", "followers": "3,772"}, "1132561256974569478": {"author": "@seokai", "content_summary": "RT @seounited: #awesomeness in einer Formel #seo https://t.co/TfRU7XHaPC", "datetime": "2019-05-26 08:17:00", "followers": "4,231"}, "1051070194754473984": {"author": "@urban_coder", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 11:20:37", "followers": "8,418"}, "1050687930640031744": {"author": "@agispof", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:01:38", "followers": "107"}, "1132522951646035968": {"author": "@SaadAlikhan1994", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 05:44:47", "followers": "213"}, "1051412026608762880": {"author": "@yapita", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 09:58:56", "followers": "896"}, "1050977998709518336": {"author": "@jaguring1", "content_summary": "\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\u306f@_Ryobot\u3055\u3093\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u3089\u5f15\u7528\u3055\u305b\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u3002\u554f\u984c\u304c\u3042\u308b\u3088\u3046\u3067\u3057\u305f\u3089\u3001\u5831\u544a\u304a\u9858\u3044\u3057\u307e\u3059\u3002\u305d\u306e\u5834\u5408\u3001\u3082\u3061\u308d\u3093\u524a\u9664\u3057\u307e\u3059\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304cBERT\u306e\u6280\u8853\u7684\u306a\u8a71\u3092\u3001\u697d\u3057\u3081\u308b\u3088\u3046\u306b\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002 target=\"_blank\" href=\"https://t.co/kXg5PnC9un\">https://t.co/kXg5PnC9un", "datetime": "2018-10-13 05:14:15", "followers": "12,765"}, "1073375312191270913": {"author": "@gaudi08", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-12-14 00:33:11", "followers": "320"}, "1051748965463482368": {"author": "@morinosuke__p", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 08:17:48", "followers": "273"}, "1050610002845556736": {"author": "@Tanaygahlot", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:51:58", "followers": "251"}, "1050551046886502401": {"author": "@mchang21", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:57:42", "followers": "132"}, "1050760554569027584": {"author": "@jaguring1", "content_summary": "SWAG\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u5e38\u8b58\u63a8\u8ad6\u30bf\u30b9\u30af\uff09\u306b\u304a\u3044\u3066\u3001BERT\u304c\u65e9\u3005\u306b\u4eba\u9593\u30ec\u30d9\u30eb\u306b\u5230\u9054\uff01\uff1f BERT\uff082018\u5e7410\u670811\u65e5\uff01\uff01\uff01\uff09 https://t.co/Kan2rWqMqu SWAG\uff082018\u5e748\u670816\u65e5\uff01\uff01\uff01\uff09 https://t.co/bSATz9jqmc https://t.co/QwpQa697mz", "datetime": "2018-10-12 14:50:13", "followers": "12,765"}, "1063918213388414982": {"author": "@FajriKoto", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-17 22:14:03", "followers": "384"}, "1059615183709003777": {"author": "@ok_mozy", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:15:21", "followers": "1,092"}, "1050808749638778880": {"author": "@singularpattern", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-12 18:01:43", "followers": "89"}, "1050962504321511425": {"author": "@nhayato_ac", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 04:12:41", "followers": "57"}, "1050815150641635328": {"author": "@_WizDom13_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:27:09", "followers": "55"}, "1050790274958536704": {"author": "@avanwykai", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:48:18", "followers": "14"}, "1050578289444749312": {"author": "@Shujian_Liu", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:45:57", "followers": "292"}, "1051343453416652801": {"author": "@yourshifer", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 05:26:26", "followers": "3"}, "1050747849250504704": {"author": "@sigitpurnomo", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:59:43", "followers": "2,663"}, "1051006897321594880": {"author": "@uwepleban", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 07:09:05", "followers": "230"}, "1180060654373412864": {"author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - pytorch-transformers - 14,520 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-10-04 10:02:39", "followers": "222"}, "1051842433674182656": {"author": "@tanosuke39", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-15 14:29:13", "followers": "751"}, "1051631701275758597": {"author": "@headcubicle", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 00:31:50", "followers": "366"}, "1050808961153331207": {"author": "@Zahid_Akhtar", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 18:02:34", "followers": "97"}, "1062574407049199617": {"author": "@Pythonista1", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-11-14 05:14:14", "followers": "249"}, "1050932526779314176": {"author": "@Yu_Yamaguchi_", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:13:34", "followers": "2,771"}, "1050846873026813952": {"author": "@ZachBessinger", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 20:33:12", "followers": "164"}, "1050652542755958784": {"author": "@dawnieando", "content_summary": "Meet BERT from Google AI @GoogleAI (great name although I am biased as it is the same name as my pomeranian) -> 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' (submitted Oct 2018) https://t.co/crUXOww65l", "datetime": "2018-10-12 07:41:01", "followers": "19,900"}, "1050560971717074946": {"author": "@kbhan0616", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:37:08", "followers": "35"}, "1146122120365301760": {"author": "@rammerdotai", "content_summary": "\"Conceptually Simple and Empirically Powerful\". An insanely simple idea that enables training a bidirectional language model, without 'seeing' future words Read more here:https://t.co/qGBRMUttng #artificialintelligence #machinelearning #datascience #d", "datetime": "2019-07-02 18:23:02", "followers": "52"}, "1050844086020841472": {"author": "@JoaoMarcosCouto", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 20:22:08", "followers": "10"}, "1060843342571888640": {"author": "@kazanagisora", "content_summary": "RT @IkuoMizuuchi: \u4eca\u65e5\u306f\u3001\u4eca\u3092\u3068\u304d\u3081\u304f\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30f3\u30cb\u30f3\u30b0\u5354\u4f1a\u306e\u7406\u4e8b\u306b\u3001BERT\u306e\u8a71\u3068\u304b\u3057\u3066\u3001\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u306e\u30d6\u30ed\u30b0\u3092\u7d39\u4ecb\u3059\u308b\u306a\u3069\u3057\u305f\u3002 \u307b\u3093\u3068\u30b9\u30d4\u30fc\u30c9\u611f\u304c\u3042\u308b\u306a\u3042\u3068\u611f\u3058\u3066\u305f\u306e\u3067\u2025 https://t.co/Aby5aa2fz7", "datetime": "2018-11-09 10:35:37", "followers": "843"}, "1050609551009927168": {"author": "@surangasms01", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:50:10", "followers": "186"}, "1205469054561067008": {"author": "@VorticonCmdr", "content_summary": "@MordyOberstein I had to look it up. This doesn't say Google is using BERT for named entity recognition but in the original paper they do compare BERT for NER with other approches https://t.co/Jgq8qLBDHK (it's the paper referenced from here: https://t.co/h", "datetime": "2019-12-13 12:46:33", "followers": "1,631"}, "1051129278627233793": {"author": "@snehaSM", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 15:15:23", "followers": "143"}, "1059614799045095424": {"author": "@reomatsumura", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-06 01:13:49", "followers": "627"}, "1050568150738059264": {"author": "@beck_daniel", "content_summary": "A new era of NLP has just begun: a dystopia where 1) English is the only language spoken, and 2) big companies ensure gatekeeping by hoarding massive computing power. But hey, SOTA on ***everything*** (including cure for cancer, world peace and spacetime t", "datetime": "2018-10-12 02:05:40", "followers": "579"}, "1050582274297683968": {"author": "@geneatcg", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:01:47", "followers": "222"}, "1050979816898363392": {"author": "@sappy_and_sappy", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:21:29", "followers": "175"}, "1050639713680351232": {"author": "@pulsebase", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:50:02", "followers": "1,705"}, "1051065017146036224": {"author": "@HouhouinK", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 11:00:02", "followers": "163"}, "1197690805982380032": {"author": "@beanstalkim", "content_summary": "@CoperniX - They released https://t.co/Np0SIQo2F4 on BERT. Does Bing have a similar doc for what you folks are doing?", "datetime": "2019-11-22 01:38:34", "followers": "3,201"}, "1052495273434464258": {"author": "@devnull90", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-17 09:43:22", "followers": "47"}, "1050820937757745158": {"author": "@TheMTank", "content_summary": "RT @beduffy1: The Language Model pre-train train keeps on chugging along. And finally we beat human F1 performance on SQuAD!!!! I was foll\u2026", "datetime": "2018-10-12 18:50:09", "followers": "77"}, "1050722928159936512": {"author": "@farhanhubble", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 12:20:42", "followers": "161"}, "1050757432480165888": {"author": "@yizhongwyz", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:37:48", "followers": "193"}, "1237152056470716416": {"author": "@kkitase", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2020-03-09 23:03:30", "followers": "4,055"}, "1058275055547965440": {"author": "@iamx9000", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 08:30:09", "followers": "9"}, "1051150217754697728": {"author": "@pedromlsreis", "content_summary": "What a time to be alive", "datetime": "2018-10-13 16:38:35", "followers": "100"}, "1050727807008018433": {"author": "@raj_desh26", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 12:40:05", "followers": "345"}, "1050989609474252800": {"author": "@KlimZaporojets", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:00:23", "followers": "26"}, "1050744383765704706": {"author": "@xpasky", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:45:57", "followers": "742"}, "1050571062050742272": {"author": "@DanielKhashabi", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 02:17:14", "followers": "867"}, "1050839768311971840": {"author": "@JustinTimesUK", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 20:04:59", "followers": "608"}, "1052868946242822145": {"author": "@Baevsky", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u043e\u0442 Google AI Language https://t.co/azZq9MgO3O", "datetime": "2018-10-18 10:28:12", "followers": "77"}, "1059596150783729664": {"author": "@safehouse10", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 23:59:43", "followers": "1,954"}, "1051989696857796608": {"author": "@wayama_ryousuke", "content_summary": "RT @keiskS: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.04805v1 [https://t.co/8DRjnqAPrR\u2026", "datetime": "2018-10-16 00:14:23", "followers": "203"}, "1050615299630718978": {"author": "@AlexGuoHan", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:13:01", "followers": "392"}, "1059597977939935232": {"author": "@Miyaran99", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-06 00:06:58", "followers": "354"}, "1050906938387488768": {"author": "@fumiaki_sato_", "content_summary": "RT @jaguring1: \u4e0a\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u30894\u30ab\u6708\u3002\u4eca\u5ea6\u306fGoogleAI\u304b\u3089\u885d\u6483\u7684\u306a\u624b\u6cd5\u304c\u767b\u5834\uff01\u518d\u3073\u5927\u91cf\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\uff08SoTA\uff09\u3092\u53e9\u304d\u51fa\u3059\u3002\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u793a\u3059\u30bf\u30b9\u30af\u3082\u3042\u308b\u3002BERT: Pre-training of Deep Bidirecti\u2026", "datetime": "2018-10-13 00:31:53", "followers": "26"}, "1176589173765156869": {"author": "@HubGenomics", "content_summary": "RT @HubBase1: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfor\u2026", "datetime": "2019-09-24 20:08:13", "followers": "130"}, "1050574030577618944": {"author": "@ytay017", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 02:29:02", "followers": "462"}, "1057932879043411968": {"author": "@healthonrails", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 09:50:28", "followers": "24"}, "1050589889786302464": {"author": "@msurd", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 03:32:03", "followers": "1,121"}, "1051117061932503041": {"author": "@esvhd", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 14:26:51", "followers": "145"}, "1131862826442731520": {"author": "@PapersTrending", "content_summary": "[7/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 6,367 \u2b50 - \ud83d\udcc4 https://t.co/6AtFwAwrX6 - \ud83d\udd17 https://t.co/SBdhq9Mhj0", "datetime": "2019-05-24 10:01:41", "followers": "222"}, "1050725097949024256": {"author": "@mohitban47", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:29:19", "followers": "4,636"}, "1050577654993235968": {"author": "@peteskomoroch", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:43:26", "followers": "47,691"}, "1050768983782232064": {"author": "@sidvash", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:23:42", "followers": "131"}, "1051279631700058112": {"author": "@jaguring1", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-14 01:12:50", "followers": "12,765"}, "1050550915789246464": {"author": "@mchang21", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 00:57:11", "followers": "132"}, "1064405754675953664": {"author": "@idokius", "content_summary": "RT @AmiRojkes: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/aH3deYdOvy", "datetime": "2018-11-19 06:31:22", "followers": "38,486"}, "1051337713268404224": {"author": "@ku21fan", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 05:03:38", "followers": "69"}, "1065666989652434946": {"author": "@VoiceTechCarl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/24ObeFx4QS >> via @VoiceTechCarl >> #NLProc #ML #AI #MachineLearning #DataScience #VoiceFirst #chatbot #chatbots #bot #bots https://t.co/b7f3KIsv5o", "datetime": "2018-11-22 18:03:04", "followers": "10,936"}, "1051075733919952896": {"author": "@damianborth", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 11:42:37", "followers": "1,434"}, "1050620943674929152": {"author": "@dennismalmgren", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:35:27", "followers": "19"}, "1050624792250834945": {"author": "@ephes", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:50:44", "followers": "248"}, "1050577502400180225": {"author": "@stanfordnlp", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:42:50", "followers": "90,659"}, "1050944283539267584": {"author": "@YejinChoinka", "content_summary": "RT @rown: Totally! But it\u2019s not the task that\u2019s solved, it\u2019s the dataset :) it remains to be seen whether collecting the data using the sam\u2026", "datetime": "2018-10-13 03:00:17", "followers": "3,652"}, "1052717345624346625": {"author": "@nlothian", "content_summary": "and BERT (https://t.co/je62ROxT2B) didn't even report language modelling numbers.", "datetime": "2018-10-18 00:25:48", "followers": "898"}, "1078408710073004032": {"author": "@onepaperperday", "content_summary": "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\": https://t.co/jUGqhwixdc #ml #nlp 2018", "datetime": "2018-12-27 21:54:07", "followers": "2,132"}, "1050647682757189632": {"author": "@yieldthought", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:21:42", "followers": "761"}, "1152302452244471808": {"author": "@Rosenchild", "content_summary": "RT @HubBucket: \u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Languag\u2026", "datetime": "2019-07-19 19:41:28", "followers": "11,933"}, "1051091873681428480": {"author": "@heartburing", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-13 12:46:45", "followers": "0"}, "1132582847833890818": {"author": "@rdiezv", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 09:42:47", "followers": "1,252"}, "1059815895650983936": {"author": "@gijigae", "content_summary": "RT @HayashiPeke: \u82f1\u8a9e\u306e\u6700\u5927\u306e\u5229\u70b9\u306f\u3001\u53d6\u5f97\u3067\u304d\u308b\u60c5\u5831\u304c\u7269\u51c4\u304f\u5897\u3048\u308b\u3053\u3068\u3060\u3068\u304a\u3082\u3046\u3002 \u8a71\u305b\u306a\u304f\u3066\u3082\u300c\u8aad\u3081\u308b\u300d\u3060\u3051\u3067\u65b0\u3057\u3044\u60c5\u5831\u304c\u624b\u306b\u5165\u308b\u3060\u3051\u3058\u3083\u306a\u304f\u3066\u3001\u540c\u3058\u3053\u3068\u3067\u3082\u3088\u308a\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u3057\u3066\u3044\u308bsource\u306b\u89e6\u308c\u3089\u308c\u305f\u308a\u3002 \u307b\u3093\u306e\u3001\u307b\u3093\u306e\u5c11\u3057\u305a\u3064\u3067\u3082\u3001\u8ad6\u6587\u306a\u3089ab\u2026", "datetime": "2018-11-06 14:32:54", "followers": "19,021"}, "1050928992788701184": {"author": "@K_Ryuichirou", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 01:59:31", "followers": "782"}, "1050694617740529665": {"author": "@juliaroz_ml", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:28:12", "followers": "476"}, "1050983948241649667": {"author": "@Singularity_43", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 05:37:54", "followers": "201"}, "1050940480723308544": {"author": "@MarchBragagnini", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 02:45:10", "followers": "30"}, "1052783472757104641": {"author": "@strategist922", "content_summary": "Pytorch implementation of Google AI's 2018 BERT, with simple annotation Ref. BERT 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Paper URL : https://t.co/KNJ3xirzZB https://t.co/fJeSrOVqoU", "datetime": "2018-10-18 04:48:34", "followers": "650"}, "1057639969789894656": {"author": "@arthurostapenko", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-31 14:26:33", "followers": "709"}, "1050680223203713024": {"author": "@madrugad0", "content_summary": "new SotA on many tasks from Google; bidirectional transformer language model, which is trained for 1M steps with 128k words/batch; for downstream tasks they train additional output layer with small lr; closely resembles OpenAI GPT, but bigger & better", "datetime": "2018-10-12 09:31:00", "followers": "299"}, "1086859119493996544": {"author": "@PaulHuang668", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-01-20 05:33:01", "followers": "1"}, "1051069395269758976": {"author": "@sher_ali84", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-13 11:17:26", "followers": "21"}, "1050755359063900162": {"author": "@KavehHassani", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 14:29:34", "followers": "161"}, "1051119937539006465": {"author": "@vtesin", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 14:38:16", "followers": "223"}, "1050581420320538624": {"author": "@mamoruk", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:58:24", "followers": "8,941"}, "1050541972644089857": {"author": "@michalwols", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:21:39", "followers": "449"}, "1050800305766424576": {"author": "@waydegilliam", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 17:28:10", "followers": "141"}, "1050841322008059905": {"author": "@pywirrarika", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 20:11:09", "followers": "331"}, "1051963837870673923": {"author": "@maria_hr", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 22:31:38", "followers": "390"}, "1050937791788400640": {"author": "@rose_miura", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:34:29", "followers": "293"}, "1051623619267301376": {"author": "@wakabaya3", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:59:43", "followers": "130"}, "1050717369264422912": {"author": "@karthik_r_n", "content_summary": "Very impressive results! great to see pre-training being pushed to limits for nlp https://t.co/yqBIzv9yUW", "datetime": "2018-10-12 11:58:36", "followers": "525"}, "1050985665037459456": {"author": "@RyoHWS", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:44:43", "followers": "82"}, "1050650105185857536": {"author": "@Ricardo_Usbeck", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:31:19", "followers": "781"}, "1051088532444463104": {"author": "@artur_nowak", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 12:33:29", "followers": "491"}, "1050691249181736960": {"author": "@J_P_Raymond", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:14:49", "followers": "227"}, "1050846479676370944": {"author": "@tuckerkirven", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 20:31:39", "followers": "71"}, "1050543823300976640": {"author": "@kentonctlee", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:29:00", "followers": "445"}, "1051609182066040832": {"author": "@wk77", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:02:21", "followers": "902"}, "1212766775248445440": {"author": "@Akanksha_Ahuja9", "content_summary": "RT @wzuidema: [13] J. Devlin, M. Chang, K. Lee and K. Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Underst\u2026", "datetime": "2020-01-02 16:05:05", "followers": "700"}, "1058016278600806400": {"author": "@bharadwajymg", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 15:21:52", "followers": "19"}, "1050631487349972992": {"author": "@tuvuumass", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:17:21", "followers": "160"}, "1050802375672369153": {"author": "@bbriniotis", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 17:36:23", "followers": "70,485"}, "1060023016069857280": {"author": "@ousta_tw", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 04:15:55", "followers": "21"}, "1196949470547591170": {"author": "@gabriel_ilharco", "content_summary": "@adityakusupati @suriyagnskr In terms of modeling, I listed some influential papers below Transformers: https://t.co/UikGf9IHf1 (also recommend this blog post: https://t.co/SNeIWirZKN) BERT: https://t.co/AJ39d0n2Bb RoBERTa: https://t.co/92pdjp5UYi T5: htt", "datetime": "2019-11-20 00:32:46", "followers": "366"}, "1050933537807818752": {"author": "@y_yammt", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:17:35", "followers": "150"}, "1051479489971011584": {"author": "@tokoroten", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 14:27:00", "followers": "21,185"}, "1050737142253670400": {"author": "@gilbane", "content_summary": "RT @fgilbane: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u201c\u2026 obtains new state-of-the-art results on e\u2026", "datetime": "2018-10-12 13:17:11", "followers": "3,910"}, "1050701722409160705": {"author": "@newsyc50", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Underst https://t.co/R3MeLLcNog (https://t.co/m5Rxr5mWOi)", "datetime": "2018-10-12 10:56:26", "followers": "6,206"}, "1058112597809983488": {"author": "@hjguyhan", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 21:44:36", "followers": "71"}, "1050584607974932480": {"author": "@HamelHusain", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:11:04", "followers": "3,632"}, "1052596210975088640": {"author": "@futuremediafbk", "content_summary": "RT @m_guerini: Exciting or scary? https://t.co/rt7Iw45oRd", "datetime": "2018-10-17 16:24:27", "followers": "803"}, "1053863504803811328": {"author": "@jaguring1", "content_summary": "RT @jaguring1: SWAG\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u5e38\u8b58\u63a8\u8ad6\u30bf\u30b9\u30af\uff09\u306b\u304a\u3044\u3066\u3001BERT\u304c\u65e9\u3005\u306b\u4eba\u9593\u30ec\u30d9\u30eb\u306b\u5230\u9054\uff01\uff1f BERT\uff082018\u5e7410\u670811\u65e5\uff01\uff01\uff01\uff09 https://t.co/Kan2rWqMqu SWAG\uff082018\u5e748\u670816\u65e5\uff01\uff01\uff01\uff09 https://t.co/bSA\u2026", "datetime": "2018-10-21 04:20:14", "followers": "12,765"}, "1119439921729531904": {"author": "@nlpmattg", "content_summary": "@emilymbender @brendan642 @yoavgo @deliprao @geomblog They used sentence ordering as a binary prediction task, with randomly-generated negatives, which last time you explicitly considered to be \"meaning\". See section 3.3.2 of the paper (for easy reference:", "datetime": "2019-04-20 03:17:30", "followers": "6,455"}, "1051982500409561088": {"author": "@montrealdotai", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-15 23:45:47", "followers": "33,955"}, "1051139346567327749": {"author": "@bys_1123", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 15:55:24", "followers": "133"}, "1132542231867535360": {"author": "@dawnieando", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 07:01:24", "followers": "19,900"}, "1050625283429031938": {"author": "@dair_ai", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:52:41", "followers": "1,395"}, "1050837367093948421": {"author": "@JoaoVictor_AC", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:55:26", "followers": "740"}, "1050672575532003330": {"author": "@hn_frontpage", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding L: https://t.co/14zbtYAA9s C: https://t.co/NB5qt6qu85", "datetime": "2018-10-12 09:00:37", "followers": "1,504"}, "1050734166373937157": {"author": "@moffattwt", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 13:05:21", "followers": "596"}, "1063607438161657856": {"author": "@link2prasad", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-17 01:39:08", "followers": "360"}, "1050789572337008642": {"author": "@mjfreshyfresh", "content_summary": "BERT is creating quite a bit of buzz in the #NLP #AI community! \"BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks\" https://t.co/WgsSfIBm3g https://t.co/2nEDBNICwu", "datetime": "2018-10-12 16:45:31", "followers": "1,460"}, "1051038724094681088": {"author": "@xuechen1994", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 09:15:33", "followers": "47"}, "1050837378418581504": {"author": "@MarchBragagnini", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:55:29", "followers": "30"}, "1050740014911508482": {"author": "@BoaMe8", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:28:36", "followers": "8"}, "1050711956913643521": {"author": "@alexjc", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:37:06", "followers": "25,260"}, "1051616644693258240": {"author": "@nohohoii", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 23:32:00", "followers": "147"}, "1050669979593404416": {"author": "@data_hpz", "content_summary": "RT @daniilmagpie: Impressive result on more than one task. Need to take a closer look https://t.co/oN3sLnNjN8", "datetime": "2018-10-12 08:50:18", "followers": "11,002"}, "1051612591590522880": {"author": "@nezuq", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:15:54", "followers": "615"}, "1050856476963684352": {"author": "@syspulse", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 21:11:22", "followers": "47"}, "1050875234243497985": {"author": "@jekbradbury", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 22:25:54", "followers": "4,327"}, "1050738170856235008": {"author": "@ken_hide", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:21:16", "followers": "250"}, "1050662445633523712": {"author": "@mormukutch", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:20:22", "followers": "30"}, "1050935053163749377": {"author": "@prajjwal_1", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 02:23:36", "followers": "348"}, "1059597021324107776": {"author": "@s_kajita", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:03:10", "followers": "12,166"}, "1050635210193170434": {"author": "@FelixNeutatz", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:32:08", "followers": "144"}, "1060360704727801856": {"author": "@waju0122", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/0yHXS1MvXV", "datetime": "2018-11-08 02:37:47", "followers": "442"}, "1051880005230301184": {"author": "@zdepablo", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 16:58:30", "followers": "821"}, "1050594776976744453": {"author": "@jaguring1", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 03:51:28", "followers": "12,765"}, "1050645909803917317": {"author": "@fbk_mt", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:14:39", "followers": "858"}, "1051594851412635648": {"author": "@KoutaR129", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 22:05:24", "followers": "268"}, "1051073269699239936": {"author": "@daytb_twy", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 11:32:50", "followers": "471"}, "1051022293986496512": {"author": "@Tarpon_red2", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 08:10:16", "followers": "2,861"}, "1059647489085587456": {"author": "@HITStales", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-11-06 03:23:43", "followers": "4,615"}, "1051133960355504133": {"author": "@tobigue_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 15:33:59", "followers": "219"}, "1051209226788491265": {"author": "@data_datum", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 20:33:04", "followers": "1,775"}, "1053435974040993792": {"author": "@tw_exception", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-20 00:01:22", "followers": "49"}, "1057826647536070657": {"author": "@ayirpelle", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 02:48:20", "followers": "2,672"}, "1050794553719091200": {"author": "@kalpeshk2011", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 17:05:19", "followers": "564"}, "1050628804832452609": {"author": "@jasonbaldridge", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-12 06:06:41", "followers": "7,922"}, "1051089040538259456": {"author": "@montrealdotai", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-13 12:35:30", "followers": "33,955"}, "1051754211392675843": {"author": "@sotongshi", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-15 08:38:39", "followers": "1,805"}, "1050743166566100994": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-12 13:41:07", "followers": "3,500"}, "1050609939142438912": {"author": "@billyinn93", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:51:43", "followers": "65"}, "1050590031432245248": {"author": "@LuhengH", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:32:37", "followers": "283"}, "1050936361421697024": {"author": "@Singularity_43", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:28:48", "followers": "201"}, "1050548296287047681": {"author": "@17facet", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:46:46", "followers": "102"}, "1050609723270025221": {"author": "@Tanaygahlot", "content_summary": "Pretraining LM + Task specific post training. LM - Bidirectional Transformer model https://t.co/kT32fS9YzV", "datetime": "2018-10-12 04:50:52", "followers": "251"}, "1050814589787676672": {"author": "@johncocacola1", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:24:56", "followers": "87"}, "1050770741732372481": {"author": "@shigekzishihara", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 15:30:41", "followers": "5,146"}, "1059867905121226753": {"author": "@ryuta23", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-06 17:59:34", "followers": "497"}, "1050722535636131840": {"author": "@EmilStenstrom", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:19:08", "followers": "1,437"}, "1050732071553921025": {"author": "@kunio_Yb", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 12:57:02", "followers": "321"}, "1188732676099989505": {"author": "@FrancastroKW", "content_summary": "Aqu\u00ed pod\u00e9is descargar el art\u00edculo cient\u00edfico donde (en 2018) se present\u00f3 a BERT. \u2705 https://t.co/pXgR68YKPX @Google #Search #Google #GoogleMyBusiness #GMB #SEO #LocalSEO #Adentity #Changes", "datetime": "2019-10-28 08:22:10", "followers": "675"}, "1050815978542653442": {"author": "@DiegoMoussallem", "content_summary": "RT @beck_daniel: A new era of NLP has just begun: a dystopia where 1) English is the only language spoken, and 2) big companies ensure gate\u2026", "datetime": "2018-10-12 18:30:27", "followers": "544"}, "1051404715018223616": {"author": "@2675Nyatszy284", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 09:29:52", "followers": "2"}, "1050624706699644929": {"author": "@araitatsuya", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:50:24", "followers": "520"}, "1050573923924901888": {"author": "@AzadeNazy", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 02:28:36", "followers": "38"}, "1050560850212343808": {"author": "@sebastien_wood", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:36:39", "followers": "304"}, "1050775843021213696": {"author": "@steven__smit", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:50:58", "followers": "15"}, "1050825527785914368": {"author": "@mmeierer", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:08:23", "followers": "139"}, "1050564282499371008": {"author": "@tarantulae", "content_summary": "RT @FrontPageHN: BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/21tZ5kxxgJ (cmts https://t.c\u2026", "datetime": "2018-10-12 01:50:18", "followers": "3,623"}, "1050806986332237824": {"author": "@waydegilliam", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 17:54:43", "followers": "141"}, "1059646665387212800": {"author": "@mannan_suzukake", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:20:26", "followers": "233"}, "1050940650676412416": {"author": "@jaguring1", "content_summary": "RT @jaguring1: @takashi119211 @Singularity_43 BERT\u306e\u885d\u6483\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u306e\u5404\u30bf\u30b9\u30af\u305d\u308c\u305e\u308c\u306e\u6027\u80fd\u5411\u4e0a\u3092\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u308b\uff08\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\uff09\u3002\u305d\u3057\u3066\u3001\u300c\u3048\u3089\u3044\u3063\u300d\u304c\u304b\u308f\u3044\u3044\u3002\u300c\u3076\u3063\u3061\u304e\u308a\u306eSoTA\u3084\u3093\uff01\u3053\u3093\u306a\u3093\u52dd\u3066\u3093\u2026", "datetime": "2018-10-13 02:45:51", "followers": "12,765"}, "1050661581569196032": {"author": "@Pechnet", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:16:56", "followers": "1,835"}, "1176370817447473152": {"author": "@HubBucket", "content_summary": "#BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transformers for Language Understanding #MachineLearning #DeepLearning @ARXIV_ORG \ud83d\udda5\ufe0fhttps://t.co/gu5abzjVB7 @HubBucket @HubAnalytics2 @", "datetime": "2019-09-24 05:40:33", "followers": "5,315"}, "1186448307633250304": {"author": "@angusbirds", "content_summary": "@disputed_proof se liga: Training of BERT was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete. https://t.co/vOHqVj1Zcb esse poder de computa\u00e7\u00e3o aqui deve renderizar uns 12 anos de FIFA, f\u00e1cil", "datetime": "2019-10-22 01:04:54", "followers": "295"}, "1052307619837239296": {"author": "@interian", "content_summary": "RT @vesko_st: The results from BERT are truly mind-blowing. BERT: Pre-training of Deep Bidirectional Transformers for Language Understandi\u2026", "datetime": "2018-10-16 21:17:42", "followers": "146"}, "1101526141041438720": {"author": "@jaguring1", "content_summary": "RT @nyaos524: \u3042\u3068\u6539\u3081\u3066BERT(https://t.co/z6RjoIHIFE)\u3068GPT-2(https://t.co/pG8CW8Q4KF)\u306e\u539f\u7a3f\u8aad\u3093\u3060\u3051\u3069\u3001\u3059\u3054\u3044\u3063\u3059\u306d\u3002\u306a\u3093\u304b\u3082\u3046\u4ed6\u306e\u8272\u3005\u306a\u30bf\u30b9\u30af\u3067\u5b66\u7fd2\u3057\u305f\u3067\u304b\u3044\u30e2\u30c7\u30eb\u3092\u4f7f\u3044\u56de\u3059\u3079\u304d\u3068\u3044\u3046\u98a8\u6f6e\u304c\u51fa\u6765\u4e0a\u304c\u3063\u3066\u3057\u2026", "datetime": "2019-03-01 16:54:32", "followers": "12,765"}, "1176354218569084929": {"author": "@Rosenchild", "content_summary": "#BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transformers for Language Understanding #MachineLearning #DeepLearning @ARXIV_ORG \ud83d\udda5\ufe0fhttps://t.co/TQjQGWAQxb @HubBucket @HubAnalytics2 @", "datetime": "2019-09-24 04:34:35", "followers": "11,933"}, "1051107143758168064": {"author": "@ElectMemo", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 13:47:26", "followers": "221"}, "1050680748917878784": {"author": "@willynguen", "content_summary": "'Pinoystartup/data-science' Top: [1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Uoajw1LEVo, see more https://t.co/LmVgIMJS6a", "datetime": "2018-10-12 09:33:05", "followers": "71"}, "1050596308975681537": {"author": "@bass_clef_", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:57:33", "followers": "37"}, "1056807271081144320": {"author": "@arxiv_flying", "content_summary": "#NAACL2019 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.04805v1 [cs\\.CL]) https://t.co/EI2w1ULRa6", "datetime": "2018-10-29 07:17:42", "followers": "1,309"}, "1051421843926614016": {"author": "@rose_miura", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 10:37:56", "followers": "293"}, "1051764823678058497": {"author": "@RavenCrow1111", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 09:20:49", "followers": "1,426"}, "1051026582427590657": {"author": "@rags080484", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 08:27:19", "followers": "82"}, "1068825674234814464": {"author": "@Root3d", "content_summary": "@NirantK @wingify Excellent talk. Awesome getting started story. TIL: https://t.co/R1k8NBN4OU https://t.co/NU29zM9l1m", "datetime": "2018-12-01 11:14:33", "followers": "616"}, "1051081138008150016": {"author": "@Montreal_AI", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-13 12:04:06", "followers": "177,481"}, "1065255911936024576": {"author": "@neuroicudoc", "content_summary": "Computational language understanding: meet \u2066@GoogleAI\u2069\u2019s #BERT \u2014\u2066 @CadeMetz\u2069 in @NYTimes; Paper in \u2066@arxiv\u2069: https://t.co/He6EaLXbM2 https://t.co/BnVf838s2a", "datetime": "2018-11-21 14:49:35", "followers": "3,465"}, "1050766153671274496": {"author": "@ecsquendor", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:12:27", "followers": "604"}, "1050540784766742529": {"author": "@zaxtax", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:16:55", "followers": "1,945"}, "1059597657767739392": {"author": "@SOLVER_LK", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:05:42", "followers": "865"}, "1050572543990497280": {"author": "@WilliamWangNLP", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 02:23:07", "followers": "5,083"}, "1057920327689355266": {"author": "@pradeepgali", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 09:00:35", "followers": "27"}, "1051405342205128704": {"author": "@y8o", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 09:32:22", "followers": "2,518"}, "1050927338882363392": {"author": "@ChanSimmer", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 01:52:57", "followers": "4"}, "1050820591740059649": {"author": "@mahesh_goud", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 18:48:47", "followers": "141"}, "1072889749189349378": {"author": "@yuvalmarton", "content_summary": "RT @alienelf: Just used BERT pre-trained models on a problem we were initially using Skip-thoughts for and have been fighting with for year\u2026", "datetime": "2018-12-12 16:23:44", "followers": "563"}, "1051107937215832064": {"author": "@stealth_hex", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-13 13:50:35", "followers": "19"}, "1050786597388607488": {"author": "@DocXavi", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:33:42", "followers": "1,920"}, "1050735258025377792": {"author": "@VGK108", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:09:41", "followers": "97"}, "1051391537244966913": {"author": "@muchafel", "content_summary": "RT @surafelml: #BERT Pre-training of Deep Bidirectional Transformers for Language Understanding: - Interesting to see how TRANSFER-LEARNI\u2026", "datetime": "2018-10-14 08:37:31", "followers": "65"}, "1050952186815901697": {"author": "@strategist922", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 03:31:41", "followers": "650"}, "1051414370779951104": {"author": "@risounomoewaifu", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 10:08:14", "followers": "517"}, "1050940687326142464": {"author": "@_ktomoya", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:46:00", "followers": "391"}, "1051583400304246784": {"author": "@kawauso_kun", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 21:19:54", "followers": "435"}, "1050940079215206400": {"author": "@mgamboacavazos", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 02:43:35", "followers": "170"}, "1059479843488772096": {"author": "@3times8", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-05 16:17:33", "followers": "331"}, "1051427939387027456": {"author": "@Yu_Yamaguchi_", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 11:02:09", "followers": "2,771"}, "1050928676810895361": {"author": "@linkoffate", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 01:58:16", "followers": "395"}, "1051190632234938368": {"author": "@kylewadegrove", "content_summary": "RT @surafelml: #BERT Pre-training of Deep Bidirectional Transformers for Language Understanding: - Interesting to see how TRANSFER-LEARNI\u2026", "datetime": "2018-10-13 19:19:11", "followers": "898"}, "1050893510579171328": {"author": "@jeremyphoward", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 23:38:32", "followers": "99,993"}, "1050613060191145984": {"author": "@rajasankar", "content_summary": "still need massive compute + weeks to train?", "datetime": "2018-10-12 05:04:07", "followers": "1,274"}, "1065644190728683520": {"author": "@KamiCule", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-22 16:32:28", "followers": "24"}, "1050568968271028224": {"author": "@adamajm", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:08:55", "followers": "145"}, "1050590457019781120": {"author": "@icoxfog417", "content_summary": "Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u5358\u8a9e\u3092\u4e88\u6e2c\u3059\u308b\u5f62\u3067\u5b66\u7fd2\u3001\u6587\u95a2\u4fc2\u5b66\u7fd2\u306e\u305f\u3081\u6b21\u306e\u6587or not\u3092\u5b66\u7fd2\u3055\u305b\u308b\u7b49\u306e\u5de5\u592b\u3092\u884c\u3063\u3066\u3044\u308b\u3002", "datetime": "2018-10-12 03:34:18", "followers": "11,476"}, "1066912657721057284": {"author": "@kz_lil_fox", "content_summary": "RT @shion_honda: Bidirectional Transformers [Devlin+, 2018] \u5927\u578b\u5316\u3057\u305f\u53cc\u65b9\u5411Transformer(BERT)\u3092\u30de\u30b9\u30af\u4ed8\u304d\u8a00\u8a9e\u30e2\u30c7\u30eb\u3068\u5f8c\u7d9a\u6587\u4e88\u6e2c\u3068\u3044\u30462\u3064\u306e\u30bf\u30b9\u30af\u3067\u4e8b\u524d\u5b66\u7fd2\u3055\u305b\u305f\u5f8c\u3001\u30bf\u30b9\u30af\u3054\u3068\u306bfine-tune\u3059\u308b\u3053\u2026", "datetime": "2018-11-26 04:32:54", "followers": "148"}, "1057829302484504581": {"author": "@shubham_stark", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 02:58:53", "followers": "181"}, "1050660985084698626": {"author": "@johnpimm", "content_summary": "BERT - language model with fine layer adjustments #NLP Similar to the @jeremyphoward approach? https://t.co/5Xwntz3ClO", "datetime": "2018-10-12 08:14:33", "followers": "194"}, "1209066633953792000": {"author": "@PapersTrending", "content_summary": "[9/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 65 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-12-23 11:02:03", "followers": "222"}, "1050951470055612416": {"author": "@Gabriel_Oguna", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 03:28:50", "followers": "1,451"}, "1050776403132567555": {"author": "@andrey_kurenkov", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:53:11", "followers": "3,416"}, "1061810197172080642": {"author": "@dreamgirl_cigar", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-12 02:37:33", "followers": "2,728"}, "1052204652798722048": {"author": "@gbiwer", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-16 14:28:32", "followers": "30"}, "1050711715221032967": {"author": "@sachinsq", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:36:08", "followers": "57"}, "1054816011407515650": {"author": "@samuelmaskell", "content_summary": "@amdev @darth pls migrate to BERT. the people are relying on you, amro. https://t.co/aOXm6RKFVp", "datetime": "2018-10-23 19:25:09", "followers": "938"}, "1050962621703184385": {"author": "@dogs381", "content_summary": "RT @jaguring1: @takashi119211 @Singularity_43 BERT\u306e\u885d\u6483\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u306e\u5404\u30bf\u30b9\u30af\u305d\u308c\u305e\u308c\u306e\u6027\u80fd\u5411\u4e0a\u3092\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u308b\uff08\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\uff09\u3002\u305d\u3057\u3066\u3001\u300c\u3048\u3089\u3044\u3063\u300d\u304c\u304b\u308f\u3044\u3044\u3002\u300c\u3076\u3063\u3061\u304e\u308a\u306eSoTA\u3084\u3093\uff01\u3053\u3093\u306a\u3093\u52dd\u3066\u3093\u2026", "datetime": "2018-10-13 04:13:09", "followers": "1,515"}, "1050978484338606080": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: \u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\u306f@_Ryobot\u3055\u3093\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u3089\u5f15\u7528\u3055\u305b\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u3002\u554f\u984c\u304c\u3042\u308b\u3088\u3046\u3067\u3057\u305f\u3089\u3001\u5831\u544a\u304a\u9858\u3044\u3057\u307e\u3059\u3002\u305d\u306e\u5834\u5408\u3001\u3082\u3061\u308d\u3093\u524a\u9664\u3057\u307e\u3059\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304cBERT\u306e\u6280\u8853\u7684\u306a\u8a71\u3092\u3001\u697d\u3057\u3081\u308b\u3088\u3046\u306b\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002 target=\"_blank\" href=\"https://t.co/\">https://t.co/\u2026", "datetime": "2018-10-13 05:16:11", "followers": "2,861"}, "1050730228593627136": {"author": "@pywirrarika", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 12:49:42", "followers": "331"}, "1050947401618313217": {"author": "@HomoSapienLCY", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 03:12:40", "followers": "79"}, "1051401893329612800": {"author": "@mofumofu1729", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 09:18:40", "followers": "302"}, "1050697103024898049": {"author": "@jamsheer2u", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:38:05", "followers": "207"}, "1051228831535820801": {"author": "@GillesMoyse", "content_summary": "RT @arxiv_cscl: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-13 21:50:58", "followers": "1,058"}, "1050923898060070913": {"author": "@kaushikv189", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 01:39:17", "followers": "20"}, "1051490454758293509": {"author": "@nakasuke_a", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 15:10:34", "followers": "35"}, "1051665585891762176": {"author": "@pipe_render", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 02:46:29", "followers": "319"}, "1189341332722126848": {"author": "@cnxt_nozomu", "content_summary": "\u30bb\u30b5\u30df\u30b9\u30c8\u30ea\u30fc\u30c8\u3001\u30d0\u30fc\u30c9\u3058\u3083\u306a\u304f\u3066\u30d0\u30fc\u30c8\u306e\u8aa4\u5b57\u3067\u3059\u3002\u3061\u306a\u307f\u306b\u82f1\u8a9e\u8ad6\u6587\u306f\u4ee5\u4e0b\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u53ef\u80fd\u3002 https://t.co/4cw70tfjKC \u30ac\u30c1\u52e2\u306a\u7686\u69d8\u306f\u3069\u3046\u305e\u3002", "datetime": "2019-10-30 00:40:45", "followers": "34,868"}, "1050836661599318016": {"author": "@ElijahWaalkes", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:52:38", "followers": "107"}, "1050882828454518784": {"author": "@K_Ryuichirou", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 22:56:05", "followers": "782"}, "1050646438227587072": {"author": "@CalcCon", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:16:45", "followers": "325"}, "1050931655433539584": {"author": "@setten_QB", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:10:06", "followers": "499"}, "1050623121915502592": {"author": "@kobi78", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:44:06", "followers": "335"}, "1050779739340726272": {"author": "@rgrrl", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:06:27", "followers": "200"}, "1050686375039766528": {"author": "@giulianobertoti", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-12 09:55:27", "followers": "81"}, "1050614596069818373": {"author": "@rewreu", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:10:13", "followers": "24"}, "1050578564641361920": {"author": "@Communicate_AI", "content_summary": "stanfordnlp: RT lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive compute is all you need. BERT from GoogleAI: SOTA results on everything https://t.co/cpPQxCzOEI. Resul\u2026", "datetime": "2018-10-12 02:47:03", "followers": "56"}, "1058188705461088256": {"author": "@fjfbupt", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 02:47:02", "followers": "59"}, "1050772060992102400": {"author": "@mariuskarma", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:35:56", "followers": "6,084"}, "1051104282504716289": {"author": "@to_m0ya", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 13:36:04", "followers": "1,031"}, "1050860981880938496": {"author": "@jaguring1", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 21:29:16", "followers": "12,765"}, "1131254718678933505": {"author": "@k_m_sowinska", "content_summary": "Google's NLP outperforms humans. Neat. https://t.co/ZkdbKwPVRR", "datetime": "2019-05-22 17:45:17", "followers": "9"}, "1050891523301158912": {"author": "@howardmeng", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 23:30:38", "followers": "44"}, "1176349998554001410": {"author": "@andrewkuznet", "content_summary": "@jwkritchie 2018 NAACL Best Paper - ELMo https://t.co/jv8L7YefiV 2019 NAACL Best Paper - BERT https://t.co/jSAgeaYPSQ", "datetime": "2019-09-24 04:17:49", "followers": "589"}, "1050737236797468672": {"author": "@jonojace", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 13:17:33", "followers": "42"}, "1133328318739013633": {"author": "@bill_slawski", "content_summary": "RT @seo_weirdo: the words \"pre-train, bidirectional representations, left and right context\" are considerable\u263a\ufe0f https://t.co/GNENx0iaRp", "datetime": "2019-05-28 11:05:02", "followers": "51,099"}, "1050618687399444480": {"author": "@dbparedes", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:26:29", "followers": "166"}, "1189656215791398912": {"author": "@Lockedown_", "content_summary": "If you want to read a technical description of how BERT works, here you go: https://t.co/8Z3B2Jy2Cd Be forewarned, there's a lot of engineering-speak here.", "datetime": "2019-10-30 21:31:59", "followers": "1,679"}, "1050646164255588352": {"author": "@JulienBalian", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:15:40", "followers": "14"}, "1050576785937707008": {"author": "@kalpeshk2011", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:39:59", "followers": "564"}, "1050556645393104897": {"author": "@kentonctlee", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:19:57", "followers": "445"}, "1050898175232372736": {"author": "@alfredplpl", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 23:57:04", "followers": "1,333"}, "1059611649236074496": {"author": "@wackunnpapa", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-11-06 01:01:18", "followers": "2,579"}, "1050983474373558272": {"author": "@Lingjinajin", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 05:36:01", "followers": "150"}, "1050616592940187648": {"author": "@anitayorker", "content_summary": "'Kenreisman/machine-learning' Top: [1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/JHilxdZcMI, see more https://t.co/VAU8peda7d", "datetime": "2018-10-12 05:18:09", "followers": "949"}, "1057921869561638912": {"author": "@LaurentJakubina", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 09:06:43", "followers": "65"}, "1050615457974181888": {"author": "@zxybazh", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:13:39", "followers": "247"}, "1066731728373313536": {"author": "@jaguring1", "content_summary": "RT @shion_honda: Bidirectional Transformers [Devlin+, 2018] \u5927\u578b\u5316\u3057\u305f\u53cc\u65b9\u5411Transformer(BERT)\u3092\u30de\u30b9\u30af\u4ed8\u304d\u8a00\u8a9e\u30e2\u30c7\u30eb\u3068\u5f8c\u7d9a\u6587\u4e88\u6e2c\u3068\u3044\u30462\u3064\u306e\u30bf\u30b9\u30af\u3067\u4e8b\u524d\u5b66\u7fd2\u3055\u305b\u305f\u5f8c\u3001\u30bf\u30b9\u30af\u3054\u3068\u306bfine-tune\u3059\u308b\u3053\u2026", "datetime": "2018-11-25 16:33:57", "followers": "12,765"}, "1050773240472166400": {"author": "@bukko_arab", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 15:40:37", "followers": "182"}, "1101375764069462016": {"author": "@nyaos524", "content_summary": "\u3042\u3068\u6539\u3081\u3066BERT(https://t.co/z6RjoIHIFE)\u3068GPT-2(https://t.co/pG8CW8Q4KF)\u306e\u539f\u7a3f\u8aad\u3093\u3060\u3051\u3069\u3001\u3059\u3054\u3044\u3063\u3059\u306d\u3002\u306a\u3093\u304b\u3082\u3046\u4ed6\u306e\u8272\u3005\u306a\u30bf\u30b9\u30af\u3067\u5b66\u7fd2\u3057\u305f\u3067\u304b\u3044\u30e2\u30c7\u30eb\u3092\u4f7f\u3044\u56de\u3059\u3079\u304d\u3068\u3044\u3046\u98a8\u6f6e\u304c\u51fa\u6765\u4e0a\u304c\u3063\u3066\u3057\u307e\u3063\u305f", "datetime": "2019-03-01 06:56:59", "followers": "454"}, "1051553059241779200": {"author": "@nskm_m", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 19:19:20", "followers": "190"}, "1050874303384670208": {"author": "@schreimoz_gz", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 22:22:12", "followers": "178"}, "1058124506177433605": {"author": "@dublinAI", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-01 22:31:55", "followers": "941"}, "1050724302226624513": {"author": "@DocXavi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:26:09", "followers": "1,920"}, "1156585426381168641": {"author": "@mayhewsw", "content_summary": "@sethhenrymorgan Haha, I understand the confusion. It's referring to works in NLP that explore what's happening in BERT, a hugely influential recent paper: https://t.co/bFjTDsyFFC", "datetime": "2019-07-31 15:20:28", "followers": "656"}, "1050775242166026241": {"author": "@karanchahal96", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:48:34", "followers": "135"}, "1119030370983448576": {"author": "@tuseeta", "content_summary": "It was fun to work on this project :)", "datetime": "2019-04-19 00:10:05", "followers": "33"}, "1051624396232712192": {"author": "@AssistedEvolve", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-15 00:02:48", "followers": "218"}, "1050803999639658496": {"author": "@Tarpon_red2", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 17:42:51", "followers": "2,861"}, "1155943988685230080": {"author": "@Ibankgrowth", "content_summary": "Never ceases to amaze me the power of Google's BERT, released a little over a year ago and along with pytorch gives you the power of human language in 10 lines of code https://t.co/VmMCQOGae5 #BERT #PyTorch", "datetime": "2019-07-29 20:51:37", "followers": "7"}, "1051475606204895232": {"author": "@RyoHWS", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 14:11:34", "followers": "82"}, "1050698415221420032": {"author": "@chs_sch", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:43:17", "followers": "291"}, "1141492888905195521": {"author": "@semiinvariant", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2019-06-19 23:48:07", "followers": "72"}, "1188797768418152448": {"author": "@AdentityES", "content_summary": "RT @FrancastroKW: Aqu\u00ed pod\u00e9is descargar el art\u00edculo cient\u00edfico donde (en 2018) se present\u00f3 a BERT. \u2705 https://t.co/pXgR68YKPX @Google #Sea\u2026", "datetime": "2019-10-28 12:40:49", "followers": "15,900"}, "1050960844324913152": {"author": "@risounomoewaifu", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 04:06:05", "followers": "517"}, "1050912854847586304": {"author": "@derekchen14", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 00:55:24", "followers": "249"}, "1050552944758845440": {"author": "@mark_riedl", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:05:15", "followers": "13,810"}, "1052983295036669952": {"author": "@thtrieu_", "content_summary": "A commonsense reasoning task is \"solved\" even before its official introduction.", "datetime": "2018-10-18 18:02:35", "followers": "608"}, "1050610728598532097": {"author": "@Trtd6Trtd", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 04:54:51", "followers": "155"}, "1050547982175629313": {"author": "@adropboxspace", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:45:31", "followers": "388"}, "1050900762362925057": {"author": "@HoddieOt", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 00:07:21", "followers": "113"}, "1051626570253824001": {"author": "@DataSciNews", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-15 00:11:27", "followers": "21,221"}, "1050911073694568454": {"author": "@SuperElectric", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 00:48:19", "followers": "65"}, "1118999370332954630": {"author": "@etiennebcp", "content_summary": "BERT feature extractor for NLP in the Wolfram Language!", "datetime": "2019-04-18 22:06:54", "followers": "74"}, "1050542780248969221": {"author": "@liviobs", "content_summary": "@sleepinyourhat @solomatovk https://t.co/ecxV4CBuSS", "datetime": "2018-10-12 00:24:51", "followers": "120"}, "1081946503566872576": {"author": "@jackccrawford", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2019-01-06 16:12:02", "followers": "8,755"}, "1050704740357681153": {"author": "@yantonov", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:08:25", "followers": "173"}, "1051304850951008256": {"author": "@Aoomsn", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-14 02:53:03", "followers": "59"}, "1050805730456944640": {"author": "@waydegilliam", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 17:49:43", "followers": "141"}, "1050980188660453376": {"author": "@079Hide", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:22:57", "followers": "204"}, "1171237995917131777": {"author": "@TinDan_", "content_summary": "RT @vikasbahirwani: Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LU\u2026", "datetime": "2019-09-10 01:44:33", "followers": "137"}, "1050926238867451904": {"author": "@ni_san2000", "content_summary": "\u30c0\u30f3\u30c8\u30c4", "datetime": "2018-10-13 01:48:35", "followers": "483"}, "1058204463062642688": {"author": "@SriwichaiPhat", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 03:49:39", "followers": "8"}, "1050721335075360771": {"author": "@mohitban47", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 12:14:22", "followers": "4,636"}, "1050811386865934336": {"author": "@jasonwu0731", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:12:12", "followers": "266"}, "1050759181769342976": {"author": "@JeanMarcJAzzi", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 14:44:45", "followers": "365"}, "1050878411357347841": {"author": "@kwlzn", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 22:38:32", "followers": "361"}, "1050791440685514752": {"author": "@ABosselut", "content_summary": "RT @rown: Totally! But it\u2019s not the task that\u2019s solved, it\u2019s the dataset :) it remains to be seen whether collecting the data using the sam\u2026", "datetime": "2018-10-12 16:52:56", "followers": "546"}, "1050691433198313473": {"author": "@bill_slawski", "content_summary": "RT @dawnieando: Meet BERT from Google AI @GoogleAI (great name although I am biased as it is the same name as my pomeranian) -> 'BERT: Pre-\u2026", "datetime": "2018-10-12 10:15:33", "followers": "51,099"}, "1050708370741764096": {"author": "@m_guerini", "content_summary": "Exciting or scary?", "datetime": "2018-10-12 11:22:51", "followers": "600"}, "1051381291072057344": {"author": "@Turchi_Marco", "content_summary": "RT @marcfede: Thanks for sharing this! https://t.co/2csID0xWQp", "datetime": "2018-10-14 07:56:48", "followers": "463"}, "1057908334127407104": {"author": "@arkaitz", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 08:12:56", "followers": "1,583"}, "1050695594186928128": {"author": "@en_zxteloiv", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 10:32:05", "followers": "78"}, "1050990301131694080": {"author": "@ChrSzegedy", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:03:08", "followers": "4,531"}, "1050900696281710592": {"author": "@aneesha", "content_summary": "RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J", "datetime": "2018-10-13 00:07:05", "followers": "1,586"}, "1050617602056179712": {"author": "@commentumir", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:22:10", "followers": "218"}, "1050992993946259457": {"author": "@ernire", "content_summary": "Deep learning has a lot of emphasis on the hardware resources...", "datetime": "2018-10-13 06:13:50", "followers": "310"}, "1050789186687635457": {"author": "@fjsosah", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 16:43:59", "followers": "602"}, "1050774773133926400": {"author": "@elasticjava", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:46:43", "followers": "1,390"}, "1146241385365794816": {"author": "@surbhirathoree", "content_summary": "RT @rammerdotai: \"Conceptually Simple and Empirically Powerful\". An insanely simple idea that enables training a bidirectional language mo\u2026", "datetime": "2019-07-03 02:16:57", "followers": "82"}, "1154542425114849280": {"author": "@Rosenchild", "content_summary": "RT @HubBucket: \u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Languag\u2026", "datetime": "2019-07-26 00:02:19", "followers": "11,933"}, "1050577820047630339": {"author": "@LaurentJakubina", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:44:05", "followers": "65"}, "1051029481257951232": {"author": "@ciscaoladipo", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 08:38:50", "followers": "889"}, "1059752221179445248": {"author": "@santarou", "content_summary": "BERT\u306e\u8ad6\u6587\u3002\u30d5\u30c4\u30fc\u306b\u6652\u3057\u3066\u308b\u3093\u3060\u306a\u30fc\u3002\u5fd7\u305f\u3051\u30fc\u3002 \uff1e https://t.co/DRcYWx2oEU https://t.co/FqNzSZR8nd", "datetime": "2018-11-06 10:19:53", "followers": "153"}, "1050778393568894976": {"author": "@dierken", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:01:06", "followers": "472"}, "1050595044376236032": {"author": "@CaimingXiong", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 03:52:32", "followers": "2,450"}, "1050775772527394816": {"author": "@kr_t", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 15:50:41", "followers": "303"}, "1050684675855794176": {"author": "@atinm7", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:48:42", "followers": "69"}, "1133352442173808645": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2019-05-28 12:40:53", "followers": "3,500"}, "1059703268140494848": {"author": "@0418kkk", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 07:05:22", "followers": "314"}, "1255603625042165760": {"author": "@MFuturetech", "content_summary": "and a wee bit more on BERT: https://t.co/Jymukihoof", "datetime": "2020-04-29 21:03:26", "followers": "740"}, "1050772990152101888": {"author": "@singularpattern", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:39:37", "followers": "89"}, "1059083041967091713": {"author": "@cackerman1", "content_summary": "Bidirectional Encoder Representations from Transformers (BERT) for TensorFlow, Keras, etc https://t.co/ThplBQXsLq https://t.co/1Os49gfEGZ https://t.co/gNDw0135hZ https://t.co/nHdiUORJYH https://t.co/EWTP43UOUu https://t.co/I7T0SCSb1r", "datetime": "2018-11-04 14:00:48", "followers": "5,542"}, "1050931800187400192": {"author": "@fuzzysphere", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:10:41", "followers": "501"}, "1132760537958367232": {"author": "@suatictemer", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 21:28:52", "followers": "311"}, "1050974807431540737": {"author": "@ArpitDotMe", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:01:34", "followers": "313"}, "1133291025269706752": {"author": "@seo_weirdo", "content_summary": "the words \"pre-train, bidirectional representations, left and right context\" are considerable\u263a\ufe0f", "datetime": "2019-05-28 08:36:50", "followers": "25"}, "1050626742858084355": {"author": "@aptr322", "content_summary": "RT @arxiv_cs_cl: https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.0480\u2026", "datetime": "2018-10-12 05:58:29", "followers": "58"}, "1124293896488361984": {"author": "@REIWA_AB", "content_summary": "@AtriaSoft https://t.co/bIQXwgJoKM", "datetime": "2019-05-03 12:45:27", "followers": "289"}, "1050766873753612295": {"author": "@ayirpelle", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:15:19", "followers": "2,672"}, "1163877523119386624": {"author": "@MishakinSergey", "content_summary": "RT @PapersTrending: [8/10] \ud83d\udcc8 - pytorch-transformers - 10,951 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/SBdhq9Mhj0", "datetime": "2019-08-20 18:16:39", "followers": "42"}, "1051375485903675392": {"author": "@david_nort", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:33:44", "followers": "721"}, "1051274034833436672": {"author": "@BehnamSabeti", "content_summary": "RT @surafelml: #BERT Pre-training of Deep Bidirectional Transformers for Language Understanding: - Interesting to see how TRANSFER-LEARNI\u2026", "datetime": "2018-10-14 00:50:36", "followers": "86"}, "1050592148356091907": {"author": "@alexrigler", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:41:01", "followers": "426"}, "1058172097745190912": {"author": "@LuoLinkai", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 01:41:02", "followers": "6"}, "1051207001319993344": {"author": "@morioka", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 20:24:14", "followers": "826"}, "1132782141316591616": {"author": "@Suzzicks", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 22:54:43", "followers": "18,053"}, "1058333739581956097": {"author": "@jsvnm", "content_summary": "RT @samim: Code and pre-trained models for Google's BERT (\"Bidirectional Encoder Representations from Transformers\": https://t.co/ruCT1oEEx\u2026", "datetime": "2018-11-02 12:23:21", "followers": "899"}, "1050608954688991232": {"author": "@ogrisel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:47:48", "followers": "28,418"}, "1050571690546102272": {"author": "@JonClarkSeattle", "content_summary": "RT @berty38: Aw man. This is going to make conversations really confusing. https://t.co/6FiYZuZwqb", "datetime": "2018-10-12 02:19:44", "followers": "1,246"}, "1051628742936801282": {"author": "@renato_umeton", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-15 00:20:05", "followers": "3,374"}, "1050960995517050880": {"author": "@tokimekishiken", "content_summary": "RT @jaguring1: @takashi119211 @Singularity_43 BERT\u306e\u885d\u6483\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u306e\u5404\u30bf\u30b9\u30af\u305d\u308c\u305e\u308c\u306e\u6027\u80fd\u5411\u4e0a\u3092\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u308b\uff08\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\uff09\u3002\u305d\u3057\u3066\u3001\u300c\u3048\u3089\u3044\u3063\u300d\u304c\u304b\u308f\u3044\u3044\u3002\u300c\u3076\u3063\u3061\u304e\u308a\u306eSoTA\u3084\u3093\uff01\u3053\u3093\u306a\u3093\u52dd\u3066\u3093\u2026", "datetime": "2018-10-13 04:06:41", "followers": "4,026"}, "1119365091948814336": {"author": "@parker_brydon", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/3w7rlwsQz0 SOTA for many #NLP tasks Available on tfhub: https://t.co/79SE2XVnAh https://t.co/ZVOPSq5eiK", "datetime": "2019-04-19 22:20:09", "followers": "149"}, "1050770403889532928": {"author": "@ksferguson", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:29:21", "followers": "16"}, "1050668646014447616": {"author": "@codekee", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:45:00", "followers": "590"}, "1051382427136880641": {"author": "@panpu333", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 08:01:19", "followers": "62"}, "1187308212346916870": {"author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 146 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-10-24 10:01:51", "followers": "222"}, "1050668727170002945": {"author": "@fran_martinez84", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:45:19", "followers": "89"}, "1050870206577475585": {"author": "@NChauville", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 22:05:56", "followers": "19"}, "1050827724846194689": {"author": "@eisokant", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 19:17:07", "followers": "4,535"}, "1050769677474062336": {"author": "@teagermylk", "content_summary": "Or maybe we need better human performance metrics. SWAG human evaluation is only on 100 samples, so the human accuracy of @rown (the expert) annotating his own dataset is in the range of 71-99% when sampling from a binomial see @GaelVaroquaux's https://t.c", "datetime": "2018-10-12 15:26:28", "followers": "914"}, "1050741593911816192": {"author": "@cosmo_bruce", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:34:52", "followers": "669"}, "1050662268898172928": {"author": "@_dileeppandey", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-12 08:19:39", "followers": "79"}, "1050773595188809730": {"author": "@Raincrash101", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:42:02", "followers": "364"}, "1051380695027736576": {"author": "@takeda25", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:54:26", "followers": "3,100"}, "1050643305090838529": {"author": "@newsycbot", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/NzcCt5psWn (cmts https://t.co/3fGFBxZPsu)", "datetime": "2018-10-12 07:04:18", "followers": "23,239"}, "1050905617135747072": {"author": "@montrealdotai", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 00:26:38", "followers": "33,955"}, "1050815315410472960": {"author": "@msurd", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 18:27:49", "followers": "1,121"}, "1050723656987373570": {"author": "@AssistedEvolve", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 12:23:35", "followers": "218"}, "1051365878283948039": {"author": "@BioDecoded", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | arXiv https://t.co/82ON7O4WNR #NLP #DeepLearning https://t.co/iPHc1u3LjG", "datetime": "2018-10-14 06:55:33", "followers": "1,135"}, "1051222257308176384": {"author": "@infinite_apesML", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 21:24:51", "followers": "145"}, "1050696364580048897": {"author": "@pczzy", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:35:08", "followers": "46"}, "1051993167724077056": {"author": "@tangxq93", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-16 00:28:10", "followers": "23"}, "1050874700459474944": {"author": "@jaguring1", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 22:23:47", "followers": "12,765"}, "1050698175533715456": {"author": "@barselona_59", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:42:20", "followers": "408"}, "1051594236515078144": {"author": "@KoutaR129", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 22:02:58", "followers": "268"}, "1051137318919098368": {"author": "@alt227Joydeep", "content_summary": "pretraining of deep bidirectional transformers (a.k.a for word embeddings for a group of models) shows good results on a lot of question answering tasks. In short a way to create better glove. https://t.co/By97OKWkrI", "datetime": "2018-10-13 15:47:20", "followers": "90"}, "1050581314943033345": {"author": "@tarantulae", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:57:58", "followers": "3,623"}, "1050617988280270849": {"author": "@katherinebailey", "content_summary": "We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Representation Decoder?", "datetime": "2018-10-12 05:23:42", "followers": "2,938"}, "1050639832865591301": {"author": "@chenlailin", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:50:30", "followers": "29"}, "1050581074454011905": {"author": "@kuwaccho0711", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 02:57:01", "followers": "8,150"}, "1050630467009933312": {"author": "@smart_grid_fr", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:13:17", "followers": "611"}, "1051022688846786560": {"author": "@battle8500", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 08:11:50", "followers": "66"}, "1050751566305812482": {"author": "@kirk_roberts", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 14:14:30", "followers": "221"}, "1052614295253479429": {"author": "@shawnmjones", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-17 17:36:19", "followers": "481"}, "1050977801463943169": {"author": "@imtechmonk", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 05:13:28", "followers": "133"}, "1051099148261285888": {"author": "@Sunghyo_Chung", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 13:15:40", "followers": "21"}, "1059482831758344193": {"author": "@Yossy_K", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-05 16:29:25", "followers": "1,176"}, "1052455669268434945": {"author": "@DriesBuyck", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-17 07:05:59", "followers": "170"}, "1050731867589115904": {"author": "@PerthMLGroup", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 12:56:13", "followers": "456"}, "1059655449518071808": {"author": "@3xv", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:55:21", "followers": "753"}, "1051304825692909568": {"author": "@Aoomsn", "content_summary": "RT @algunion: New SOTA on almost all NLP tasks: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https:/\u2026", "datetime": "2018-10-14 02:52:57", "followers": "59"}, "1232017620250943488": {"author": "@TecTecqwertyui", "content_summary": "RT @icoxfog417: \u5b9f\u88c5\u898b\u305f\u9650\u308a\u307e\u3063\u305f\u304fNER\u3089\u3057\u3055\u3092\u611f\u3058\u306a\u304b\u3063\u305f\u304c\u3001BERT\u306e\u8ad6\u6587\u3092\u307f\u308b\u3068CRF\u3092\u4f7f\u308f\u305a\u5165\u529bToken\u306e\u6700\u521d\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u5206\u985e\u6a5f\u306b\u304b\u3051\u3066\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3088\u3046\u3060\u306a(5.3)\u3002\u307b\u307c\u5206\u985e\u306e\u5ef6\u9577\u3067\u5b9f\u88c5\u3067\u304d\u3066\u7cbe\u5ea6\u3082\u51fa\u308b\u306a\u3089\u3042\u308a\u304c\u305f\u3044\u3068\u3053\u308d\u3060\u3002 https://\u2026", "datetime": "2020-02-24 19:01:05", "followers": "3"}, "1059072817910575105": {"author": "@Singularity_43", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-04 13:20:11", "followers": "201"}, "1061273766637002753": {"author": "@HubBucket", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-10 15:05:58", "followers": "5,315"}, "1051419578524299264": {"author": "@usagisan2020", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 10:28:56", "followers": "579"}, "1064308092597342213": {"author": "@theChrisChua", "content_summary": "RT @v_vashishta: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/1s4Xa6RE0e #nlproc #MachineL\u2026", "datetime": "2018-11-19 00:03:17", "followers": "2,136"}, "1060026710911926272": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-11-07 04:30:36", "followers": "2,861"}, "1058101372057542656": {"author": "@TensorFlow", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 21:00:00", "followers": "203,537"}, "1050592300999426049": {"author": "@Allen_A_N", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 03:41:38", "followers": "447"}, "1050678629925933056": {"author": "@kostyainua", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:24:40", "followers": "34"}, "1060711288177778688": {"author": "@itimika_next", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-09 01:50:52", "followers": "161"}, "1050734450932424704": {"author": "@FrAnDeSaR", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:06:29", "followers": "218"}, "1050837375394504704": {"author": "@sanjaykamath", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 19:55:28", "followers": "608"}, "1051143589789671424": {"author": "@dr_levan", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 16:12:15", "followers": "170"}, "1050961024768077824": {"author": "@tokimekishiken", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 04:06:48", "followers": "4,026"}, "1055976408994869249": {"author": "@alvations", "content_summary": "Elmo: https://t.co/Qv9vnpauMV Bert: https://t.co/P9LJIWVENl Oscar: https://t.co/C2pyOxGsHW", "datetime": "2018-10-27 00:16:09", "followers": "1,006"}, "1051348184214065152": {"author": "@HundertSteine", "content_summary": "RT @TSaodake: BERT\u304c\u3059\u3054\u3044\u3089\u3057\u3044 https://t.co/h0mMPCboG9", "datetime": "2018-10-14 05:45:14", "followers": "770"}, "1050821499492491267": {"author": "@PurvaTendulkar", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 18:52:23", "followers": "204"}, "1050784633472868353": {"author": "@WillemMadoc", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 16:25:53", "followers": "8"}, "1051100923856998401": {"author": "@knccch", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 13:22:43", "followers": "111"}, "1050622392920301570": {"author": "@puromputo", "content_summary": "RT @AltangChagnaa: \u0425\u04e9\u04e9\u0445, \u0448\u0438\u043d\u044d \u044d\u0440\u0438\u043d \u044d\u0445\u043b\u044d\u0432 \u0433\u044d\u043d\u044d \u04af\u04af? https://t.co/J4bjDkE1gZ", "datetime": "2018-10-12 05:41:12", "followers": "2,044"}, "1050748492098998272": {"author": "@arxivml", "content_summary": "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", Jacob Devlin, Ming-Wei Chang, K\u2026 https://t.co/3KdhVSdvyY", "datetime": "2018-10-12 14:02:17", "followers": "773"}, "1231932335056015360": {"author": "@morioka", "content_summary": "RT @icoxfog417: \u5b9f\u88c5\u898b\u305f\u9650\u308a\u307e\u3063\u305f\u304fNER\u3089\u3057\u3055\u3092\u611f\u3058\u306a\u304b\u3063\u305f\u304c\u3001BERT\u306e\u8ad6\u6587\u3092\u307f\u308b\u3068CRF\u3092\u4f7f\u308f\u305a\u5165\u529bToken\u306e\u6700\u521d\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u5206\u985e\u6a5f\u306b\u304b\u3051\u3066\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3088\u3046\u3060\u306a(5.3)\u3002\u307b\u307c\u5206\u985e\u306e\u5ef6\u9577\u3067\u5b9f\u88c5\u3067\u304d\u3066\u7cbe\u5ea6\u3082\u51fa\u308b\u306a\u3089\u3042\u308a\u304c\u305f\u3044\u3068\u3053\u308d\u3060\u3002 https://\u2026", "datetime": "2020-02-24 13:22:11", "followers": "826"}, "1050602300450922497": {"author": "@tcs2711", "content_summary": "\"massive computer is all you need\"", "datetime": "2018-10-12 04:21:22", "followers": "56"}, "1050712786601500672": {"author": "@stealth_hex", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:40:24", "followers": "19"}, "1050840547584331777": {"author": "@super_pitanga", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 20:08:04", "followers": "83"}, "1051128483227869184": {"author": "@NurmaMSU", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 15:12:14", "followers": "11"}, "1051391315290742784": {"author": "@regenschauer490", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 08:36:38", "followers": "143"}, "1051125412456292353": {"author": "@fkariminejadasl", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 15:00:01", "followers": "63"}, "1051608911436894208": {"author": "@RND_png", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-14 23:01:17", "followers": "3,999"}, "1051402220581736448": {"author": "@_suk1yak1", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 09:19:58", "followers": "588"}, "1050791619979632640": {"author": "@nova77t", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:53:39", "followers": "625"}, "1072652967109632000": {"author": "@gragtah", "content_summary": "RT @profillic: Google open sourced a new technique for NLP pre-training called BERT. With this release, anyone in the world can train their\u2026", "datetime": "2018-12-12 00:42:50", "followers": "8,216"}, "1050654478066569216": {"author": "@PeterNikolow", "content_summary": "RT @dawnieando: Meet BERT from Google AI @GoogleAI (great name although I am biased as it is the same name as my pomeranian) -> 'BERT: Pre-\u2026", "datetime": "2018-10-12 07:48:42", "followers": "4,758"}, "1050661813887459328": {"author": "@_florianmai", "content_summary": "This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo .", "datetime": "2018-10-12 08:17:51", "followers": "359"}, "1051375797464776706": {"author": "@KatoOrigami", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 07:34:58", "followers": "109"}, "1051380569303474176": {"author": "@takeda25", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 07:53:56", "followers": "3,100"}, "1050607767919030272": {"author": "@lirong11", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:43:05", "followers": "23"}, "1050618372046577666": {"author": "@SNechyporchuk", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:25:14", "followers": "28"}, "1140321028867948544": {"author": "@jcchinhui", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-06-16 18:11:34", "followers": "256"}, "1050755315069636611": {"author": "@rown", "content_summary": "Totally! But it\u2019s not the task that\u2019s solved, it\u2019s the dataset :) it remains to be seen whether collecting the data using the same methods - just more powerful language models and adversarial filters - would fix the robustness problem.", "datetime": "2018-10-12 14:29:23", "followers": "1,670"}, "1051449007006867456": {"author": "@heartburing", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-14 12:25:52", "followers": "0"}, "1051105462110674944": {"author": "@shfaithy", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 13:40:45", "followers": "4"}, "1051160407660851200": {"author": "@zhuangh", "content_summary": "RT @quocleix: Great progress in NLP using unsupervised pre-trained bidirectional language model. https://t.co/OKdRzWETP4", "datetime": "2018-10-13 17:19:05", "followers": "249"}, "1050722842902327296": {"author": "@nackpan", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 12:20:21", "followers": "153"}, "1057851284374478848": {"author": "@liqiangniu", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 04:26:14", "followers": "59"}, "1050911758771089408": {"author": "@SingingData", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 00:51:02", "followers": "455"}, "1206700779580489728": {"author": "@nikrangerseo", "content_summary": "Thank you @dawnieando for turning me onto this, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding By Jacob Devlin, Ming-Wei Chang, Kenton Lee & Kristina Toutanova Technical, helpfully insightful. \ud83d\udcaf\ud83d\udc4c\ud83c\udffd https://t.co/r0m9b", "datetime": "2019-12-16 22:20:59", "followers": "230"}, "1050610826103611398": {"author": "@nearlimbo", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:55:15", "followers": "87"}, "1059602644321263617": {"author": "@Kappa1st", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:25:31", "followers": "599"}, "1059801574011105281": {"author": "@moscwa_", "content_summary": "\u3067\u3066\u304f\u308b\u5973\u306e\u5b50\u304c\u53ef\u611b\u3044\uff08\u9055", "datetime": "2018-11-06 13:36:00", "followers": "372"}, "1050891385362993152": {"author": "@howardmeng", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 23:30:05", "followers": "44"}, "1050571010141900800": {"author": "@ArkaSadhu29", "content_summary": "Crazy moment for nlp", "datetime": "2018-10-12 02:17:02", "followers": "109"}, "1050614550926581766": {"author": "@pulsebase", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:10:03", "followers": "1,705"}, "1137161751223341056": {"author": "@tswaterman", "content_summary": "somehow I got the wrong orthography for BERT into my head -- it's always been capital 'E'. Sorry guys! https://t.co/uE1cq8ewds (it's ELMo with the funny name: https://t.co/U19DgOvmoB)", "datetime": "2019-06-08 00:57:43", "followers": "1,347"}, "1050625358028914688": {"author": "@ibelmopan", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:52:59", "followers": "1,111"}, "1051998575939317766": {"author": "@jason_han214", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-16 00:49:40", "followers": "9"}, "1051198721080193024": {"author": "@hiroaki_tanaka", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 19:51:20", "followers": "1,212"}, "1050661486933110784": {"author": "@iamquadr", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:16:33", "followers": "596"}, "1059604891339911168": {"author": "@adacola", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:34:27", "followers": "946"}, "1050936972166877184": {"author": "@prototechno", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:31:14", "followers": "4,825"}, "1132821265809461248": {"author": "@portentint", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 01:30:11", "followers": "34,498"}, "1064394689770590208": {"author": "@AmiRojkes", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/aH3deYdOvy", "datetime": "2018-11-19 05:47:24", "followers": "3,033"}, "1050692178467536896": {"author": "@maling0715", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:18:30", "followers": "108"}, "1051293473410699264": {"author": "@KouroshMeshgi", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-14 02:07:50", "followers": "616"}, "1050608472629248001": {"author": "@kaineko", "content_summary": "RT @KRiver1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/mojVIr1rLE Masked Language Model\u2026", "datetime": "2018-10-12 04:45:53", "followers": "1,377"}, "1171228039620907009": {"author": "@vikasbahirwani", "content_summary": "Get up to date on #NLP #DeepLearning in 3 easy steps? It is fun! 1) Read Transformers by @ashVaswani (https://t.co/LUftwroVOp) 2) Quickly review CoVe, ELMo, GPT and GPT2 via. https://t.co/CzHQjBlCTT. Thanx @lilianweng. 3) Read BERT https://t.co/ZnrRHD", "datetime": "2019-09-10 01:04:59", "followers": "203"}, "1050932879272755200": {"author": "@jaguring1", "content_summary": "@takashi119211 @Singularity_43 BERT\u306e\u885d\u6483\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u306e\u5404\u30bf\u30b9\u30af\u305d\u308c\u305e\u308c\u306e\u6027\u80fd\u5411\u4e0a\u3092\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u308b\uff08\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\uff09\u3002\u305d\u3057\u3066\u3001\u300c\u3048\u3089\u3044\u3063\u300d\u304c\u304b\u308f\u3044\u3044\u3002\u300c\u3076\u3063\u3061\u304e\u308a\u306eSoTA\u3084\u3093\uff01\u3053\u3093\u306a\u3093\u52dd\u3066\u3093\u308f\u3063\uff01\u300d\u3044\u3084\uff5e\u3001\u307b\u3093\u3068\u3059\u3054\u3044\u3067\u3059\u3002 https://t.co/kXg5PnC9un", "datetime": "2018-10-13 02:14:58", "followers": "12,765"}, "1050774807615336450": {"author": "@kamilsindi", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:46:51", "followers": "399"}, "1050677422763401218": {"author": "@japanAmi2015", "content_summary": "#Google #ArtificialIntelligence #semantics", "datetime": "2018-10-12 09:19:52", "followers": "180"}, "1050622867572834304": {"author": "@roeeaharoni", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:43:05", "followers": "1,081"}, "1124977643974275072": {"author": "@PapersTrending", "content_summary": "[10/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 5,880 \u2b50 - \ud83d\udcc4 https://t.co/6AtFwAwrX6 - \ud83d\udd17 https://t.co/SBdhq9Mhj0", "datetime": "2019-05-05 10:02:26", "followers": "222"}, "1054961413926592512": {"author": "@michaeld7", "content_summary": "https://t.co/fEcIODcIe5", "datetime": "2018-10-24 05:02:55", "followers": "307"}, "1050584715311210499": {"author": "@panku_god", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:11:29", "followers": "72"}, "1050910325841620992": {"author": "@kingyo222", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 00:45:21", "followers": "240"}, "1052344591251173376": {"author": "@__youki__", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-16 23:44:36", "followers": "502"}, "1198044367954436096": {"author": "@YadKonrad", "content_summary": "Weekly Cross-Cultural Words (WCCW): BERT: it could mean so much to different communities. 1) a reference to Bert Kreischer, (The Machine) , a comedian, @bertkreischer. 2) or or A state of art Machine Learning / Language model from the big G-labs https://", "datetime": "2019-11-23 01:03:30", "followers": "1,692"}, "1050640756438851584": {"author": "@kairuu4647", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 06:54:10", "followers": "195"}, "1050797968666710016": {"author": "@deeplearning4j", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 17:18:53", "followers": "25,311"}, "1068424191941193734": {"author": "@exg_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-30 08:39:12", "followers": "69"}, "1132638401511940096": {"author": "@mrcuration", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 13:23:32", "followers": "19"}, "1050577208660480001": {"author": "@eve_yk", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 02:41:39", "followers": "485"}, "1050713359400783872": {"author": "@scott_bot", "content_summary": "RT @newsyc50: BERT: Pre-Training of Deep Bidirectional Transformers for Language Underst https://t.co/R3MeLLcNog (https://t.co/m5Rxr5mWOi)", "datetime": "2018-10-12 11:42:40", "followers": "6,259"}, "1058740601494036480": {"author": "@kazanagisora", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-11-03 15:20:04", "followers": "843"}, "1052536498094886912": {"author": "@nullbytep", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-17 12:27:10", "followers": "45"}, "1050624749968084992": {"author": "@kakrafoon2", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:50:34", "followers": "17"}, "1058117601526718467": {"author": "@dklt_", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 22:04:29", "followers": "5"}, "1050590098406817792": {"author": "@LuhengH", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 03:32:53", "followers": "283"}, "1051138046240874498": {"author": "@ruka_funaki", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 15:50:14", "followers": "1,304"}, "1082961598304657409": {"author": "@erwtokritos", "content_summary": "RT @AUEBNLPGroup: Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0\u2026", "datetime": "2019-01-09 11:25:40", "followers": "671"}, "1050621168573530112": {"author": "@JeanMarcJAzzi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:36:20", "followers": "365"}, "1050792662671007744": {"author": "@algunion", "content_summary": "New SOTA on almost all NLP tasks: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NbddJtDIxG", "datetime": "2018-10-12 16:57:48", "followers": "741"}, "1189510654585126913": {"author": "@fnieves1", "content_summary": "@cackerman1 Posted... https://t.co/dLHuEdX7jy", "datetime": "2019-10-30 11:53:34", "followers": "2,668"}, "1051374927322865664": {"author": "@_Ryobot", "content_summary": "\u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://t.co/lI7697HdHQ", "datetime": "2018-10-14 07:31:30", "followers": "4,386"}, "1050809974727823362": {"author": "@jsaltr", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 18:06:35", "followers": "137"}, "1050546389569167362": {"author": "@rodolfor", "content_summary": "Note that this is from Google in Seattle, not DeepMind", "datetime": "2018-10-12 00:39:12", "followers": "4,492"}, "1051515402566610944": {"author": "@anshulkundaje", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-14 16:49:42", "followers": "7,143"}, "1050758676439478272": {"author": "@subhobrata1", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 14:42:45", "followers": "305"}, "1059750840955633667": {"author": "@moai501", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 10:14:24", "followers": "3,133"}, "1050891177023692802": {"author": "@ceobillionaire", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 23:29:15", "followers": "163,819"}, "1231927935340204032": {"author": "@icoxfog417", "content_summary": "\u5b9f\u88c5\u898b\u305f\u9650\u308a\u307e\u3063\u305f\u304fNER\u3089\u3057\u3055\u3092\u611f\u3058\u306a\u304b\u3063\u305f\u304c\u3001BERT\u306e\u8ad6\u6587\u3092\u307f\u308b\u3068CRF\u3092\u4f7f\u308f\u305a\u5165\u529bToken\u306e\u6700\u521d\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u5206\u985e\u6a5f\u306b\u304b\u3051\u3066\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3088\u3046\u3060\u306a(5.3)\u3002\u307b\u307c\u5206\u985e\u306e\u5ef6\u9577\u3067\u5b9f\u88c5\u3067\u304d\u3066\u7cbe\u5ea6\u3082\u51fa\u308b\u306a\u3089\u3042\u308a\u304c\u305f\u3044\u3068\u3053\u308d\u3060\u3002 https://t.co/B4gEcVqQ0r", "datetime": "2020-02-24 13:04:42", "followers": "11,476"}, "1050772074984349698": {"author": "@rantistan", "content_summary": "Buah... A somewhat anticipated result but awesome stuff nonetheless.", "datetime": "2018-10-12 15:35:59", "followers": "92"}, "1063042467170181120": {"author": "@searcholic_jp", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-15 12:14:09", "followers": "453"}, "1050623633691824128": {"author": "@RichardSocher", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:46:08", "followers": "62,400"}, "1050742842853875714": {"author": "@drorhilman", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:39:50", "followers": "54"}, "1051279235304775681": {"author": "@jaguring1", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 01:11:16", "followers": "12,765"}, "1051212632265355264": {"author": "@vsikka", "content_summary": "RT @pranavrajpurkar: Paper here: https://t.co/c78X5OLMtn", "datetime": "2018-10-13 20:46:36", "followers": "252,146"}, "1050774829077553153": {"author": "@ioanauoft", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:46:56", "followers": "352"}, "1132587935793127429": {"author": "@hisham_elamir", "content_summary": "I am currently reading the #BERT #paper by @GoogleAI team, https://t.co/ehPWfkKZRi #NLP #DeepLearning", "datetime": "2019-05-26 10:03:01", "followers": "56"}, "1050769571500691458": {"author": "@pliang279", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 15:26:02", "followers": "2,232"}, "1050731408568868864": {"author": "@raj_desh26", "content_summary": "2017 was the year of Bi-LSTM with attention. In 2018, they aren't even in the baseline comparison table!", "datetime": "2018-10-12 12:54:24", "followers": "345"}, "1050704540822134785": {"author": "@codekee", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:07:38", "followers": "590"}, "1075058384473350144": {"author": "@ProwessConsult", "content_summary": "Heya, BERT: researchers at @Google develop #AI that can finish your s\u0336a\u0336n\u0336d\u0336w\u0336i\u0336c\u0336h\u0336e\u0336s\u0336 sentences. https://t.co/vPWrYTDsHU https://t.co/zXkl7YlIgG", "datetime": "2018-12-18 16:01:07", "followers": "390"}, "1057264708976173056": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-30 13:35:24", "followers": "843"}, "1050785472887312384": {"author": "@EvpokPadding", "content_summary": "Oh, so much common sense. The Winograd schema challenge should be easy then", "datetime": "2018-10-12 16:29:14", "followers": "929"}, "1051114512181579778": {"author": "@VikasMalhotra08", "content_summary": "In case anyone is interested in understanding the paper and lead author\u2019s thought, here is his Reddit thread discussing the paper: https://t.co/jH39XZS6YR", "datetime": "2018-10-13 14:16:43", "followers": "17"}, "1050882579300212736": {"author": "@jaguring1", "content_summary": "\u4e0a\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u30894\u30ab\u6708\u3002\u4eca\u5ea6\u306fGoogleAI\u304b\u3089\u885d\u6483\u7684\u306a\u624b\u6cd5\u304c\u767b\u5834\uff01\u518d\u3073\u5927\u91cf\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\uff08SoTA\uff09\u3092\u53e9\u304d\u51fa\u3059\u3002\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u793a\u3059\u30bf\u30b9\u30af\u3082\u3042\u308b\u3002BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Kan2rWqMqu", "datetime": "2018-10-12 22:55:06", "followers": "12,765"}, "1050643860676702208": {"author": "@microth", "content_summary": "Cool, so now we have solved English and can finally look at more interesting languages?", "datetime": "2018-10-12 07:06:31", "followers": "623"}, "1130370055924002816": {"author": "@dangermouse_77", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-05-20 07:09:57", "followers": "1,333"}, "1050810088661893120": {"author": "@Zahid_Akhtar", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 18:07:02", "followers": "97"}, "1051309328223756288": {"author": "@kobi78", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 03:10:50", "followers": "335"}, "1248660247910195201": {"author": "@DanHLawReporter", "content_summary": "BERT (the technology in focus in the talk): https://t.co/Fw65E2JvWz", "datetime": "2020-04-10 17:12:56", "followers": "4,021"}, "1051994011320242179": {"author": "@import_godiva", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-16 00:31:32", "followers": "1,244"}, "1050732495908610049": {"author": "@MJahangeerQ", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:58:43", "followers": "5"}, "1219641008587268098": {"author": "@keiohtani", "content_summary": "\u00bb [1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/zIlfv0E2cB", "datetime": "2020-01-21 15:20:50", "followers": "204"}, "1050758141225385984": {"author": "@IntuitMachine", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:40:37", "followers": "4,812"}, "1051682545744666625": {"author": "@derekchen14", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-15 03:53:52", "followers": "249"}, "1050841772656652289": {"author": "@dirkvandenpoel", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 20:12:56", "followers": "5,910"}, "1051899483427143680": {"author": "@sermakarevich", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/9ox1k7XC2L", "datetime": "2018-10-15 18:15:54", "followers": "287"}, "1050665342693863427": {"author": "@dawnieando", "content_summary": "@DavidAmerland Here is another one -> https://t.co/crUXOww65l 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' (research by Google and submitted only yesterday). Natural language understanding next level", "datetime": "2018-10-12 08:31:52", "followers": "19,900"}, "1051722047032709120": {"author": "@shogookamoto", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 06:30:50", "followers": "275"}, "1052416890881105921": {"author": "@ForeverWeDream", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-17 04:31:54", "followers": "2,166"}, "1050853277238202369": {"author": "@feedmari", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 20:58:39", "followers": "128"}, "1051319477210570752": {"author": "@mnrmja007", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-14 03:51:10", "followers": "70"}, "1050620330845253633": {"author": "@morioka", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:33:01", "followers": "826"}, "1152316882067828736": {"author": "@Rosenchild", "content_summary": "RT @HubBucket: \u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Languag\u2026", "datetime": "2019-07-19 20:38:48", "followers": "11,933"}, "1231931783949635585": {"author": "@luda_green", "content_summary": "RT @icoxfog417: \u5b9f\u88c5\u898b\u305f\u9650\u308a\u307e\u3063\u305f\u304fNER\u3089\u3057\u3055\u3092\u611f\u3058\u306a\u304b\u3063\u305f\u304c\u3001BERT\u306e\u8ad6\u6587\u3092\u307f\u308b\u3068CRF\u3092\u4f7f\u308f\u305a\u5165\u529bToken\u306e\u6700\u521d\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u5206\u985e\u6a5f\u306b\u304b\u3051\u3066\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3088\u3046\u3060\u306a(5.3)\u3002\u307b\u307c\u5206\u985e\u306e\u5ef6\u9577\u3067\u5b9f\u88c5\u3067\u304d\u3066\u7cbe\u5ea6\u3082\u51fa\u308b\u306a\u3089\u3042\u308a\u304c\u305f\u3044\u3068\u3053\u308d\u3060\u3002 https://\u2026", "datetime": "2020-02-24 13:20:00", "followers": "3"}, "1050894711404122112": {"author": "@walterdebrouwer", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:43:18", "followers": "4,643"}, "1050982518709735424": {"author": "@__sukanta__", "content_summary": "RT @beck_daniel: A new era of NLP has just begun: a dystopia where 1) English is the only language spoken, and 2) big companies ensure gate\u2026", "datetime": "2018-10-13 05:32:13", "followers": "30"}, "1051104258290987008": {"author": "@knccch", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 13:35:58", "followers": "111"}, "1050726814908264448": {"author": "@upaver20", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 12:36:08", "followers": "406"}, "1050952036706062336": {"author": "@Gabriel_Oguna", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 03:31:05", "followers": "1,451"}, "1051347847600254976": {"author": "@jaguring1", "content_summary": "RT @TSaodake: BERT\u304c\u3059\u3054\u3044\u3089\u3057\u3044 https://t.co/h0mMPCboG9", "datetime": "2018-10-14 05:43:54", "followers": "12,765"}, "1050897482102784000": {"author": "@MatthewTeschke", "content_summary": "I can\u2019t wait until they release the code at the end of the month. Looking forward to trying it out", "datetime": "2018-10-12 23:54:19", "followers": "197"}, "1050716991634513921": {"author": "@alexey_r", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:57:06", "followers": "1,709"}, "1051181597674729472": {"author": "@visarASU", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 18:43:17", "followers": "238"}, "1051329410643582976": {"author": "@TSaodake", "content_summary": "BERT\u304c\u3059\u3054\u3044\u3089\u3057\u3044", "datetime": "2018-10-14 04:30:38", "followers": "19"}, "1051188448701403136": {"author": "@lastzero", "content_summary": "Google to the rest of the world: Solving this problem was easy! All you need is massive compute power.", "datetime": "2018-10-13 19:10:30", "followers": "740"}, "1050934930652454913": {"author": "@muktabh", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 02:23:07", "followers": "781"}, "1050773162474921984": {"author": "@arora_manuel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:40:19", "followers": "345"}, "1132539493683097600": {"author": "@MediaWyse", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 06:50:31", "followers": "4,070"}, "1257847944088182784": {"author": "@Rotten_Fruits", "content_summary": "BERT\u306e\u8ad6\u6587\u8aad\u3093\u3067\u308b\u3093\u3060\u3051\u3069\u3001\"we believe\"\u3068\u304b\u51fa\u3066\u304d\u3066\u3001\u8ad6\u6587\u306a\u306e\u306b\u3042\u308a\u306a\u306e\u304b\u306a\u3063\u3066\u601d\u3063\u305f\u3002 https://t.co/pPbdwQbphv", "datetime": "2020-05-06 01:41:34", "followers": "136"}, "1128652294541561856": {"author": "@pavlov99", "content_summary": "Did anyone work with Google's BERT NLP model? https://t.co/3hfeBM3ZdS This bidirectional approach seems to work well outperforming human performance by 2.0%. Only takes 340M parameters to train.", "datetime": "2019-05-15 13:24:11", "followers": "109"}, "1118971026304577544": {"author": "@tecmvaind", "content_summary": "WolframResearch: New in the #WolframNetRepo: extract text features w/ a BERT model trained on BookCorpus & Wikipedia https://t.co/ezQ0l46mOj Thanks Jacob Devlin, Ming-Wei Chang, Kenton Lee & Kristina Toutanova for creating the model https://t.co/5", "datetime": "2019-04-18 20:14:16", "followers": "38"}, "1051611210410409984": {"author": "@jippori6_6", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 23:10:25", "followers": "321"}, "1131500479924592640": {"author": "@PapersTrending", "content_summary": "[9/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 6,321 \u2b50 - \ud83d\udcc4 https://t.co/6AtFwAwrX6 - \ud83d\udd17 https://t.co/SBdhq9Mhj0", "datetime": "2019-05-23 10:01:51", "followers": "222"}, "1050740921413726208": {"author": "@cghosh_", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:32:12", "followers": "275"}, "1050940164636200960": {"author": "@mogamogamachine", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:43:55", "followers": "1,987"}, "1050767765118771200": {"author": "@ayirpelle", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:18:52", "followers": "2,672"}, "1059657506702548992": {"author": "@kuronekococochi", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-06 04:03:31", "followers": "1,525"}, "1050776988334678017": {"author": "@ioanauoft", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:55:31", "followers": "352"}, "1050763890286452738": {"author": "@sxren", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:03:28", "followers": "1,417"}, "1089957668385681409": {"author": "@itsjustkonrad", "content_summary": "#TransferLearning is not a new idea, it was previously applied on images. The difference was BERT. https://t.co/wsAZXQYpGW #NLP @malte_pietsch #AIMonday #ThePlaceBerlin #AI", "datetime": "2019-01-28 18:45:33", "followers": "929"}, "1050721965017690112": {"author": "@ksasao", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 12:16:52", "followers": "8,989"}, "1052519799413080064": {"author": "@robodasha", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-17 11:20:49", "followers": "381"}, "1189736116229984256": {"author": "@asfilippini", "content_summary": "@pthenq1 @vivichuleta Igual es un paso nom\u00e1s. Por cierto, cuando tenga tiempo deglutir\u00e9 el paper: https://t.co/nPwFuQbumM", "datetime": "2019-10-31 02:49:28", "followers": "1,347"}, "1050928497739292672": {"author": "@kurama554101", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 01:57:33", "followers": "162"}, "1089201222157025281": {"author": "@pauanita", "content_summary": "RT @MLHispano: El club de lectura de papers est\u00e1 m\u00e1s que en marcha \ud83d\udc9c Est\u00e1 semana ha tocado ... \ud83d\udd2c: https://t.co/7hketEs1Zp Os animamos a u\u2026", "datetime": "2019-01-26 16:39:42", "followers": "65"}, "1060603008277336065": {"author": "@jaguring1", "content_summary": "RT @IkuoMizuuchi: \u4eca\u65e5\u306f\u3001\u4eca\u3092\u3068\u304d\u3081\u304f\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30f3\u30cb\u30f3\u30b0\u5354\u4f1a\u306e\u7406\u4e8b\u306b\u3001BERT\u306e\u8a71\u3068\u304b\u3057\u3066\u3001\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u306e\u30d6\u30ed\u30b0\u3092\u7d39\u4ecb\u3059\u308b\u306a\u3069\u3057\u305f\u3002 \u307b\u3093\u3068\u30b9\u30d4\u30fc\u30c9\u611f\u304c\u3042\u308b\u306a\u3042\u3068\u611f\u3058\u3066\u305f\u306e\u3067\u2025 https://t.co/Aby5aa2fz7", "datetime": "2018-11-08 18:40:36", "followers": "12,765"}, "1050649789790965760": {"author": "@mikestrattonNET", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:30:04", "followers": "4,483"}, "1125453229788209154": {"author": "@JamesLeeUC", "content_summary": "@Ted_Underwood https://t.co/cmPDFNiuZ6", "datetime": "2019-05-06 17:32:14", "followers": "874"}, "1050741706201620480": {"author": "@KSKSKSKS2", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/7h6t5stW5W", "datetime": "2018-10-12 13:35:19", "followers": "216"}, "1051060131322970112": {"author": "@joekina", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/bqtnUAVvwo https://t.co/nESDbBX8fb", "datetime": "2018-10-13 10:40:37", "followers": "394"}, "1050872321735622657": {"author": "@313V", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 22:14:20", "followers": "518"}, "1050547236386435072": {"author": "@LJL20", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:42:34", "followers": "3"}, "1213802555748364288": {"author": "@juantomas", "content_summary": "RT @wzuidema: [13] J. Devlin, M. Chang, K. Lee and K. Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Underst\u2026", "datetime": "2020-01-05 12:40:55", "followers": "3,321"}, "1050801062658146304": {"author": "@ValeDik4", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 17:31:10", "followers": "419"}, "1050697132527767552": {"author": "@teagermylk", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:38:12", "followers": "914"}, "1050559170234241025": {"author": "@BayesForDays", "content_summary": "Massive compute sure does sound wasteful", "datetime": "2018-10-12 01:29:59", "followers": "2,439"}, "1050572322329878534": {"author": "@kentonctlee", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 02:22:14", "followers": "445"}, "1050577011461115904": {"author": "@jaguring1", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/SkTI7Bkg4C \u3068\u3046\u3068\u3046\u304d\u305f\uff01\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u9054\u6210\uff01\uff01(Google A.I.)\u3002 \u8cea\u554f\u5fdc\u7b54\u30bf\u30b9\u30af\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8SQuAD v1.1\u306e\u8a71\u3002\u3057\u304b\u3082\u3001\u5287\u7684\u306a\u6027\u80fd\u5411\u4e0a\u3002\u3059\u3054\u3044 https://t.co/IdKgpUq6k4", "datetime": "2018-10-12 02:40:52", "followers": "12,765"}, "1059603457181511680": {"author": "@_7iva", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:28:45", "followers": "1,087"}, "1050719729491431430": {"author": "@andyhickl", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 12:07:59", "followers": "5,958"}, "1249302004490371072": {"author": "@adamwdb", "content_summary": "Kembali k NLP Baru baca BERT ... ketinggalan bgt https://t.co/zslb8wZC9g", "datetime": "2020-04-12 11:43:03", "followers": "497"}, "1050974358766784512": {"author": "@PRONOjits", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 04:59:47", "followers": "2,245"}, "1050721665804591104": {"author": "@sharon_smith_1", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/aIWMgrRYgB", "datetime": "2018-10-12 12:15:41", "followers": "103"}, "1059966434770481152": {"author": "@TomiyaAkio", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 00:31:05", "followers": "3,209"}, "1055836671097229312": {"author": "@sudharsan2020", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-26 15:00:53", "followers": "363"}, "1072651383734706176": {"author": "@profillic", "content_summary": "Google open sourced a new technique for NLP pre-training called BERT. With this release, anyone in the world can train their own state-of-the-art question answering system paper: https://t.co/LMzq3NMdr8 More like this: https://t.co/VXK89jUoZH #NLP #Neur", "datetime": "2018-12-12 00:36:33", "followers": "2,107"}, "1050592514237915136": {"author": "@christian_hudon", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 03:42:29", "followers": "255"}, "1051040894244270086": {"author": "@tanoross", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 09:24:11", "followers": "250"}, "1050730223933767680": {"author": "@surafelml", "content_summary": "#BERT Pre-training of Deep Bidirectional Transformers for Language Understanding: - Interesting to see how TRANSFER-LEARNING through pre-training benefits downstream tasks - Novelty: pre-train deep Bi-directional Transformer language model https://t.co", "datetime": "2018-10-12 12:49:41", "followers": "365"}, "1187739805150515200": {"author": "@dawnieando", "content_summary": "@joeyizzy Here's the paper then too to read whilst on your flight -> https://t.co/7KCbVcRhc3", "datetime": "2019-10-25 14:36:51", "followers": "19,900"}, "1059669621043675137": {"author": "@karinto5656", "content_summary": "\u4eca\u66f4\u306a\u304c\u3089\u77e5\u3063\u3066\u9707\u3048\u3066\u308b\uff01 \u3067\u3082\u65e5\u672c\u8a9e\u306e\u5834\u5408\u3069\u3046\u306a\u308b\u304b\u308f\u304b\u3089\u3093\u3051\u3069\u306d\u2026", "datetime": "2018-11-06 04:51:40", "followers": "22"}, "1050569400116498432": {"author": "@jmhessel", "content_summary": "BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentence-follow-predictor is out on arxiv! Improvements are mostly via bidirectionality and ditching language modeling! https://t.co/1PlEu", "datetime": "2018-10-12 02:10:38", "followers": "1,254"}, "1132703772357091328": {"author": "@pedro_matias", "content_summary": "\u201coutperforming human performance by 2.0%.\u201d Go BERT \ud83d\ude42", "datetime": "2019-05-26 17:43:18", "followers": "1,068"}, "1050549141539938304": {"author": "@skrish_13", "content_summary": "izzout", "datetime": "2018-10-12 00:50:08", "followers": "247"}, "1051559263011786752": {"author": "@hellorahulk", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 19:43:59", "followers": "264"}, "1059060310214987776": {"author": "@gijigae", "content_summary": "BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.co/QeH5mIbCWZ\uff09\u306b\u306f\u3053\u308c\u3089\u306e\u60c5\u5831\u3092\u6d3b\u7528\u3057\u305f\u3044\u304b\u3089\u304c\u7406\u7531\u3067\u53c2\u52a0\u3055\u308c\u3066\u308b\u65b9\u3082\u591a\u3044\u3067\u3059\u3002 https://t.co/VnjlwWzV6j", "datetime": "2018-11-04 12:30:29", "followers": "19,021"}, "1050666872239054849": {"author": "@eisokant", "content_summary": "RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J", "datetime": "2018-10-12 08:37:57", "followers": "4,535"}, "1051034621171945472": {"author": "@emanlapponi", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 08:59:15", "followers": "210"}, "1050654602880679936": {"author": "@battle8500", "content_summary": "RT @berty38: Aw man. This is going to make conversations really confusing. https://t.co/6FiYZuZwqb", "datetime": "2018-10-12 07:49:12", "followers": "66"}, "1050567074182033408": {"author": "@BlyNotes", "content_summary": "RT @FrontPageHN: BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/21tZ5kxxgJ (cmts https://t.c\u2026", "datetime": "2018-10-12 02:01:23", "followers": "192"}, "1082620886727909377": {"author": "@gijigae", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2019-01-08 12:51:48", "followers": "19,021"}, "1050609980359864320": {"author": "@rajatmonga", "content_summary": "Big step forward on natural language understanding", "datetime": "2018-10-12 04:51:53", "followers": "13,857"}, "1050776980939931648": {"author": "@santty128", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:55:29", "followers": "694"}, "1050579517771735040": {"author": "@georgemasto", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:50:50", "followers": "106"}, "1050968310987124736": {"author": "@Octavian_ai", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 04:35:46", "followers": "149"}, "1050586534867398657": {"author": "@KalaiRamea", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:18:43", "followers": "741"}, "1051479838588948480": {"author": "@GRGSIBERIA", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 14:28:23", "followers": "2,523"}, "1050835473524875264": {"author": "@tiagomluis", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:47:55", "followers": "86"}, "1227357217742979072": {"author": "@kritika_garg", "content_summary": "RT @IvanM08106184: Google's new Bidirectional Encoder Representations from Transformers (BERT) helps understand the nuances and context of\u2026", "datetime": "2020-02-11 22:22:18", "followers": "87"}, "1050894716001296386": {"author": "@generativist", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 23:43:19", "followers": "18,499"}, "1050610781648171008": {"author": "@rajhans_samdani", "content_summary": "Solid results from a solid team!", "datetime": "2018-10-12 04:55:04", "followers": "492"}, "1050952558565617665": {"author": "@avin_regmi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 03:33:10", "followers": "7"}, "1058045010975707137": {"author": "@kartik_godawat", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 17:16:02", "followers": "148"}, "1050583304842371079": {"author": "@arvind_io", "content_summary": "@GaryMarcus Things are changing : https://t.co/g5k70XjsLT and multiple other recent work in nlp", "datetime": "2018-10-12 03:05:53", "followers": "1,141"}, "1050585727237541889": {"author": "@newsyc20", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Underst https://t.co/KfYonlagAo (https://t.co/d6KQDhKFeK)", "datetime": "2018-10-12 03:15:30", "followers": "15,958"}, "1091882068898910214": {"author": "@bhushanbrb1", "content_summary": "#WeekendRead #unlimitedResourceToRead In love with this site! After completing BERT https://t.co/9bjfOARO7P stumbled upon a free resource for latest state-of-the-art ML papers and code :) https://t.co/Ba2Odw6k0p Believe me, t\u2026https://t.co/JVoIDzC5Cn htt", "datetime": "2019-02-03 02:12:26", "followers": "137"}, "1050713647968911361": {"author": "@hamzaharkous", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:43:49", "followers": "520"}, "1051044664415944704": {"author": "@RaccoonBlack_", "content_summary": "\u306f\u3084\u3044", "datetime": "2018-10-13 09:39:10", "followers": "10"}, "1051981058290388992": {"author": "@ceobillionaire", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-15 23:40:03", "followers": "163,819"}, "1051040571412729856": {"author": "@allpratik", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 09:22:54", "followers": "300"}, "1059598026803621888": {"author": "@Miyaran99", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:07:10", "followers": "354"}, "1050575530297151488": {"author": "@xmlee97", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 02:34:59", "followers": "11"}, "1059648541474283520": {"author": "@tonets", "content_summary": "SOTA\u3059\u304d\u306b\u306f\u582a\u3089\u3093\u306a", "datetime": "2018-11-06 03:27:54", "followers": "5,131"}, "1051624568756916229": {"author": "@Mepha_nikolove", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 00:03:30", "followers": "75"}, "1145292522442412037": {"author": "@cackerman1", "content_summary": "BERT (Bidirectional Encoder Representations from Transformers) https://t.co/CcVVWhetTf https://t.co/l3oMa0iQoq https://t.co/sbMIVSNLUo https://t.co/ThplBQXsLq https://t.co/JtlJrQlFXT https://t.co/UrO2iXDMuB https://t.co/XfLrng2VQt BioBert https://t.co/mqlp", "datetime": "2019-06-30 11:26:30", "followers": "5,542"}, "1059615017832566784": {"author": "@KuboBook", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:14:41", "followers": "7,314"}, "1059654608207790081": {"author": "@t_teruya", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:52:00", "followers": "578"}, "1051303670703304705": {"author": "@K_Ryuichirou", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 02:48:22", "followers": "782"}, "1050982168015634432": {"author": "@shubham_stark", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 05:30:49", "followers": "181"}, "1050733666371084290": {"author": "@PiotrZelasko", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:03:22", "followers": "156"}, "1051266041106427904": {"author": "@kmt_t", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 00:18:50", "followers": "701"}, "1053944119750352897": {"author": "@niku9Tenhou", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-21 09:40:34", "followers": "1,145"}, "1050584660244230145": {"author": "@parashifter_R", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 03:11:16", "followers": "351"}, "1050589267808813057": {"author": "@ArthurCamara", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:29:35", "followers": "589"}, "1050628332126003200": {"author": "@wrik_basu21", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:04:48", "followers": "23"}, "1176924680323833858": {"author": "@HubBucket", "content_summary": "RT @HubBaseDB: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-25 18:21:24", "followers": "5,315"}, "1050602910541725696": {"author": "@kacky24", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 04:23:47", "followers": "21"}, "1050612435567050759": {"author": "@matsuu_zatsu", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/2yy12dlLKg", "datetime": "2018-10-12 05:01:38", "followers": "341"}, "1050998790163845120": {"author": "@itota", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:36:52", "followers": "87"}, "1050701462387322880": {"author": "@chibiscottish", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:55:24", "followers": "145"}, "1159888509269946368": {"author": "@Rosenchild", "content_summary": "RT @HubBucket: \u2695\ufe0f#HealthIT @Microsoft makes it easier to build Bidirectional Encoder Representations from Transformers - #BERT for Languag\u2026", "datetime": "2019-08-09 18:05:44", "followers": "11,933"}, "1058109743665963008": {"author": "@lostinio", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 21:33:16", "followers": "19"}, "1050587073932013568": {"author": "@ivanajwilliams", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:20:52", "followers": "867"}, "1155618061598121985": {"author": "@PostPCEra", "content_summary": "BERT Explained: A language model for NLP - state-of-the-art results in a wide variety of NLP tasks, including Question Answering, Natural Language Inference - Microsoft makes BERT NLP model better https://t.co/wH0lGXnaEx paper https://t.co/eb2gl78GV2 ht", "datetime": "2019-07-28 23:16:30", "followers": "214"}, "1082901954211270657": {"author": "@AUEBNLPGroup", "content_summary": "Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0u, https://t.co/AcNq92BHj9. Code: https://t.co/7U4Syr57aE, https://t.co/GsqjZZqu04. Central AUEB buildings, room A36. All welcome.", "datetime": "2019-01-09 07:28:39", "followers": "511"}, "1050643878137614336": {"author": "@MarklDouthwaite", "content_summary": "This could be useful - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding #ArtificialIntelligence #technology #DeepLearning https://t.co/t7Ap2YmHoj", "datetime": "2018-10-12 07:06:35", "followers": "837"}, "1050584758265282563": {"author": "@bmorphism", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:11:39", "followers": "246"}, "1050625250730242048": {"author": "@hustwj", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:52:34", "followers": "122"}, "1059330463418839042": {"author": "@kogeda92", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-05 06:23:58", "followers": "247"}, "1050774304239939585": {"author": "@akshitac8", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:44:51", "followers": "227"}, "1051006185166450694": {"author": "@JqmmRrT7HdTvvAJ", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 07:06:15", "followers": "213"}, "1050736645035708416": {"author": "@_ahani_", "content_summary": "Awesome!", "datetime": "2018-10-12 13:15:12", "followers": "288"}, "1050703137357455361": {"author": "@atsuc", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 11:02:03", "followers": "54"}, "1051844417097613314": {"author": "@nardtree", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 14:37:06", "followers": "8,954"}, "1065514820844306432": {"author": "@PSH_Lewis", "content_summary": "@AndrewMendez19 @uclmr Sure :) I\u2019ll point you to this thread : https://t.co/QnWTe4Qdeg MuppetShow works by running the muppet model on some text, then we select the hidden state of the word we\u2019re interested in, and do MuppetShow searches for similar vector", "datetime": "2018-11-22 07:58:24", "followers": "1,050"}, "1051498120272642049": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-14 15:41:02", "followers": "3,500"}, "1050551682994724864": {"author": "@tangjianpku", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 01:00:14", "followers": "1,035"}, "1072899082639851522": {"author": "@sseraphini", "content_summary": "RT @profillic: Google open sourced a new technique for NLP pre-training called BERT. With this release, anyone in the world can train their\u2026", "datetime": "2018-12-12 17:00:49", "followers": "4,395"}, "1072209716644696064": {"author": "@sangha_deb", "content_summary": "Bert: the recent paper (https://t.co/raCCbWSWFf) from google on pretrained vectors. #nlp #DeepLearning #MachineLearning https://t.co/QvQn1dkg3a", "datetime": "2018-12-10 19:21:31", "followers": "536"}, "1050688190573674496": {"author": "@fbalbach", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:02:40", "followers": "182"}, "1050608372557406208": {"author": "@ogrisel", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 04:45:30", "followers": "28,418"}, "1228812856323452928": {"author": "@AntonioLopardo3", "content_summary": "Day 56 #100DaysOfMLCode I finally studied the BERT paper. The Masked LM task and all the exploits they used during pre-training to improve on the downstream tasks are truly impressive. https://t.co/zQRcASEZmW", "datetime": "2020-02-15 22:46:29", "followers": "31"}, "1226802763801284611": {"author": "@Ritmonegro", "content_summary": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/j6iRMwemLI", "datetime": "2020-02-10 09:39:06", "followers": "1,860"}, "1050764035791233026": {"author": "@alexhock", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 15:04:03", "followers": "193"}, "1050733814715047936": {"author": "@okiu_sanjyo", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 13:03:57", "followers": "650"}, "1050903756269907974": {"author": "@mugahayaliey", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 00:19:14", "followers": "1,008"}, "1208342009456611328": {"author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 57 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-12-21 11:02:39", "followers": "222"}, "1050584045992562688": {"author": "@cghosh_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:08:50", "followers": "275"}, "1176382237547401217": {"author": "@HubBucket", "content_summary": "RT @HubBucket: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-24 06:25:56", "followers": "5,315"}, "1057760856576942080": {"author": "@tori_tukune29", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-31 22:26:55", "followers": "859"}, "1182909771520696321": {"author": "@Ronmemo1", "content_summary": "ABST100\u30e1\u30e2\uff1a63 BERT\uff082018\uff09\u2192https://t.co/oh1n1niF0Y\u3000\uff0cBidirectional Encoder Representations from Transformers\u306e\u7565\uff0c\u3064\u307e\u308a\u53cc\u65b9\u5411\u6027\u306eTransformer\uff0c\u300cBERT is conceptually simple and empirically powerful.\u300d\u305a\u3044\u3076\u3093\u3068\u81ea\u4fe1\u6e80\u3005\u306a\u6587\u7ae0", "datetime": "2019-10-12 06:44:01", "followers": "63"}, "1051022487973068800": {"author": "@BFelbo", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 08:11:02", "followers": "292"}, "1051771607306235904": {"author": "@masaponte", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 09:47:46", "followers": "63"}, "1052293390618910720": {"author": "@manoja328", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-16 20:21:09", "followers": "237"}, "1050576903747166208": {"author": "@keiskS", "content_summary": "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.04805v1 [https://t.co/8DRjnqAPrR])\" https://t.co/BSvG4rblTM", "datetime": "2018-10-12 02:40:27", "followers": "707"}, "1050631883585806336": {"author": "@vksbhandary", "content_summary": "Wow amazing results from @GoogleAI", "datetime": "2018-10-12 06:18:55", "followers": "282"}, "1051272780031770626": {"author": "@YunyiYang2", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-14 00:45:37", "followers": "2"}, "1050709369917243392": {"author": "@tejscript", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:26:49", "followers": "73"}, "1050752175515820032": {"author": "@AarneTalman", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:16:55", "followers": "335"}, "1060134374111305728": {"author": "@SKataoka44", "content_summary": "\u4eca\u3053\u308c\u77e5\u3063\u305f(\u7b11) https://t.co/NaG0XnN8Uo", "datetime": "2018-11-07 11:38:25", "followers": "5"}, "1177073133649457152": {"author": "@SRoyLee", "content_summary": "ROTD: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/c1HCGPwyFQ", "datetime": "2019-09-26 04:11:18", "followers": "260"}, "1050872557077901312": {"author": "@tackman", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 22:15:16", "followers": "1,554"}, "1050967315871817728": {"author": "@wggongsmu", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-13 04:31:48", "followers": "14"}, "1132561463657287680": {"author": "@bywoda", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 08:17:49", "followers": "412"}, "1050716727078604801": {"author": "@vksbhandary", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:56:03", "followers": "282"}, "1050659459381452801": {"author": "@texttheater", "content_summary": "RT @nsaphra: Sad glances at academic compute resources https://t.co/dgOkoQREEH", "datetime": "2018-10-12 08:08:30", "followers": "623"}, "1051502176407887873": {"author": "@chriswolfvision", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 15:57:09", "followers": "1,313"}, "1050584506393014272": {"author": "@fukutax", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:10:39", "followers": "653"}, "1060320484720488448": {"author": "@_e1337", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 23:57:58", "followers": "862"}, "1050855398830432258": {"author": "@elyase", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 21:07:05", "followers": "67"}, "1050865814302146560": {"author": "@ChiefScientist", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 21:48:28", "followers": "5,416"}, "1050698881632219136": {"author": "@AwanaAhn", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:45:09", "followers": "24"}, "1050655986963243010": {"author": "@misterkardas", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:54:42", "followers": "315"}, "1089553619735609346": {"author": "@MLHispano", "content_summary": "Las representaciones BERT prentrenadas pueden ajustarse con solo 1 capa de salida adicional para crear modelos con amplia gama de tareas (Respuesta-pregunta, inferencia de lenguaje,... : sin tareas sustanciales) \ud83d\udd2c https://t.co/2iuBXTcQEQ Ven a comentar", "datetime": "2019-01-27 16:00:00", "followers": "7,292"}, "1051016191051948032": {"author": "@guccini_off", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 07:46:01", "followers": "1"}, "1051618608256675841": {"author": "@prototechno", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding / \u201c[1810.04805] BERT: Pre-trainin\u2026\u201d https://t.co/eOmdysVUnc", "datetime": "2018-10-14 23:39:48", "followers": "4,825"}, "1051143722593918977": {"author": "@dr_levan", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 16:12:47", "followers": "170"}, "1051849314010636290": {"author": "@shalomfrance", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 14:56:33", "followers": "752"}, "1133006309303808001": {"author": "@invsujitdas", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 13:45:29", "followers": "2,425"}, "1101201674973126656": {"author": "@SimpleATeam", "content_summary": "@Cornell presents BERT, a new language representation model. BERT reportedly achieved impressive results on eleven natural language processing tasks. #NLP #bidirectinaltransformers #deeplearning #machinetranslation #SQuAD #intelligentcontent https://t.co/x", "datetime": "2019-02-28 19:25:13", "followers": "1,070"}, "1050640219077136385": {"author": "@sappy_and_sappy", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 06:52:02", "followers": "175"}, "1050704695151521792": {"author": "@LesiaTkacz", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 11:08:15", "followers": "227"}, "1058605638538813441": {"author": "@BioDecoded", "content_summary": "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing | Google AI Blog https://t.co/WG2EYApABU https://t.co/1wx3xQsl2l https://t.co/CM5idba1k1 #NLP #DeepLearning https://t.co/xnkndAaOyr", "datetime": "2018-11-03 06:23:46", "followers": "1,135"}, "1135420356615761920": {"author": "@wordmetrics", "content_summary": "This is an excellent explanation of BERT. https://t.co/2Arb3EFxX4 Original Google AI paper here: https://t.co/yhp9vkKNRR #nlproc #ai #google #textclassification #seo #machinelearning #ml https://t.co/S9MPMMLQPU", "datetime": "2019-06-03 05:38:02", "followers": "36"}, "1050938838065893376": {"author": "@tsatie", "content_summary": "RT @jaguring1: @takashi119211 @Singularity_43 BERT\u306e\u885d\u6483\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u306e\u5404\u30bf\u30b9\u30af\u305d\u308c\u305e\u308c\u306e\u6027\u80fd\u5411\u4e0a\u3092\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u308b\uff08\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\uff09\u3002\u305d\u3057\u3066\u3001\u300c\u3048\u3089\u3044\u3063\u300d\u304c\u304b\u308f\u3044\u3044\u3002\u300c\u3076\u3063\u3061\u304e\u308a\u306eSoTA\u3084\u3093\uff01\u3053\u3093\u306a\u3093\u52dd\u3066\u3093\u2026", "datetime": "2018-10-13 02:38:39", "followers": "704"}, "1051000124506349568": {"author": "@makingAGI", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:42:10", "followers": "89"}, "1050551992484093957": {"author": "@vishivish18", "content_summary": "RT JatanaHQ \"[R] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/uG6yzStbU8\"", "datetime": "2018-10-12 01:01:27", "followers": "105"}, "1050736309059358720": {"author": "@__snehal__", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:13:52", "followers": "338"}, "1119313661447823361": {"author": "@gopalpsarma", "content_summary": "Looking forward to sitting down with WL12 and diving into all of the new neural network functionality. @WolframResearch @etiennebcp", "datetime": "2019-04-19 18:55:47", "followers": "217"}, "1053863474349060096": {"author": "@jaguring1", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-21 04:20:06", "followers": "12,765"}, "1051543918406455298": {"author": "@cesncn", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 18:43:01", "followers": "14"}, "1051665105048416257": {"author": "@mingszuliang", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 02:44:34", "followers": "246"}, "1050735882263777280": {"author": "@sqcai", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:12:10", "followers": "2,399"}, "1050546530782871552": {"author": "@yatskar", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:39:45", "followers": "921"}, "1050715952592105473": {"author": "@jordnb", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:52:59", "followers": "1,219"}, "1051042651783528449": {"author": "@pyykkis81", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 09:31:10", "followers": "260"}, "1050883042145787905": {"author": "@tomo_makes", "content_summary": "RT @jaguring1: \u4e0a\u306e\u30c4\u30a4\u30fc\u30c8\u304b\u30894\u30ab\u6708\u3002\u4eca\u5ea6\u306fGoogleAI\u304b\u3089\u885d\u6483\u7684\u306a\u624b\u6cd5\u304c\u767b\u5834\uff01\u518d\u3073\u5927\u91cf\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30bf\u30b9\u30af\u3067\u6700\u9ad8\u6027\u80fd\uff08SoTA\uff09\u3092\u53e9\u304d\u51fa\u3059\u3002\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u793a\u3059\u30bf\u30b9\u30af\u3082\u3042\u308b\u3002BERT: Pre-training of Deep Bidirecti\u2026", "datetime": "2018-10-12 22:56:56", "followers": "1,054"}, "1050658132567838720": {"author": "@yoquankara", "content_summary": "New Language model from Google \"BERT\", which scores higher than ELMo and Generative Pre-trained Transformer. Impressive! https://t.co/4SUY5Ln90z Masking input seems to get more and more applications.", "datetime": "2018-10-12 08:03:13", "followers": "45"}, "1050579992583655424": {"author": "@kartik_144", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:52:43", "followers": "19"}, "1050781519441096704": {"author": "@r_akshaynath", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:13:31", "followers": "23"}, "1051002317305200640": {"author": "@daniel_heres", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:50:53", "followers": "50"}, "1057805427038175232": {"author": "@ErmiaBivatan", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 01:24:01", "followers": "817"}, "1058342913619316738": {"author": "@angel_genov", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 12:59:48", "followers": "43"}, "1050797842368000005": {"author": "@DanBarg", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 17:18:23", "followers": "13"}, "1050984086007820288": {"author": "@a2uky", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 05:38:27", "followers": "841"}, "1051110702537007104": {"author": "@PerthMLGroup", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 14:01:34", "followers": "456"}, "1051109481721950208": {"author": "@monkey_d_george", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 13:56:43", "followers": "2,090"}, "1050584493587816448": {"author": "@iamknighton", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 03:10:36", "followers": "425"}, "1050920403055468544": {"author": "@linbo_pythoner", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 01:25:23", "followers": "40"}, "1051113691259625472": {"author": "@PranjalDaga1", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 14:13:27", "followers": "207"}, "1051062963094736897": {"author": "@__youki__", "content_summary": "\u53c2\u8003\u306b\u306a\u308a\u307e\u3059", "datetime": "2018-10-13 10:51:52", "followers": "502"}, "1059614771509481472": {"author": "@reomatsumura", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:13:42", "followers": "627"}, "1050915529987432450": {"author": "@JungeAlexander", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 01:06:02", "followers": "551"}, "1051033444598173697": {"author": "@_sh_8_", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 08:54:35", "followers": "383"}, "1118969837412323328": {"author": "@WolframResearch", "content_summary": "New in the #WolframNetRepo: extract text features w/ a BERT model trained on BookCorpus & Wikipedia https://t.co/kxDgMdJu5A Thanks Jacob Devlin, Ming-Wei Chang, Kenton Lee & Kristina Toutanova for creating the model https://t.co/uc7FhHduLa #NLP #n", "datetime": "2019-04-18 20:09:33", "followers": "41,939"}, "1050978062186237952": {"author": "@miyeah27", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:14:30", "followers": "57"}, "1051414261975535616": {"author": "@usagisan2020", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 10:07:49", "followers": "579"}, "1050552381526568960": {"author": "@mchang21", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 01:03:00", "followers": "132"}, "1059386621747847168": {"author": "@cop4587", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-05 10:07:07", "followers": "48"}, "1050804102358216704": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: \u2460 BERT https://t.co/HIplHpINZL", "datetime": "2018-10-12 17:43:15", "followers": "2,861"}, "1050876833573625856": {"author": "@eleurent", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-12 22:32:16", "followers": "848"}, "1059604908117176321": {"author": "@d27robo", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:34:31", "followers": "593"}, "1051488116995813376": {"author": "@stealthinu", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 15:01:17", "followers": "1,055"}, "1050697706224652294": {"author": "@NyatzAnger", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:40:28", "followers": "360"}, "1051112818232025089": {"author": "@xamat", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 14:09:59", "followers": "19,084"}, "1050669895787065345": {"author": "@jalners", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:49:58", "followers": "18"}, "1050609869739315201": {"author": "@YW_climber", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:51:26", "followers": "32"}, "1050874918420795393": {"author": "@nelscorrea", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 22:24:39", "followers": "535"}, "1052374899430383616": {"author": "@minhpham", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-17 01:45:02", "followers": "85"}, "1050577495848677376": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 02:42:48", "followers": "2,861"}, "1107680929932148740": {"author": "@gorban", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2019-03-18 16:31:28", "followers": "1,270"}, "1050763926156251136": {"author": "@alexhock", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 15:03:36", "followers": "193"}, "1050923747694301185": {"author": "@kaushikv189", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-13 01:38:41", "followers": "20"}, "1050550827851501568": {"author": "@bttyeo", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:56:50", "followers": "5,861"}, "1050718079859249152": {"author": "@dabadone2", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:01:26", "followers": "7"}, "1050557144741830656": {"author": "@jonathanrraiman", "content_summary": "Incredible achievement in NLP Pretraining from @GoogleAI with BERT. I assume translation will fall to this approach next?", "datetime": "2018-10-12 01:21:56", "followers": "674"}, "1050750983867912197": {"author": "@akhavr", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 14:12:11", "followers": "975"}, "1121010863664697344": {"author": "@vitiokm", "content_summary": "I have lived in a darkness until this day https://t.co/Q9dUhzss3Z", "datetime": "2019-04-24 11:19:51", "followers": "55"}, "1212727374594367490": {"author": "@wzuidema", "content_summary": "[13] J. Devlin, M. Chang, K. Lee and K. Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://t.co/UBltI8MhaY @toutanova", "datetime": "2020-01-02 13:28:32", "followers": "929"}, "1058706850332401664": {"author": "@jaguring1", "content_summary": "@Ken_Yamamura \u305d\u308c\u3089\u306b\u6311\u6226\u3057\u305f\u7814\u7a76\u306f\u3044\u304f\u3064\u304b\u3042\u3063\u3066\u3001\u90e8\u5206\u7684\u306b\u306f\u5b9f\u73fe\u3057\u3066\u3044\u308b\u6c17\u304c\u3057\u307e\u3059\u304c\u3001\u672c\u8cea\u7684\u306b\u306f\u3053\u308c\u304b\u3089\u3060\u3068\u601d\u3044\u307e\u3059\u3002\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u306e\u8a71\u3060\u3068\u3001\u3053\u308c\u304c\u4eca\u306e\u6700\u5148\u7aef\u3067\u3059\u3002\u5e38\u8b58\u63a8\u8ad6\u30bf\u30b9\u30afswag\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3082\u4eba\u9593\u30ec\u30d9\u30eb\u3060\u3068\u601d\u3044\u307e\u3059\u3002 https://t.co/e4uYHsbwZF", "datetime": "2018-11-03 13:05:57", "followers": "12,765"}, "1050696856982966272": {"author": "@ceobillionaire", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 10:37:06", "followers": "163,819"}, "1050912077278109696": {"author": "@Adam_Chidlow", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 00:52:18", "followers": "105"}, "1050758420817575936": {"author": "@Melnus_", "content_summary": "RT @jaguring1: \u2460 BERT https://t.co/HIplHpINZL", "datetime": "2018-10-12 14:41:44", "followers": "411"}, "1057875396526370816": {"author": "@beanwen", "content_summary": "Great work!", "datetime": "2018-11-01 06:02:03", "followers": "18"}, "1050865132857958400": {"author": "@IlyaOrson", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 21:45:46", "followers": "78"}, "1051168137847148544": {"author": "@leehomyc", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 17:49:48", "followers": "273"}, "1057760356750114816": {"author": "@came1223pg", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-31 22:24:55", "followers": "1,472"}, "1050676995497943040": {"author": "@SegmentorFault", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 09:18:11", "followers": "65"}, "1187824079002312705": {"author": "@dawnieando", "content_summary": "@Suzzicks @BritneyMuller @AlexisKSanders My dog was here first. My Bert is 6 years old. https://t.co/QKoiNtV3Jy", "datetime": "2019-10-25 20:11:43", "followers": "19,900"}, "1178029957521563649": {"author": "@Amazone97358351", "content_summary": "RT @HubBucket: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-28 19:33:23", "followers": "7"}, "1050611166173487110": {"author": "@treasured_write", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:56:36", "followers": "99"}, "1187670622878478336": {"author": "@PapersTrending", "content_summary": "[6/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 150 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-10-25 10:01:57", "followers": "222"}, "1053070684044185600": {"author": "@mihail_eric", "content_summary": "I think this is symptomatic of the difficulty of the benchmarks we are designing. That and we haven\u2019t yet really honed in on a testable kernel of what \u201creasoning\u201d or \u201ccommon sense\u201d are. Datasets like clevr I believe also were \u201csolved\u201d very soon after rele", "datetime": "2018-10-18 23:49:50", "followers": "871"}, "1050976678099337216": {"author": "@Davidwang420", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 05:09:00", "followers": "36"}, "1050558380358524928": {"author": "@hiconcep", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 01:26:50", "followers": "135,428"}, "1057196548453531648": {"author": "@EvpokPadding", "content_summary": "Okay, the #BERT article (https://t.co/unZakIxijd) and the @googlecloud page (https://t.co/olYEwkCJQn) seem to be contradicting each other regarding what a \u201cpod\u201d is. According to the page it looks like 64 (v2) TPU/pod, but If I understand the article correc", "datetime": "2018-10-30 09:04:33", "followers": "929"}, "1207074154593181696": {"author": "@FBWM8888", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Kan2rWqMqu", "datetime": "2019-12-17 23:04:39", "followers": "553"}, "1050975668849205248": {"author": "@CHAOYUEHE", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:05:00", "followers": "39"}, "1051078208416833536": {"author": "@DanielCanueto", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 11:52:27", "followers": "206"}, "1059616689715736576": {"author": "@alytile", "content_summary": "[2018] Jacob Devlin, et. al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://t.co/tKSKiiJSZz BERT=Bidirectional Encoder Representations from Transformers https://t.co/H3aCffrmL6", "datetime": "2018-11-06 01:21:20", "followers": "669"}, "1051491000126820353": {"author": "@guccini_off", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 15:12:44", "followers": "1"}, "1058492885920825346": {"author": "@DevHunterYZ", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-02 22:55:44", "followers": "87"}, "1058102508843921408": {"author": "@TensorflowTural", "content_summary": "RT bsaeta: I really enjoyed reading jalammar's The Illustrated Transformer https://t.co/Fp6k3j0nX4. Transformer-style architectures are very promising, and underpin the latest SotA NLP such as BERT https://t.co/HvYtXuowm4 and Mesh TensorFlow https://t.co/I", "datetime": "2018-11-01 21:04:31", "followers": "315"}, "1051182968998252544": {"author": "@Reza_Zadeh", "content_summary": "Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sentence X immediately follow sentence A? Transfer learning showing great strides in language, as it does with Computer Vision. Paper:", "datetime": "2018-10-13 18:48:44", "followers": "25,193"}, "1075498423770464256": {"author": "@shinji_kono", "content_summary": "BERT\u3063\u3066\u3001\u6587\u7ae0\u306e\u7a74\u57cb\u3081\u554f\u984c\u306a\u306e\u304b\u3002\u53d7\u9a13\u52c9\u5f37\u3067\u305f\u304f\u3055\u3093\u89e3\u3044\u305f\u306a\u3002 https://t.co/ZEVfXd653L", "datetime": "2018-12-19 21:09:40", "followers": "6,349"}, "1189342693299838976": {"author": "@1amageek", "content_summary": "RT @cnxt_nozomu: \u30bb\u30b5\u30df\u30b9\u30c8\u30ea\u30fc\u30c8\u3001\u30d0\u30fc\u30c9\u3058\u3083\u306a\u304f\u3066\u30d0\u30fc\u30c8\u306e\u8aa4\u5b57\u3067\u3059\u3002\u3061\u306a\u307f\u306b\u82f1\u8a9e\u8ad6\u6587\u306f\u4ee5\u4e0b\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u53ef\u80fd\u3002 https://t.co/4cw70tfjKC \u30ac\u30c1\u52e2\u306a\u7686\u69d8\u306f\u3069\u3046\u305e\u3002", "datetime": "2019-10-30 00:46:09", "followers": "3,152"}, "1050788998887534593": {"author": "@AtulAcharya", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:43:14", "followers": "1,882"}, "1050909368412073984": {"author": "@nikiparmar09", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 00:41:33", "followers": "1,967"}, "1051504587973091328": {"author": "@ringo00617", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-14 16:06:44", "followers": "17"}, "1050780508001452032": {"author": "@ashishashiya", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:09:30", "followers": "7"}, "1050563490979561472": {"author": "@BrundageBot", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova https://t.co/UHdlkqRABo", "datetime": "2018-10-12 01:47:09", "followers": "3,887"}, "1051746341544636416": {"author": "@kosuke_tsujino", "content_summary": "BERT\u3001\u5b9f\u88c5\u304c\u516c\u958b\u3055\u308c\u305f\u3089(10\u6708\u4e2d\u3089\u3057\u3044)\u65e5\u672c\u8a9e\u30e2\u30c7\u30eb\u3092\u30af\u30e9\u30a6\u30c9\u30d5\u30a1\u30f3\u30c7\u30a3\u30f3\u30b0\u3067\u4f5c\u3063\u3066\u5171\u6709\u3059\u308b\u4eba\u3068\u304b\u3044\u306a\u3044\u3060\u308d\u3046\u304b... https://t.co/orRjADHKkm", "datetime": "2018-10-15 08:07:22", "followers": "472"}, "1050786122756964354": {"author": "@karlafej", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 16:31:48", "followers": "159"}, "1052402927263014912": {"author": "@CaitlinnBrianna", "content_summary": "RT @johnpimm: BERT - language model with fine layer adjustments #NLP Similar to the @jeremyphoward approach? https://t.co/5Xwntz3ClO", "datetime": "2018-10-17 03:36:25", "followers": "15"}, "1051038892584038405": {"author": "@mcraddock", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 09:16:14", "followers": "2,772"}, "1050623513491427328": {"author": "@lespetitescases", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:45:39", "followers": "2,134"}, "1057983976806211584": {"author": "@roundtrip", "content_summary": "\u201cOur academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https://t.co/VBaiBHF0vc.\u201d @Google", "datetime": "2018-11-01 13:13:31", "followers": "1,932"}, "1051188554355879936": {"author": "@rejectionking", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 19:10:56", "followers": "2,251"}, "1050578846360248320": {"author": "@amitunix", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:48:10", "followers": "229"}, "1050867772949852160": {"author": "@haiyongw", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 21:56:15", "followers": "41"}, "1050662722457624577": {"author": "@storfisk", "content_summary": "BERT is the word", "datetime": "2018-10-12 08:21:28", "followers": "94"}, "1050727305927172097": {"author": "@sravanankaraju", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:38:05", "followers": "186"}, "1086859485308641280": {"author": "@PaulHuang668", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2019-01-20 05:34:28", "followers": "1"}, "1051184857198579712": {"author": "@loretoparisi", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 18:56:14", "followers": "1,062"}, "1051619545213890560": {"author": "@ktansai", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 23:43:32", "followers": "2,587"}, "1050545801204658176": {"author": "@sammthomson", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:36:51", "followers": "1,042"}, "1050598519306838023": {"author": "@Tarpon_red2", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 04:06:20", "followers": "2,861"}, "1050667210039906304": {"author": "@shfaithy", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 08:39:17", "followers": "4"}, "1050945981213159425": {"author": "@faye73875464", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-13 03:07:02", "followers": "1"}, "1050581847531376640": {"author": "@thefeed2000", "content_summary": "https://t.co/qe8E25Sv2r BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", "datetime": "2018-10-12 03:00:05", "followers": "318"}, "1050837102118748162": {"author": "@hanjinda", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 19:54:23", "followers": "12"}, "1051415124685115398": {"author": "@carpent13823815", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 10:11:14", "followers": "115"}, "1050653946857975809": {"author": "@daniilmagpie", "content_summary": "Impressive result on more than one task. Need to take a closer look", "datetime": "2018-10-12 07:46:35", "followers": "904"}, "1053723560617816064": {"author": "@u39kun", "content_summary": "@GoogleAI 's BERT paper (https://t.co/IARGHdvzip) says code and pre-trained weights will be available before the end of October 2018. This will be game changing for NLP practitioners who would be able to simply download and finetune it to perform many NLP", "datetime": "2018-10-20 19:04:08", "followers": "273"}, "1050793860165709824": {"author": "@ananya__g", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 17:02:33", "followers": "130"}, "1051263379258527745": {"author": "@antimon2", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 00:08:15", "followers": "964"}, "1132713363589361664": {"author": "@anilthakur2u", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 18:21:25", "followers": "161"}, "1051042666773995520": {"author": "@smart_grid_fr", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 09:31:13", "followers": "611"}, "1059611674062266369": {"author": "@GeorgeChiesa", "content_summary": "Important contribution to #AI from Google https://t.co/3WwRWwRA8t", "datetime": "2018-11-06 01:01:24", "followers": "1,844"}, "1051143323921068032": {"author": "@RocckYoung", "content_summary": "An step forward to AGI !", "datetime": "2018-10-13 16:11:12", "followers": "4"}, "1050594875123494925": {"author": "@omarsar0", "content_summary": "That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @RichardSocher suggested, we would like these mechanisms to be tested on a wider variety of tasks and even more complex tasks. https:", "datetime": "2018-10-12 03:51:51", "followers": "5,219"}, "1050815470314504192": {"author": "@msurd", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 18:28:25", "followers": "1,121"}, "1050772035297660928": {"author": "@etzioni", "content_summary": "For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03", "datetime": "2018-10-12 15:35:50", "followers": "24,267"}, "1051016456601653248": {"author": "@guccini_off", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 07:47:04", "followers": "1"}, "1059636628086374400": {"author": "@kuippa", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 02:40:33", "followers": "1,461"}, "1050575705942175744": {"author": "@ahmaurya", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:35:41", "followers": "0"}, "1057791018236432384": {"author": "@ankurhandos", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 00:26:46", "followers": "5,089"}, "1051844529102311424": {"author": "@Untergrund_idol", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 14:37:32", "followers": "308"}, "1050696232816050177": {"author": "@PSH_Lewis", "content_summary": "Loads of excitement about BERT: https://t.co/XPce6qBLLu. I think the simplicity of the approach is the most interesting part. Do think its a bit early to be \"a new era in NLP\" just yet though...", "datetime": "2018-10-12 10:34:37", "followers": "1,050"}, "1050643761259114496": {"author": "@_reachsumit", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:06:07", "followers": "66"}, "1051486997032759296": {"author": "@viirya", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-14 14:56:50", "followers": "115"}, "1050692120749727751": {"author": "@m__dehghani", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:18:17", "followers": "1,649"}, "1051428625784823808": {"author": "@rose_miura", "content_summary": "BERT\uff1a\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306e\u6df1\u5c64\u53cc\u65b9\u5411\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u4e8b\u524d\u8a13\u7df4 \u672c\u7a3f\u3067\u306f\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u304b\u3089\u306e\u53cc\u65b9\u5411\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u8868\u73fe\u3092\u8868\u3059BERT\u3068\u3044\u3046\u65b0\u3057\u3044\u8a00\u8a9e\u8868\u73fe\u30e2\u30c7\u30eb\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 https://t.co/21vKnrxuC5", "datetime": "2018-10-14 11:04:53", "followers": "293"}, "1050716929223262208": {"author": "@Manish_Saraswt", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:56:51", "followers": "1,249"}, "1051627129622810624": {"author": "@yujiorama", "content_summary": "\u201c[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d https://t.co/Mq1FqMaGEC #web", "datetime": "2018-10-15 00:13:40", "followers": "734"}, "1051674927781228544": {"author": "@3000manJPY", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 03:23:36", "followers": "51"}, "1062891739986444288": {"author": "@T_Rugby_H", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-15 02:15:13", "followers": "100"}, "1050692688167620608": {"author": "@baibaidamashii2", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 10:20:32", "followers": "577"}, "1050877878164951042": {"author": "@olgaiv39", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 22:36:25", "followers": "90"}, "1050758641828139008": {"author": "@rajarishis", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 14:42:37", "followers": "161"}, "1050591480719400960": {"author": "@akkikiki", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:38:22", "followers": "640"}, "1050884793892782080": {"author": "@DH_FBK", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 23:03:54", "followers": "1,860"}, "1050980956272701440": {"author": "@ken_hide", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:26:00", "followers": "250"}, "1081243790927687682": {"author": "@SiQuizas", "content_summary": "RT @pabloc_ds: The NLP model was born in Google and it is called BERT, here's the paper: https://t.co/wDT3Hru7vF #AI #NLP", "datetime": "2019-01-04 17:39:42", "followers": "2,166"}, "1051203490196930560": {"author": "@ChurchillMic", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 20:10:17", "followers": "164"}, "1050894843310936065": {"author": "@rags080484", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 23:43:49", "followers": "82"}, "1051376049760522240": {"author": "@sughimsi", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:35:58", "followers": "232"}, "1050939759009226752": {"author": "@tomo_makes", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-13 02:42:18", "followers": "1,054"}, "1051081343944314882": {"author": "@LiGillis", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-13 12:04:55", "followers": "11,096"}, "1050561687001202690": {"author": "@HNTweets", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding: https://t.co/xkVLMUdxIM Comments: https://t.co/c7yvDNEnPH", "datetime": "2018-10-12 01:39:59", "followers": "17,713"}, "1051647542516039680": {"author": "@anthonyawuley", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 01:34:47", "followers": "82"}, "1059659783593091072": {"author": "@Nekomikado", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 04:12:34", "followers": "101"}, "1050933180939661312": {"author": "@a2uky", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:16:10", "followers": "841"}, "1051375852951199744": {"author": "@AshiData", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 07:35:11", "followers": "258"}, "1050907249797943297": {"author": "@JeremiahHarmsen", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 00:33:07", "followers": "1,090"}, "1050938244194430976": {"author": "@nikhilbalaji", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 02:36:17", "followers": "231"}, "1050637797600550912": {"author": "@shonosuke_i", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:42:25", "followers": "1,074"}, "1051110877615669248": {"author": "@PerthMLGroup", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 14:02:16", "followers": "456"}, "1050714160617742343": {"author": "@marc_data", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:45:51", "followers": "187"}, "1057264748918538240": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-30 13:35:33", "followers": "843"}, "1050682264382820354": {"author": "@_dmh", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-12 09:39:07", "followers": "1,122"}, "1051499095620276226": {"author": "@vardi", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 15:44:54", "followers": "7,188"}, "1050949658044977152": {"author": "@ASreesaila", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 03:21:38", "followers": "25"}, "1051647310738862081": {"author": "@ReedRoof", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-15 01:33:52", "followers": "28"}, "1132470175880495104": {"author": "@bill_slawski", "content_summary": "No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 02:15:04", "followers": "51,099"}, "1050722647988944896": {"author": "@LeanderHasty", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:19:35", "followers": "83"}, "1059612749280436224": {"author": "@FrostyDesign_JP", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:05:40", "followers": "1,584"}, "1050589295373770752": {"author": "@maithra_raghu", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 03:29:41", "followers": "5,629"}, "1050633319447031809": {"author": "@alexpoms", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:24:37", "followers": "267"}, "1050592203297284101": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyrbYpu", "datetime": "2018-10-12 03:41:14", "followers": "3,500"}, "1051490070409043968": {"author": "@kazanagisora", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 15:09:03", "followers": "843"}, "1051291898625880064": {"author": "@iskander", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-14 02:01:35", "followers": "2,172"}, "1050661378590044160": {"author": "@betterhn50", "content_summary": "53 \u2013 BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/TRbEgDSZeP", "datetime": "2018-10-12 08:16:07", "followers": "112"}, "1050575828319436801": {"author": "@chubbymaggie", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:36:10", "followers": "248"}, "1058003317270269952": {"author": "@PerthMLGroup", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 14:30:22", "followers": "456"}, "1050635183982968836": {"author": "@morioka", "content_summary": "RT @icoxfog417: Bi-directional\u306eTransformer\u3092\u4e8b\u524d\u5b66\u7fd2\u3057\u3001QA\u3084\u6587\u95a2\u4fc2\u63a8\u8ad6\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u8ee2\u79fb\u3057\u305f\u7814\u7a76\u3002ELMo\u306e\u53cc\u65b9\u5411\u6027\u3068\u3001OpenAI\u306eTransformer\u8ee2\u79fb\u3092\u30df\u30c3\u30af\u30b9\u3057\u305f\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u5b66\u7fd2\u306f\u53cc\u65b9\u5411\u3067\u306a\u3044\u306e\u3067crop\u3057\u305f\u2026", "datetime": "2018-10-12 06:32:02", "followers": "826"}, "1051593842598825985": {"author": "@akuzubasli", "content_summary": "Check BERT by \u2066@GoogleAI\u2069 which outperforms human in language understanding tasks! https://t.co/VOLkgk90QV", "datetime": "2018-10-14 22:01:24", "followers": "313"}, "1051490369794240512": {"author": "@kazanagisora", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 15:10:14", "followers": "843"}, "1051134217931743232": {"author": "@sonfackserge", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 15:35:01", "followers": "488"}, "1050544177413533701": {"author": "@VisualBI", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:30:24", "followers": "200"}, "1059506949048299520": {"author": "@bamboo4031", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-05 18:05:15", "followers": "235"}, "1050978471634038784": {"author": "@jd_mashiro", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:16:08", "followers": "1,150"}, "1050680060343214080": {"author": "@albarjip", "content_summary": "New state of the art results in a wide range of NLP tasks by using very deep bidirectional pretrained models https://t.co/g1JN5gXbSQ", "datetime": "2018-10-12 09:30:21", "followers": "537"}, "1192423969699368961": {"author": "@davdittrich", "content_summary": "RT @freakonometrics: \"Computers Are Learning to #Read\u2014But They're Still Not So Smart\" https://t.co/jDMdTuoEeh with GLUE https://t.co/Bn3Xyxudzf, Word2Vec https://t.co/lErdRh6jf7, BERT https://t.co/uzSvLXlnBs (a lot of BERT, actually)", "datetime": "2019-11-07 12:50:03", "followers": "7,857"}, "1050772953632268288": {"author": "@moreymat", "content_summary": "RT @teagermylk: Or maybe we need better human performance metrics. SWAG human evaluation is only on 100 samples, so the human accuracy of @\u2026", "datetime": "2018-10-12 15:39:29", "followers": "442"}, "1051298790412447744": {"author": "@vincentzlt", "content_summary": "At the same time far more expensive\ud83d\ude02", "datetime": "2018-10-14 02:28:58", "followers": "96"}, "1050686578589229056": {"author": "@Yu_Yamaguchi_", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 09:56:15", "followers": "2,771"}, "1052870229464965121": {"author": "@ApprovedAmerica", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-18 10:33:18", "followers": "425"}, "1051150579693699076": {"author": "@IgorCarron", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 16:40:02", "followers": "4,890"}, "1051263503636385793": {"author": "@thakurrajanand", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-14 00:08:45", "followers": "129"}, "1051208507549200391": {"author": "@singularpattern", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 20:30:13", "followers": "89"}, "1050546863575887878": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-12 00:41:05", "followers": "3,500"}, "1050830105453494272": {"author": "@ChikaObuah", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-12 19:26:35", "followers": "912"}, "1050744153573871616": {"author": "@HeathFGordon", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:45:02", "followers": "143"}, "1050895567855923200": {"author": "@posadajd", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 23:46:42", "followers": "223"}, "1061310847585148928": {"author": "@speakerjohnash", "content_summary": "https://t.co/mn7V973Uwq", "datetime": "2018-11-10 17:33:18", "followers": "211"}, "1060023526772461569": {"author": "@star_orihime", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-11-07 04:17:57", "followers": "600"}, "1089522510318784512": {"author": "@miweru", "content_summary": "@FaktenprueferIn mir ist aufgefallen, dass es die kontextualisierten Wortvektoren auf deutsch noch gar nicht gibt. Eine Quelle w\u00e4re: https://t.co/reuvZJRO2G (das ist in dieser Form noch recht neu, daher gibt es genau dazu vielleicht noch keine Fachliteratu", "datetime": "2019-01-27 13:56:23", "followers": "433"}, "1059618945425002497": {"author": "@toofuya", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:30:18", "followers": "1,401"}, "1050826422888075269": {"author": "@dannyehb", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:11:57", "followers": "291"}, "1059495379735998465": {"author": "@michaellgraves", "content_summary": "RT @cackerman1: Bidirectional Encoder Representations from Transformers (BERT) for TensorFlow, Keras, etc https://t.co/ThplBQXsLq https:/\u2026", "datetime": "2018-11-05 17:19:17", "followers": "6,010"}, "1050821728363008000": {"author": "@luk_augustyniak", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 18:53:18", "followers": "184"}, "1050570860216639488": {"author": "@GiovanniToschi", "content_summary": "#RT @JatanaHQ: [R] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/MDfzwUI1M9", "datetime": "2018-10-12 02:16:26", "followers": "4,453"}, "1050976067857526784": {"author": "@jaguring1", "content_summary": "\u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1 \u6700\u5f8c\u306e\u753b\u50cf\u306f\u3001\u4e8c\u65e5\u524d\u306b\u3042\u3063\u305f\u885d\u6483\uff08GoogleAI\uff09 https://t.co/Kan2rWqMqu https://t.co/FiKwB3tiMx", "datetime": "2018-10-13 05:06:35", "followers": "12,765"}, "1064349831420874753": {"author": "@Rosenchild", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-19 02:49:09", "followers": "11,933"}, "1051316870547197952": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-14 03:40:49", "followers": "3,500"}, "1050664527019180033": {"author": "@seb_ruder", "content_summary": "RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J", "datetime": "2018-10-12 08:28:38", "followers": "44,898"}, "1050933435768832000": {"author": "@PGajjewar", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 02:17:11", "followers": "33"}, "1232224255976230915": {"author": "@HocWithSukashi", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2020-02-25 08:42:10", "followers": "93"}, "1063550955663241216": {"author": "@digiKinesis", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-11-16 21:54:42", "followers": "910"}, "1125054784875008000": {"author": "@mickey24", "content_summary": "@beatinaniwa https://t.co/aClS9UwzNq", "datetime": "2019-05-05 15:08:57", "followers": "1,964"}, "1050742963561586689": {"author": "@vi_shall_c", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 13:40:19", "followers": "15"}, "1050721977370075137": {"author": "@manaalfar", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 12:16:55", "followers": "2,522"}, "1058166944921202688": {"author": "@ErmiaBivatan", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 01:20:34", "followers": "817"}, "1051737692076199936": {"author": "@va13rik", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-15 07:33:00", "followers": "32"}, "1065365903502647296": {"author": "@Johannes_Welbl", "content_summary": "@oliverbdw4 @uclmr Here you go! https://t.co/OZhTlYrGn9 and https://t.co/8yWwTsPe7s", "datetime": "2018-11-21 22:06:39", "followers": "390"}, "1097377777852985344": {"author": "@keigohtr", "content_summary": "\u4eca\u66f4\u306a\u304c\u3089BERT\u306e\u3053\u306e\u90e8\u5206\u304c\u306a\u305cwork\u3059\u308b\u306e\u304b\u308f\u304b\u3089\u305a\u8abf\u3079\u59cb\u3081\u305f\u3002\u306a\u305c\u3060\uff1f > The input embeddings is the sum of the token embeddings, the segmentation embeddings and the position embeddings. https://t.co/rCmJoob7K5", "datetime": "2019-02-18 06:10:25", "followers": "1,673"}, "1051406518191456256": {"author": "@tksakaki", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 09:37:02", "followers": "4,615"}, "1050939338299564032": {"author": "@discexOmnes", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:40:38", "followers": "155"}, "1051056031210434560": {"author": "@ashiato45", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 10:24:20", "followers": "963"}, "1051259756524929024": {"author": "@lucy3_li", "content_summary": "RT @mihail_eric: WIth ELMo and BERT already out, I predict within the next 1-2 years we'll be able to do a @sesamestreet re-run using nothi\u2026", "datetime": "2018-10-13 23:53:52", "followers": "649"}, "1051623644248432640": {"author": "@ryos_", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:59:49", "followers": "867"}, "1051007135163830272": {"author": "@SturzaMihai23", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 07:10:02", "followers": "25"}, "1051674606635868160": {"author": "@hmichu", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 03:22:20", "followers": "335"}, "1051657985657716736": {"author": "@lrushx", "content_summary": "impressive results!", "datetime": "2018-10-15 02:16:17", "followers": "3"}, "1050703275241037825": {"author": "@chibiscottish", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 11:02:36", "followers": "145"}, "1050652296948809767": {"author": "@mdigangiPA", "content_summary": "RT @marcfede: Thanks for sharing this! https://t.co/2csID0xWQp", "datetime": "2018-10-12 07:40:02", "followers": "367"}, "1050813889129132032": {"author": "@npinto", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 18:22:09", "followers": "2,535"}, "1050572280063975424": {"author": "@donovanOng_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:22:04", "followers": "48"}, "1050735126697693184": {"author": "@loretoparisi", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 13:09:10", "followers": "1,062"}, "1051150819473727488": {"author": "@erwtokritos", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 16:40:59", "followers": "671"}, "1050789482922827776": {"author": "@requesthandler", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 16:45:10", "followers": "23"}, "1050909402784391169": {"author": "@takuoko1", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 00:41:41", "followers": "2,161"}, "1050775671314599936": {"author": "@irodov_rg", "content_summary": "Contextual pretrained models ftw! Good resource for SOTA on many common tasks.", "datetime": "2018-10-12 15:50:17", "followers": "206"}, "1050547692714246145": {"author": "@mrdrozdov", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 00:44:22", "followers": "872"}, "1128610914654023680": {"author": "@_hideoyamada", "content_summary": "BERT\u306f\u5727\u5012\u7684\u306a\u6027\u80fd\u3092\u51fa\u3057\u3066\u3044\u308b https://t.co/EBxJ3OmzoG #dl4practitioners", "datetime": "2019-05-15 10:39:45", "followers": "87"}, "1050548547790073856": {"author": "@JaVergar", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 00:47:46", "followers": "183"}, "1050584428488011779": {"author": "@cristipph", "content_summary": "Pre-trained Transformers. State of the art on multiple NLP tasks. 2 points better than humans on SQuAD. Fun times!", "datetime": "2018-10-12 03:10:21", "followers": "3"}, "1050737117993848834": {"author": "@SanjibNarzary", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:17:05", "followers": "259"}, "1050643657554980864": {"author": "@neuropuff", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/UBpsDr8RYK", "datetime": "2018-10-12 07:05:42", "followers": "813"}, "1226439018834849792": {"author": "@pinoystartup", "content_summary": "The BERT paper by Jacob Devlin et al. Will be attending his lecture in person. https://t.co/YIpPS9pO5C", "datetime": "2020-02-09 09:33:42", "followers": "595"}, "1051609489692999680": {"author": "@kurama554101", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:03:34", "followers": "162"}, "1051054236987879424": {"author": "@y_shindoh", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 10:17:12", "followers": "368"}, "1055463531460493313": {"author": "@reiver", "content_summary": "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova https://t.co/tifGqSrrBG (machine learning)", "datetime": "2018-10-25 14:18:10", "followers": "3,449"}, "1051664464624279554": {"author": "@stealthinu", "content_summary": "Transformer\u3092\u5927\u898f\u6a21\u306b\u3057\u3066\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5de5\u592b\u3057\u3066\u5b66\u7fd2\u3055\u305b\u308b\u3053\u3068\u3067\u3076\u3063\u3061\u304e\u308a\u306e\u6027\u80fd\u306b\u3002\u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3053\u308c\u3082\u3046\u4eba\u9593\u8d85\u3048\u3066\u308b\u306e\u3067\u306f\uff1f / \u201c[1810.04805] BERT: Pre-training of Deep Bi\u2026\u201d https://t.co/IgS4pfhG5l", "datetime": "2018-10-15 02:42:02", "followers": "1,055"}, "1052847486832914433": {"author": "@AAllhorn", "content_summary": "BERT > ELMo ?! https://t.co/vorK9IOxUE", "datetime": "2018-10-18 09:02:56", "followers": "4"}, "1051021029819146240": {"author": "@one_meets_seven", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 08:05:15", "followers": "338"}, "1050545619188809730": {"author": "@liviobs", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 00:36:08", "followers": "120"}, "1058540118196273152": {"author": "@alayaran", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-03 02:03:25", "followers": "94"}, "1059705258992054273": {"author": "@obnym", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 07:13:16", "followers": "134"}, "1050727451138150400": {"author": "@seb_ruder", "content_summary": "It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task that will only be introduced at @emnlp2018 in November! We need even more challenging tasks! BERT: https://t.co/jJmVoH1632 SWAG: htt", "datetime": "2018-10-12 12:38:40", "followers": "44,898"}, "1051293403042861056": {"author": "@KouroshMeshgi", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-14 02:07:34", "followers": "616"}, "1050612260039680002": {"author": "@Trtd6Trtd", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:00:56", "followers": "155"}, "1119627269750661121": {"author": "@mikibrd", "content_summary": "RT @gopalpsarma: Looking forward to sitting down with WL12 and diving into all of the new neural network functionality. @WolframResearch @\u2026", "datetime": "2019-04-20 15:41:57", "followers": "286"}, "1050919539951558656": {"author": "@rishabh16_", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-13 01:21:58", "followers": "387"}, "1050934432469798912": {"author": "@muktabh", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 02:21:08", "followers": "781"}, "1231930457962381312": {"author": "@FBWM8888", "content_summary": "RT @icoxfog417: \u5b9f\u88c5\u898b\u305f\u9650\u308a\u307e\u3063\u305f\u304fNER\u3089\u3057\u3055\u3092\u611f\u3058\u306a\u304b\u3063\u305f\u304c\u3001BERT\u306e\u8ad6\u6587\u3092\u307f\u308b\u3068CRF\u3092\u4f7f\u308f\u305a\u5165\u529bToken\u306e\u6700\u521d\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u5206\u985e\u6a5f\u306b\u304b\u3051\u3066\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3088\u3046\u3060\u306a(5.3)\u3002\u307b\u307c\u5206\u985e\u306e\u5ef6\u9577\u3067\u5b9f\u88c5\u3067\u304d\u3066\u7cbe\u5ea6\u3082\u51fa\u308b\u306a\u3089\u3042\u308a\u304c\u305f\u3044\u3068\u3053\u308d\u3060\u3002 https://\u2026", "datetime": "2020-02-24 13:14:43", "followers": "553"}, "1051622812878692352": {"author": "@oyanakama", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:56:31", "followers": "374"}, "1051127692211306496": {"author": "@debashis_dutta", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 15:09:05", "followers": "13,243"}, "1058337237132136448": {"author": "@cghosh_", "content_summary": "RT @samim: Code and pre-trained models for Google's BERT (\"Bidirectional Encoder Representations from Transformers\": https://t.co/ruCT1oEEx\u2026", "datetime": "2018-11-02 12:37:14", "followers": "275"}, "1059612766426689536": {"author": "@FrostyDesign_JP", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-06 01:05:44", "followers": "1,584"}, "1050605246659981312": {"author": "@jasonbaldridge", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 04:33:04", "followers": "7,922"}, "1051694315301265408": {"author": "@htchien", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 04:40:38", "followers": "1,374"}, "1050710909654630400": {"author": "@VietMinhVU1", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:32:56", "followers": "5"}, "1050751380753854465": {"author": "@eifuentes", "content_summary": "transfer learning for language models is a massive inflection point for machine learning progress. expecting/excited to see the explosion of applications like we saw when AlexNet/VGG made massive leaps with ImageNet.", "datetime": "2018-10-12 14:13:45", "followers": "223"}, "1050826546309554176": {"author": "@prodigyaj", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 19:12:26", "followers": "125"}, "1051024070421041152": {"author": "@MktO740123", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 08:17:20", "followers": "53"}, "1059620110409728001": {"author": "@Okakomy", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:34:55", "followers": "704"}, "1052372644262080513": {"author": "@minhpham", "content_summary": "RT @arxiv_cs_cl: https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.0480\u2026", "datetime": "2018-10-17 01:36:05", "followers": "85"}, "1050723076898344962": {"author": "@camchenry", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:21:17", "followers": "195"}, "1051547291822252032": {"author": "@DrMarkCLewis", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 18:56:25", "followers": "1,070"}, "1050963088927801344": {"author": "@nhayato_ac", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:15:01", "followers": "57"}, "1051218487354351616": {"author": "@__phanhoang__", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 21:09:52", "followers": "219"}, "1132727912271142913": {"author": "@Beetle_SEO", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 19:19:14", "followers": "2,441"}, "1132985283442618368": {"author": "@thejimbirch", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 12:21:56", "followers": "949"}, "1050840008763019267": {"author": "@alienelf", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 20:05:56", "followers": "6,225"}, "1050665307994447873": {"author": "@geepokey", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:31:44", "followers": "67"}, "1051619221405388800": {"author": "@heyhuphup", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 23:42:15", "followers": "27"}, "1050929209923649536": {"author": "@wakadori_Mk2", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:00:23", "followers": "305"}, "1132800636129599489": {"author": "@MuhMoussa", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 00:08:12", "followers": "2,274"}, "1059865225015578629": {"author": "@hoshi_takanori", "content_summary": "RT @HayashiPeke: \u82f1\u8a9e\u306e\u6700\u5927\u306e\u5229\u70b9\u306f\u3001\u53d6\u5f97\u3067\u304d\u308b\u60c5\u5831\u304c\u7269\u51c4\u304f\u5897\u3048\u308b\u3053\u3068\u3060\u3068\u304a\u3082\u3046\u3002 \u8a71\u305b\u306a\u304f\u3066\u3082\u300c\u8aad\u3081\u308b\u300d\u3060\u3051\u3067\u65b0\u3057\u3044\u60c5\u5831\u304c\u624b\u306b\u5165\u308b\u3060\u3051\u3058\u3083\u306a\u304f\u3066\u3001\u540c\u3058\u3053\u3068\u3067\u3082\u3088\u308a\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u3057\u3066\u3044\u308bsource\u306b\u89e6\u308c\u3089\u308c\u305f\u308a\u3002 \u307b\u3093\u306e\u3001\u307b\u3093\u306e\u5c11\u3057\u305a\u3064\u3067\u3082\u3001\u8ad6\u6587\u306a\u3089ab\u2026", "datetime": "2018-11-06 17:48:55", "followers": "1,000"}, "1050978427350573058": {"author": "@Tarpon_red2", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 05:15:57", "followers": "2,861"}, "1050860520654422016": {"author": "@egrefen", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-12 21:27:26", "followers": "23,208"}, "1050581700047036416": {"author": "@akahana_1", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:59:30", "followers": "867"}, "1132823007142010880": {"author": "@EmarketerSEO", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-27 01:37:06", "followers": "1,297"}, "1050586473165131776": {"author": "@gaialive", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:18:28", "followers": "927"}, "1050548440806043649": {"author": "@d_q_nguyen", "content_summary": "RT @arxiv_cs_cl: https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.0480\u2026", "datetime": "2018-10-12 00:47:21", "followers": "166"}, "1053065071004397569": {"author": "@GersonVizcarra", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-18 23:27:32", "followers": "21"}, "1050789858946502658": {"author": "@matasramon", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:46:39", "followers": "159"}, "1050547475885502464": {"author": "@lal_yash", "content_summary": "RT @arxiv_cs_cl: https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.0480\u2026", "datetime": "2018-10-12 00:43:31", "followers": "58"}, "1059608914835079168": {"author": "@knenet", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:50:26", "followers": "947"}, "1050983839542046722": {"author": "@yama_no_nigiwai", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:37:28", "followers": "424"}, "1053195083338706944": {"author": "@AleMas2100", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-19 08:04:09", "followers": "54"}, "1050587757062643712": {"author": "@JDeanThomas", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:23:34", "followers": "216"}, "1051559833713823744": {"author": "@poliu2s", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-14 19:46:16", "followers": "196"}, "1059685131181408256": {"author": "@if2was1", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-06 05:53:17", "followers": "427"}, "1051419821517156352": {"author": "@usagisan2020", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 10:29:54", "followers": "579"}, "1050778470672990209": {"author": "@PipCruncher", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:01:24", "followers": "127"}, "1050548081303965696": {"author": "@rosstaylor90", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:45:55", "followers": "1,088"}, "1050786449946226689": {"author": "@ortizsjesus", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:33:06", "followers": "263"}, "1189355506512650240": {"author": "@bill_slawski", "content_summary": "@ChadMCrabtree @seo_theory Without a doubt, start with the paper that started the whole thing off: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/uhlZp3t7Ik", "datetime": "2019-10-30 01:37:04", "followers": "51,099"}, "1189486484702814208": {"author": "@cackerman1", "content_summary": "Bidirectional Encoder Representations from Transformers (BERT) https://t.co/l3oMa0iQoq https://t.co/sbMIVSNLUo https://t.co/ThplBQXsLq https://t.co/CcVVWhetTf https://t.co/MydOuC40h9 https://t.co/9UH6pMV0uo) https://t.co/7PZfpZIKw3", "datetime": "2019-10-30 10:17:32", "followers": "5,542"}, "1051022625760137216": {"author": "@kawauso_kun", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 08:11:35", "followers": "435"}, "1052975677903970305": {"author": "@gragtah", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-18 17:32:19", "followers": "8,216"}, "1059667393172013058": {"author": "@wasp_dragon", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 04:42:48", "followers": "763"}, "1050945021187305472": {"author": "@siazeroli", "content_summary": "Just knew it from THU Machine Reading Comprehension Seminar. Really GREATTT Research.", "datetime": "2018-10-13 03:03:13", "followers": "21"}, "1050833680082452480": {"author": "@arxiv_cscl", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyrbYpu", "datetime": "2018-10-12 19:40:47", "followers": "3,500"}, "1050979261127872512": {"author": "@olanleed", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 05:19:16", "followers": "1,655"}, "1180273329250459648": {"author": "@HubBucket", "content_summary": "RT @HubBaseDB: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-10-05 00:07:44", "followers": "5,315"}, "1050958399049994241": {"author": "@danyhaddad43", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 03:56:22", "followers": "88"}, "1050989588402110464": {"author": "@KlimZaporojets", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 06:00:18", "followers": "26"}, "1050625822246154241": {"author": "@hustwj", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:54:50", "followers": "122"}, "1050746373300047872": {"author": "@AshiData", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 13:53:51", "followers": "258"}, "1050736422028812288": {"author": "@aCraigPfeifer", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:14:19", "followers": "771"}, "1051664867034202113": {"author": "@vvakame", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 02:43:37", "followers": "8,929"}, "1051090998925905920": {"author": "@HussAnders", "content_summary": "An impressive push of SOTA in language modelling! And a brilliant example of the power of transfer learning and clever \u201clearning task/target engineering\u201d. https://t.co/lm2skuWUAT #GoogleA #NLP", "datetime": "2018-10-13 12:43:17", "followers": "53"}, "1050929156232339456": {"author": "@Trtd6Trtd", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:00:10", "followers": "155"}, "1050953069666676736": {"author": "@tianshi5940", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 03:35:12", "followers": "35"}, "1050880921220591616": {"author": "@gkaraml", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 22:48:30", "followers": "129"}, "1050969397718847489": {"author": "@0xAlsaheel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 04:40:05", "followers": "2,565"}, "1050958605472751619": {"author": "@amitunix", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 03:57:12", "followers": "229"}, "1207131653459468289": {"author": "@sesquipedale", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/Kan2rWqMqu", "datetime": "2019-12-18 02:53:08", "followers": "1,876"}, "1050958265956360192": {"author": "@chaoticneural", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-13 03:55:51", "followers": "586"}, "1050611043683119104": {"author": "@likesky3", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 04:56:06", "followers": ""}, "1051505409255727105": {"author": "@rodoume", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 16:10:00", "followers": "363"}, "1050751546093268992": {"author": "@jaguring1", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:14:25", "followers": "12,765"}, "1180223079433326598": {"author": "@Rosenchild", "content_summary": "RT @HubBaseDB: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-10-04 20:48:04", "followers": "11,933"}, "1050785365794144257": {"author": "@adnanmasood", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:28:48", "followers": "971"}, "1050804676856139776": {"author": "@quocleix", "content_summary": "Great progress in NLP using unsupervised pre-trained bidirectional language model.", "datetime": "2018-10-12 17:45:32", "followers": "18,260"}, "1051083372859088896": {"author": "@rose_miura", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-13 12:12:58", "followers": "293"}, "1057959260787630080": {"author": "@johnmvore", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 11:35:18", "followers": "195"}, "1051093536526008320": {"author": "@nicholasmalaya", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 12:53:22", "followers": "222"}, "1050608502194855936": {"author": "@mahesh_goud", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:46:00", "followers": "141"}, "1050925881894400000": {"author": "@_Ryobot", "content_summary": "GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd https://t.co/dAr3Lq9rPi", "datetime": "2018-10-13 01:47:10", "followers": "4,386"}, "1050547427160322048": {"author": "@arxiv_cs_cl", "content_summary": "https://t.co/wiDcDUMpjO BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (arXiv:1810.04805v1 [https://t.co/HW5RVw4UkE]) #NLProc", "datetime": "2018-10-12 00:43:19", "followers": "4,204"}, "1050556087206899712": {"author": "@cometyang", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:17:44", "followers": "37"}, "1050650086454132736": {"author": "@PiotrCzapla", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-12 07:31:15", "followers": "844"}, "1059809730250518528": {"author": "@azarasiasa", "content_summary": "\u5f37\u3044\u3068\u3057\u304b\u8a00\u3048\u306a\u3044\u3002 \u3069\u306e\u3088\u3046\u306a\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u3082\u5148\u884c\u7814\u7a76\u3092\u4e0a\u56de\u308b\u7d50\u679c\u3068\u304b\u5316\u3051\u7269\u304b\u3088\u3002", "datetime": "2018-11-06 14:08:24", "followers": "31"}, "1058269493237927943": {"author": "@cghosh_", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 08:08:03", "followers": "275"}, "1057934617552908288": {"author": "@nishitian", "content_summary": "Google introduced a new language representation model called BERT https://t.co/5P8PU1RLqZ https://t.co/7FZv7b2cYR", "datetime": "2018-11-01 09:57:22", "followers": "199"}, "1050652446555484160": {"author": "@futsaludy", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 07:40:38", "followers": "101"}, "1050807539246358528": {"author": "@sunasaji", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 17:56:55", "followers": "1,310"}, "1050667475098947584": {"author": "@ivo_es", "content_summary": "RT @_florianmai: This is becoming more evident than ever considering the BERT results: https://t.co/uuaj8U1Mbo . https://t.co/uwGucuAN4J", "datetime": "2018-10-12 08:40:21", "followers": "353"}, "1052020410273824769": {"author": "@_suk1yak1", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-16 02:16:26", "followers": "588"}, "1133958604619280385": {"author": "@tobyminion", "content_summary": "Finally lazy me read the BERT paper. Super interesting. It inspires me of adapting the similar mechism onto pattern embedding and emotion analysis. https://t.co/iqL3U3ePye", "datetime": "2019-05-30 04:49:33", "followers": "109"}, "1050926318181728256": {"author": "@Miles_Brundage", "content_summary": "RT @katherinebailey: We already had EMLo, now we have BERT. Will someone please build a Bidirectional Generative Bayesian Implicit Represen\u2026", "datetime": "2018-10-13 01:48:54", "followers": "25,741"}, "1050580653312434176": {"author": "@KoutaR129", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 02:55:21", "followers": "268"}, "1050744361166745600": {"author": "@GrantDeLozier", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:45:52", "followers": "73"}, "1058086356042141696": {"author": "@ceobillionaire", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 20:00:20", "followers": "163,819"}, "1050977122699890688": {"author": "@s_0samu", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 05:10:46", "followers": "1,337"}, "1132554032214683648": {"author": "@samtmcr", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 07:48:17", "followers": "507"}, "1050936382275768320": {"author": "@Singularity_43", "content_summary": "RT @jaguring1: @takashi119211 @Singularity_43 BERT\u306e\u885d\u6483\u3002\u308a\u3087\u307c\u3063\u3068\u3055\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u306e\u5404\u30bf\u30b9\u30af\u305d\u308c\u305e\u308c\u306e\u6027\u80fd\u5411\u4e0a\u3092\u307e\u3068\u3081\u3066\u304f\u308c\u3066\u308b\uff08\u6700\u5f8c\u306e\u30b9\u30e9\u30a4\u30c9\uff09\u3002\u305d\u3057\u3066\u3001\u300c\u3048\u3089\u3044\u3063\u300d\u304c\u304b\u308f\u3044\u3044\u3002\u300c\u3076\u3063\u3061\u304e\u308a\u306eSoTA\u3084\u3093\uff01\u3053\u3093\u306a\u3093\u52dd\u3066\u3093\u2026", "datetime": "2018-10-13 02:28:53", "followers": "201"}, "1051665739197833216": {"author": "@upgm", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 02:47:05", "followers": "171"}, "1051079517777866753": {"author": "@Montreal_AI", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 11:57:39", "followers": "177,481"}, "1050898917217292289": {"author": "@ougai_quantum", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-13 00:00:01", "followers": "914"}, "1051184878170120193": {"author": "@hexpheus", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 18:56:19", "followers": "160"}, "1135439102667415553": {"author": "@bill_slawski", "content_summary": "RT @wordmetrics: This is an excellent explanation of BERT. https://t.co/2Arb3EFxX4 Original Google AI paper here: https://t.co/yhp9vkKNRR\u2026", "datetime": "2019-06-03 06:52:32", "followers": "51,099"}, "1050804409096011776": {"author": "@RexDouglass", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 17:44:28", "followers": "1,410"}, "1058090609490124800": {"author": "@Montreal_AI", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 20:17:14", "followers": "177,481"}, "1050619073917120512": {"author": "@mihail_eric", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 05:28:01", "followers": "871"}, "1050690550813351936": {"author": "@_josh_meyer_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:12:02", "followers": "1,080"}, "1051812340193058816": {"author": "@itochat", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 12:29:38", "followers": "749"}, "1050816046213550080": {"author": "@therealaseifert", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 18:30:43", "followers": "34"}, "1060318995994226688": {"author": "@biac", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 23:52:03", "followers": "1,749"}, "1052187972265697281": {"author": "@hvroussel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-16 13:22:15", "followers": "341"}, "1059714024030461952": {"author": "@saishi_PhD", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 07:48:06", "followers": "58"}, "1057792535718969344": {"author": "@bensprecher", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 00:32:48", "followers": "523"}, "1050766908641796097": {"author": "@Xiaolei33", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 15:15:27", "followers": "90"}, "1059029572576923648": {"author": "@cm3", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-04 10:28:20", "followers": "650"}, "1082902121014534144": {"author": "@ionandrou", "content_summary": "RT @AUEBNLPGroup: Next meeting, Tue. 15 Jan, 17:15-19:00: BERT discussion. Paper: https://t.co/2SvS4lHPtM. See also: https://t.co/Su5jbFkm0\u2026", "datetime": "2019-01-09 07:29:19", "followers": "650"}, "1051454804264869893": {"author": "@takatoh1", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 12:48:55", "followers": "697"}, "1050660935130476545": {"author": "@storfisk", "content_summary": "Amazing NLP results using pretraining: https://t.co/fcSIxzO7Xn \"BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks\"", "datetime": "2018-10-12 08:14:21", "followers": "94"}, "1051003790692929536": {"author": "@makingAGI", "content_summary": "RT @etzioni: For a more challenging QA task, check out: https://t.co/vyN1pgB0tK Hey @GoogleAI, can your BERT do that? \ud83d\ude03 https://t.co/4rk\u2026", "datetime": "2018-10-13 06:56:45", "followers": "89"}, "1051019825429041152": {"author": "@RTjmora", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 08:00:28", "followers": "23"}, "1051454991217524736": {"author": "@nishiken", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 12:49:39", "followers": "75"}, "1059194355485831168": {"author": "@ChoimiraiHQ", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-04 21:23:07", "followers": "1,727"}, "1052009143928639488": {"author": "@CrubClub", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-16 01:31:39", "followers": "373"}, "1050559833076711424": {"author": "@ericflo", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:32:37", "followers": "7,376"}, "1189676562305564672": {"author": "@lockedowndesign", "content_summary": "RT @Lockedown_: If you want to read a technical description of how BERT works, here you go: https://t.co/8Z3B2Jy2Cd Be forewarned, there's\u2026", "datetime": "2019-10-30 22:52:50", "followers": "316"}, "1050718003262828544": {"author": "@jeandut14000", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 12:01:08", "followers": "43"}, "1050565969435746304": {"author": "@yatskar", "content_summary": "RT @dipanjand: New preprint from our team in Seattle. State of the art on everything! https://t.co/iFxi4J6BdL", "datetime": "2018-10-12 01:57:00", "followers": "921"}, "1050925202018725888": {"author": "@1chromicorn", "content_summary": "RT @mmbollmann: Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher'\u2026", "datetime": "2018-10-13 01:44:28", "followers": "15"}, "1050710748983427072": {"author": "@dginev", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 11:32:18", "followers": "351"}, "1051044833991647232": {"author": "@ruka_funaki", "content_summary": "RT @arxiv_cscl: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-13 09:39:50", "followers": "1,304"}, "1051375387849048064": {"author": "@makoFALAR1229", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:33:20", "followers": "1,091"}, "1050815182199504900": {"author": "@yanamalaajay", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 18:27:17", "followers": "115"}, "1050581790358994945": {"author": "@sam_fgza", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:59:52", "followers": "13"}, "1050683266762706944": {"author": "@richardm_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 09:43:06", "followers": "795"}, "1051101918569721856": {"author": "@alessandroleite", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 13:26:40", "followers": "376"}, "1051456496775647232": {"author": "@Tanaygahlot", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-14 12:55:38", "followers": "251"}, "1208704301104877569": {"author": "@PapersTrending", "content_summary": "[8/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 60 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/1N00deZjGV", "datetime": "2019-12-22 11:02:16", "followers": "222"}, "1050601308967690243": {"author": "@jmullenbach_", "content_summary": "really unbelievable how effective the LM pre-training method is. we're already beating a single human at the brand new common-sense SWAG dataset!", "datetime": "2018-10-12 04:17:25", "followers": "34"}, "1052210873228390406": {"author": "@KataokaShotaro", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-16 14:53:15", "followers": "44"}, "1051431615136595974": {"author": "@minemaz", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 11:16:46", "followers": "1,538"}, "1050989047634513920": {"author": "@Yu_Yamaguchi_", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 05:58:10", "followers": "2,771"}, "1050754312295632896": {"author": "@avin_regmi", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:25:24", "followers": "7"}, "1050783508614901760": {"author": "@ArkaSadhu29", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 16:21:25", "followers": "109"}, "1050576223846445057": {"author": "@miguelalonsojr", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:37:45", "followers": "1,063"}, "1052094897417871360": {"author": "@liddias79", "content_summary": "RT @johnpimm: BERT - language model with fine layer adjustments #NLP Similar to the @jeremyphoward approach? https://t.co/5Xwntz3ClO", "datetime": "2018-10-16 07:12:25", "followers": "9"}, "1051724979807629312": {"author": "@11shubh_laabh11", "content_summary": "RT @arxiv_cscl: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-15 06:42:29", "followers": "66"}, "1113393742726057984": {"author": "@u1PH7A85flztdwf", "content_summary": "RT @miyayou: \u6628\u65e5\u306e\u30b9\u30af\u30a6\u30a7\u30a2\u30fb\u30a8\u30cb\u30c3\u30af\u30b9 \u793e\u5185\u30b2\u30fc\u30e0AI\u30bb\u30df\u30ca\u30fc\uff08\u7b2c\uff12\uff14\uff18\u56de\uff09\u3067\u306f\u3001Google \u306eBERT\u3092\u89e3\u8aac\u3057\u307e\u3057\u305f\u3002\u601d\u60f3\u7684\u306b\u3082\u3068\u3066\u3082\u9762\u767d\u304f\u3001\u30b2\u30fc\u30e0\u30c7\u30b6\u30a4\u30ca\u30fc\u304b\u3089\u30b5\u30fc\u30d0\u30fc\u6280\u8853\u8005\u307e\u3067\u305f\u304f\u3055\u3093\u306e\u65b9\u306b\u3054\u53c2\u52a0\u3044\u305f\u3060\u304d\u8b70\u8ad6\u3082\u767d\u71b1\u3057\u307e\u3057\u305f\u3002 https://t.co/\u2026", "datetime": "2019-04-03 10:52:08", "followers": "393"}, "1057947954458185728": {"author": "@ken_hide", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 10:50:22", "followers": "250"}, "1051108098012864512": {"author": "@JesParent", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 13:51:13", "followers": "985"}, "1050778969983934466": {"author": "@SldKent", "content_summary": "\u041d\u0430\u043f\u0438\u0441\u0430\u043b \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043e\u0431\u0437\u043e\u0440 \u0441\u0442\u0430\u0442\u044c\u0438: https://t.co/RRBp4l1NUk \ud83e\udd13", "datetime": "2018-10-12 16:03:23", "followers": "56"}, "1050749618668097536": {"author": "@alexginsca", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 14:06:45", "followers": "126"}, "1050754756937842688": {"author": "@mihail_eric", "content_summary": "WIth ELMo and BERT already out, I predict within the next 1-2 years we'll be able to do a @sesamestreet re-run using nothing but deep learning for #nlproc language modeling papers.", "datetime": "2018-10-12 14:27:10", "followers": "871"}, "1059605823997960192": {"author": "@goodstoriez", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 00:38:09", "followers": "3,668"}, "1069716415119159296": {"author": "@BrianRoemmele", "content_summary": "Wonderful academic paper on #VoiceFirst AI I showed last month: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d BERT use currently accepted AI/ML techniques, that are far better for sentence intent extraction. https://", "datetime": "2018-12-03 22:14:02", "followers": "97,533"}, "1051041766516908032": {"author": "@smart_grid_fr", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 09:27:39", "followers": "611"}, "1050611500837044225": {"author": "@vgpiyer", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:57:55", "followers": "130"}, "1051184011580588033": {"author": "@vcjha", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 18:52:53", "followers": "1,941"}, "1050747900173721600": {"author": "@polag01", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 13:59:56", "followers": "14"}, "1050621631373094912": {"author": "@puneethmishra", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:38:11", "followers": "556"}, "1051482216151179264": {"author": "@mzntaka0", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 14:37:50", "followers": "1,048"}, "1051612816069644288": {"author": "@tokoroten", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:16:48", "followers": "21,185"}, "1060098934360555520": {"author": "@mimic007", "content_summary": "1810.04805.pdf https://t.co/p4pdVTEm3W", "datetime": "2018-11-07 09:17:36", "followers": "138"}, "1051671772372684801": {"author": "@twittyoota", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-15 03:11:04", "followers": "714"}, "1050644874637135874": {"author": "@kuronosec", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:10:32", "followers": "361"}, "1057798054877806594": {"author": "@fjfbupt", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 00:54:43", "followers": "59"}, "1059630970091892736": {"author": "@hirosehideo", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-06 02:18:04", "followers": "1,880"}, "1058219452628590592": {"author": "@harshtiku", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-02 04:49:12", "followers": "6"}, "1051094570103173120": {"author": "@PrincesOluebube", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 12:57:28", "followers": "1,734"}, "1132792798900555779": {"author": "@LoukilAymen", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 23:37:04", "followers": "2,164"}, "1051171816641843201": {"author": "@suneelmarthi", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 18:04:25", "followers": "754"}, "1051815186473070593": {"author": "@Quebec_AI", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 12:40:56", "followers": "160,790"}, "1051191859060428800": {"author": "@gabeibagon", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 19:24:04", "followers": "72"}, "1198044455892398088": {"author": "@drahmadbazzi", "content_summary": "RT @YadKonrad: Weekly Cross-Cultural Words (WCCW): BERT: it could mean so much to different communities. 1) a reference to Bert Kreischer\u2026", "datetime": "2019-11-23 01:03:51", "followers": "1,880"}, "1051007909071532032": {"author": "@JW1jcNSvy3fjWHU", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 07:13:06", "followers": "22"}, "1050798369902391296": {"author": "@AndreyDulub", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 17:20:28", "followers": "44"}, "1051174874243190785": {"author": "@linekin", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 18:16:34", "followers": "97"}, "1057829864668041217": {"author": "@cghosh_", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 03:01:07", "followers": "275"}, "1051626907966484481": {"author": "@wsiya", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-15 00:12:47", "followers": "685"}, "1050905708005380097": {"author": "@montrealdotai", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 00:27:00", "followers": "33,955"}, "1051654590591000576": {"author": "@idgmatrix", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-15 02:02:47", "followers": "4,465"}, "1050701301632270336": {"author": "@chibiscottish", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:54:46", "followers": "145"}, "1051039617602875392": {"author": "@ko_ash", "content_summary": "RT @jaguring1: \u4eba\u5de5\u77e5\u80fd\u6280\u8853\u306e\u6700\u524d\u7dda\uff08\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u7de8\uff09\uff5e\u3053\u306e4\u304b\u6708\u306e\u885d\u6483\uff5e \u5927\u91cf\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3001\u5927\u5e45\u306a\u6027\u80fd\u5411\u4e0a\uff01 \u30bf\u30b9\u30af\u306b\u3088\u3063\u3066\u306f\u3001\u4eba\u9593\u8d8a\u3048\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\uff01 \u5de6\u4e8c\u3064\u306e\u753b\u50cf\u306f\u30016\u6708\u306b\u3042\u3063\u305f\u885d\u6483\uff08OpenAI\uff09 https://t.co/LmmvIm3mR1\u2026", "datetime": "2018-10-13 09:19:06", "followers": "230"}, "1051251547882053633": {"author": "@ceobillionaire", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-13 23:21:14", "followers": "163,819"}, "1050719680854446081": {"author": "@daveaitel", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 12:07:48", "followers": "25,813"}, "1059648506594439173": {"author": "@isahaya", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:27:45", "followers": "565"}, "1050657700466454530": {"author": "@hackernewsj", "content_summary": "BERT\uff1a\u8a00\u8a9e\u7406\u89e3\u306e\u305f\u3081\u306e\u53cc\u65b9\u5411\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u4e88\u5099\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 https://t.co/ThbvWZPyHM", "datetime": "2018-10-12 08:01:30", "followers": "948"}, "1086855654256476160": {"author": "@PaulHuang668", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2019-01-20 05:19:15", "followers": "1"}, "1050996618449825792": {"author": "@abhimskywalker", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 06:28:15", "followers": "538"}, "1051299784902860800": {"author": "@ryo_masumura", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 02:32:55", "followers": "425"}, "1059655418576687105": {"author": "@tossi_F2", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:55:13", "followers": "89"}, "1050658368543580162": {"author": "@hackernewsrobot", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/wEsrg0uAN7", "datetime": "2018-10-12 08:04:10", "followers": "250"}, "1202849502123589634": {"author": "@andysun1222", "content_summary": "RT @WolframResearch: New in the #WolframNetRepo: extract text features w/ a BERT model trained on BookCorpus & Wikipedia https://t.co/kxDgM\u2026", "datetime": "2019-12-06 07:17:23", "followers": "27"}, "1118970837359525888": {"author": "@theChrisChua", "content_summary": "RT @WolframResearch: New in the #WolframNetRepo: extract text features w/ a BERT model trained on BookCorpus & Wikipedia https://t.co/kxDgM\u2026", "datetime": "2019-04-18 20:13:31", "followers": "2,136"}, "1051622260191055872": {"author": "@u_akihiro", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:54:19", "followers": "7,529"}, "1058746891758981122": {"author": "@emezac", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-03 15:45:04", "followers": "14"}, "1132573812418056192": {"author": "@jasonmbarnard", "content_summary": "That last sentence almost slips by unnoticed \"outperforming human performance by 2.0%.\" https://t.co/srMLiCQQvr", "datetime": "2019-05-26 09:06:53", "followers": "3,591"}, "1051400326236975104": {"author": "@fumiaki_sato_", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 09:12:26", "followers": "26"}, "1059798162452439040": {"author": "@eleansa", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-06 13:22:26", "followers": "234"}, "1050659238484140033": {"author": "@FlorianDreher", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:07:37", "followers": "85"}, "1219520607940825088": {"author": "@minxy_xoo", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2020-01-21 07:22:25", "followers": "20"}, "1072853565717340160": {"author": "@alienelf", "content_summary": "Just used BERT pre-trained models on a problem we were initially using Skip-thoughts for and have been fighting with for years. BERT completely destroyed skip-thoughts. The fine-tuning did wonders! So excited! #NLProc #DeepLearning https://t.co/LSWAJ5G4C", "datetime": "2018-12-12 13:59:57", "followers": "6,225"}, "1050698760521732098": {"author": "@Ted_Underwood", "content_summary": "RT @jmhessel: BERT, the new high-performing, pretrained l\u0336a\u0336n\u0336g\u0336u\u0336a\u0336g\u0336e\u0336 \u0336m\u0336o\u0336d\u0336e\u0336l\u0336 bidirectional-word-filler-inner-and-does-this-sentenc\u2026", "datetime": "2018-10-12 10:44:40", "followers": "8,829"}, "1050555428977827840": {"author": "@omerlevy_", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:15:07", "followers": "2,341"}, "1050579926523375616": {"author": "@KRiver1", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:52:27", "followers": "1,871"}, "1050896796086493184": {"author": "@howardmeng", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:51:35", "followers": "44"}, "1051021247377596416": {"author": "@x_meta", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 08:06:07", "followers": "183"}, "1050989803486044160": {"author": "@KlimZaporojets", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 06:01:10", "followers": "26"}, "1050553164313825280": {"author": "@adn_twitts", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:06:07", "followers": "73"}, "1051080900451098624": {"author": "@fumiaki_sato_", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 12:03:09", "followers": "26"}, "1085216719969312768": {"author": "@efernandez", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-01-15 16:46:43", "followers": "6,422"}, "1058134993602375680": {"author": "@TyzenLee", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 23:13:36", "followers": "477"}, "1051783070351810560": {"author": "@DMfun", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-15 10:33:19", "followers": "163"}, "1051113364536090625": {"author": "@akbirkhan", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 14:12:09", "followers": "228"}, "1051375235843481600": {"author": "@itayevron", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 07:32:44", "followers": "19"}, "1051026101957361664": {"author": "@battle8500", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 08:25:24", "followers": "66"}, "1050960964273635328": {"author": "@tokimekishiken", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:06:34", "followers": "4,026"}, "1050577557521723392": {"author": "@rbhar90", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:43:03", "followers": "8,541"}, "1051840723899875328": {"author": "@puneethmishra", "content_summary": "RT @arxiv_cscl: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/NkvlyqUnxW", "datetime": "2018-10-15 14:22:25", "followers": "556"}, "1050790738114445312": {"author": "@RatnRajiv", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 16:50:09", "followers": "630"}, "1058145763929206785": {"author": "@kuro0531", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-01 23:56:24", "followers": "138"}, "1132770621123903490": {"author": "@A_K_Anderson", "content_summary": "RT @bill_slawski: No, it's not LSI that Google is Using. It is BERT! https://t.co/uhlZp3bwjK", "datetime": "2019-05-26 22:08:56", "followers": "4,107"}, "1051147239492595712": {"author": "@ht0ma", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 16:26:45", "followers": "151"}, "1050654577266122753": {"author": "@battle8500", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:49:06", "followers": "66"}, "1050785159685926912": {"author": "@Tim_Dettmers", "content_summary": "This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! Simple, but lots of compute. What does it mean for NLP? We do not know yet, but it will change how we do NLP and think about it for sure", "datetime": "2018-10-12 16:27:59", "followers": "5,581"}, "1178029932112277504": {"author": "@Rosenchild", "content_summary": "RT @HubBucket: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-28 19:33:17", "followers": "11,933"}, "1050596847264223232": {"author": "@daiquocng", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:59:42", "followers": "85"}, "1145108660076654593": {"author": "@_stefan_munich", "content_summary": "Btw: BERT paper got an update (one month ago): It now mentions the usage of the maximal document context for NER ;) So make sure, that you \"print out\" the latest version: https://t.co/PASpU39HGd", "datetime": "2019-06-29 23:15:54", "followers": "585"}, "1051296448065753088": {"author": "@hannah0n", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-14 02:19:40", "followers": "29"}, "1050997687657451520": {"author": "@morioka", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 06:32:29", "followers": "826"}, "1050634995222507520": {"author": "@mariuskarma", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 06:31:17", "followers": "6,084"}, "1132547986557480960": {"author": "@analyticsagentu", "content_summary": "RT: seounited: #awesomeness in einer Formel #seo https://t.co/xYfGUCzapb #seounited #analytics #google", "datetime": "2019-05-26 07:24:16", "followers": "81"}, "1050763402442723328": {"author": "@63556poiuytrewq", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 15:01:32", "followers": "417"}, "1050667941216149505": {"author": "@Cedias_", "content_summary": "BERT https://t.co/pt2fjPLD1i is one more reason to have a look at Transformers (https://t.co/za7t7McxAm). Apparently Attention is really all you need !", "datetime": "2018-10-12 08:42:12", "followers": "340"}, "1050927941134086144": {"author": "@wayama_ryousuke", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 01:55:21", "followers": "203"}, "1050642008992186368": {"author": "@jittat", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:59:09", "followers": "2,960"}, "1050624239919759360": {"author": "@RichmanRonald", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 05:48:33", "followers": "888"}, "1238039551207227392": {"author": "@cackerman1", "content_summary": "https://t.co/qMZc57GnK6 https://t.co/dW0d2KRGQH https://t.co/ITCElyach3 https://t.co/ThplBQXsLq Google NLP: Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA), BERT, RoBERTa, XLNet, ALBERT, T5, others https://t.co/q0fHi", "datetime": "2020-03-12 09:50:05", "followers": "5,542"}, "1050552734947000320": {"author": "@plusepsilon", "content_summary": "When is ERNIE (Eager Recurrent Networks In-between Embeddings) coming out? \ud83d\ude06", "datetime": "2018-10-12 01:04:24", "followers": "723"}, "1050545142225100801": {"author": "@kaz_yos", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 00:34:14", "followers": "4,144"}, "1051008682841071616": {"author": "@ralphbrooks", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 07:16:11", "followers": "137"}, "1052374615262019584": {"author": "@minhpham", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-17 01:43:55", "followers": "85"}, "1050948289653235712": {"author": "@0oMetao0", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 03:16:12", "followers": "460"}, "1050892094636666880": {"author": "@Aryan46339976", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 23:32:54", "followers": "47"}, "1051429691955920896": {"author": "@rose_miura", "content_summary": "\u6700\u8fd1\u306e\u8a00\u8a9e\u8868\u73fe\u30e2\u30c7\u30eb\u3068\u306f\u7570\u306a\u308a\u3001BERT\u306f\u3001\u3059\u3079\u3066\u306e\u5c64\u306e\u5de6\u53f3\u4e21\u65b9\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u4e00\u7dd2\u306b\u8abf\u6574\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u6df1\u5c64\u306e\u53cc\u65b9\u5411\u8868\u73fe\u3092\u4e8b\u524d\u8a13\u7df4\u3059\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 https://t.co/3s70jMuGK6", "datetime": "2018-10-14 11:09:07", "followers": "293"}, "1052019325240889344": {"author": "@7142857", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-16 02:12:07", "followers": "837"}, "1059648488948944896": {"author": "@future731_robo", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:27:41", "followers": "960"}, "1052003433983565824": {"author": "@jaguring1", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-16 01:08:58", "followers": "12,765"}, "1051147941656707072": {"author": "@Tro2B", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 16:29:33", "followers": "32"}, "1051617373403271168": {"author": "@tbm_hiro", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-14 23:34:54", "followers": "69"}, "1050568465097068544": {"author": "@ctl_walt_del", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:06:55", "followers": "27"}, "1062871726613118976": {"author": "@vodkamomo", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-15 00:55:41", "followers": "323"}, "1057792735556452352": {"author": "@rosinality", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 00:33:35", "followers": "1,821"}, "1206933768545677312": {"author": "@huomaa_minut", "content_summary": "RT @norpadon: @kazdalevsky @0player @skazal_net \u0414\u0430 \u043d\u0435\u0442, \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0442 \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0443\u0436\u0435. BERT (https://t.co/iDSlodd9nw) \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u0435 \u0443 \u0433\u0443\u0433\u043b\u0430 \u0432\u2026", "datetime": "2019-12-17 13:46:48", "followers": "155"}, "1050742823123791872": {"author": "@HIM72", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:39:45", "followers": "55"}, "1051613520884715521": {"author": "@Snark86", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 23:19:36", "followers": "307"}, "1050549263111868416": {"author": "@LJL20", "content_summary": "https://t.co/MlcYbUtADB latest good news", "datetime": "2018-10-12 00:50:37", "followers": "3"}, "1050633799506124800": {"author": "@danouc", "content_summary": "AI 2% better than humans https://t.co/zkHODsn1Ae", "datetime": "2018-10-12 06:26:32", "followers": "1,214"}, "1050549249811697664": {"author": "@skrish_13", "content_summary": "@adinamwilliams https://t.co/WJzkS4kdjR", "datetime": "2018-10-12 00:50:34", "followers": "247"}, "1059613822225272833": {"author": "@guccii", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-06 01:09:56", "followers": "111"}, "1059653066675302400": {"author": "@nekodevjp", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 03:45:53", "followers": "177"}, "1058334175223263233": {"author": "@dwhitena", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-02 12:25:04", "followers": "4,037"}, "1053359366953304064": {"author": "@bigdata", "content_summary": "Advancing the state-of-the-art for eleven #NLproc tasks \ud83d\udcd6 BERT (Bidirectional Encoder Representations from Transformers) from @GoogleAI https://t.co/ZLyDLkjGrs https://t.co/pL0XZQmaSG", "datetime": "2018-10-19 18:56:58", "followers": "44,640"}, "1051688740584714240": {"author": "@hiroaki_tanaka", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-15 04:18:29", "followers": "1,212"}, "1050580571871678466": {"author": "@MarkShmulevich", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:55:01", "followers": "2,721"}, "1060319840181792768": {"author": "@udoooom", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-07 23:55:24", "followers": "2,176"}, "1053863540103110657": {"author": "@kazuki232", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-21 04:20:22", "followers": "2,325"}, "1050540589148565504": {"author": "@rosinality", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:16:09", "followers": "1,821"}, "1067914305989947396": {"author": "@BarrySlaff", "content_summary": "BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. https", "datetime": "2018-11-28 22:53:06", "followers": "119"}, "1051430008764297217": {"author": "@rose_miura", "content_summary": "\u305d\u306e\u7d50\u679c\u3001\u4e8b\u524d\u8a13\u7df4\u3057\u305fBERT\u306e\u8868\u73fe\u306f1\u3064\u306e\u51fa\u529b\u5c64\u306e\u307f\u3067\u5fae\u8abf\u6574\u3067\u304d\u3001\u5e45\u5e83\u3044\u30bf\u30b9\u30af\u306b\u5bfe\u5fdc\u3059\u308b\u6700\u5148\u7aef\u306e\u30e2\u30c7\u30eb\u3092\u5f62\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 https://t.co/3s70jMuGK6", "datetime": "2018-10-14 11:10:23", "followers": "293"}, "1053642040263753728": {"author": "@cyberandy", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-20 13:40:12", "followers": "1,682"}, "1050612499668590594": {"author": "@ToJans", "content_summary": "Improving natural language AI by randomly masking a word and having the AI defer it from the context. https://t.co/WUu7nQvpGs", "datetime": "2018-10-12 05:01:54", "followers": "1,860"}, "1050822952810233856": {"author": "@wjarek", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 18:58:09", "followers": "2,583"}, "1050561609310060544": {"author": "@andybaoxv", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 01:39:40", "followers": "17"}, "1059628782443683841": {"author": "@nardtree", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-11-06 02:09:23", "followers": "8,954"}, "1059738596838535168": {"author": "@kazanagisora", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-06 09:25:45", "followers": "843"}, "1050947846436876288": {"author": "@63556poiuytrewq", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 03:14:26", "followers": "417"}, "1059612370765459457": {"author": "@nyororon2180000", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 01:04:10", "followers": "2,047"}, "1059494278274015232": {"author": "@kmoriyama", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-11-05 17:14:55", "followers": "9,096"}, "1050610154599632898": {"author": "@deliprao", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-12 04:52:34", "followers": "12,955"}, "1050662377371328512": {"author": "@albarjip", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:20:05", "followers": "537"}, "1050698810110836738": {"author": "@aneesha", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 10:44:52", "followers": "1,586"}, "1051484123133440001": {"author": "@ken_hide", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 14:45:25", "followers": "250"}, "1050718605749440512": {"author": "@SproutCats", "content_summary": "CAT HACKER: BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/B9pbVN9OYl", "datetime": "2018-10-12 12:03:31", "followers": "111"}, "1177014094131617793": {"author": "@Rosenchild", "content_summary": "RT @HubBaseDB: #BioBert - Bidirectional Encoder Representations from Transformers for #Biomedical #Text #Mining Deep Bidirectional Transfo\u2026", "datetime": "2019-09-26 00:16:42", "followers": "11,933"}, "1050581960945328128": {"author": "@2ManyKevinParks", "content_summary": "Wow they beat the human baseline.", "datetime": "2018-10-12 03:00:33", "followers": "56"}, "1050616874587672576": {"author": "@vesko_st", "content_summary": "The results from BERT are truly mind-blowing. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova https://t.co/Hmv5HS3xrj", "datetime": "2018-10-12 05:19:17", "followers": "1,070"}, "1059907280164937728": {"author": "@puddlingdrive", "content_summary": "RT @HayashiPeke: \u82f1\u8a9e\u306e\u6700\u5927\u306e\u5229\u70b9\u306f\u3001\u53d6\u5f97\u3067\u304d\u308b\u60c5\u5831\u304c\u7269\u51c4\u304f\u5897\u3048\u308b\u3053\u3068\u3060\u3068\u304a\u3082\u3046\u3002 \u8a71\u305b\u306a\u304f\u3066\u3082\u300c\u8aad\u3081\u308b\u300d\u3060\u3051\u3067\u65b0\u3057\u3044\u60c5\u5831\u304c\u624b\u306b\u5165\u308b\u3060\u3051\u3058\u3083\u306a\u304f\u3066\u3001\u540c\u3058\u3053\u3068\u3067\u3082\u3088\u308a\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u3057\u3066\u3044\u308bsource\u306b\u89e6\u308c\u3089\u308c\u305f\u308a\u3002 \u307b\u3093\u306e\u3001\u307b\u3093\u306e\u5c11\u3057\u305a\u3064\u3067\u3082\u3001\u8ad6\u6587\u306a\u3089ab\u2026", "datetime": "2018-11-06 20:36:02", "followers": "103"}, "1232453933865398274": {"author": "@aceyuan", "content_summary": "@yudapearl BERT stands for Bidirectional Encoder Representations from Transformers, a training technique for natural language processing (NLP) applications https://t.co/nvPKM8GtHA and https://t.co/YCaqevrXNm", "datetime": "2020-02-25 23:54:50", "followers": "6"}, "1050772632755494913": {"author": "@singularpattern", "content_summary": "RT @teagermylk: Or maybe we need better human performance metrics. SWAG human evaluation is only on 100 samples, so the human accuracy of @\u2026", "datetime": "2018-10-12 15:38:12", "followers": "89"}, "1178642926131240961": {"author": "@Hironsan13", "content_summary": "@xxCENTRALxx \u30c6\u30ad\u30b9\u30c8\u306e\u985e\u4f3c\u5ea6\u3068\u3044\u3046\u3053\u3068\u3067\u3042\u308c\u3070\u3001BERT\u306e\u8ad6\u6587\u4e2d\u3067\u884c\u3063\u3066\u3044\u308bSTS(Semantic Textual Similarity)\u306e\u8a55\u4fa1\u304c\u305d\u308c\u306b\u3042\u305f\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3044\u307e\u3059\u3002 https://t.co/UB9pSEkCwu", "datetime": "2019-09-30 12:09:06", "followers": "3,084"}, "1050740104862543873": {"author": "@AndLukyane", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:28:57", "followers": "1,324"}, "1050544679383527424": {"author": "@JonClarkSeattle", "content_summary": "BERT: Fresh pretraining results from colleagues here at Google Seattle. Nice improvements on QA, GLUE, and NER. https://t.co/wM4GoHc22b", "datetime": "2018-10-12 00:32:24", "followers": "1,246"}, "1235115915382894592": {"author": "@victorvicpal", "content_summary": "RT @SpirosDenaxas: I really liked this paper \"A Primer in BERTology: What we know about how BERT works\" by @annargrs & coauthors, gives a r\u2026", "datetime": "2020-03-04 08:12:36", "followers": "675"}, "1050840060260683782": {"author": "@alxcnwy", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 20:06:08", "followers": "1,602"}, "1050575738804400129": {"author": "@shanerai", "content_summary": "Wow. BERT, new language representation model, from #GoogleAI. Look at those new benchmark results: GLUE 80.4% (7.6% abs improvement), MultiNLI 86.7 (5.6% abs improvement), SQuAD v1.1 q&a Test F1 to 93.2 (1.5% abs improvement). https://t.co/XkAchHGGft", "datetime": "2018-10-12 02:35:49", "followers": "387"}, "1050654626586877952": {"author": "@mmbollmann", "content_summary": "Is it just me, or isn't \"massive compute is all you need\" quite the opposite of \"fun time ahead\" from a curious researcher's point of view?", "datetime": "2018-10-12 07:49:17", "followers": "755"}, "1050690591959523329": {"author": "@_josh_meyer_", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 10:12:12", "followers": "1,080"}, "1051202729509412864": {"author": "@HamelHusain", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 20:07:15", "followers": "3,632"}, "1051303027309604864": {"author": "@dontsentouin", "content_summary": "RT @kyoun: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google) https://t.co/R7eMbzWPuE ELMo\uff08\u53cc\u65b9\u5411RNN\u306e\u7d50\u5408\u2026", "datetime": "2018-10-14 02:45:48", "followers": "211"}, "1050623413998374917": {"author": "@HaHuyKhanh", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-12 05:45:16", "followers": "23"}, "1119444827463213056": {"author": "@danielscarvalho", "content_summary": "RT @gopalpsarma: Looking forward to sitting down with WL12 and diving into all of the new neural network functionality. @WolframResearch @\u2026", "datetime": "2019-04-20 03:36:59", "followers": "2,262"}, "1050819659904245762": {"author": "@ChikaObuah", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-12 18:45:04", "followers": "912"}, "1050761176768860160": {"author": "@deadcatbouncepn", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 14:52:41", "followers": "1,543"}, "1053049213238632448": {"author": "@barbarikon", "content_summary": "RT @thtrieu_: A commonsense reasoning task is \"solved\" even before its official introduction. https://t.co/lcRmTlicg5", "datetime": "2018-10-18 22:24:31", "followers": "1,712"}, "1081235115186028545": {"author": "@pabloc_ds", "content_summary": "The NLP model was born in Google and it is called BERT, here's the paper: https://t.co/wDT3Hru7vF #AI #NLP", "datetime": "2019-01-04 17:05:14", "followers": "1,145"}, "1050721817348857857": {"author": "@tunguz", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 12:16:17", "followers": "10,081"}, "1058141594346389505": {"author": "@martin_wicke", "content_summary": "RT @bsaeta: I really enjoyed reading @jalammar's The Illustrated Transformer https://t.co/cwDDNUffJA. Transformer-style architectures are v\u2026", "datetime": "2018-11-01 23:39:50", "followers": "4,884"}, "1050890308068749312": {"author": "@ceobillionaire", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 23:25:48", "followers": "163,819"}, "1050987516315230208": {"author": "@SamXu15", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 05:52:04", "followers": "5"}, "1050939872318373889": {"author": "@nanjakorewa", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 02:42:45", "followers": "212"}, "1084090606476054528": {"author": "@johnkthompson60", "content_summary": "An interesting research paper on NLP improvements. Very technical, but intriguing if you are interested in NLP. 15 minute read. https://t.co/bjjptgotwb", "datetime": "2019-01-12 14:11:56", "followers": "1,008"}, "1057806405992923136": {"author": "@yaohomeway", "content_summary": "RT @hardmaru: Bidirectional Encoder Representations from Transformers is released! BERT is a method of pre-training language representation\u2026", "datetime": "2018-11-01 01:27:54", "followers": "11"}, "1050665658583666688": {"author": "@schnee", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 08:33:08", "followers": "427"}, "1204276803457691649": {"author": "@stephan_mir", "content_summary": "Google : \u201cBERT is conceptually simple and empirically powerful\u201d. (Cf. Arxiv: https://t.co/q9kMLmQnNe). Simple ? \u00e9l\u00e9gante a minima! Et 1,5 ans entre les r\u00e9sultats scientifiques et le d\u00e9ploiement quasi worldwide (70 pays...). Impressionnant! https://t.co/yLV", "datetime": "2019-12-10 05:48:58", "followers": "18"}, "1050732652419043328": {"author": "@anmarasovic", "content_summary": "Hear, hear!", "datetime": "2018-10-12 12:59:20", "followers": "532"}, "1051006381216722944": {"author": "@pavelkordik", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-13 07:07:02", "followers": "934"}, "1059808781046030338": {"author": "@morioka", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-11-06 14:04:38", "followers": "826"}, "1094708414637322240": {"author": "@LP2348", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-02-10 21:23:19", "followers": "66"}, "1050925872276881409": {"author": "@chaudao19", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 01:47:07", "followers": "6"}, "1051104852422656001": {"author": "@khushal2898", "content_summary": "RT @earnmyturns: Fresh from my team: one pre-training model, many tasks improved https://t.co/VHdEYlhsii", "datetime": "2018-10-13 13:38:20", "followers": "26"}, "1050893207280574464": {"author": "@jeremyphoward", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 23:37:19", "followers": "99,993"}, "1051482418773774336": {"author": "@t_ibis22", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 14:38:38", "followers": "123"}, "1050541805266264064": {"author": "@sksq96", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:20:59", "followers": "506"}, "1050768175065776129": {"author": "@bsaeta", "content_summary": "RT @rajatmonga: Big step forward on natural language understanding https://t.co/SG5jhc3LmV", "datetime": "2018-10-12 15:20:29", "followers": "2,285"}, "1173537471977709569": {"author": "@PapersTrending", "content_summary": "[5/10] \ud83d\udcc8 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 287 \u2b50 - \ud83d\udcc4 https://t.co/g9ESCVCrxk - \ud83d\udd17 https://t.co/tvmmNM6XVV", "datetime": "2019-09-16 10:01:51", "followers": "222"}, "1052127457137123328": {"author": "@jbmrlt", "content_summary": "RT @nicogontier: https://t.co/1uOnUoWRO1 Very relevant article with the recent release of the BERT model (https://t.co/5fm5PlgQy8) from Goo\u2026", "datetime": "2018-10-16 09:21:48", "followers": "7"}, "1051106903646846976": {"author": "@AssistedEvolve", "content_summary": "RT @Tim_Dettmers: This is the most important step in NLP in months \u2014 big! Make sure to read the BERT paper even if you are doing CV etc! S\u2026", "datetime": "2018-10-13 13:46:29", "followers": "218"}, "1059070082221887491": {"author": "@jaguring1", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-04 13:09:18", "followers": "12,765"}, "1221259640920363008": {"author": "@ShinobuKinjo1", "content_summary": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://t.co/CMvCrE315o Masks the input tokens randomly to obtain a bidirectional pre-trained model and introduces a binarized next sentence prediction for a model to underst", "datetime": "2020-01-26 02:32:42", "followers": "18"}, "1051044170213646336": {"author": "@ruka_funaki", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 09:37:12", "followers": "1,304"}, "1060318750740668416": {"author": "@biac", "content_summary": "RT @gijigae: BERT\u306b\u95a2\u3059\u308b\u8ad6\u6587\u306f10\u670811\u65e5\u306b @arxiv\uff08\u67fb\u8aad\u524d\u306e\u8ad6\u6587\u3092\u767b\u9332\uff09\u306b\u63b2\u8f09\uff08 target=\"_blank\" href=\"https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv\">https://t.co/xXdufE1j76\uff09\u3055\u308c\u8ab0\u3067\u3082\u8aad\u3081\u307e\u3059\u3002arXiv \u306b\u6295\u7a3f\u3055\u308c\u3066\u308b\u8ad6\u6587\u6570\u306f\u67081\u4e07\u4ef6\u3092\u8efd\u304f\u8d85\u3048\u3066\u3044\u308b\u3002 \u591a\u8aad\u30c1\u30e3\u30ec\u30f3\u30b8\uff08\ud83d\udc49https://t.c\u2026", "datetime": "2018-11-07 23:51:04", "followers": "1,749"}, "1050638607348715520": {"author": "@nerigenio", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 06:45:38", "followers": "71"}, "1051375687913721856": {"author": "@suripenchan", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-14 07:34:32", "followers": "607"}, "1051007700723687426": {"author": "@wk77", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 07:12:17", "followers": "902"}, "1070646434570100737": {"author": "@_mishy", "content_summary": "The thing that strikes me most about the BERT paper is that there are only 4 authors. If this had been a physics paper there would be over 100 names listed https://t.co/CDSPHFg0ud", "datetime": "2018-12-06 11:49:36", "followers": "301"}, "1057754372879204353": {"author": "@kouki_tanak", "content_summary": "RT @_Ryobot: \u7d50\u5c40BERT\u306e\u4f55\u304c\u6050\u308d\u3057\u3044\u304b\u3063\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u4e00\u56de\u5b66\u7fd2\u3055\u305b\u3068\u3051\u3070\u304a\u305d\u3089\u304f\u6a5f\u68b0\u7ffb\u8a33\u3084\u5bfe\u8a71\u751f\u6210\u3068\u304b\u3082\u6c4e\u7528\u7684\u306b\u30d6\u30fc\u30b9\u30c8\u3067\u304d\u308b\u4e07\u80fd\u85ac\u306a\u306e\u306b\u526f\u4f5c\u7528\u304c\u306a\u3044\u3068\u3053\u306a\u3093\u3060\u3088\u306a\u3041\uff08\u3060\u304b\u3089pretrain\u30e2\u30c7\u30eb\u304c\u516c\u958b\u3055\u308c\u305f\u3089NLP\u5168\u57df\u3067\u304f\u305d\u6d41\u884c\u308b\u3068\u601d\u3063\u3066\u308b\uff09 https://\u2026", "datetime": "2018-10-31 22:01:09", "followers": "458"}, "1050754585516621825": {"author": "@odan3240", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 14:26:29", "followers": "1,231"}, "1050592453978251264": {"author": "@akf", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 03:42:14", "followers": "819"}, "1051620938037452800": {"author": "@dtvsakuranejp", "content_summary": "\u3082\u3046\u4e00\u822c\u4eba\u304c\u3044\u304b\u306b\u30b3\u30ce\u30e4\u30ed\u30fc\u30b3\u30ce\u30e4\u30ed\u30fc\u3068NOVA\u3068\u304bGABA\u306b\u901a\u3063\u3066\u3082\u3069\u3046\u306b\u3082\u306a\u3089\u306a\u3044\u3002 \u30cd\u30a4\u30c6\u30a3\u30d6\u3068\u3057\u3066\u751f\u307e\u308c\u5909\u308f\u308b\u306e\u3092\u5f85\u3064\u304b", "datetime": "2018-10-14 23:49:04", "followers": "482"}, "1050611515642994688": {"author": "@shashivelur", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:57:59", "followers": "1,142"}, "1050544440501243905": {"author": "@mrdrozdov", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 00:31:27", "followers": "872"}, "1050961311058690049": {"author": "@imenurok", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-13 04:07:57", "followers": "2,042"}, "1154026525886488576": {"author": "@Pardoe_AI", "content_summary": "RT @RobertoGEMartin: #AI #NLP #BERT #OpenSource Microsoft open-sources scripts and notebooks to pre-train BERT natural language model with\u2026", "datetime": "2019-07-24 13:52:19", "followers": "13,720"}, "1050561648858030080": {"author": "@angsuman", "content_summary": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding https://t.co/hi30aMiIBM", "datetime": "2018-10-12 01:39:50", "followers": "7,943"}, "1052154741696778241": {"author": "@PerthMLGroup", "content_summary": "RT @Reza_Zadeh: Two unsupervised tasks together give a great features for many language tasks: Task 1: Fill in the blank. Task 2: Does sen\u2026", "datetime": "2018-10-16 11:10:13", "followers": "456"}, "1050624359629385730": {"author": "@peteskomoroch", "content_summary": "RT @omarsar0: That's BERT right there in a nutshell. ELMo be like what just happened there? Pretty remarkable results by the way. But as @R\u2026", "datetime": "2018-10-12 05:49:01", "followers": "47,691"}, "1050727859239575552": {"author": "@camchenry", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-12 12:40:17", "followers": "195"}, "1050934130647547904": {"author": "@PGajjewar", "content_summary": "RT @seb_ruder: It's amazing how fast #NLProc is moving these days. We have now reached super-human performance on SWAG, a commonsense task\u2026", "datetime": "2018-10-13 02:19:56", "followers": "33"}, "1050920168258383877": {"author": "@linbo_pythoner", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-13 01:24:27", "followers": "40"}, "1050644658852749312": {"author": "@mopodono", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 07:09:41", "followers": "32"}, "1050597030685335552": {"author": "@pamin2222", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 04:00:25", "followers": "134"}, "1068126035059761153": {"author": "@codonomics", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-29 12:54:26", "followers": "514"}, "1050724083221061632": {"author": "@essentialskill", "content_summary": "Getting to know BERT (Bidirectional Encoder Representations from Transformers) as introduced here by @dawnieando", "datetime": "2018-10-12 12:25:17", "followers": "5,041"}, "1166796942002839553": {"author": "@fatmasgadelrab", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2019-08-28 19:37:23", "followers": "389"}, "1050755653285736448": {"author": "@_1651324671212", "content_summary": "RT @jaguring1: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\uff08\u8a00\u8a9e\u7406\u89e3\uff09 https://t.co/Kan2rWqMqu https://t.co/\u2026", "datetime": "2018-10-12 14:30:44", "followers": "908"}, "1050549755032559619": {"author": "@iwontbecreative", "content_summary": "Nice ideas: - Masked LM training to train bidir LMs together rather than simply concatenate - Sentence embeddings (for every token) in addition to positional and token Massive model: - 1m steps @128k words/batch - 64 TPU chips... Huge improvements over", "datetime": "2018-10-12 00:52:34", "followers": "449"}, "1050737451386441730": {"author": "@nmh226", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 13:18:24", "followers": "6"}, "1050827761135181825": {"author": "@sudotong", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 19:17:16", "followers": "894"}, "1050709493711958016": {"author": "@WeinroT_xc", "content_summary": "RT @seb_ruder: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: SOTA on 11 tasks. Main additions: - Bidir\u2026", "datetime": "2018-10-12 11:27:19", "followers": "9"}, "1083326150443552768": {"author": "@cnerg", "content_summary": "Reading Group : BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding |Seminar Room 107(CSE), 6.00PM| Paper link: https://t.co/Ksl40GJwha", "datetime": "2019-01-10 11:34:16", "followers": "421"}, "1050572857103802368": {"author": "@klaimet", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 02:24:22", "followers": "4"}, "1059237132714180608": {"author": "@daisuzu", "content_summary": "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing https://t.co/cpa0EOG7AV https://t.co/zcMOhgrD42 https://t.co/O4pLyVupcM", "datetime": "2018-11-05 00:13:06", "followers": "1,248"}, "1051948954772881409": {"author": "@miki_iwa", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-15 21:32:29", "followers": "54"}, "1051373175471304704": {"author": "@Bachfischer", "content_summary": "Bidirectional Encoder Representations from Transformers (BERT) achieves SoTA results in eleven NLP tasks - looks like language modelling will have a great impact in the NLP space! https://t.co/b2UkMNgYs3", "datetime": "2018-10-14 07:24:33", "followers": "65"}, "1131081101135499265": {"author": "@cosimo_fedeli", "content_summary": "#DataScience Pick of the week: Pre-training of deep bidirectional transformers for language understanding https://t.co/SAtLULtkOZ", "datetime": "2019-05-22 06:15:23", "followers": "67"}, "1062723496567103489": {"author": "@Yingjie_Hu", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-11-14 15:06:40", "followers": "643"}, "1050608478878756868": {"author": "@thembani_p", "content_summary": "RT @kchonyc: the paper behind BERT is now online: https://t.co/kKDzq3GF5W BERT: Pre-training of Deep Bidirectional Transformers for Langu\u2026", "datetime": "2018-10-12 04:45:55", "followers": "193"}, "1050580599235174400": {"author": "@kvikas572", "content_summary": "RT @lmthang: A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive\u2026", "datetime": "2018-10-12 02:55:08", "followers": "63"}, "1053980717858734080": {"author": "@deadcatbouncepn", "content_summary": "RT @jaguring1: \u73fe\u72b6\u3060\u3068RST\u306e\u63a8\u8ad6\u3001\u30a4\u30e1\u30fc\u30b8\u540c\u5b9a\u3001\u5177\u4f53\u4f8b\u540c\u5b9a\u306f\u3069\u306e\u3050\u3089\u3044\u3067\u304d\u308b\u306e\u3060\u308d\u3046\uff1f\u3042\u3068\u3001\u3053\u308c\u304b\u3089\u306e\u767a\u5c55\u3067\u3069\u3053\u307e\u3067\u3044\u3051\u305d\u3046\u306a\u3093\u3060\u308d\u3046\uff1f\u65b0\u4e95\u7d00\u5b50\u3055\u3093\u306f\u3001\u3053\u308c\u3089\u30678\u5272\u9054\u6210\u3057\u305f\u3089\u300c\u4fe1\u3058\u3066\u3042\u3052\u308b\u304b\u306a\u300d\u3068\u8a00\u3063\u3066\u305f\u308f\u3051\u3060\u3051\u3069\u3002https://t.co/kXg5PnC9un", "datetime": "2018-10-21 12:05:59", "followers": "1,543"}, "1057756609718366208": {"author": "@Gamma_ijk", "content_summary": "RT @_Ryobot: GLUE\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5168\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af\u3067\u3076\u3063\u3061\u304e\u308a\u306eSOTA\uff01 \u8cea\u7591\u5fdc\u7b54\u30bf\u30b9\u30af\u306eSQuAD\u3067\u3082SOTA\uff01 \u8a71\u984c\u306e\u624b\u6cd5BERT\u3063\u3066\u306a\u306b\u3063\uff1f\uff0824\u5c6416\u6ce8\u610f\u30d8\u30c3\u30c9\u306eTransformer\uff01\uff1f\uff09 paper: https://t.co/1yapHc0RTd\u2026", "datetime": "2018-10-31 22:10:02", "followers": "529"}}, "completed": "1", "queriedAt": "2020-05-16 10:45:46", "citation_id": "49541835"}