{"citation_id": "3011284", "completed": "1", "queriedAt": "2020-05-14 14:18:59", "tab": "twitter", "twitter": {"571269427099860992": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "723", "datetime": "2015-02-27 11:23:39", "author": "@tsubame959"}, "649802253503623170": {"content_summary": "[1412.6980] Adam: A Method for Stochastic Optimization: [1412.6980] Adam: A Method for Stochastic\u2026 http://t.co/YKo9SRROdR [fav]", "followers": "200", "datetime": "2015-10-02 04:25:04", "author": "@overleo"}, "955749122727821312": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "400", "datetime": "2018-01-23 10:28:42", "author": "@zakizaki_4daime"}, "557577975588081664": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "3,595", "datetime": "2015-01-20 16:38:43", "author": "@moustaki"}, "678652619984867328": {"content_summary": "RT @pavelkordik: Adam #optimization method deals with sparse gradients and non-stationary objectives https://t.co/39rgkBneUG Beats #RMSProp\u2026", "followers": "1,303", "datetime": "2015-12-20 19:06:08", "author": "@LoopGM"}, "557650312119738368": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "442", "datetime": "2015-01-20 21:26:09", "author": "@moreymat"}, "814120154749513729": {"content_summary": "https://t.co/rQw5HgkUXm", "followers": "205", "datetime": "2016-12-28 14:45:45", "author": "@ymizushi"}, "1207729555156410368": {"content_summary": "[Research Paper] Adam: A Method for Stochastic Optimization https://t.co/CkmSHCypra #MachineLearning", "followers": "2,269", "datetime": "2019-12-19 18:28:59", "author": "@adnan_hashmi"}, "955545267058569217": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "224", "datetime": "2018-01-22 20:58:39", "author": "@ElectronNest"}, "598690870666203136": {"content_summary": "\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/JRQ4L4Pzpy http://t.co/nWnMhblJ2S", "followers": "244", "datetime": "2015-05-14 03:26:41", "author": "@s_ryosky"}, "976216762604646400": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "2,067", "datetime": "2018-03-20 21:59:48", "author": "@yo_ehara"}, "574549210143420416": {"content_summary": "Adam\u5b9f\u88c5\u7c21\u5358\u305d\u3046\u306a\u306e\u3067Caffe\u3067\u4f7f\u3063\u3066\u307f\u305f\u3044\u306a http://t.co/YFmIXU2BoF", "followers": "1,720", "datetime": "2015-03-08 12:36:20", "author": "@mooopan"}, "813060958075686912": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/yJEZpbr5eu", "followers": "1,333", "datetime": "2016-12-25 16:36:53", "author": "@data_wizard"}, "558060205171548162": {"content_summary": "RNNs for text analysis: http://t.co/98W4Kzjlab steeper sigmoid gate (http://t.co/BG52hizZ4e) \u3084 adam (http://t.co/ebMPdJj63Y) \u306a\u3069\u6700\u65b0\u6280\u8853\u6e80\u8f09", "followers": "1,256", "datetime": "2015-01-22 00:34:55", "author": "@takahi_i"}, "1109726599002243072": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/pXLXW9PtQS (Popularity:18.0) #Machine_Learning", "followers": "93", "datetime": "2019-03-24 08:00:13", "author": "@poqaa_ai"}, "932854256721215488": {"content_summary": "@rishmishra @adamnash @thogge ironically, Adam may be the solution https://t.co/TZ8Fs8CKSV", "followers": "2,018", "datetime": "2017-11-21 06:12:41", "author": "@alphafishing"}, "575448085502914560": {"content_summary": "http://t.co/WNiflYv1Lh http://t.co/jvKuFRMxZZ", "followers": "13", "datetime": "2015-03-11 00:08:09", "author": "@adamistpocket"}, "648165806644170752": {"content_summary": "\u5143\u8ad6\u6587\u306f\u3053\u308c\u3089\u3057\u3044\u3002\u4eca\u65e5\u306f\u7720\u3044\u304b\u3089\u660e\u65e5\u306e\u591c\u8aad\u307f\u307e\u3059\u3002 > Diederik Kingma & Jimmy Ba. Adam: A Method for Stochastic Optimization http://t.co/7N2z8MZjK7", "followers": "2,161", "datetime": "2015-09-27 16:02:25", "author": "@asas_mimi"}, "627095881020391427": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "269", "datetime": "2015-07-31 12:38:03", "author": "@white_output"}, "571138386037071872": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "1,427", "datetime": "2015-02-27 02:42:57", "author": "@yamakatu"}, "955949600153649152": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "688", "datetime": "2018-01-23 23:45:20", "author": "@a_hasimoto"}, "1189408985293770752": {"content_summary": "RT @o_guest: Well, this is *ehem* peak something. \ud83e\udd23 \"Equal contribution. Author ordering determined by coin flip over a Google Hangout.\"\u2026", "followers": "7,444", "datetime": "2019-10-30 05:09:34", "author": "@o_guest"}, "832150088764051456": {"content_summary": "Adam: A method for stochastic optimization - https://t.co/hocB1axXFT", "followers": "173", "datetime": "2017-02-16 08:50:16", "author": "@DataHaskell"}, "572000631071719424": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "339", "datetime": "2015-03-01 11:49:12", "author": "@_willr"}, "572947869239529472": {"content_summary": "Adam: A Method for Stochastic Optimization http://t.co/bRZa5hmVo6 #arxiv", "followers": "219", "datetime": "2015-03-04 02:33:11", "author": "@fsantiba"}, "572000044183760898": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "1,877", "datetime": "2015-03-01 11:46:52", "author": "@masahiro_sakai"}, "571303836393934848": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "424", "datetime": "2015-02-27 13:40:23", "author": "@RobbyMeals"}, "949837170662981642": {"content_summary": "Here is the original published paper describing \"Adam\", a technique for stochastic gradient descent optimizations, in #MachineLearning: https://t.co/g7FJhqHygB", "followers": "33", "datetime": "2018-01-07 02:56:43", "author": "@tayallred"}, "907677382022230017": {"content_summary": "RT @simjue: Sympathetic guys :D https://t.co/DjmGG9WPhq https://t.co/7NB0p3bz0a", "followers": "1,451", "datetime": "2017-09-12 18:48:46", "author": "@matthiasendler"}, "574549747748339712": {"content_summary": "1\u4ef6\u306e\u30b3\u30e1\u30f3\u30c8 http://t.co/h2BhNwrNr6 \u201c[1412.6980] Adam: A Method for Stochastic Optimization\u201d http://t.co/1n207MwXac", "followers": "2,143", "datetime": "2015-03-08 12:38:29", "author": "@Drunkar"}, "1237888843430281217": {"content_summary": "\u30aa\u30ea\u30b8\u30ca\u30eb\u3092\u8aad\u3080\uff1a Adam: A Method for Stochastic Optimization https://t.co/xbYJulxl7D", "followers": "224", "datetime": "2020-03-11 23:51:13", "author": "@ElectronNest"}, "819761641814298625": {"content_summary": "\u201c[1412.6980v8] Adam: A Method for Stochastic Optimization\u201d https://t.co/5xIublWIWU", "followers": "7,818", "datetime": "2017-01-13 04:23:00", "author": "@call_me_nots"}, "571318721802670080": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "824", "datetime": "2015-02-27 14:39:32", "author": "@morioka"}, "627116747661795328": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "810", "datetime": "2015-07-31 14:00:58", "author": "@xiangze750"}, "860970783887294465": {"content_summary": "RT @NancyYaco: ADAM: A method for Stochastic Optimization #machinelearning https://t.co/0yNxZ0xLn1", "followers": "4,944", "datetime": "2017-05-06 21:33:25", "author": "@shakamunyi"}, "955757115657764865": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "963", "datetime": "2018-01-23 11:00:28", "author": "@ashiato45"}, "1037943612594683904": {"content_summary": "RT @richmond_umagat: Learned about Adam optimization algorithm for training #NeuralNetworks. It combines Momentum and Hinton's #RMSProp to\u2026", "followers": "5,362", "datetime": "2018-09-07 06:00:15", "author": "@100DaysOfMLCode"}, "562081584224813056": {"content_summary": "ADAM\u304c\u3044\u3044\u3088\u3063\u3066\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u3002\u3082\u30461\u672c\u30d9\u30af\u30c8\u30eb\u304c\u5fc5\u8981\u3067\u3059\u304c\u3001\u5b9f\u88c5\u5de5\u592b\u3059\u308c\u3070\u30a2\u30bb\u30f3\u30d6\u30ea\u66f8\u304b\u306a\u304f\u3066\u3082SSE\u306b\u306f\u4e57\u305b\u3089\u308c\u308b\u3002\u305f\u3060\u3001\u30e1\u30e2\u30ea\u5e2f\u57df\u30cd\u30c3\u30af\u306b\u306a\u3063\u3066\u308b\u307d\u304f\u3066\u3001\u5897\u3048\u305f\u5206\u306e\u30e1\u30e2\u30ea\u304c\u618e\u3044 http://t.co/7ukDHKpj5Z", "followers": "12,457", "datetime": "2015-02-02 02:54:27", "author": "@unnonouno"}, "583251893616533504": {"content_summary": "Not very exciting paper but big impact nonetheless! http://t.co/NcVJBnLAnI Adam: A Method for Stochastic Optimization http://t.co/4Gq4HX8KiP", "followers": "25,260", "datetime": "2015-04-01 12:57:42", "author": "@alexjc"}, "900459412149739520": {"content_summary": "learned about the adam algorithm for mini-batch gradient decent https://t.co/QBw0TdLSIA thanks to https://t.co/kWkL4WVXqf by @AndrewYNg", "followers": "194", "datetime": "2017-08-23 20:47:08", "author": "@lsbardel"}, "955574679506333698": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,667", "datetime": "2018-01-22 22:55:31", "author": "@Scaled_Wurm"}, "572643798867689472": {"content_summary": "Adam: A Method for Stochastic Optimization http://t.co/PBYPXXYNZc #arxiv", "followers": "219", "datetime": "2015-03-03 06:24:55", "author": "@fsantiba"}, "627092806364868609": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "1,655", "datetime": "2015-07-31 12:25:50", "author": "@olanleed"}, "574504758699913216": {"content_summary": "Adagrad\u3088\u308a\u3082\u3044\u3051\u3066\u308b\u3089\u3057\u3044\u306e\u3067 Adam: A Method for Stochastic Optimization http://t.co/iYwDpgoaED", "followers": "1,371", "datetime": "2015-03-08 09:39:42", "author": "@k_ishiguro"}, "547542326843682816": {"content_summary": "Adam: A Method for Stochastic Optimization http://t.co/oJMlYxB0Cm #arxiv", "followers": "787", "datetime": "2014-12-24 00:00:38", "author": "@arXivStats"}, "955666341159030784": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,655", "datetime": "2018-01-23 04:59:45", "author": "@olanleed"}, "807386697423949824": {"content_summary": "RT @beam2d: @aonotas Adam\u3067\u3084\u3063\u3066\u308b\u8ad6\u6587\u3092\u3071\u3063\u3068\u898b\u3064\u3051\u3089\u308c\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001 Adam\u306e\u5143\u8ad6\u6587\u306b\u304a\u3051\u308b\u7406\u8ad6\u89e3\u6790\u3067\u306f\u5b66\u7fd2\u7387\u30921/\u221at\u3067\u6e1b\u8870\u3055\u305b\u305f\u5834\u5408\u306b\u53ce\u675f\u3059\u308b\u3053\u3068\u304c\u8a3c\u660e\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7406\u8ad6\u7684\u306b\u3082\u6e1b\u8870\u3055\u305b\u308b\u306e\u306f\u5fc5\u8981\u306a\u306f\u305a\u3067\u3059 https://t.co/Y1BL\u2026", "followers": "4,019", "datetime": "2016-12-10 00:49:23", "author": "@ballforest"}, "950568832539746305": {"content_summary": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION https://t.co/llV3ulIVZz", "followers": "4,136", "datetime": "2018-01-09 03:24:05", "author": "@kaz_yos"}, "1101523801387393024": {"content_summary": "https://t.co/cLa16qhK2p Adam", "followers": "102", "datetime": "2019-03-01 16:45:14", "author": "@naka4ma_"}, "955676187308666880": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "22", "datetime": "2018-01-23 05:38:53", "author": "@Lampmat"}, "664277016410267649": {"content_summary": "[1412.6980v8] Adam: A Method for Stochastic Optimization: (Submitted on 22 Dec 2014 (v1), last revised 23 Jul\u2026 https://t.co/M0Al7hEFxU [ml]", "followers": "200", "datetime": "2015-11-11 03:02:37", "author": "@overleo"}, "571168271954653187": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "1,655", "datetime": "2015-02-27 04:41:42", "author": "@olanleed"}, "926177013475442688": {"content_summary": "RT @__ice9: Interesting discussion of ADAM, a somewhat cleaner alternative to stochastic gradient descent: https://t.co/wjDIPt2ufi", "followers": "625", "datetime": "2017-11-02 19:59:42", "author": "@generuso"}, "843485007280836608": {"content_summary": "@neurosci_doc I wrote my own. https://t.co/qQ0FUDsT8q", "followers": "2,674", "datetime": "2017-03-19 15:31:11", "author": "@memming"}, "955661614937661441": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,769", "datetime": "2018-01-23 04:40:58", "author": "@jaialkdanel"}, "1108639438974783488": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/pXLXW9PtQS (Popularity:29.2) #Machine_Learning", "followers": "93", "datetime": "2019-03-21 08:00:14", "author": "@poqaa_ai"}, "695630874323369984": {"content_summary": "Adam is my new hero! It can optimize 4 hidden layers without any pretraining. https://t.co/9cZ33qQHpo #ml #deeplearning #adam #optimizer", "followers": "107", "datetime": "2016-02-05 15:31:39", "author": "@SnurakBill"}, "987833736812052480": {"content_summary": "[1412.6980] Adam: A Method for Stochastic Optimization \u3068\u308a\u3042\u3048\u305aMendeley\u306b\u30dd\u30a4\u3057\u305f\u304c\u5206\u304b\u3089\u3093\u3068\u601d\u308f\u308c\uff0e https://t.co/LPei4oKLi1", "followers": "294", "datetime": "2018-04-21 23:21:30", "author": "@pcneko5"}, "583369081337868288": {"content_summary": "Not very exciting paper but big impact nonetheless! http://t.co/NcVJBnLAnI Adam: A Method for Stochastic Optimization http://t.co/4Gq4HX8KiP", "followers": "72", "datetime": "2015-04-01 20:43:22", "author": "@gmazars"}, "1045832214800474113": {"content_summary": "@joemeno Two common choices are to involve higher order derivatives (Newton\u2019s method) or to use some kind of inertia term (Adam, RMSProp). Adam paper: https://t.co/RU8n0Yzsod", "followers": "62", "datetime": "2018-09-29 00:26:45", "author": "@blogczak"}, "802475456490414080": {"content_summary": "@nfunato \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092ADAM\u3067\u6700\u9069\u5316\u3057\u3066\u3044\u308b\u306e\u3067\u305d\u306e\u8ad6\u6587\u3067\u3059\u3002https://t.co/fQNX8KdzqL \u3061\u3083\u3093\u3068Emacs\u3067\u3059", "followers": "567", "datetime": "2016-11-26 11:33:52", "author": "@masatoi0"}, "571290105685499905": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "334", "datetime": "2015-02-27 12:45:49", "author": "@nel215"}, "598055001253085184": {"content_summary": "@mclduk try Adam as well, I've had more luck with that than any other ada* method: http://t.co/B0bO76uL4Q still prefer Nesterov+SGD though!", "followers": "37,494", "datetime": "2015-05-12 09:19:58", "author": "@sedielem"}, "571278014090080256": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "1,667", "datetime": "2015-02-27 11:57:47", "author": "@Scaled_Wurm"}, "955650907143720960": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "151", "datetime": "2018-01-23 03:58:26", "author": "@mitch_sus"}, "913908381819326464": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/WmjpAhMvdp", "followers": "1,775", "datetime": "2017-09-29 23:28:32", "author": "@data_datum"}, "674527769997414400": {"content_summary": "Adam seems to use norm of something influenced by gradient, so I am afraid they might conflict to each other. https://t.co/s2VpoyxLOJ", "followers": "191", "datetime": "2015-12-09 09:55:27", "author": "@satoshi2373"}, "967336505898348545": {"content_summary": "Paper on ADAM algorithm to optimize gradient descent has this written in footer of the 1st page \"Equal contribution. Author ordering determined by coin flip over a Google Hangout\" \ud83d\ude07 https://t.co/GdNse3bp5P @dpkingma @OpenAI @UofT #MachineLearning #Artifici", "followers": "28", "datetime": "2018-02-24 09:52:49", "author": "@IshaanArora95"}, "706017863547691008": {"content_summary": "Adam: An alternative to SGD for optimizing functions that are ill-conditioned. https://t.co/CFBUlsAYis", "followers": "1,675", "datetime": "2016-03-05 07:25:50", "author": "@nimalan"}, "611470174789566464": {"content_summary": "\u201c1412.6980v4.pdf\u201d http://t.co/1UJOg4OPfO", "followers": "3,333", "datetime": "2015-06-18 09:47:04", "author": "@ainame"}, "557943908559110144": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "1,307", "datetime": "2015-01-21 16:52:48", "author": "@dna_nerd"}, "955661519173242887": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,769", "datetime": "2018-01-23 04:40:36", "author": "@jaialkdanel"}, "810419779706044416": {"content_summary": "https://t.co/UKp7KCSDA2 \u3088\u308a(\u8aa4\u8a33\u3063\u3066\u305f\u3089\u3054\u3081\u3093\u306a\u3055\u3044) https://t.co/zHF04BObQ7", "followers": "471", "datetime": "2016-12-18 09:41:47", "author": "@daytb_twy"}, "571306987322892288": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "53", "datetime": "2015-02-27 13:52:54", "author": "@MktO740123"}, "905493499222491136": {"content_summary": "Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions https://t.co/hL8qIMKTC3", "followers": "956", "datetime": "2017-09-06 18:10:48", "author": "@Kingwulf"}, "1246906546665279490": {"content_summary": "@miggan94 @DotCSV @DeepMind Buenas Miguel. El m\u00e9todo Adam surgi\u00f3 en 2015 https://t.co/q6JziWL7O8 mientras que el paper original de @DeepMind fue presentado unos a\u00f1os antes, en 2013 https://t.co/OqRDcZS2xj. Un saludo! \ud83d\ude0a", "followers": "274", "datetime": "2020-04-05 21:04:21", "author": "@AIMatesanz"}, "761005584359362560": {"content_summary": "RT @markgerstein: A solution to a perennial problem: Co-first \"author ordering determined by coin flip\" - eg https://t.co/87xmrber7X & http\u2026", "followers": "27", "datetime": "2016-08-04 01:07:24", "author": "@EDISON_MITO"}, "707812455427264512": {"content_summary": "@SuryaGanguli how does ur saddle-free method (https://t.co/wcesLpvsem) compare to methods like ADAM (https://t.co/wtQBF2GeK1) in practice?", "followers": "282", "datetime": "2016-03-10 06:16:54", "author": "@AaronAndalman"}, "955665360560144384": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "165", "datetime": "2018-01-23 04:55:52", "author": "@piro0316"}, "593368529732173824": {"content_summary": "\u201c[1412.6980] Adam: A Method for Stochastic Optimization\u201d http://t.co/EUiPt0XY1G", "followers": "83", "datetime": "2015-04-29 10:57:36", "author": "@fqz7c3"}, "558010988835143680": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "42,874", "datetime": "2015-01-21 21:19:21", "author": "@DeepLearningHub"}, "1227648704284258304": {"content_summary": "https://t.co/2q9JlgVD7c", "followers": "287", "datetime": "2020-02-12 17:40:34", "author": "@horomaki"}, "860968718825926661": {"content_summary": "ADAM: A method for Stochastic Optimization #machinelearning https://t.co/0yNxZ0xLn1", "followers": "346", "datetime": "2017-05-06 21:25:13", "author": "@NancyYaco"}, "575274722876772352": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "902", "datetime": "2015-03-10 12:39:16", "author": "@wk77"}, "571273358483927040": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "199", "datetime": "2015-02-27 11:39:17", "author": "@stsincerity"}, "992812788727861249": {"content_summary": "[1412.6980] Adam: A Method for Stochastic Optimization https://t.co/J4RWnzLtgY", "followers": "383", "datetime": "2018-05-05 17:06:29", "author": "@surfnm"}, "627106594061316097": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "334", "datetime": "2015-07-31 13:20:37", "author": "@nel215"}, "598692317009063936": {"content_summary": "\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/JRQ4L4Pzpy http://t.co/nWnMhblJ2S", "followers": "1,055", "datetime": "2015-05-14 03:32:26", "author": "@takanashi1986"}, "557678044962627584": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "825", "datetime": "2015-01-20 23:16:21", "author": "@fly51fly"}, "1095387617678553095": {"content_summary": "Today I looked into stochastic gradient descent #sgd and how to optimize a path with it \ud83d\ude03 (https://t.co/s3VhyvroPR) https://t.co/qz5OW5Hyrb", "followers": "231", "datetime": "2019-02-12 18:22:14", "author": "@ct2034"}, "557871649291530241": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "218", "datetime": "2015-01-21 12:05:40", "author": "@CannavoSonia"}, "955649338570547205": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "78", "datetime": "2018-01-23 03:52:12", "author": "@sumvo_bass"}, "839519925286809600": {"content_summary": "#Adam: gradient-based optimization of stochastic objective functions via adaptive estimates of lower-order moments https://t.co/xjiu8gCO8A", "followers": "789", "datetime": "2017-03-08 16:55:22", "author": "@stojanovic"}, "922131975707561984": {"content_summary": "\u30c6\u30f3\u30bd\u30ebadam optimizer https://t.co/i4hunTNb6I", "followers": "68", "datetime": "2017-10-22 16:06:10", "author": "@imnynmi"}, "955661689197813760": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "303", "datetime": "2018-01-23 04:41:16", "author": "@utsushiiro"}, "559508489605623808": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "71", "datetime": "2015-01-26 00:29:53", "author": "@HwanyJung"}, "1141616249790763008": {"content_summary": "RT @HBAkirmak: The Adam stochastic optimization whitepaper fm 2015. \u201cThe method is straightforward to implement, is computationally efficie\u2026", "followers": "403", "datetime": "2019-06-20 07:58:18", "author": "@mwubahimana"}, "955661019178721280": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "4,512", "datetime": "2018-01-23 04:38:36", "author": "@ayafmy"}, "908338706830041088": {"content_summary": "Confused abt \u25b3 between adam & RMSprop + momentum (like I was)? Turns out reading the paper is helpful. #DeepLearning https://t.co/h4RQhjregk https://t.co/CtbmkOnhHV", "followers": "3,813", "datetime": "2017-09-14 14:36:38", "author": "@NicholasStrayer"}, "571130721911205889": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "4,311", "datetime": "2015-02-27 02:12:29", "author": "@ibaibabaibai"}, "955690867683082240": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "247", "datetime": "2018-01-23 06:37:13", "author": "@wz7ce"}, "555487698384084992": {"content_summary": "@bradneuberg Paper here: http://t.co/u7BZUREStA It's a generalized SGD, different hyper-parameter settings correspond to NAG, rmsprop, etc.", "followers": "33,897", "datetime": "2015-01-14 22:12:42", "author": "@AlecRad"}, "997592683463036929": {"content_summary": "@nsaphra @mhdempsey @jackclarkSF @algorithmia Nobody is saying that\u2019s necessary. But there are far easier ways to explain what Adam does, even a parenthetical explanation of momentum, than this trainwreck of words https://t.co/129zBJXj6J", "followers": "4,465", "datetime": "2018-05-18 21:40:04", "author": "@jGage718"}, "664277019832676352": {"content_summary": "1412.6980v8.pdf: Published as a conference paper at ICLR 2015 ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\u2026 https://t.co/W4pJrmYpZX [ml]", "followers": "200", "datetime": "2015-11-11 03:02:37", "author": "@overleo"}, "1189181807083655168": {"content_summary": "RT @o_guest: Well, this is *ehem* peak something. \ud83e\udd23 \"Equal contribution. Author ordering determined by coin flip over a Google Hangout.\"\u2026", "followers": "578", "datetime": "2019-10-29 14:06:51", "author": "@M_Moutoussis"}, "557671138365820929": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "3,102", "datetime": "2015-01-20 22:48:55", "author": "@kastnerkyle"}, "926152765180600320": {"content_summary": "why is ADAM so damn nice? https://t.co/tqSNSS6ilp", "followers": "122", "datetime": "2017-11-02 18:23:20", "author": "@cottonwoodfluff"}, "571932339867029504": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "256", "datetime": "2015-03-01 07:17:50", "author": "@dizzy_my_future"}, "1068062083286224896": {"content_summary": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has... https://t.co", "followers": "18", "datetime": "2018-11-29 08:40:18", "author": "@JpalvesUnico"}, "1226969941527625740": {"content_summary": "RT @roldan_matias: Adam optimization algorithm paper: https://t.co/81j90E6V45 #DeepLearning #MachineLearning #ai", "followers": "5,149", "datetime": "2020-02-10 20:43:24", "author": "@clairebotai"}, "571129969084010496": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "1,720", "datetime": "2015-02-27 02:09:30", "author": "@mooopan"}, "700339691476766720": {"content_summary": "\u305f\u307e\u305f\u307e\u898b\u304b\u3051\u305fAdam\u306e\u5b9f\u88c5\u304c\u3069\u3046\u3082\u81ea\u5206\u306e\u77e5\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u9055\u3063\u305f\u306e\u3067\u6539\u3081\u3066\u8abf\u3079\u3066\u307f\u305f\u3089\u3001\u7d50\u69cb\u983b\u7e41\u306b\u66f4\u65b0\u3055\u308c\u3066\u3044\u305f\u3002v7\u306e\u304c\u5206\u304b\u308a\u3084\u3059\u3044\u3068\u601d\u3063\u305f\u3051\u3069v8\u3067\u65b0\u8b0e\u6982\u5ff5\u304c\u767b\u5834\u3057\u3066\u3044\u308b\u2026\u3002 https://t.co/PtYWcQVhDr", "followers": "1,903", "datetime": "2016-02-18 15:22:48", "author": "@ak11"}, "725196026219470848": {"content_summary": "TIL that ADAM is able to jump out of local minima. Wow. https://t.co/n51e31YZyn", "followers": "1,889", "datetime": "2016-04-27 05:33:00", "author": "@ericmjl"}, "956125831205593093": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "221", "datetime": "2018-01-24 11:25:36", "author": "@kog"}, "598677683237560320": {"content_summary": "\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/JRQ4L4Pzpy http://t.co/nWnMhblJ2S", "followers": "1,529", "datetime": "2015-05-14 02:34:17", "author": "@myui"}, "742287521535524864": {"content_summary": "@aUDocomosoftJp \u307c\u304f\u304c\u3044\u3063\u305fadam\u306f\u6700\u9069\u5316\u6cd5\u306e\u3084\u3064https://t.co/0WPRTQvEox\u3002\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u5b9f\u88c5\u3055\u308c\u3066\u308b\u3084\u3064\u306fmomentum\u4ed8\u304d\u306e\u52fe\u914d\u6cd5\u3067\u3001adam\u306f\u52fe\u914d\u306e2\u6b21\u306e\u30e2\u30fc\u30e1\u30f3\u30c8\u3082\u8003\u616e\u3057\u3066\u308b\u3084\u3064\u3002\u3067\u3043\u30fc\u3077\u3089\u30fc\u306b\u3093\u3050\u3068\u304b\u3060\u3068adam\u304c\u6700\u8fd1\u3088\u304f\u4f7f\u308f\u308c\u308b", "followers": "125", "datetime": "2016-06-13 09:28:30", "author": "@WahWah_wa"}, "955842196443357184": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,192", "datetime": "2018-01-23 16:38:32", "author": "@hrs1985"}, "955585162783309825": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,457", "datetime": "2018-01-22 23:37:11", "author": "@St_Hakky"}, "894923282197204993": {"content_summary": "Sympathetic guys :D https://t.co/DjmGG9WPhq https://t.co/7NB0p3bz0a", "followers": "146", "datetime": "2017-08-08 14:08:32", "author": "@simjue"}, "955628075236057088": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "842", "datetime": "2018-01-23 02:27:42", "author": "@jnishi"}, "571151796221005824": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "21,804", "datetime": "2015-02-27 03:36:14", "author": "@vaaaaanquish"}, "955724507280416768": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "868", "datetime": "2018-01-23 08:50:53", "author": "@SING_A_WELL"}, "1189068267501113345": {"content_summary": "RT @o_guest: Well, this is *ehem* peak something. \ud83e\udd23 \"Equal contribution. Author ordering determined by coin flip over a Google Hangout.\"\u2026", "followers": "889", "datetime": "2019-10-29 06:35:41", "author": "@Lightrider"}, "627088796987756544": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "1,529", "datetime": "2015-07-31 12:09:54", "author": "@myui"}, "571306954183684097": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "315", "datetime": "2015-02-27 13:52:46", "author": "@yohekawag"}, "997232797914947585": {"content_summary": "ICLR paper https://t.co/RaDLZwZZrH \u2026 finds a mistake in proof of ADAM [https://t.co/5JO73U5SJT , 8k citations] and gives a 1D convex counter example where ADAM fails to converge to minimum. Also, SGD can beat ADAM (both algorithms optimized) https://t.co", "followers": "356", "datetime": "2018-05-17 21:50:01", "author": "@TravisEGibson"}, "722640237168619522": {"content_summary": "Adam: A Method for Stochastic Optimization. https://t.co/gckakpItlP (by Diederik P. Kingma and Jimmy Lei Ba) #optimization #machinelearning", "followers": "2,307", "datetime": "2016-04-20 04:17:13", "author": "@SoloGen"}, "1183520865113333760": {"content_summary": "... (where dropout prevents overfitting). Adam: A Method for Stochastic Optimization (30 Jan 2017*), Kingma, et al. ArXiv(org)/pdf/1412.6980v9 https://t.co/oRsudqgNSE *note the paper was first published as a conference paper at ICLR 2015. ...", "followers": "215", "datetime": "2019-10-13 23:12:17", "author": "@PolandCherieM"}, "1209026778791940096": {"content_summary": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION https://t.co/p8BmQqxGRP", "followers": "219", "datetime": "2019-12-23 08:23:41", "author": "@lolanbro"}, "998849422971887616": {"content_summary": "https://t.co/63wdNDDvn2 ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION", "followers": "546", "datetime": "2018-05-22 08:53:54", "author": "@garywang"}, "1141615475455139842": {"content_summary": "The Adam stochastic optimization whitepaper fm 2015. \u201cThe method is straightforward to implement, is computationally efficient, has little memory requirements, & is well suited for problems that are large in terms of data/parameters.\u201d https://t.co/jobN", "followers": "281", "datetime": "2019-06-20 07:55:14", "author": "@HBAkirmak"}, "955608334823866368": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "4,856", "datetime": "2018-01-23 01:09:16", "author": "@sonots"}, "1072057838770503680": {"content_summary": "The original Adam paper by @dpkingma and Jimmy Lei Ba: https://t.co/0B7SeOFOsP \"Author ordering determined by coin flip over a Google Hangout.\"", "followers": "31", "datetime": "2018-12-10 09:18:01", "author": "@_lorenzkuhn"}, "982589571845193728": {"content_summary": "For the neural net peeps, here's the paper that brought about the ADAM optimization algorithm. Enjoy! :) https://t.co/5K2jNsWxHk", "followers": "479", "datetime": "2018-04-07 12:03:04", "author": "@adlersantos"}, "598685947362869248": {"content_summary": "\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/JRQ4L4Pzpy http://t.co/nWnMhblJ2S", "followers": "2,257", "datetime": "2015-05-14 03:07:07", "author": "@shunji_umetani"}, "557576221169819649": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "7,210", "datetime": "2015-01-20 16:31:45", "author": "@fastml_extra"}, "926163474316656640": {"content_summary": "Interesting discussion of ADAM, a somewhat cleaner alternative to stochastic gradient descent: https://t.co/wjDIPt2ufi", "followers": "4,795", "datetime": "2017-11-02 19:05:54", "author": "@__ice9"}, "980079933463650306": {"content_summary": "Just reading the Adam optimisation paper https://t.co/MNSOsnCZEI and noticed footnote \"Equal contribution. Author ordering determined by coin flip over a Google Hangout.\" Nice one!", "followers": "605", "datetime": "2018-03-31 13:50:39", "author": "@gerardjgorman"}, "673656670183145473": {"content_summary": "In my optimization purposes, I use Adam algorithm for gradient optimization https://t.co/aoOemB7pi3 #TensorFlow", "followers": "107", "datetime": "2015-12-07 00:14:00", "author": "@SnurakBill"}, "955743339902484480": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "1,380", "datetime": "2018-01-23 10:05:43", "author": "@shinohara0919"}, "1189065398144163841": {"content_summary": "Well, this is *ehem* peak something. \ud83e\udd23 \"Equal contribution. Author ordering determined by coin flip over a Google Hangout.\" https://t.co/8dH84sPvfA https://t.co/jLbTTvAvHY", "followers": "7,444", "datetime": "2019-10-29 06:24:17", "author": "@o_guest"}, "955649338499198976": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "295", "datetime": "2018-01-23 03:52:12", "author": "@sanbo_bass"}, "571238934627221504": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "244", "datetime": "2015-02-27 09:22:29", "author": "@s_ryosky"}, "557826018015465473": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "135", "datetime": "2015-01-21 09:04:21", "author": "@mehdidc"}, "571152975202271232": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "501", "datetime": "2015-02-27 03:40:55", "author": "@fuzzysphere"}, "598680962755526657": {"content_summary": "\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/JRQ4L4Pzpy http://t.co/nWnMhblJ2S", "followers": "2,124", "datetime": "2015-05-14 02:47:19", "author": "@echizen_tm"}, "571171650223210498": {"content_summary": "[1412.6980] Adam: A Method for Stochastic Optimization: [1412.6980] Adam: A Method for Stochastic\u2026 http://t.co/Mz9iei81yL [fav]", "followers": "200", "datetime": "2015-02-27 04:55:07", "author": "@overleo"}, "1161903860325240839": {"content_summary": "RT @rickwierenga: @subhamkr23 @jeremyphoward \u201cAdam: A Method for Stochastic Optimization\u201d by @dpkingma and Jimmy Ba. https://t.co/Awx92Ihe\u2026", "followers": "1,451", "datetime": "2019-08-15 07:34:02", "author": "@Gabriel_Oguna"}, "955667289881260032": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "725", "datetime": "2018-01-23 05:03:32", "author": "@yamo_o"}, "557587038640156673": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "31", "datetime": "2015-01-20 17:14:44", "author": "@cswhjiang"}, "568503002291671040": {"content_summary": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION http://t.co/mWCuZzIkmq", "followers": "473", "datetime": "2015-02-19 20:10:52", "author": "@Udibr"}, "571230315529646080": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "185", "datetime": "2015-02-27 08:48:14", "author": "@amaprg"}, "955660977805996032": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "16,954", "datetime": "2018-01-23 04:38:27", "author": "@kazoo04"}, "1162879860525678592": {"content_summary": "@theshawwn @kaggle Good question! To be honest I'm not sure. They are described as \"Exponential decay rates for the moment estimates\". If you are going to tune this I think in general you want them closer to 1. The standard values come from the original r", "followers": "751", "datetime": "2019-08-18 00:12:18", "author": "@carlolepelaars"}, "1246281332122169346": {"content_summary": "\u539f\u672c\u8aad\u3080\u3068\u3053\u3053\u3089\u8fba\u3088\u304f\u5206\u304b\u308b\u3002 https://t.co/pI2qqwjHyg", "followers": "224", "datetime": "2020-04-04 03:39:59", "author": "@ElectronNest"}, "997257865462140928": {"content_summary": "RT @TravisEGibson: ICLR paper https://t.co/RaDLZwZZrH \u2026 finds a mistake in proof of ADAM [https://t.co/5JO73U5SJT , 8k citations] and giv\u2026", "followers": "1,099", "datetime": "2018-05-17 23:29:37", "author": "@samsinai"}, "1116886675458641920": {"content_summary": "@Atrix256 @rygorous @kenpex https://t.co/ibn5Hh5qBj", "followers": "3,376", "datetime": "2019-04-13 02:11:48", "author": "@MoldyMagnet"}, "807439436736626688": {"content_summary": "RT @beam2d: @aonotas Adam\u3067\u3084\u3063\u3066\u308b\u8ad6\u6587\u3092\u3071\u3063\u3068\u898b\u3064\u3051\u3089\u308c\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001 Adam\u306e\u5143\u8ad6\u6587\u306b\u304a\u3051\u308b\u7406\u8ad6\u89e3\u6790\u3067\u306f\u5b66\u7fd2\u7387\u30921/\u221at\u3067\u6e1b\u8870\u3055\u305b\u305f\u5834\u5408\u306b\u53ce\u675f\u3059\u308b\u3053\u3068\u304c\u8a3c\u660e\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7406\u8ad6\u7684\u306b\u3082\u6e1b\u8870\u3055\u305b\u308b\u306e\u306f\u5fc5\u8981\u306a\u306f\u305a\u3067\u3059 https://t.co/Y1BL\u2026", "followers": "1,655", "datetime": "2016-12-10 04:18:57", "author": "@olanleed"}, "851513046577156097": {"content_summary": "Please stop putting \"ADAM\" in all caps--it's not an acronym. It should be \"Adam\". https://t.co/9jeuxMTbbM", "followers": "1,353", "datetime": "2017-04-10 19:11:45", "author": "@unixpickle"}, "1209215432948797445": {"content_summary": "\uadf8\ub9ac\uace0 \ub0b4\uc77c\uc740 ADAM paper\ub97c \uc77d\uaca0\uc9c0. \ub514\ud14c\uc77c\uc774 \ub9e4\ubc88 \uae30\uc5b5\uc774 \uc548 \ub09c\ub2e4. https://t.co/f3B73iDNgc", "followers": "1,101", "datetime": "2019-12-23 20:53:20", "author": "@_stevenyoo_"}, "824299831673974786": {"content_summary": "Just tested implementing ADAM myself. It's SGD upgraded, super simple and works amazingly well. https://t.co/eE6OMy3oh3", "followers": "246", "datetime": "2017-01-25 16:56:09", "author": "@joose_rajamaeki"}, "732890555940806656": {"content_summary": "RT @pavelkordik: Adam optimization https://t.co/39rgkBneUG looks good for gradient learning https://t.co/dAopHg2OQ4 of #NeuralNetworks #dat\u2026", "followers": "19,293", "datetime": "2016-05-18 11:08:19", "author": "@homeAIinfo"}, "571520129785192448": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "725", "datetime": "2015-02-28 03:59:51", "author": "@yamo_o"}, "807393204844707840": {"content_summary": "RT @beam2d: @aonotas Adam\u3067\u3084\u3063\u3066\u308b\u8ad6\u6587\u3092\u3071\u3063\u3068\u898b\u3064\u3051\u3089\u308c\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001 Adam\u306e\u5143\u8ad6\u6587\u306b\u304a\u3051\u308b\u7406\u8ad6\u89e3\u6790\u3067\u306f\u5b66\u7fd2\u7387\u30921/\u221at\u3067\u6e1b\u8870\u3055\u305b\u305f\u5834\u5408\u306b\u53ce\u675f\u3059\u308b\u3053\u3068\u304c\u8a3c\u660e\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7406\u8ad6\u7684\u306b\u3082\u6e1b\u8870\u3055\u305b\u308b\u306e\u306f\u5fc5\u8981\u306a\u306f\u305a\u3067\u3059 https://t.co/Y1BL\u2026", "followers": "338", "datetime": "2016-12-10 01:15:15", "author": "@one_meets_seven"}, "593271867651162113": {"content_summary": "\u201c[1412.6980] Adam: A Method for Stochastic Optimization\u201d http://t.co/EUiPt0XY1G", "followers": "5,851", "datetime": "2015-04-29 04:33:30", "author": "@hamadakoichi"}, "1162636488376406016": {"content_summary": "RT @TivadarDanka: @tkasasagi In CS it is more common, see for example the ADAM paper: https://t.co/lfn7XF7i8H In math, order of authorship\u2026", "followers": "16,522", "datetime": "2019-08-17 08:05:14", "author": "@tkasasagi"}, "1023685049265217536": {"content_summary": "RS: Adapting learning rates. One learning rate per parameter. Adagrad, RMSProp, Adam (the most dominant method that essentially combines RMSProp with momentum) #JSM2018 https://t.co/1CxsDIJg0d", "followers": "16,447", "datetime": "2018-07-29 21:41:49", "author": "@michaelhoffman"}, "977607029987557376": {"content_summary": "Adam: A Method for Stochastic Optimization paper https://t.co/cr6QsOaYBs", "followers": "149", "datetime": "2018-03-24 18:04:13", "author": "@parker_brydon"}, "627104323894312964": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "2,060", "datetime": "2015-07-31 13:11:36", "author": "@toshi_k_datasci"}, "627086342820773888": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "152", "datetime": "2015-07-31 12:00:09", "author": "@y_tag"}, "1108639444049883137": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/m8cHw6NYWD (Popularity:29.2) #Machine_Learning", "followers": "53", "datetime": "2019-03-21 08:00:15", "author": "@PoQaa_cs"}, "955743273124950016": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "2,515", "datetime": "2018-01-23 10:05:27", "author": "@bori_so1"}, "955715801398493184": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "302", "datetime": "2018-01-23 08:16:18", "author": "@mofumofu1729"}, "955660194066808832": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "824", "datetime": "2018-01-23 04:35:20", "author": "@morioka"}, "976213493085978624": {"content_summary": "Adam\u4ee5\u964d\u306e\u52fe\u914d\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u306f\u3053\u3061\u3089\u3082\u3054\u53c2\u7167\u4e0b\u3055\u3044\u3002 https://t.co/bHN0nNVpCe", "followers": "2,043", "datetime": "2018-03-20 21:46:48", "author": "@dosei_sanga"}, "572698659168509953": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "83", "datetime": "2015-03-03 10:02:55", "author": "@selminage"}, "1141618249492074497": {"content_summary": "RT @HBAkirmak: The Adam stochastic optimization whitepaper fm 2015. \u201cThe method is straightforward to implement, is computationally efficie\u2026", "followers": "378", "datetime": "2019-06-20 08:06:15", "author": "@tsspl2006"}, "898576577402994688": {"content_summary": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION https://t.co/QHRrHPuuGN", "followers": "8,563", "datetime": "2017-08-18 16:05:25", "author": "@siah"}, "558722745195048961": {"content_summary": "Really nice stochastic optimization paper from Jimmy Ba & Diederik Kingma. Simple impl'n, only req. 1st order info. http://t.co/XFFFjRImJY", "followers": "702", "datetime": "2015-01-23 20:27:37", "author": "@lzamparo"}, "678646448125321216": {"content_summary": "Adam #optimization method deals with sparse gradients and non-stationary objectives https://t.co/39rgkBneUG Beats #RMSProp! #machinelearning", "followers": "934", "datetime": "2015-12-20 18:41:36", "author": "@pavelkordik"}, "1038252068895698944": {"content_summary": "Impressive performance. #Adam #AdaMax #StochasticOptimization #AdaGrad #RMSProp #Training #ANN #NeuralNetworks #DeepLearning #GradientDescent #FirstOrderMethods https://t.co/IwSYvgkoe3", "followers": "189", "datetime": "2018-09-08 02:25:57", "author": "@oneinfinitezero"}, "909309990581682176": {"content_summary": "Go Adam go, go Adam go, goooooooo Adam. (this assignment taught us about the Adam algorithm optimising technique) https://t.co/fsQ0OtKbk6 https://t.co/t7XpYL7hi5", "followers": "2,033", "datetime": "2017-09-17 06:56:10", "author": "@sighmon"}, "926315318384791552": {"content_summary": "RT @__ice9: Interesting discussion of ADAM, a somewhat cleaner alternative to stochastic gradient descent: https://t.co/wjDIPt2ufi", "followers": "4", "datetime": "2017-11-03 05:09:16", "author": "@Naturalistech"}, "598229841012465664": {"content_summary": "Adam: A Method for Stochastic Optimization http://t.co/WG3JxmtLir", "followers": "285", "datetime": "2015-05-12 20:54:43", "author": "@thomasquintana"}, "924312288693227535": {"content_summary": "Coursera\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u8b1b\u7fa9\u3002\u52fe\u914d\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3067\u30e2\u30fc\u30e1\u30f3\u30bf\u30e0\u3068RMSprop\u3092\u6271\u3063\u305f\u5f8c\u3001Adam\u3092\u7fd2\u3063\u3066\u30b3\u30a2\u90e8\u5206\u306e\u307f\u5b9f\u88c5\u3057\u305f\u3002\u3060\u3093\u3060\u3093\u672c\u683c\u7684\u307d\u3044\u5185\u5bb9\u306b\u306a\u3063\u3066\u3044\u304f\u3002Adam\u306e\u8ad6\u6587\u3082\u8a00\u53ca\u3055\u308c\u3066\u305f\u306e\u3067\u4eca\u5ea6\u8aad\u3080\u3002\u3002 https://t.co/5up1BtuBCb", "followers": "356", "datetime": "2017-10-28 16:29:57", "author": "@zero310"}, "722795610777083904": {"content_summary": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION https://t.co/7m4JVsIrav", "followers": "397", "datetime": "2016-04-20 14:34:37", "author": "@yunyun_anoma"}, "571279139883208704": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "3", "datetime": "2015-02-27 12:02:15", "author": "@bohdaisuke10"}, "707530465335336962": {"content_summary": "Adam\u306e\u8ad6\u6587 https://t.co/FsezA08Zq6", "followers": "15", "datetime": "2016-03-09 11:36:22", "author": "@CSG77947106"}, "598678955051208704": {"content_summary": "\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/JRQ4L4Pzpy http://t.co/nWnMhblJ2S", "followers": "39,503", "datetime": "2015-05-14 02:39:20", "author": "@TJO_datasci"}, "558059790690451456": {"content_summary": "RNNs for text analysis: http://t.co/98W4Kzjlab steeper sigmoid gate (http://t.co/BG52hizZ4e) \u3084 adam (http://t.co/ebMPdJj63Y) \u306a\u3069\u6700\u65b0\u6280\u8853\u6e80\u8f09", "followers": "764", "datetime": "2015-01-22 00:33:17", "author": "@yuutat"}, "955559184338075649": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "2,451", "datetime": "2018-01-22 21:53:57", "author": "@wraikny_"}, "572002332373696512": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "701", "datetime": "2015-03-01 11:55:57", "author": "@kmt_t"}, "736118856016494592": {"content_summary": "RT @natebrix: ADAM: A Method for Stochastic Optimization: https://t.co/1vHMvO1ikk #tensorflow #orms", "followers": "303", "datetime": "2016-05-27 08:56:26", "author": "@minhhanu"}, "557803647690887170": {"content_summary": "Adam: A Method for Stochastic Optimization (good for sparse gradients): http://t.co/wrjoa9pPmG Python/Theano code: https://t.co/NMQXiwfgrx", "followers": "28,418", "datetime": "2015-01-21 07:35:27", "author": "@ogrisel"}, "1109726602416422913": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/m8cHw6NYWD (Popularity:18.0) #Machine_Learning", "followers": "53", "datetime": "2019-03-24 08:00:14", "author": "@PoQaa_cs"}, "1226969919671226370": {"content_summary": "Adam optimization algorithm paper: https://t.co/81j90E6V45 #DeepLearning #MachineLearning #ai", "followers": "200", "datetime": "2020-02-10 20:43:19", "author": "@roldan_matias"}, "571157806939680768": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "150", "datetime": "2015-02-27 04:00:07", "author": "@y_yammt"}, "860970862006153217": {"content_summary": "RT @NancyYaco: ADAM: A method for Stochastic Optimization #machinelearning https://t.co/0yNxZ0xLn1", "followers": "4,056", "datetime": "2017-05-06 21:33:44", "author": "@scottbot_17"}, "807440342312067072": {"content_summary": "RT @beam2d: @aonotas Adam\u3067\u3084\u3063\u3066\u308b\u8ad6\u6587\u3092\u3071\u3063\u3068\u898b\u3064\u3051\u3089\u308c\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001 Adam\u306e\u5143\u8ad6\u6587\u306b\u304a\u3051\u308b\u7406\u8ad6\u89e3\u6790\u3067\u306f\u5b66\u7fd2\u7387\u30921/\u221at\u3067\u6e1b\u8870\u3055\u305b\u305f\u5834\u5408\u306b\u53ce\u675f\u3059\u308b\u3053\u3068\u304c\u8a3c\u660e\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7406\u8ad6\u7684\u306b\u3082\u6e1b\u8870\u3055\u305b\u308b\u306e\u306f\u5fc5\u8981\u306a\u306f\u305a\u3067\u3059 https://t.co/Y1BL\u2026", "followers": "1,667", "datetime": "2016-12-10 04:22:33", "author": "@Scaled_Wurm"}, "955491537491668994": {"content_summary": "1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWeightDecay\u3092\u8003\u616e\u3057\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/JJayCjDroD 4.\u3044\u3084\u8a3c\u660e\u9593\u9055\u3063\u3066\u308b\u3057 \u2192https://t.co/znLJ5GHwVj", "followers": "2,043", "datetime": "2018-01-22 17:25:09", "author": "@dosei_sanga"}, "562087175051677697": {"content_summary": "ADAM\u304c\u3044\u3044\u3088\u3063\u3066\u6559\u3048\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u3002\u3082\u30461\u672c\u30d9\u30af\u30c8\u30eb\u304c\u5fc5\u8981\u3067\u3059\u304c\u3001\u5b9f\u88c5\u5de5\u592b\u3059\u308c\u3070\u30a2\u30bb\u30f3\u30d6\u30ea\u66f8\u304b\u306a\u304f\u3066\u3082SSE\u306b\u306f\u4e57\u305b\u3089\u308c\u308b\u3002\u305f\u3060\u3001\u30e1\u30e2\u30ea\u5e2f\u57df\u30cd\u30c3\u30af\u306b\u306a\u3063\u3066\u308b\u307d\u304f\u3066\u3001\u5897\u3048\u305f\u5206\u306e\u30e1\u30e2\u30ea\u304c\u618e\u3044 http://t.co/7ukDHKpj5Z", "followers": "2,257", "datetime": "2015-02-02 03:16:40", "author": "@shunji_umetani"}, "705668846133194752": {"content_summary": "RT @ak11: \u305f\u307e\u305f\u307e\u898b\u304b\u3051\u305fAdam\u306e\u5b9f\u88c5\u304c\u3069\u3046\u3082\u81ea\u5206\u306e\u77e5\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u9055\u3063\u305f\u306e\u3067\u6539\u3081\u3066\u8abf\u3079\u3066\u307f\u305f\u3089\u3001\u7d50\u69cb\u983b\u7e41\u306b\u66f4\u65b0\u3055\u308c\u3066\u3044\u305f\u3002v7\u306e\u304c\u5206\u304b\u308a\u3084\u3059\u3044\u3068\u601d\u3063\u305f\u3051\u3069v8\u3067\u65b0\u8b0e\u6982\u5ff5\u304c\u767b\u5834\u3057\u3066\u3044\u308b\u2026\u3002 https://t.co/PtYWcQVhDr", "followers": "259", "datetime": "2016-03-04 08:18:58", "author": "@j_inbar"}, "663785570582196224": {"content_summary": "Adam: A Method for Stochastic Optimization https://t.co/8bLWisVZcs", "followers": "4,683", "datetime": "2015-11-09 18:29:47", "author": "@bradneuberg"}, "571127743464677376": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "18,250", "datetime": "2015-02-27 02:00:39", "author": "@hillbig"}, "812011714292121600": {"content_summary": "#machinelearning community; any empirical intuition when to (or not to) use ADAM? (vs others like AdaGrad etc) https://t.co/sc5hR4dwja", "followers": "867", "datetime": "2016-12-22 19:07:33", "author": "@DanielKhashabi"}, "623465836972408832": {"content_summary": "@sylvan5 ADAM\u306f\u3053\u308c\u306e\u5b9f\u88c5\u3067\u3059\u306d\u3002\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u8ad6\u6587\u306e\u63a8\u5968\u5024\u3067\u826f\u3044\u306e\u3067AdaGrad\u306a\u3069\u306b\u6bd4\u3079\u3066\u5b9f\u884c\u304c\u697d\u3067\u3059 http://t.co/3deyRowENH", "followers": "1,655", "datetime": "2015-07-21 12:13:33", "author": "@olanleed"}, "955669312718618625": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "44", "datetime": "2018-01-23 05:11:34", "author": "@CharaBentoPapa"}, "955740403189268481": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "381", "datetime": "2018-01-23 09:54:03", "author": "@vintersn0w"}, "955665929651699712": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "0", "datetime": "2018-01-23 04:58:07", "author": "@digital_gori"}, "828412577386369025": {"content_summary": "Adam\u306e\u8ad6\u6587 / \u201c[1412.6980] Adam: A Method for Stochastic Optimization\u201d https://t.co/8evFXO9CJa", "followers": "4,054", "datetime": "2017-02-06 01:18:44", "author": "@chezou"}, "571307229032230912": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "4,019", "datetime": "2015-02-27 13:53:52", "author": "@ballforest"}, "955950844876222464": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "874", "datetime": "2018-01-23 23:50:16", "author": "@guicho271828"}, "955661091090063360": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "406", "datetime": "2018-01-23 04:38:54", "author": "@upaver20"}, "1186958943051866112": {"content_summary": "ABST100\u30e1\u30e2\uff1a77 Adam\uff082014\uff09\u2192https://t.co/Dt2QHnLpSe\u3000\uff0c\u6700\u9069\u5316\u306e\u4ee3\u8868\u7684\u624b\u6cd5\uff0c\u8ad6\u6587\u4e2d\u306eregret\uff08\u5f8c\u6094\uff09\u3063\u3066\u306a\u3093\u3084\u306d\u3093\u3068\u601d\u3063\u305f\u304c\u30ea\u30b0\u30ec\u30c3\u30c8\u89e3\u6790\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308b\u307f\u305f\u3044\u306a\u306e\u3067\u8abf\u3079\u3066\u307f\u308b", "followers": "63", "datetime": "2019-10-23 10:53:59", "author": "@Ronmemo1"}, "732889409146753024": {"content_summary": "Adam optimization https://t.co/39rgkBneUG looks good for gradient learning https://t.co/dAopHg2OQ4 of #NeuralNetworks #datascience #convnet", "followers": "934", "datetime": "2016-05-18 11:03:46", "author": "@pavelkordik"}, "807383455482814464": {"content_summary": "@aonotas Adam\u3067\u3084\u3063\u3066\u308b\u8ad6\u6587\u3092\u3071\u3063\u3068\u898b\u3064\u3051\u3089\u308c\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001 Adam\u306e\u5143\u8ad6\u6587\u306b\u304a\u3051\u308b\u7406\u8ad6\u89e3\u6790\u3067\u306f\u5b66\u7fd2\u7387\u30921/\u221at\u3067\u6e1b\u8870\u3055\u305b\u305f\u5834\u5408\u306b\u53ce\u675f\u3059\u308b\u3053\u3068\u304c\u8a3c\u660e\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7406\u8ad6\u7684\u306b\u3082\u6e1b\u8870\u3055\u305b\u308b\u306e\u306f\u5fc5\u8981\u306a\u306f\u305a\u3067\u3059 https://t.co/Y1BL1E56s9", "followers": "4,631", "datetime": "2016-12-10 00:36:30", "author": "@beam2d"}, "945681712855572480": {"content_summary": "... And, the most popular associated optimizer: Adam (historic gradient reweighting w decay and momentum) : https://t.co/zKAbX7dDp2 #DeepLearning #CNN #RNN #AI https://t.co/NCxB29hLrk", "followers": "373", "datetime": "2017-12-26 15:44:24", "author": "@rsoper72"}, "1162632805563678720": {"content_summary": "@tkasasagi In CS it is more common, see for example the ADAM paper: https://t.co/lfn7XF7i8H In math, order of authorship is always alphabetical. Lost count of how many WTFs I have heard when explaining this to life scientists :D", "followers": "268", "datetime": "2019-08-17 07:50:36", "author": "@TivadarDanka"}, "1121962454882275328": {"content_summary": "Adam\u304c\u5b66\u7fd2\u7387\u8abf\u6574\u624b\u6cd5\u3060\u3068\u304b\u3067\u306f\u306a\u3044\u3068\u304b\u3055\u3042\u3001\u305d\u3082\u305d\u3082\u8457\u8005\u305d\u3093\u306a\u3053\u3068\u8a00\u3063\u3066\u306a\u3044\u3088\u306d https://t.co/0TNVmWg0mb https://t.co/PXpJt3sB19", "followers": "4,615", "datetime": "2019-04-27 02:21:08", "author": "@HITStales"}, "733777305068310531": {"content_summary": "ADAM: A Method for Stochastic Optimization: https://t.co/1vHMvO1ikk #tensorflow #orms", "followers": "912", "datetime": "2016-05-20 21:51:57", "author": "@natebrix"}, "571270130480472066": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "2,124", "datetime": "2015-02-27 11:26:27", "author": "@echizen_tm"}, "573947651038707712": {"content_summary": "Adagrad*RMSProp = Adam: A Method for Stochastic Optimization (update) http://t.co/4XXrEQhiZ0", "followers": "1,036", "datetime": "2015-03-06 20:45:58", "author": "@kleinsound"}, "583910002487726080": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "229", "datetime": "2015-04-03 08:32:47", "author": "@ochiai_tetu"}, "825177678013722624": {"content_summary": "I _really_ like the Adam algorithm for stochastic optimization. Works well in practice & makes sense in theory. https://t.co/UULFzVozHB", "followers": "2,042", "datetime": "2017-01-28 03:04:24", "author": "@davidjayharris"}, "1251902301121531905": {"content_summary": "@GlobalEqTiming @the_chart_life This ADAM? https://t.co/hIeayqG1mW", "followers": "915", "datetime": "2020-04-19 15:55:42", "author": "@Oh_So_Gordo"}, "955608521529212928": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "668", "datetime": "2018-01-23 01:10:00", "author": "@yusuke1118"}, "571174178981675008": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "229", "datetime": "2015-02-27 05:05:10", "author": "@ochiai_tetu"}, "760838633167388674": {"content_summary": "A solution to a perennial problem: Co-first \"author ordering determined by coin flip\" - eg https://t.co/87xmrber7X & https://t.co/vwVgDLoS2a", "followers": "6,445", "datetime": "2016-08-03 14:04:00", "author": "@markgerstein"}, "1161753426423775238": {"content_summary": "@subhamkr23 @jeremyphoward \u201cAdam: A Method for Stochastic Optimization\u201d by @dpkingma and Jimmy Ba. https://t.co/Awx92IheC6", "followers": "1,557", "datetime": "2019-08-14 21:36:15", "author": "@rickwierenga"}, "581070986478374912": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "2,176", "datetime": "2015-03-26 12:31:33", "author": "@wednesdaymuse"}, "804134799606693888": {"content_summary": "[1412.6980] Adam: A Method for Stochastic Optimization https://t.co/jtSAaKhoMI", "followers": "376", "datetime": "2016-12-01 01:27:30", "author": "@learnlinksfeed"}, "1037943610874900481": {"content_summary": "Learned about Adam optimization algorithm for training #NeuralNetworks. It combines Momentum and Hinton's #RMSProp to speed up convergence of #GradientDescent. The underlying concept is exponentially weighted moving averages. #100daysOfMLCode #DeepLearning", "followers": "175", "datetime": "2018-09-07 06:00:15", "author": "@richmond_umagat"}, "616575414379483137": {"content_summary": "Adam\u306e\u03b1\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308b\u304b\u3082\u3060\u3051\u3069\u30010.001\u4ee5\u5916\u306b0.01\u306f\u8a66\u3057\u3066\u3044\u3044\u304b\u3082\u3002 http://t.co/yUTll93JUl", "followers": "512", "datetime": "2015-07-02 11:53:28", "author": "@tmasada"}, "691383712315932672": {"content_summary": "Unrelated: done training my 27 layer deep baby. From scratch, for the first time. Protip: Use ADAM (https://t.co/Vk74duzguo).", "followers": "226", "datetime": "2016-01-24 22:14:56", "author": "@nasim_rahaman"}, "598702423612346369": {"content_summary": "myui\u3055\u3093\u306fTwitter\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059: \"\u8efd\u3081\u306a\u30a4\u30f3\u30bf\u30fc\u30f3\u8ab2\u984c\u3068\u3057\u3066\u306fADAM\u3068\u304bNbSVM\u3092hivemall\u306b\u5b9f\u88c5\u3059\u308b\u3068\u304b\u3082\u3042\u308a\u304b\u306a\u3002http://t.co/t7gh7ASkSg\u2026 http://t.co/kbDT6nelok [fav]", "followers": "200", "datetime": "2015-05-14 04:12:35", "author": "@overleo"}, "571130945090162688": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "39,503", "datetime": "2015-02-27 02:13:23", "author": "@TJO_datasci"}, "547512036452626432": {"content_summary": "Adam: A Method for Stochastic Optimization http://t.co/oJMlYxB0Cm #arxiv", "followers": "787", "datetime": "2014-12-23 22:00:16", "author": "@arXivStats"}, "571273186257416192": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "2,257", "datetime": "2015-02-27 11:38:36", "author": "@shunji_umetani"}, "571147642413010944": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "50", "datetime": "2015-02-27 03:19:44", "author": "@takuya_araki"}, "571196484277772288": {"content_summary": "\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "221", "datetime": "2015-02-27 06:33:48", "author": "@kog"}, "736243667581698049": {"content_summary": "reading about Adam : moving averages estimates updates to learning rate w/correction for initialization bias. https://t.co/66eiXo5kYf", "followers": "65", "datetime": "2016-05-27 17:12:23", "author": "@orsonady"}, "938161075723358208": {"content_summary": "By far the best stochastic optimization algorithm \ud83e\udd23: https://t.co/28eR2w17VH", "followers": "411", "datetime": "2017-12-05 21:40:05", "author": "@adambehrens"}, "955661619459112961": {"content_summary": "RT @dosei_sanga: 1.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u7121\u9650\u6b21\u5143\u3067\u6271\u3063\u3066\u306a\u3044\u304b\u3089 \u2192https://t.co/GKalnj8hva 2.Adam\u304c\u30c0\u30e1\u306a\u306e\u306f\u76ee\u7684\u95a2\u6570\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u306a\u3044\u304b\u3089 \u2192https://t.co/VKt65INjvq 3.Adam\u304c\u30c0\u30e1\u306a\u306e\u306fWei\u2026", "followers": "655", "datetime": "2018-01-23 04:41:00", "author": "@yuto3048"}, "674900746882691072": {"content_summary": "@tensorflow this is so cool: most documentation contains links to papers: def Adam(learning_rate): \"\"\" based on: https://t.co/RmVQxIJaPk", "followers": "5", "datetime": "2015-12-10 10:37:31", "author": "@Voice_Actions"}, "627104477540028416": {"content_summary": "\u4eca\u65e5\u306f\u4f1a\u793e\u306e\u4eba\u306bADAM\u8ad6\u6587\u306e\u5909\u9077\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u3082\u3089\u3063\u305f\u3002\u4eca\u3067\u306fv8\u307e\u3067\u66f4\u65b0\u3055\u308c\u3066\u3044\u3066\u3001v6\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u03bb\u304c\u306a\u304f\u306a\u3063\u305f\u3089\u3057\u3044\u3002http://t.co/4HGq93Qo3l", "followers": "1,201", "datetime": "2015-07-31 13:12:13", "author": "@shora_kujira16"}, "571131992978296832": {"content_summary": "\u30e1\u30e2\u30e1\u30e2\u3063\u3068 \u03c6(..) RT @hillbig: \u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002ry) http://t.co/l28nRdImSr", "followers": "251", "datetime": "2015-02-27 02:17:32", "author": "@hal9801"}, "1221743404893331456": {"content_summary": "\u30a2\u30c3\u3001\u30a2\u30c3\u3001\u30a2\u30c3\u3002\u3002\u3002\uff82\uff97\uff72(T_T) \u3042\u308c\u3001\u4ffa\u3001\u306a\u3093\u3067\u6ce3\u3044\u3066\u308b\u3093\u3060\u308d\uff1f\uff1f\ud83d\ude22 \u300c\u304d\u3061\u3093\u3068\u904e\u53bb\u3092\u5f8c\u6094\u3057\u306a\u3044\u304b\u3089\u300d", "followers": "1,867", "datetime": "2020-01-27 10:35:01", "author": "@TakashiSasaki"}, "818252435479105537": {"content_summary": "ADAM performed better than SGD+Nesterov on MNIST. Adaptive learning rate FTW! https://t.co/RgRv49RMwN", "followers": "8", "datetime": "2017-01-09 00:25:57", "author": "@j1mehta"}, "1049675001933299712": {"content_summary": "RT @hillbig: \u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u306bADAM\u3092\u4f7f\u3044\u306f\u3058\u3081\u305f\u304c\u3001\u53ce\u675f\u6027\u80fd\u304c\u3044\u3044\u306e\u3068\u3001\u4eca\u306e\u3068\u3053\u308d\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3044\u3058\u3089\u305a\u8ad6\u6587\u63a8\u5968\u306e\u5024\u3067\u554f\u984c\u306a\u304f\u3067\u304d\u3066\u304a\u52e7\u3081\u3002Adagrad\u3084RMSprop\u306b\u4ee3\u308f\u308b\u6700\u9069\u5316\u306e\u5b9a\u756a\u306b\u306a\u308a\u305d\u3046 http://t.co/i0M3oCJiKO", "followers": "1,213", "datetime": "2018-10-09 14:56:37", "author": "@ekusoyt"}}}