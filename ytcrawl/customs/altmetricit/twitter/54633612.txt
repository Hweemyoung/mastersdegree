{"twitter": {"1092084641689976838": {"author": "@macroeyeshealth", "datetime": "2019-02-03 15:37:23", "content_summary": "\ud83d\udcf0\"Escaping Saddle Points with Adaptive Gradient Methods\" @GoogleResearch @optiML https://t.co/qKNlQRAa01 #DeepLearning #Optimization https://t.co/5G29QX1B0i", "followers": "323"}, "1090122831298363392": {"author": "@I_eric_Y", "datetime": "2019-01-29 05:41:51", "content_summary": "RT @StatsPapers: Escaping Saddle Points with Adaptive Gradient Methods. https://t.co/RujyNzX6sA", "followers": "602"}, "1094665600985493504": {"author": "@DSPonFPGA", "datetime": "2019-02-10 18:33:11", "content_summary": "Escaping Saddle Points with Adaptive Gradient Methods https://t.co/6gShAbhFtY", "followers": "405"}, "1090253693084205056": {"author": "@arxivml", "datetime": "2019-01-29 14:21:51", "content_summary": "\"Escaping Saddle Points with Adaptive Gradient Methods\", Matthew Staib, Sashank J\uff0e Reddi, Satyen Kale, Sanjiv Kumar\u2026 https://t.co/AK7G9Llobu", "followers": "779"}, "1090315948018544641": {"author": "@arXiv__ml", "datetime": "2019-01-29 18:29:13", "content_summary": "#arXiv #machinelearning [cs.LG] Escaping Saddle Points with Adaptive Gradient Methods. (arXiv:1901.09149v1 [cs.LG]) https://t.co/G2zQrdVPba Adaptive methods such as Adam and RMSProp are widely used in deep learning but are not well understood. In this pap", "followers": "1,743"}, "1090084182859177985": {"author": "@arxiv_cs_LG", "datetime": "2019-01-29 03:08:16", "content_summary": "Escaping Saddle Points with Adaptive Gradient Methods. Matthew Staib, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra https://t.co/W98wVoM4mu", "followers": "318"}, "1090087593876701184": {"author": "@StatsPapers", "datetime": "2019-01-29 03:21:49", "content_summary": "Escaping Saddle Points with Adaptive Gradient Methods. https://t.co/RujyNzX6sA", "followers": "5,454"}, "1090070400942440448": {"author": "@StatMLPapers", "datetime": "2019-01-29 02:13:30", "content_summary": "Escaping Saddle Points with Adaptive Gradient Methods. (arXiv:1901.09149v1 [cs.LG]) https://t.co/hpoHuJHlP7", "followers": "9,706"}, "1090079994284519430": {"author": "@BrundageBot", "datetime": "2019-01-29 02:51:38", "content_summary": "Escaping Saddle Points with Adaptive Gradient Methods. Matthew Staib, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra https://t.co/PpZNZIBsNb", "followers": "3,890"}, "1090331906145701889": {"author": "@mlmemoirs", "datetime": "2019-01-29 19:32:38", "content_summary": "#arXiv #machinelearning [cs.LG] Escaping Saddle Points with Adaptive Gradient Methods. (arXiv:1901.09149v1 [cs.LG]) https://t.co/9msBas51Dj Adaptive methods such as Adam and RMSProp are widely used in deep learning but are not well understood. In this pap", "followers": "1,260"}}, "queriedAt": "2020-05-21 20:15:53", "completed": "1", "citation_id": "54633612", "tab": "twitter"}