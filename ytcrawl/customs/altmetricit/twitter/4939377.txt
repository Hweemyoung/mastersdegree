{"citation_id": "4939377", "queriedAt": "2020-05-09 13:58:35", "completed": "0", "twitter": {"1029008035912531968": {"followers": "526", "content_summary": "2 - We leverage attention's ability to model loosely ordered sequences, as shown in @colinraffel's https://t.co/2pEks4rltP to predict the number of primary vertices in an event from VP readouts. 4/10 https://t.co/GuUYZES6Cw", "author": "@dtsbourg", "datetime": "2018-08-13 14:13:28"}, "684807976515813376": {"followers": "2,915", "content_summary": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems https://t.co/LcpwC0Ne80", "author": "@evolvingstuff", "datetime": "2016-01-06 18:45:19"}, "682450338792652800": {"followers": "148", "content_summary": "https://t.co/XaCUXtTXL3 Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "author": "@y_yammt", "datetime": "2015-12-31 06:36:54"}}, "tab": "twitter"}