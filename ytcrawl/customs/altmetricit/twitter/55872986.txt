{"tab": "twitter", "completed": "1", "twitter": {"1101143198561374216": {"author": "@johnnyprothero", "followers": "184", "datetime": "2019-02-28 15:32:51", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101313749057658880": {"author": "@stealthinu", "followers": "1,055", "datetime": "2019-03-01 02:50:33", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100558706624741376": {"author": "@svlevine", "followers": "18,478", "datetime": "2019-02-27 00:50:17", "content_summary": "Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruction and then optimizing it with RL results in substantially better instruction following https://t.co/relOydZbC9 w/ Justin Fu, @"}, "1099717659212595200": {"author": "@theChrisChua", "followers": "2,136", "datetime": "2019-02-24 17:08:16", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Followin\u2026"}, "1101045665621331968": {"author": "@ykilcher", "followers": "965", "datetime": "2019-02-28 09:05:17", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100783486166695936": {"author": "@AbdullahMdKhan", "followers": "16", "datetime": "2019-02-27 15:43:29", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100779511040200704": {"author": "@bwrado1", "followers": "61", "datetime": "2019-02-27 15:27:41", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100997546665603072": {"author": "@sguada", "followers": "1,436", "datetime": "2019-02-28 05:54:05", "content_summary": "RT @chelseabfinn: Neat work from Justin, Anoop, Sergey, and Sergio on enabling agents to learn from language instructions! https://t.co/1p4\u2026"}, "1101438111689535488": {"author": "@pavan_yekabote", "followers": "30", "datetime": "2019-03-01 11:04:44", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100560158151692288": {"author": "@iandanforth", "followers": "1,951", "datetime": "2019-02-27 00:56:03", "content_summary": "This is excellent. An instruction *should* alter the value assigned to components of an observation."}, "1101367113392877569": {"author": "@shubh_300595", "followers": "66", "datetime": "2019-03-01 06:22:36", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1099581657663717376": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-02-24 08:07:51", "content_summary": "RT @StatMLPapers: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG\u2026"}, "1100683392733962241": {"author": "@brargk", "followers": "20", "datetime": "2019-02-27 09:05:45", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101323231707848704": {"author": "@cddadr", "followers": "599", "datetime": "2019-03-01 03:28:14", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100841557232513024": {"author": "@chelseabfinn", "followers": "23,776", "datetime": "2019-02-27 19:34:14", "content_summary": "Neat work from Justin, Anoop, Sergey, and Sergio on enabling agents to learn from language instructions!"}, "1101289476289060865": {"author": "@ainewsantenna", "followers": "55", "datetime": "2019-03-01 01:14:06", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101071826644422657": {"author": "@Volton007", "followers": "132", "datetime": "2019-02-28 10:49:15", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101103772145729537": {"author": "@dannyehb", "followers": "301", "datetime": "2019-02-28 12:56:11", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101299843195068416": {"author": "@SquirrelYellow", "followers": "175", "datetime": "2019-03-01 01:55:18", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101326915770540034": {"author": "@morioka", "followers": "822", "datetime": "2019-03-01 03:42:53", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101285598701088768": {"author": "@auratus", "followers": "1,571", "datetime": "2019-03-01 00:58:42", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100578781935222784": {"author": "@viktor_m81", "followers": "118", "datetime": "2019-02-27 02:10:04", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100082277613105157": {"author": "@mmitchell_ai", "followers": "10,025", "datetime": "2019-02-25 17:17:08", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1100837183903150082": {"author": "@gshashank84", "followers": "64", "datetime": "2019-02-27 19:16:51", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100658270459576321": {"author": "@heghbalz", "followers": "1,308", "datetime": "2019-02-27 07:25:55", "content_summary": "RT @sharadvikram: Cool ICLR paper by Justin Fu et al, coming out of a Google AI internship! https://t.co/lWeFKoiTPn"}, "1100621953667420160": {"author": "@bbc1183", "followers": "130", "datetime": "2019-02-27 05:01:36", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100909678467309568": {"author": "@ElectronNest", "followers": "230", "datetime": "2019-02-28 00:04:55", "content_summary": "RT @sharadvikram: Cool ICLR paper by Justin Fu et al, coming out of a Google AI internship! https://t.co/lWeFKoiTPn"}, "1098825834150924290": {"author": "@windx0303", "followers": "1,199", "datetime": "2019-02-22 06:04:28", "content_summary": "RT @arxiv_org: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Follo... https://t.co/XmkNbkeqOq https:/\u2026"}, "1100776074319806464": {"author": "@bcmcmahan", "followers": "563", "datetime": "2019-02-27 15:14:02", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101030256595484673": {"author": "@KrishaMehta2", "followers": "135", "datetime": "2019-02-28 08:04:03", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1099812233658261504": {"author": "@sguada", "followers": "1,436", "datetime": "2019-02-24 23:24:04", "content_summary": "Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu, Anoop Korattikara, @svlevine , @sguada now available in arxiv. https://t.co/O2WNjx6zJV"}, "1100894275414446082": {"author": "@miguelalonsojr", "followers": "1,068", "datetime": "2019-02-27 23:03:43", "content_summary": "RT @chelseabfinn: Neat work from Justin, Anoop, Sergey, and Sergio on enabling agents to learn from language instructions! https://t.co/1p4\u2026"}, "1100748230537760768": {"author": "@aadubredu", "followers": "63", "datetime": "2019-02-27 13:23:23", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100572754774941696": {"author": "@sguada", "followers": "1,436", "datetime": "2019-02-27 01:46:07", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100960852750942208": {"author": "@iamknighton", "followers": "426", "datetime": "2019-02-28 03:28:16", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100025492722216960": {"author": "@cghosh_", "followers": "278", "datetime": "2019-02-25 13:31:29", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1099156823267889153": {"author": "@bwrado1", "followers": "61", "datetime": "2019-02-23 03:59:42", "content_summary": "RT @arxiv_org: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Follo... https://t.co/XmkNbkeqOq https:/\u2026"}, "1100844974378110976": {"author": "@2000Qiu", "followers": "10", "datetime": "2019-02-27 19:47:49", "content_summary": "RT @chelseabfinn: Neat work from Justin, Anoop, Sergey, and Sergio on enabling agents to learn from language instructions! https://t.co/1p4\u2026"}, "1100560703398637568": {"author": "@ericjang11", "followers": "17,768", "datetime": "2019-02-27 00:58:13", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101288298125778944": {"author": "@hakuturu583", "followers": "1,742", "datetime": "2019-03-01 01:09:25", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101285808286326785": {"author": "@TheCoolMuseum", "followers": "4,080", "datetime": "2019-03-01 00:59:32", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101002035028205568": {"author": "@blissunplugged", "followers": "24", "datetime": "2019-02-28 06:11:55", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101897865327243264": {"author": "@and_papers", "followers": "18", "datetime": "2019-03-02 17:31:38", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100731265349812235": {"author": "@ceobillionaire", "followers": "163,562", "datetime": "2019-02-27 12:15:58", "content_summary": "RT @sharadvikram: Cool ICLR paper by Justin Fu et al, coming out of a Google AI internship! https://t.co/lWeFKoiTPn"}, "1101059898178191361": {"author": "@cole_hurwitz", "followers": "232", "datetime": "2019-02-28 10:01:51", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1099580964131368960": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-02-24 08:05:05", "content_summary": "RT @arxiv_org: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Follo... https://t.co/XmkNbkeqOq https:/\u2026"}, "1101354152108515328": {"author": "@jaguring1", "followers": "13,476", "datetime": "2019-03-01 05:31:06", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100575647536218113": {"author": "@RichEverts", "followers": "333", "datetime": "2019-02-27 01:57:36", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100611967532724227": {"author": "@Hossameldin400", "followers": "18", "datetime": "2019-02-27 04:21:56", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100640715057623040": {"author": "@jw1401VM", "followers": "8", "datetime": "2019-02-27 06:16:10", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100655073976299520": {"author": "@kadarakos", "followers": "277", "datetime": "2019-02-27 07:13:13", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101288155985006592": {"author": "@yuki_mimu", "followers": "1,182", "datetime": "2019-03-01 01:08:51", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1126904317703794688": {"author": "@NandoDF", "followers": "77,292", "datetime": "2019-05-10 17:38:20", "content_summary": "RT @sguada: @svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/O2WNjx6z\u2026"}, "1100625951732453378": {"author": "@saikrishna_gvs", "followers": "673", "datetime": "2019-02-27 05:17:30", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100733970910711809": {"author": "@unsorsodicorda", "followers": "738", "datetime": "2019-02-27 12:26:43", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101155885764743168": {"author": "@d_kangin", "followers": "118", "datetime": "2019-02-28 16:23:16", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1126917987338833920": {"author": "@farhanhubble", "followers": "159", "datetime": "2019-05-10 18:32:39", "content_summary": "RT @sguada: @svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/O2WNjx6z\u2026"}, "1100722859188592641": {"author": "@jmrozanec", "followers": "80", "datetime": "2019-02-27 11:42:34", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100638538264166401": {"author": "@jastner109", "followers": "194", "datetime": "2019-02-27 06:07:31", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101292274485407744": {"author": "@shiosainouta", "followers": "1,099", "datetime": "2019-03-01 01:25:13", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100640822096203782": {"author": "@jw1401VM", "followers": "8", "datetime": "2019-02-27 06:16:35", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100027619683299330": {"author": "@udmrzn", "followers": "1,348", "datetime": "2019-02-25 13:39:56", "content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Followin\u2026"}, "1098791054667669504": {"author": "@yapp1e", "followers": "50", "datetime": "2019-02-22 03:46:16", "content_summary": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG]) https://t.co/xMnvu8hl7k Reinforcement learning is a promising framework for solving control problems, but its use in practical situ"}, "1127542234826432513": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-05-12 11:53:12", "content_summary": "RT @sguada: @svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/O2WNjx6z\u2026"}, "1100759099740954624": {"author": "@IntelligenceTV", "followers": "56,127", "datetime": "2019-02-27 14:06:35", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101386872989790208": {"author": "@00001V", "followers": "78", "datetime": "2019-03-01 07:41:07", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1099343294411108353": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-02-23 16:20:40", "content_summary": "RT @StatMLPapers: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG\u2026"}, "1100612220759662592": {"author": "@Epsilon_Lee", "followers": "88", "datetime": "2019-02-27 04:22:56", "content_summary": "#rl"}, "1098783774555426816": {"author": "@BrundageBot", "followers": "3,903", "datetime": "2019-02-22 03:17:20", "content_summary": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama https://t.co/G9ZsbiiDjP"}, "1101308539971108865": {"author": "@KSKSKSKS2", "followers": "216", "datetime": "2019-03-01 02:29:51", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100922338361565184": {"author": "@KSKSKSKS2", "followers": "216", "datetime": "2019-02-28 00:55:14", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100748431960748038": {"author": "@chiefaiofficers", "followers": "239", "datetime": "2019-02-27 13:24:11", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1126530000352624642": {"author": "@bbc1183", "followers": "130", "datetime": "2019-05-09 16:50:56", "content_summary": "https://t.co/SN2f69dwoI"}, "1098765452069740547": {"author": "@StatMLPapers", "followers": "9,736", "datetime": "2019-02-22 02:04:32", "content_summary": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG]) https://t.co/KWyKtsAwWC"}, "1098827826260832257": {"author": "@arxivml", "followers": "783", "datetime": "2019-02-22 06:12:23", "content_summary": "\"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\", Justin Fu, Anoop K\u2026 https://t.co/lbwiYmyMIN"}, "1101026925194153984": {"author": "@NandoDF", "followers": "77,292", "datetime": "2019-02-28 07:50:49", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101314752087711744": {"author": "@ararabo", "followers": "1,331", "datetime": "2019-03-01 02:54:32", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100557982583029760": {"author": "@arxiv_pop", "followers": "712", "datetime": "2019-02-27 00:47:25", "content_summary": "2019/02/20 \u6295\u7a3f 4\u4f4d LG(Machine Learning) From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following https://t.co/ZJjrdyFRwz 7 Tweets 21 Retweets 69 Favorites"}, "1127208230768103424": {"author": "@tomaxent", "followers": "4", "datetime": "2019-05-11 13:45:59", "content_summary": "RT @sguada: @svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/O2WNjx6z\u2026"}, "1100602302354898945": {"author": "@EliSennesh", "followers": "650", "datetime": "2019-02-27 03:43:31", "content_summary": ">But people can communicate objectives to each other simply by describing or demonstrating them. So, how are people doing that? That's the interesting question!"}, "1100828153189085184": {"author": "@DanBarg", "followers": "13", "datetime": "2019-02-27 18:40:58", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100740039020040193": {"author": "@Otaboroid", "followers": "61", "datetime": "2019-02-27 12:50:50", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100861591636111360": {"author": "@SamiraEKahou", "followers": "599", "datetime": "2019-02-27 20:53:51", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100669239575367680": {"author": "@sysbiobot", "followers": "37", "datetime": "2019-02-27 08:09:30", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100026627160002560": {"author": "@EricSchles", "followers": "2,078", "datetime": "2019-02-25 13:36:00", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1101260825183035394": {"author": "@Montreal_IA", "followers": "159,742", "datetime": "2019-02-28 23:20:15", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100566446826876928": {"author": "@sharadvikram", "followers": "169", "datetime": "2019-02-27 01:21:03", "content_summary": "Cool ICLR paper by Justin Fu et al, coming out of a Google AI internship!"}, "1101488210000003074": {"author": "@wasp_dragon", "followers": "763", "datetime": "2019-03-01 14:23:48", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1127026688733220864": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-05-11 01:44:36", "content_summary": "RT @sguada: @svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/O2WNjx6z\u2026"}, "1100660928830492673": {"author": "@honasu", "followers": "275", "datetime": "2019-02-27 07:36:29", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100908950646648832": {"author": "@aditpandas", "followers": "37", "datetime": "2019-02-28 00:02:02", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101284848310734848": {"author": "@icoxfog417", "followers": "11,544", "datetime": "2019-03-01 00:55:43", "content_summary": "\u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u306fLSTM\u3067\u51e6\u7406\u3057\u753b\u50cf\u5074\u30d9\u30af\u30c8\u30eb\u3068\u5185\u7a4d\u3092\u53d6\u3063\u3066\u3044\u308b"}, "1099840008784097280": {"author": "@RubenEVillegas", "followers": "558", "datetime": "2019-02-25 01:14:26", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1100159912208928769": {"author": "@bgalbraith", "followers": "962", "datetime": "2019-02-25 22:25:37", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1100661731817414656": {"author": "@caglarml", "followers": "2,072", "datetime": "2019-02-27 07:39:40", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100778384349528064": {"author": "@__jm", "followers": "245", "datetime": "2019-02-27 15:23:12", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101291358600323072": {"author": "@hrs1985", "followers": "1,199", "datetime": "2019-03-01 01:21:35", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101316516253884418": {"author": "@shtsno24", "followers": "1,864", "datetime": "2019-03-01 03:01:33", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100617036521103360": {"author": "@HaoTan5", "followers": "198", "datetime": "2019-02-27 04:42:04", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100593478713503745": {"author": "@cghosh_", "followers": "278", "datetime": "2019-02-27 03:08:28", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1106043773258088449": {"author": "@theiscoresearch", "followers": "153", "datetime": "2019-03-14 04:05:59", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100911238312161280": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-02-28 00:11:07", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100566773475074048": {"author": "@tejscript", "followers": "73", "datetime": "2019-02-27 01:22:20", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100682329201160192": {"author": "@sandipanh05", "followers": "80", "datetime": "2019-02-27 09:01:31", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100618428866531328": {"author": "@andrey_kurenkov", "followers": "3,416", "datetime": "2019-02-27 04:47:36", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100163039956033537": {"author": "@_LXAI", "followers": "1,941", "datetime": "2019-02-25 22:38:03", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1100641688312197120": {"author": "@ayirpelle", "followers": "2,682", "datetime": "2019-02-27 06:20:02", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100713617236865025": {"author": "@miguelalonsojr", "followers": "1,068", "datetime": "2019-02-27 11:05:51", "content_summary": "RT @sharadvikram: Cool ICLR paper by Justin Fu et al, coming out of a Google AI internship! https://t.co/lWeFKoiTPn"}, "1100710085695696896": {"author": "@stl950116", "followers": "11", "datetime": "2019-02-27 10:51:49", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101296130430652416": {"author": "@kav_kazuna", "followers": "31", "datetime": "2019-03-01 01:40:33", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1101271648114696192": {"author": "@PerthMLGroup", "followers": "456", "datetime": "2019-03-01 00:03:16", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100584836769685505": {"author": "@j0nlussier", "followers": "916", "datetime": "2019-02-27 02:34:07", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1098890506170445824": {"author": "@indy9000", "followers": "356", "datetime": "2019-02-22 10:21:27", "content_summary": "RT @arxiv_org: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Follo... https://t.co/XmkNbkeqOq https:/\u2026"}, "1100220096822370304": {"author": "@miguelalonsojr", "followers": "1,068", "datetime": "2019-02-26 02:24:46", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1100563231171641344": {"author": "@abhshkdz", "followers": "3,216", "datetime": "2019-02-27 01:08:16", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100563464844701701": {"author": "@miguelalonsojr", "followers": "1,068", "datetime": "2019-02-27 01:09:12", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101368057551675393": {"author": "@LazyOp", "followers": "10", "datetime": "2019-03-01 06:26:22", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1126885813831098368": {"author": "@sguada", "followers": "1,436", "datetime": "2019-05-10 16:24:49", "content_summary": "@svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/O2WNjx6zJV"}, "1101275806624436224": {"author": "@flrgsr", "followers": "304", "datetime": "2019-03-01 00:19:47", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1098770113526988800": {"author": "@udmrzn", "followers": "1,348", "datetime": "2019-02-22 02:23:03", "content_summary": "RT @StatMLPapers: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG\u2026"}, "1104918283566735360": {"author": "@maru8xx", "followers": "12", "datetime": "2019-03-11 01:33:41", "content_summary": "https://t.co/5m8E1iNSvI #\u8aad\u3080"}, "1100888751964712961": {"author": "@Bihua_chen", "followers": "36", "datetime": "2019-02-27 22:41:46", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100619575471812608": {"author": "@bhargavbardipur", "followers": "431", "datetime": "2019-02-27 04:52:09", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100618107465285632": {"author": "@tomaxent", "followers": "4", "datetime": "2019-02-27 04:46:19", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100595637211594757": {"author": "@goloskokovic", "followers": "131", "datetime": "2019-02-27 03:17:02", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100845722843340800": {"author": "@NLP_Grayming", "followers": "25", "datetime": "2019-02-27 19:50:47", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1099935240632627200": {"author": "@DocXavi", "followers": "1,923", "datetime": "2019-02-25 07:32:51", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1100020329697763329": {"author": "@bousmalis", "followers": "3,356", "datetime": "2019-02-25 13:10:58", "content_summary": "RT @sguada: Our @iclr2019 paper \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" Justin Fu,\u2026"}, "1099717579738963968": {"author": "@arXiv__ml", "followers": "1,779", "datetime": "2019-02-24 17:07:57", "content_summary": "#arXiv #machinelearning [cs.LG] From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG]) https://t.co/oPbcIy4bpl Reinforcement learning is a promising framework for solving control problem"}, "1101427300959023104": {"author": "@ElectronNest", "followers": "230", "datetime": "2019-03-01 10:21:46", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100820338139488257": {"author": "@OmarUFlorez", "followers": "1,237", "datetime": "2019-02-27 18:09:55", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1099329031957180416": {"author": "@AssistedEvolve", "followers": "219", "datetime": "2019-02-23 15:24:00", "content_summary": "RT @arxiv_org: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Follo... https://t.co/XmkNbkeqOq https:/\u2026"}, "1098782282767306752": {"author": "@deep_rl", "followers": "862", "datetime": "2019-02-22 03:11:25", "content_summary": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following - Justin Fu https://t.co/U3x11wLljK"}, "1098825204325744640": {"author": "@arxiv_org", "followers": "12,787", "datetime": "2019-02-22 06:01:58", "content_summary": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Follo... https://t.co/XmkNbkeqOq https://t.co/pREa0vULjT"}, "1099735017666826245": {"author": "@mlmemoirs", "followers": "1,269", "datetime": "2019-02-24 18:17:14", "content_summary": "#arXiv #machinelearning [cs.LG] From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG]) https://t.co/bWDLR0mOHr Reinforcement learning is a promising framework for solving control problem"}, "1100858797138931713": {"author": "@vasan_ashwin", "followers": "36", "datetime": "2019-02-27 20:42:44", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1098772125568126976": {"author": "@minsuk_chang", "followers": "758", "datetime": "2019-02-22 02:31:03", "content_summary": "RT @StatMLPapers: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG\u2026"}, "1101260542851903492": {"author": "@Quebec_AI", "followers": "160,304", "datetime": "2019-02-28 23:19:08", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1100570133880283136": {"author": "@waynemystir", "followers": "58", "datetime": "2019-02-27 01:35:42", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1177257913426927618": {"author": "@subhobrata1", "followers": "307", "datetime": "2019-09-26 16:25:33", "content_summary": "RT @chelseabfinn: Neat work from Justin, Anoop, Sergey, and Sergio on enabling agents to learn from language instructions! https://t.co/1p4\u2026"}, "1100585891469365248": {"author": "@ceobillionaire", "followers": "163,562", "datetime": "2019-02-27 02:38:19", "content_summary": "RT @svlevine: Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruc\u2026"}, "1101285087872606208": {"author": "@Miyaran99", "followers": "357", "datetime": "2019-03-01 00:56:40", "content_summary": "RT @icoxfog417: \u9006\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306b\u3001\u63a8\u5b9a\u5bfe\u8c61\u3067\u3042\u308b\u5831\u916c\u95a2\u6570\u306b\u8a00\u8a9e\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u3051\u3092\u5c0e\u5165\u3057\u305f\u7814\u7a76(\u7aef\u7684\u306b\u306f\u3001\u6307\u793a\u901a\u308a\u52d5\u3051\u305f\u3089\u5831\u916c\u304c\u4e0e\u3048\u3089\u308c\u308b)\u3002\u8a00\u8a9e\u306b\u3088\u308b\u6307\u793a\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u5831\u916c\u306e\u8ee2\u79fb\u6027\u304c\u9ad8\u304f\u306a\u308b\u3068\u306e\u3053\u3068\u3002\u624b\u6cd5\u306fMax Entropy\u30d9\u30fc\u30b9\u3067\u3001\u8a00\u8a9e\u6307\u793a\u2026"}, "1100570237164969984": {"author": "@EloraHL", "followers": "305", "datetime": "2019-02-27 01:36:06", "content_summary": "RT @sharadvikram: Cool ICLR paper by Justin Fu et al, coming out of a Google AI internship! https://t.co/lWeFKoiTPn"}, "1099068653553606656": {"author": "@Bihua_chen", "followers": "36", "datetime": "2019-02-22 22:09:21", "content_summary": "RT @StatMLPapers: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. (arXiv:1902.07742v1 [cs.LG\u2026"}, "1128028909369344000": {"author": "@berkeley_ai", "followers": "31,251", "datetime": "2019-05-13 20:07:04", "content_summary": "@FarshidFaal @svlevine \"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following\" https://t.co/PBZV7O32l1"}}, "citation_id": "55872986", "queriedAt": "2020-06-04 00:02:40"}