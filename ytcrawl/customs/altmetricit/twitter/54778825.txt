{"citation_id": "54778825", "completed": "1", "queriedAt": "2020-05-14 11:19:13", "tab": "twitter", "twitter": {"1091344639443910656": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "225", "datetime": "2019-02-01 14:36:52", "author": "@sagarpath"}, "1092844775642779649": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "4,825", "datetime": "2019-02-05 17:57:53", "author": "@prototechno"}, "1095298368199368705": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "489", "datetime": "2019-02-12 12:27:35", "author": "@valentinilla"}, "1091576974982623232": {"content_summary": "The Evolved Transformer https://t.co/5cQQRCHgW1", "followers": "2,042", "datetime": "2019-02-02 06:00:06", "author": "@imenurok"}, "1091417321258786816": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "46", "datetime": "2019-02-01 19:25:41", "author": "@helboukkouri"}, "1095480241743630336": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "19", "datetime": "2019-02-13 00:30:17", "author": "@MassBassLol"}, "1091475816477835265": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "770", "datetime": "2019-02-01 23:18:07", "author": "@aCraigPfeifer"}, "1095179553234800641": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "5,903", "datetime": "2019-02-12 04:35:27", "author": "@enakai00"}, "1091379115221934086": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "388", "datetime": "2019-02-01 16:53:52", "author": "@adropboxspace"}, "1091652175078420482": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "98", "datetime": "2019-02-02 10:58:55", "author": "@treasured_write"}, "1091977096950251520": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "8,922", "datetime": "2019-02-03 08:30:02", "author": "@ailurus1991"}, "1091268904305844224": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "52", "datetime": "2019-02-01 09:35:56", "author": "@hoangcuong0605"}, "1095489875162292224": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "613", "datetime": "2019-02-13 01:08:34", "author": "@KouroshMeshgi"}, "1091497867930263553": {"content_summary": "RT @fabmilo: #AI #DeepLearning #Evolution & #AutoML applied to #seq2seq creates a better, more accurate, more efficient architecture than t\u2026", "followers": "4,884", "datetime": "2019-02-02 00:45:45", "author": "@martin_wicke"}, "1092796409814884353": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "149", "datetime": "2019-02-05 14:45:41", "author": "@y_yammt"}, "1095426140683026432": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "42", "datetime": "2019-02-12 20:55:18", "author": "@MARTINSARABIA16"}, "1094162560796872704": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "21", "datetime": "2019-02-09 09:14:17", "author": "@kacky24"}, "1091205838348333056": {"content_summary": "RT @evolvingstuff: The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Tra\u2026", "followers": "4,825", "datetime": "2019-02-01 05:25:20", "author": "@prototechno"}, "1091370970466533377": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "21", "datetime": "2019-02-01 16:21:30", "author": "@binh_vu54321"}, "1091619274253352961": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "732", "datetime": "2019-02-02 08:48:10", "author": "@PeleteiroAna"}, "1095315728285093888": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "245", "datetime": "2019-02-12 13:36:34", "author": "@__jm"}, "1093612423305662475": {"content_summary": "We've also extended the method with some simple evolutionary algorithm techniques (that's the blue line in the chart) similar to https://t.co/FNfrkr9GVS and https://t.co/eC3D49PMKe \u2013 the two work really well together, especially as the algorithm progresses", "followers": "2,228", "datetime": "2019-02-07 20:48:14", "author": "@hichaelmart"}, "1091203719436283904": {"content_summary": "The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B\" ht", "followers": "2,925", "datetime": "2019-02-01 05:16:54", "author": "@evolvingstuff"}, "1141679127960752129": {"content_summary": "RT @DataScienceNIG: TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, lev\u2026", "followers": "98", "datetime": "2019-06-20 12:08:10", "author": "@treasured_write"}, "1095930632666894336": {"content_summary": "\uc5b4\ub5bb\uac8c \uc0dd\uac01\ud574 \ubcf4\uba74 computation\uc744 \ub54c\ub824\ubc15\uc544 neural architecture search\ub97c \ud574\ub3c4 \uc0ac\ub78c\uc774 \ucd5c\uc801\ud654\ud55c \uac83\uc5d0 \ube44\ud574 \ucd5c\uc885 \uc131\ub2a5\uc740 \ud070 \ucc28\uc774\uac00 \uc5c6\ub2e4\uace0 \ubcfc \uc218\ub3c4 \uc788\uc74c. \uc5b4\ucc28\ud53c \uc0ac\ub78c\uc774 Transformer\ub97c \ub514\uc790\uc778\ud560 \ub54c small size\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ucd5c\uc801\ud654\ud558\ub824\uace0 \ub514\uc790\uc778\ud55c \uac83\ub3c4 \uc544\ub2d0 \ud14c\uace0.", "followers": "8,523", "datetime": "2019-02-14 06:19:58", "author": "@d_ijk_stra"}, "1092105545765539840": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "3,515", "datetime": "2019-02-03 17:00:27", "author": "@Haavepaja"}, "1095177251040112641": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "151,453", "datetime": "2019-02-12 04:26:18", "author": "@JeffDean"}, "1091489892293595136": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "68", "datetime": "2019-02-02 00:14:03", "author": "@diegovogeid"}, "1097687430055030784": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-19 02:40:52", "author": "@arxiv_cscl"}, "1092872456233086976": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "93", "datetime": "2019-02-05 19:47:52", "author": "@0xhexhex"}, "1095222196371378176": {"content_summary": "Amazing! Did you test on other tasks? I\u2019d love to know whether the new architecture is task optimal or globally optimal.", "followers": "604", "datetime": "2019-02-12 07:24:54", "author": "@JustinTimesUK"}, "1091583612204523520": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "241", "datetime": "2019-02-02 06:26:28", "author": "@Heavy02011"}, "1095239198351679488": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "216", "datetime": "2019-02-12 08:32:28", "author": "@KSKSKSKS2"}, "1092446427974979584": {"content_summary": "RT @mino98: \"The Evolved Transformer\" (https://t.co/GPXQUUl8gX): hopefully useful to squeeze out little (?) gains on smaller models... but\u2026", "followers": "5,338", "datetime": "2019-02-04 15:34:59", "author": "@100DaysOfMLCode"}, "1095203903577051136": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "766", "datetime": "2019-02-12 06:12:13", "author": "@narphorium"}, "1095197578818408448": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "227", "datetime": "2019-02-12 05:47:05", "author": "@cruseakshay"}, "1091510546636070913": {"content_summary": "Our new work using evolution to discover better Transformer architecture.", "followers": "35", "datetime": "2019-02-02 01:36:08", "author": "@crazydonkey200"}, "1092873820250136577": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "296", "datetime": "2019-02-05 19:53:18", "author": "@rose_miura"}, "1091311110093967360": {"content_summary": "The Evolved Transformer Paper by So et al.: https://t.co/Kyj84MwOme #artificalintelligence #MachineLearning #NeuralComputing #EvolutionaryComputing #research", "followers": "179,056", "datetime": "2019-02-01 12:23:38", "author": "@Montreal_AI"}, "1092203788314660865": {"content_summary": "> Progressive Dynamic Hurdles \u4e00\u7a2e\u306eGrid Search\u306b\u898b\u3048\u308b\u306e\u3060\u304c\u30fb\u30fb\u30fb\uff1f The Evolved Transformer https://t.co/xHRiFoocHS", "followers": "1,673", "datetime": "2019-02-03 23:30:49", "author": "@keigohtr"}, "1095160634151325703": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "767", "datetime": "2019-02-12 03:20:16", "author": "@bradenjhancock"}, "1091265905944813569": {"content_summary": "The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smaller, mobile-friendly, Evolved Transformer with only ~7M parameters outperforms the original Transformer by 0.7 BLEU on WMT14 EN-DE.\u201d", "followers": "81,077", "datetime": "2019-02-01 09:24:01", "author": "@hardmaru"}, "1095208227527045120": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "2,310", "datetime": "2019-02-12 06:29:24", "author": "@Vikram_Tiwari"}, "1091744139585163265": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "172", "datetime": "2019-02-02 17:04:21", "author": "@ggdupont"}, "1095197814827728896": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "83", "datetime": "2019-02-12 05:48:01", "author": "@FlorianDreher"}, "1146005324375023617": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "1,143", "datetime": "2019-07-02 10:38:55", "author": "@iBotamon"}, "1093716171487170560": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-08 03:40:30", "author": "@arxiv_cscl"}, "1091399221897498626": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "372", "datetime": "2019-02-01 18:13:46", "author": "@Gab___riela"}, "1095243704804171776": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "366", "datetime": "2019-02-12 08:50:22", "author": "@ashishawasthi"}, "1094486256900292609": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-10 06:40:32", "author": "@arxiv_cscl"}, "1140251935724883968": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "180", "datetime": "2019-06-16 13:37:00", "author": "@toshi033"}, "1092799238323560448": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "1,247", "datetime": "2019-02-05 14:56:56", "author": "@daisuzu"}, "1091424468168331264": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "649", "datetime": "2019-02-01 19:54:05", "author": "@cortexelation"}, "1092503596460064768": {"content_summary": "\"The Evolved Transformer\", David R\uff0e So, Chen Liang, Quoc V\uff0e Le https://t.co/IuU1nsgHij", "followers": "767", "datetime": "2019-02-04 19:22:09", "author": "@arxivml"}, "1091696527431618562": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,063", "datetime": "2019-02-02 13:55:09", "author": "@miguelalonsojr"}, "1092492722676600833": {"content_summary": "RT @mino98: \"The Evolved Transformer\" (https://t.co/GPXQUUl8gX): hopefully useful to squeeze out little (?) gains on smaller models... but\u2026", "followers": "73", "datetime": "2019-02-04 18:38:57", "author": "@nitincodes"}, "1092876273137811456": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "273", "datetime": "2019-02-05 20:03:02", "author": "@morinosuke__p"}, "1095679423402119169": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "66", "datetime": "2019-02-13 13:41:45", "author": "@shubh_300595"}, "1141046112142471169": {"content_summary": "Google\u2019s Evolved Transformer translation tasks https://t.co/xEaoMXAkBO https://t.co/HugWwSLSx2 https://t.co/oumkpVC09y https://t.co/LOvjBiNjYV", "followers": "5,524", "datetime": "2019-06-18 18:12:47", "author": "@cackerman1"}, "1091273188363915264": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,456", "datetime": "2019-02-01 09:52:57", "author": "@Gabriel_Oguna"}, "1091368518434590720": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "340", "datetime": "2019-02-01 16:11:46", "author": "@Cedias_"}, "1091293991067361280": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "164,152", "datetime": "2019-02-01 11:15:37", "author": "@ceobillionaire"}, "1140197773024714754": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "220", "datetime": "2019-06-16 10:01:47", "author": "@eiichisugiyama1"}, "1091504995218862081": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "215", "datetime": "2019-02-02 01:14:04", "author": "@AssistedEvolve"}, "1092803042934697984": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,247", "datetime": "2019-02-05 15:12:03", "author": "@daisuzu"}, "1091342728473071619": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "165", "datetime": "2019-02-01 14:29:17", "author": "@dbparedes"}, "1091259867195674624": {"content_summary": "RT @evolvingstuff: The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Tra\u2026", "followers": "4,254", "datetime": "2019-02-01 09:00:01", "author": "@weballergy"}, "1091402918660579329": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "399", "datetime": "2019-02-01 18:28:27", "author": "@djsaunde"}, "1091394555981766656": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,882", "datetime": "2019-02-01 17:55:13", "author": "@ThomasScialom"}, "1091179826235490305": {"content_summary": "\"The Evolved Transformer\", David R\uff0e So, Chen Liang, Quoc V\uff0e Le https://t.co/IuU1nsgHij", "followers": "767", "datetime": "2019-02-01 03:41:58", "author": "@arxivml"}, "1097749014542471168": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "23", "datetime": "2019-02-19 06:45:34", "author": "@bairn_moon"}, "1130846094131261440": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BB0A4", "followers": "3,489", "datetime": "2019-05-21 14:41:33", "author": "@arxiv_cscl"}, "1091438585276964867": {"content_summary": "Just like neural networks, genetic algorithms are enjoying a renaissance which is cool to see", "followers": "3,872", "datetime": "2019-02-01 20:50:11", "author": "@jpatanooga"}, "1106851304062750720": {"content_summary": "The Evolved Transformer https://t.co/6vCqxpUFul", "followers": "1,425", "datetime": "2019-03-16 09:34:49", "author": "@jreuben1"}, "1091631225058529280": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "45", "datetime": "2019-02-02 09:35:40", "author": "@artuskg"}, "1141727191035731973": {"content_summary": "\ud83d\udc40", "followers": "392", "datetime": "2019-06-20 15:19:09", "author": "@Felipessalvador"}, "1095581858652291072": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,937", "datetime": "2019-02-13 07:14:04", "author": "@vcjha"}, "1091499327166410752": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "113", "datetime": "2019-02-02 00:51:33", "author": "@tranlaman"}, "1091462298793697280": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "296", "datetime": "2019-02-01 22:24:25", "author": "@rose_miura"}, "1095399890237227009": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "378", "datetime": "2019-02-12 19:11:00", "author": "@RyanAEMetz"}, "1092983639363604486": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "7", "datetime": "2019-02-06 03:09:40", "author": "@mindscalehq"}, "1095235168053403649": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "331", "datetime": "2019-02-12 08:16:27", "author": "@pywirrarika"}, "1091483429001031680": {"content_summary": "#AI #DeepLearning #Evolution & #AutoML applied to #seq2seq creates a better, more accurate, more efficient architecture than the best #human #made. With \"only\" 16 TPU v2", "followers": "440", "datetime": "2019-02-01 23:48:22", "author": "@fabmilo"}, "1140347775021772800": {"content_summary": "Applying AutoML to Transformer Architectures | Google AI Blog https://t.co/m25RlH2h6j https://t.co/P5s0dYTRfj #NLP #DeepLearning https://t.co/0kbf2c6Djp", "followers": "1,128", "datetime": "2019-06-16 19:57:50", "author": "@BioDecoded"}, "1095177565034012672": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "2,098", "datetime": "2019-02-12 04:27:33", "author": "@toru_inoue"}, "1091765628665561089": {"content_summary": "#arXiv #machinelearning [cs.LG] The Evolved Transformer. (arXiv:1901.11117v1 [cs.LG]) https://t.co/ySOiqBgiMZ Recent works have highlighted the strengths of the Transformer architecture for dealing with sequence tasks. At the same time, neural architectur", "followers": "1,700", "datetime": "2019-02-02 18:29:44", "author": "@arXiv__ml"}, "1091198193969188864": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "267", "datetime": "2019-02-01 04:54:57", "author": "@kotti_sasikanth"}, "1091956088088481793": {"content_summary": "Transformers architectures like BERT are gigantic beasts atm. Making them smaller/faster is worth the effort and simplifies their application! Good effort here!", "followers": "29", "datetime": "2019-02-03 07:06:33", "author": "@luckluckierluke"}, "1091287227466842113": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "4,615", "datetime": "2019-02-01 10:48:44", "author": "@HITStales"}, "1092232593288835074": {"content_summary": "Will this be the answer to serving Transformer-based models for the companies without TPU?", "followers": "79", "datetime": "2019-02-04 01:25:17", "author": "@SijunHe"}, "1093408708648935424": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "6", "datetime": "2019-02-07 07:18:45", "author": "@XenonDim"}, "1091483602133663744": {"content_summary": "RT @fabmilo: #AI #DeepLearning #Evolution & #AutoML applied to #seq2seq creates a better, more accurate, more efficient architecture than t\u2026", "followers": "2,136", "datetime": "2019-02-01 23:49:04", "author": "@theChrisChua"}, "1095335497860145152": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "296", "datetime": "2019-02-12 14:55:07", "author": "@rose_miura"}, "1140386770841829376": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "697", "datetime": "2019-06-16 22:32:48", "author": "@takatoh1"}, "1097868730011435009": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-19 14:41:17", "author": "@arxiv_cscl"}, "1095202595239473152": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "148", "datetime": "2019-02-12 06:07:01", "author": "@zouwei_cn"}, "1140406559584841728": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "7", "datetime": "2019-06-16 23:51:26", "author": "@cactoid"}, "1140241477227839488": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "633", "datetime": "2019-06-16 12:55:27", "author": "@rkakamilan"}, "1095195570728140800": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,344", "datetime": "2019-02-12 05:39:06", "author": "@jesse_vig"}, "1095187534034976769": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "844", "datetime": "2019-02-12 05:07:10", "author": "@candidosales"}, "1095175367181123584": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "164,152", "datetime": "2019-02-12 04:18:49", "author": "@ceobillionaire"}, "1091459890520809472": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "93", "datetime": "2019-02-01 22:14:50", "author": "@0xhexhex"}, "1096502332232212480": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "635", "datetime": "2019-02-15 20:11:42", "author": "@lievAnastazia"}, "1093086665348792321": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "226", "datetime": "2019-02-06 09:59:04", "author": "@ElectronNest"}, "1092213892585910272": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "5,545", "datetime": "2019-02-04 00:10:59", "author": "@__MLT__"}, "1091607466155008000": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,410", "datetime": "2019-02-02 08:01:15", "author": "@RexDouglass"}, "1093701097296846849": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BB0A4", "followers": "3,489", "datetime": "2019-02-08 02:40:36", "author": "@arxiv_cscl"}, "1091183765454434304": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "99,844", "datetime": "2019-02-01 03:57:37", "author": "@jeremyphoward"}, "1095242567111692288": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "29", "datetime": "2019-02-12 08:45:51", "author": "@chenlailin"}, "1095359514872672259": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "723", "datetime": "2019-02-12 16:30:33", "author": "@ArchieIndian"}, "1091714980280320000": {"content_summary": "Great result of Evolved Transformer, especially for mobile-friendly ~7M params model. The only concern is its architecture search likely takes another explaining step away from human (!?), given that DL in its current status is already blackbox...", "followers": "45", "datetime": "2019-02-02 15:08:29", "author": "@yoquankara"}, "1091398521998127106": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "7", "datetime": "2019-02-01 18:10:59", "author": "@Akash16408623"}, "1091197378852311040": {"content_summary": "The Evolved Transformer https://t.co/akr2N1dUx0", "followers": "34,267", "datetime": "2019-02-01 04:51:43", "author": "@fantasysite"}, "1091352864620929027": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "103", "datetime": "2019-02-01 15:09:33", "author": "@Coder26237717"}, "1091430031107207169": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "141", "datetime": "2019-02-01 20:16:11", "author": "@mahesh_goud"}, "1092044580093128705": {"content_summary": "RT @fabmilo: #AI #DeepLearning #Evolution & #AutoML applied to #seq2seq creates a better, more accurate, more efficient architecture than t\u2026", "followers": "164,152", "datetime": "2019-02-03 12:58:11", "author": "@ceobillionaire"}, "1091515276338683905": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "21,733", "datetime": "2019-02-02 01:54:55", "author": "@vaaaaanquish"}, "1095466098110222336": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "360", "datetime": "2019-02-12 23:34:05", "author": "@amansoni"}, "1091364738309607429": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "15", "datetime": "2019-02-01 15:56:44", "author": "@The_UBD"}, "1091195704712687616": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "613", "datetime": "2019-02-01 04:45:04", "author": "@KouroshMeshgi"}, "1095489621809491969": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "22", "datetime": "2019-02-13 01:07:33", "author": "@lirong11"}, "1091485931389865984": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "111", "datetime": "2019-02-01 23:58:19", "author": "@ID9112"}, "1091371648404594689": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "27", "datetime": "2019-02-01 16:24:12", "author": "@mahesh21aug"}, "1091600947673993216": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "7,014", "datetime": "2019-02-02 07:35:21", "author": "@mscavazzin"}, "1091369308964368384": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "608", "datetime": "2019-02-01 16:14:54", "author": "@thtrieu_"}, "1141694409081282560": {"content_summary": "RT @DataScienceNIG: TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, lev\u2026", "followers": "72", "datetime": "2019-06-20 13:08:53", "author": "@David_veracruz"}, "1095232254756962306": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "47", "datetime": "2019-02-12 08:04:52", "author": "@xuechen1994"}, "1091445563416039424": {"content_summary": "RT @jpatanooga: Just like neural networks, genetic algorithms are enjoying a renaissance which is cool to see https://t.co/JkemSA1bKX", "followers": "1,080", "datetime": "2019-02-01 21:17:55", "author": "@Atabey_Kaygun"}, "1141612596392595456": {"content_summary": "RT @DataScienceNIG: TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, lev\u2026", "followers": "416", "datetime": "2019-06-20 07:43:47", "author": "@FunminiyiOlada1"}, "1092862772252151808": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "163", "datetime": "2019-02-05 19:09:23", "author": "@AaronGauthier1"}, "1093689223885045765": {"content_summary": "https://t.co/RV9gcN8uER The Evolved Transformer. (arXiv:1901.11117v2 [cs.LG] UPDATED) #NLProc", "followers": "4,198", "datetime": "2019-02-08 01:53:25", "author": "@arxiv_cs_cl"}, "1095195371343618048": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "35", "datetime": "2019-02-12 05:38:18", "author": "@tianshi5940"}, "1095169069400186880": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "129", "datetime": "2019-02-12 03:53:48", "author": "@itscientist"}, "1091348061291311105": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "824", "datetime": "2019-02-01 14:50:28", "author": "@morioka"}, "1140241114307239936": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "72", "datetime": "2019-06-16 12:54:00", "author": "@semiinvariant"}, "1091369022032175104": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "194", "datetime": "2019-02-01 16:13:46", "author": "@jastner109"}, "1092728995995475968": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "226", "datetime": "2019-02-05 10:17:49", "author": "@ElectronNest"}, "1091343310499672064": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "564", "datetime": "2019-02-01 14:31:36", "author": "@gazay"}, "1091266545701908481": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "229", "datetime": "2019-02-01 09:26:33", "author": "@m3yrin"}, "1092624951109025792": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "824", "datetime": "2019-02-05 03:24:22", "author": "@morioka"}, "1095282929029140481": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "36", "datetime": "2019-02-12 11:26:14", "author": "@kuz44ma69"}, "1091266147230593024": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "13,987", "datetime": "2019-02-01 09:24:58", "author": "@_rockt"}, "1092624954187608064": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "431", "datetime": "2019-02-05 03:24:23", "author": "@kawauso_kun"}, "1092053301799993344": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "456", "datetime": "2019-02-03 13:32:51", "author": "@PerthMLGroup"}, "1098077640743763968": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "11", "datetime": "2019-02-20 04:31:25", "author": "@AmoomalExpert"}, "1091335538253021184": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "97", "datetime": "2019-02-01 14:00:43", "author": "@kgwmath"}, "1091393672338460672": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,602", "datetime": "2019-02-01 17:51:43", "author": "@alxcnwy"}, "1091319445555838976": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "2,071", "datetime": "2019-02-01 12:56:46", "author": "@EricSchles"}, "1130861043415171077": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-05-21 15:40:57", "author": "@arxiv_cscl"}, "1091342693941293062": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "592", "datetime": "2019-02-01 14:29:09", "author": "@codekee"}, "1095222620453302273": {"content_summary": "Also, is there any explanation / intuition as to why it prefers odd numbers of convolutions?", "followers": "604", "datetime": "2019-02-12 07:26:35", "author": "@JustinTimesUK"}, "1091283835315253248": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "172", "datetime": "2019-02-01 10:35:16", "author": "@ggdupont"}, "1091325145908535297": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "36", "datetime": "2019-02-01 13:19:25", "author": "@kuz44ma69"}, "1092640732157001728": {"content_summary": "RT @arxiv_in_review: #ICML2019 The Evolved Transformer. (arXiv:1901.11117v1 [cs\\.LG]) https://t.co/dyPgfDZIkj", "followers": "782", "datetime": "2019-02-05 04:27:05", "author": "@muktabh"}, "1091269247483797505": {"content_summary": "We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Transformer itself. The found architecture, Evolved Transformer, is better and more efficient, especially for small size models. Link:", "followers": "18,260", "datetime": "2019-02-01 09:37:18", "author": "@quocleix"}, "1091375511182499842": {"content_summary": "RT @evolvingstuff: The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Tra\u2026", "followers": "266", "datetime": "2019-02-01 16:39:33", "author": "@brandonkindred"}, "1091401802229137408": {"content_summary": "The Evolved Transformer - nuovo risultato per la neuroevolution: \"At a much smaller \u2013 mobile-friendly \u2013 model size of \u223c7M parameters, the Evolved Transformer outperforms the Transformer by 0.7 BLEU on WMT\u201914 EnglishGerman.\"", "followers": "700", "datetime": "2019-02-01 18:24:01", "author": "@iaml_it"}, "1091334681608507393": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "190", "datetime": "2019-02-01 13:57:18", "author": "@oneinfinitezero"}, "1092648622318006272": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "16", "datetime": "2019-02-05 04:58:26", "author": "@honyohonyopo"}, "1091155729057304577": {"content_summary": "The Evolved Transformer. (arXiv:1901.11117v1 [cs.LG]) https://t.co/IBP8kUrDZT Recent works have highlighted the strengths of the Transformer architecture for dealing with sequence tasks. At the same time, neural architecture search has advanced to the poi", "followers": "48", "datetime": "2019-02-01 02:06:13", "author": "@yapp1e"}, "1092229411200532480": {"content_summary": "https://t.co/RV9gcN8uER The Evolved Transformer. (arXiv:1901.11117v1 [cs.LG]) #NLProc", "followers": "4,198", "datetime": "2019-02-04 01:12:38", "author": "@arxiv_cs_cl"}, "1138602588075569152": {"content_summary": "A good example was David So with \u00abThe evolved transformer\u00bb https://t.co/6YyH6tTqYH https://t.co/sG76kt3PkH", "followers": "5,874", "datetime": "2019-06-12 00:23:05", "author": "@alfcnz"}, "1091169410436390912": {"content_summary": "\"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "25,578", "datetime": "2019-02-01 03:00:35", "author": "@Miles_Brundage"}, "1095761142054408192": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "2,535", "datetime": "2019-02-13 19:06:29", "author": "@npinto"}, "1092670019194871808": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "150", "datetime": "2019-02-05 06:23:28", "author": "@ranadayo"}, "1091611896115937280": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "61", "datetime": "2019-02-02 08:18:51", "author": "@nickmaxkelsen"}, "1091223300242661376": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "335", "datetime": "2019-02-01 06:34:43", "author": "@TheLeanAcademic"}, "1140144290653298688": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "1,762", "datetime": "2019-06-16 06:29:16", "author": "@jaialkdanel"}, "1091402371996008448": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "734", "datetime": "2019-02-01 18:26:17", "author": "@unsorsodicorda"}, "1091269973018763267": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "206", "datetime": "2019-02-01 09:40:11", "author": "@neil_uai"}, "1092446404465909762": {"content_summary": "\"The Evolved Transformer\" (https://t.co/GPXQUUl8gX): hopefully useful to squeeze out little (?) gains on smaller models... but when papers casually mention 16xTPUv2 and 8xP100 GPUs and hundreds of millions weights, then AI feudalism is real! \ud83d\ude2f #100DaysOfM", "followers": "189", "datetime": "2019-02-04 15:34:54", "author": "@mino98"}, "1091318005076762624": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "821", "datetime": "2019-02-01 12:51:02", "author": "@ErmiaBivatan"}, "1091358115826622464": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "274", "datetime": "2019-02-01 15:30:25", "author": "@cghosh_"}, "1091824032264187904": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,348", "datetime": "2019-02-02 22:21:49", "author": "@udmrzn"}, "1091276167422844928": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,058", "datetime": "2019-02-01 10:04:47", "author": "@DanielOCL"}, "1091708432850272256": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "", "datetime": "2019-02-02 14:42:28", "author": "@AaaLee"}, "1091921280561893376": {"content_summary": "ANN architecture search for Transformer network:", "followers": "26", "datetime": "2019-02-03 04:48:14", "author": "@6963616e74646f7"}, "1091571007268151296": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "415", "datetime": "2019-02-02 05:36:23", "author": "@rahulunair"}, "1091190056122007552": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "291", "datetime": "2019-02-01 04:22:37", "author": "@Shujian_Liu"}, "1092987548563955714": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "380", "datetime": "2019-02-06 03:25:12", "author": "@harujoh"}, "1095225526749253632": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "13", "datetime": "2019-02-12 07:38:08", "author": "@2000Qiu"}, "1092997020518772737": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "296", "datetime": "2019-02-06 04:02:51", "author": "@rose_miura"}, "1138673475957264384": {"content_summary": "RT @alfcnz: A good example was David So with \u00abThe evolved transformer\u00bb https://t.co/6YyH6tTqYH https://t.co/sG76kt3PkH", "followers": "353", "datetime": "2019-06-12 05:04:46", "author": "@indy9000"}, "1095453958385082368": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "234", "datetime": "2019-02-12 22:45:50", "author": "@atlanticailabs"}, "1140077278958772225": {"content_summary": "LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u3067\u3082\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u5206\u304b\u308c\u3066\u3044\u308b\u3002 https://t.co/teMRdJahlX", "followers": "3,311", "datetime": "2019-06-16 02:02:59", "author": "@yoh_okuno"}, "1091584779642822656": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "8", "datetime": "2019-02-02 06:31:06", "author": "@tchaye59"}, "1091414310784561153": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "158", "datetime": "2019-02-01 19:13:43", "author": "@farhanhubble"}, "1091511508956336128": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "557", "datetime": "2019-02-02 01:39:57", "author": "@puneethmishra"}, "1095328653104140288": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "2,674", "datetime": "2019-02-12 14:27:55", "author": "@ayirpelle"}, "1095681708328796161": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "179", "datetime": "2019-02-13 13:50:50", "author": "@Gabriel_Maicas"}, "1091947382961553408": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "269", "datetime": "2019-02-03 06:31:58", "author": "@ReginaMeszlenyi"}, "1091272395858591744": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "218", "datetime": "2019-02-01 09:49:48", "author": "@collectedview"}, "1092236548224024577": {"content_summary": "#aboveMyHeadButCool", "followers": "241", "datetime": "2019-02-04 01:41:00", "author": "@andreinot"}, "1091389118641332224": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "18", "datetime": "2019-02-01 17:33:37", "author": "@bhskrdtt"}, "1095252171355054080": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "297", "datetime": "2019-02-12 09:24:01", "author": "@westis96"}, "1140410980825833472": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "281", "datetime": "2019-06-17 00:09:00", "author": "@nuratokura"}, "1095207560775258112": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "4,884", "datetime": "2019-02-12 06:26:45", "author": "@martin_wicke"}, "1095178348303503360": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "436", "datetime": "2019-02-12 04:30:40", "author": "@krtk"}, "1091369231034339329": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "194", "datetime": "2019-02-01 16:14:35", "author": "@jastner109"}, "1098983244354048000": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "53", "datetime": "2019-02-22 16:29:58", "author": "@Surreabral"}, "1091601874589933568": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "13,857", "datetime": "2019-02-02 07:39:02", "author": "@rajatmonga"}, "1095179581156478977": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "79", "datetime": "2019-02-12 04:35:34", "author": "@trashcanmagic"}, "1111015399024480256": {"content_summary": "\"The Evolved Transformer\": https://t.co/27bl3mXbe5 #ml #2019", "followers": "2,132", "datetime": "2019-03-27 21:21:27", "author": "@onepaperperday"}, "1091510661849509893": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "565", "datetime": "2019-02-02 01:36:35", "author": "@BluFlame"}, "1091531308541460480": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "30", "datetime": "2019-02-02 02:58:38", "author": "@chenfeiyang2018"}, "1091395556906278912": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "4,802", "datetime": "2019-02-01 17:59:12", "author": "@IntuitMachine"}, "1104204389856751618": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "57", "datetime": "2019-03-09 02:16:56", "author": "@daniloagst"}, "1091298970859520001": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "459", "datetime": "2019-02-01 11:35:24", "author": "@dolhani"}, "1091443796133531648": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "36", "datetime": "2019-02-01 21:10:53", "author": "@Nowigence"}, "1091402473242120192": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "935", "datetime": "2019-02-01 18:26:41", "author": "@OmarUFlorez"}, "1095363251498635264": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "613", "datetime": "2019-02-12 16:45:24", "author": "@mattmcd"}, "1091237239751794688": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "9", "datetime": "2019-02-01 07:30:06", "author": "@just_anh"}, "1091555777548935169": {"content_summary": "AutoML evolves Transformer architecture using evolutionary algo.", "followers": "158", "datetime": "2019-02-02 04:35:52", "author": "@luiginardi"}, "1091702525684723712": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "99", "datetime": "2019-02-02 14:18:59", "author": "@kornesh"}, "1095315711621160960": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "245", "datetime": "2019-02-12 13:36:30", "author": "@__jm"}, "1091771673177673729": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "183", "datetime": "2019-02-02 18:53:45", "author": "@0xNaN"}, "1091175608565014530": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "164,152", "datetime": "2019-02-01 03:25:12", "author": "@ceobillionaire"}, "1091658412000038912": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "216", "datetime": "2019-02-02 11:23:42", "author": "@KSKSKSKS2"}, "1095236214695870464": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,047", "datetime": "2019-02-12 08:20:36", "author": "@loretoparisi"}, "1091684627696234496": {"content_summary": "Cc @decodyng", "followers": "605", "datetime": "2019-02-02 13:07:52", "author": "@rharang"}, "1091586615519006720": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "302", "datetime": "2019-02-02 06:38:24", "author": "@mofumofu1729"}, "1091423952604413952": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "934", "datetime": "2019-02-01 19:52:02", "author": "@pavelkordik"}, "1091419062343675904": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "23", "datetime": "2019-02-01 19:32:36", "author": "@bairn_moon"}, "1091528642738282496": {"content_summary": "The Evolved Transformer - https://t.co/9D1COPFWPY https://t.co/ekeciZsKuh", "followers": "195", "datetime": "2019-02-02 02:48:02", "author": "@hereticreader"}, "1091588696313217026": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "304", "datetime": "2019-02-02 06:46:40", "author": "@flrgsr"}, "1095540905564614656": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "150", "datetime": "2019-02-13 04:31:20", "author": "@rodgzilla"}, "1095696431774601216": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "218", "datetime": "2019-02-13 14:49:21", "author": "@marqoz"}, "1091695458781679616": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "971", "datetime": "2019-02-02 13:50:54", "author": "@DasfNYC"}, "1095216578004582400": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "19", "datetime": "2019-02-12 07:02:34", "author": "@bharadwajymg"}, "1091296521545502720": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "733", "datetime": "2019-02-01 11:25:40", "author": "@Pythonner"}, "1103249103578951682": {"content_summary": "Te Evolved Transformer. This paper uses architecture search to find a better Transformer achitecture. https://t.co/hlS0d68zO7", "followers": "30", "datetime": "2019-03-06 11:00:58", "author": "@serendeepia"}, "1091343475130327040": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "293", "datetime": "2019-02-01 14:32:15", "author": "@keratin7"}, "1091266396523323392": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,670", "datetime": "2019-02-01 09:25:58", "author": "@StuartReid1929"}, "1091311795959083008": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "163", "datetime": "2019-02-01 12:26:22", "author": "@HouhouinK"}, "1091376060447498240": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "2,244", "datetime": "2019-02-01 16:41:44", "author": "@CharlotteHase"}, "1091593952505950208": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "71", "datetime": "2019-02-02 07:07:33", "author": "@aeonblack"}, "1140231991272472576": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "1,071", "datetime": "2019-06-16 12:17:45", "author": "@_milktea6"}, "1140599894064787461": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "142", "datetime": "2019-06-17 12:39:40", "author": "@chappy_nkmr"}, "1140446915865169921": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "2,360", "datetime": "2019-06-17 02:31:47", "author": "@ngr_t"}, "1091368107375874049": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "3,104", "datetime": "2019-02-01 16:10:08", "author": "@eturner303"}, "1091264655308275712": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "199", "datetime": "2019-02-01 09:19:03", "author": "@mathpluscode"}, "1092637616376348672": {"content_summary": "#ICML2019 The Evolved Transformer. (arXiv:1901.11117v1 [cs\\.LG]) https://t.co/dyPgfDZIkj", "followers": "1,300", "datetime": "2019-02-05 04:14:42", "author": "@arxiv_in_review"}, "1091434069727105025": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "45", "datetime": "2019-02-01 20:32:14", "author": "@artuskg"}, "1091390336377581568": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "15", "datetime": "2019-02-01 17:38:27", "author": "@Abdulla37154295"}, "1141785346490994694": {"content_summary": "RT @DataScienceNIG: TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, lev\u2026", "followers": "80", "datetime": "2019-06-20 19:10:14", "author": "@assoulix"}, "1091355369144807424": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "4,891", "datetime": "2019-02-01 15:19:31", "author": "@IgorCarron"}, "1095179842323124224": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "4,925", "datetime": "2019-02-12 04:36:36", "author": "@waweru"}, "1095205638706159616": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "59", "datetime": "2019-02-12 06:19:06", "author": "@liqiangniu"}, "1091271890327416833": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "215", "datetime": "2019-02-01 09:47:48", "author": "@AssistedEvolve"}, "1092286084350906368": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "791", "datetime": "2019-02-04 04:57:50", "author": "@TDLS_TO"}, "1091706970191659008": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,649", "datetime": "2019-02-02 14:36:39", "author": "@m__dehghani"}, "1091433324847947776": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "350", "datetime": "2019-02-01 20:29:17", "author": "@dkislyuk"}, "1095203461405261824": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "889", "datetime": "2019-02-12 06:10:27", "author": "@ciscaoladipo"}, "1095325076138803201": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "82", "datetime": "2019-02-12 14:13:42", "author": "@daiquocng"}, "1091362584622780418": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "539", "datetime": "2019-02-01 15:48:11", "author": "@phisch124"}, "1091271684944998400": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "353", "datetime": "2019-02-01 09:46:59", "author": "@indy9000"}, "1091190615621206017": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "782", "datetime": "2019-02-01 04:24:50", "author": "@muktabh"}, "1141621179750522880": {"content_summary": "RT @DataScienceNIG: TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, lev\u2026", "followers": "1,481", "datetime": "2019-06-20 08:17:54", "author": "@galsenai"}, "1095209612330954753": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "33", "datetime": "2019-02-12 06:34:54", "author": "@duong6991"}, "1091302103501332480": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "231", "datetime": "2019-02-01 11:47:51", "author": "@aiskoaskosd"}, "1091271697699729408": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "2,653", "datetime": "2019-02-01 09:47:02", "author": "@mosko_mule"}, "1096116637848297472": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,001", "datetime": "2019-02-14 18:39:05", "author": "@SassonMargaliot"}, "1091341566151667713": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "934", "datetime": "2019-02-01 14:24:40", "author": "@pavelkordik"}, "1095224721216425984": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "4,698", "datetime": "2019-02-12 07:34:56", "author": "@ML_deep"}, "1091415508778274816": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "2,285", "datetime": "2019-02-01 19:18:29", "author": "@bsaeta"}, "1091278679336902656": {"content_summary": "The Evolved Transformer https://t.co/OIg1AtuZ0H Neural Architecture Search\u306a\u3069\u3092\u7528\u3044\u3066Transformer\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u63a2\u7d22\u3057\u3001\u7ffb\u8a33\u30bf\u30b9\u30af\u3067\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u306f Transformer \u306e2\u500d\u306e\u52b9\u7387\u3001\u5c0f\u3055\u306a\u30e2\u30c7\u30eb(\u30d1\u30e9\u30e1\u30fc\u30bf\u65707M\u4ee5\u4e0b)\u3067\u3082WMT Eng/Ger \u3067 Transformer \u3092\u4e0a\u56de\u3063\u305f\u3068\u306e\u3053\u3068\u3002\u4e2d\u8eab\u8aad\u3093\u3067\u307f\u308b\u3002", "followers": "764", "datetime": "2019-02-01 10:14:46", "author": "@cfiken"}, "1091562674238873601": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "56", "datetime": "2019-02-02 05:03:16", "author": "@brong78"}, "1092236461481562113": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-04 01:40:39", "author": "@arxiv_cscl"}, "1091166613359841281": {"content_summary": "RT @BrundageBot: The Evolved Transformer. David R. So, Chen Liang, and Quoc V. Le https://t.co/srITOa3P4o", "followers": "1,828", "datetime": "2019-02-01 02:49:28", "author": "@rosinality"}, "1140231604943519744": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "155", "datetime": "2019-06-16 12:16:13", "author": "@Trtd6Trtd"}, "1091453852023312384": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "25,311", "datetime": "2019-02-01 21:50:51", "author": "@deeplearning4j"}, "1091386638943510528": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "83", "datetime": "2019-02-01 17:23:46", "author": "@edorado93"}, "1091151353106305025": {"content_summary": "The Evolved Transformer. (arXiv:1901.11117v1 [cs.LG]) https://t.co/H381Q8k81A", "followers": "9,664", "datetime": "2019-02-01 01:48:49", "author": "@StatMLPapers"}, "1092947200236290051": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "57", "datetime": "2019-02-06 00:44:53", "author": "@nhayato_ac"}, "1091333898565373952": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "392", "datetime": "2019-02-01 13:54:12", "author": "@Felipessalvador"}, "1091321508562120705": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "544", "datetime": "2019-02-01 13:04:58", "author": "@altamborrino"}, "1091754831365124096": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "536", "datetime": "2019-02-02 17:46:50", "author": "@sangha_deb"}, "1091428319248113665": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "424", "datetime": "2019-02-01 20:09:23", "author": "@iamknighton"}, "1091311710261178368": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "2,399", "datetime": "2019-02-01 12:26:01", "author": "@sqcai"}, "1095243678258352130": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "366", "datetime": "2019-02-12 08:50:16", "author": "@ashishawasthi"}, "1091547595380379648": {"content_summary": "RT @gsksantosh: Evolved Transformer performing better than the original with less params. https://t.co/scTsNcl9lt", "followers": "1,410", "datetime": "2019-02-02 04:03:21", "author": "@RexDouglass"}, "1092960340290097152": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "36", "datetime": "2019-02-06 01:37:06", "author": "@nama_greentea"}, "1095214383276441602": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "592", "datetime": "2019-02-12 06:53:51", "author": "@codekee"}, "1091926351181840385": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "5", "datetime": "2019-02-03 05:08:23", "author": "@argmix_ml"}, "1091701523170672640": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "6,225", "datetime": "2019-02-02 14:15:00", "author": "@alienelf"}, "1091782844026945541": {"content_summary": "#arXiv #machinelearning [cs.LG] The Evolved Transformer. (arXiv:1901.11117v1 [cs.LG]) https://t.co/QdQdxnpNkI Recent works have highlighted the strengths of the Transformer architecture for dealing with sequence tasks. At the same time, neural architectur", "followers": "1,247", "datetime": "2019-02-02 19:38:09", "author": "@mlmemoirs"}, "1091500127557173256": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "26", "datetime": "2019-02-02 00:54:44", "author": "@Yugi_NHC"}, "1091597932678152192": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "62", "datetime": "2019-02-02 07:23:22", "author": "@samhardyhey"}, "1095180848360550400": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,670", "datetime": "2019-02-12 04:40:36", "author": "@StuartReid1929"}, "1095276312711888898": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "255", "datetime": "2019-02-12 10:59:56", "author": "@josipK"}, "1091619180489728001": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "109", "datetime": "2019-02-02 08:47:48", "author": "@joseluisalcala"}, "1105682440931008512": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "26", "datetime": "2019-03-13 04:10:11", "author": "@Quanguet"}, "1092793983548444672": {"content_summary": "The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u69cb\u9020\u3001Swish\u306e\u4f7f\u7528\u3002(\u4e00\u77acconv?!fully-connected=1x1 conv\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d/activation\u306e\u9078\u629e\u80a2\u304c\u5c11\u306a\u3044\u2026) https://t.co/WBWENujvXc", "followers": "2,653", "datetime": "2019-02-05 14:36:03", "author": "@mosko_mule"}, "1091568415381311490": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,878", "datetime": "2019-02-02 05:26:05", "author": "@meltemataynsnt"}, "1091324743918211073": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "4,631", "datetime": "2019-02-01 13:17:49", "author": "@mohitban47"}, "1091276370003546114": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,063", "datetime": "2019-02-01 10:05:36", "author": "@rosstaylor90"}, "1091162699512365056": {"content_summary": "The Evolved Transformer. David R. So, Chen Liang, and Quoc V. Le https://t.co/srITOa3P4o", "followers": "3,858", "datetime": "2019-02-01 02:33:55", "author": "@BrundageBot"}, "1091204700291063809": {"content_summary": "RT @evolvingstuff: The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Tra\u2026", "followers": "1,002", "datetime": "2019-02-01 05:20:48", "author": "@ThomasMiconi"}, "1091313646020378624": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "2,071", "datetime": "2019-02-01 12:33:43", "author": "@EricSchles"}, "1091538491622584321": {"content_summary": "Evolved Transformer performing better than the original with less params.", "followers": "248", "datetime": "2019-02-02 03:27:10", "author": "@gsksantosh"}, "1091667856289419264": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "39", "datetime": "2019-02-02 12:01:13", "author": "@ChrissMonsantos"}, "1091440495270588416": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "363", "datetime": "2019-02-01 20:57:46", "author": "@nzkronus"}, "1091396897447784449": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "164,152", "datetime": "2019-02-01 18:04:32", "author": "@ceobillionaire"}, "1091338924587401218": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "426", "datetime": "2019-02-01 14:14:10", "author": "@alexrigler"}, "1091684020067491840": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "76,883", "datetime": "2019-02-02 13:05:27", "author": "@NandoDF"}, "1091636991530336257": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "45", "datetime": "2019-02-02 09:58:35", "author": "@jeandut14000"}, "1092416760047751168": {"content_summary": "The Evolved Transformer The next-gen Transformer is called the \"Evolved Transformer\" and \"demonstrates consistent improvement over the original Transformer on four well-established language tasks\u201d #ArtificialIntelligence #Technology https://t.co/vxaJCICg3", "followers": "628", "datetime": "2019-02-04 13:37:06", "author": "@AntoMon"}, "1092415967643295744": {"content_summary": "The Evolved Transformer The next-gen Transformer is called the \"Evolved Transformer\" and \"demonstrates consistent improvement over the original Transformer on four well-established language tasks\u201d #ArtificialIntelligence #Technology https://t.co/t4dmZbxs9", "followers": "134", "datetime": "2019-02-04 13:33:57", "author": "@joyenergynews"}, "1095588683497926656": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "22", "datetime": "2019-02-13 07:41:11", "author": "@lirong11"}, "1091187079978016769": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "2", "datetime": "2019-02-01 04:10:47", "author": "@sivia89024"}, "1091503584045281280": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "327", "datetime": "2019-02-02 01:08:28", "author": "@shurain"}, "1091369474178105344": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "89", "datetime": "2019-02-01 16:15:33", "author": "@margusb"}, "1140301932663169025": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "78", "datetime": "2019-06-16 16:55:41", "author": "@watchdog20xx"}, "1091701068109512706": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "2,674", "datetime": "2019-02-02 14:13:12", "author": "@ayirpelle"}, "1095386536827281408": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "141", "datetime": "2019-02-12 18:17:56", "author": "@mahesh_goud"}, "1095242838135226368": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "223", "datetime": "2019-02-12 08:46:55", "author": "@ozgurorman"}, "1091538787677687808": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "282", "datetime": "2019-02-02 03:28:21", "author": "@vksbhandary"}, "1092614519195488256": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "12,756", "datetime": "2019-02-05 02:42:55", "author": "@jaguring1"}, "1091364293155540992": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "16", "datetime": "2019-02-01 15:54:58", "author": "@RHsudlc"}, "1093912641867825152": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-08 16:41:12", "author": "@arxiv_cscl"}, "1098303894336294912": {"content_summary": "@RichmanRonald @Aerobotics_SA 100%. There has been some work in other domains. This was shared on Twitter recently: https://t.co/VOUxL1p01B :-). IMO using AutoML to refinine architectures like this is the way forward. We have good priors, but we are also c", "followers": "1,670", "datetime": "2019-02-20 19:30:28", "author": "@StuartReid1929"}, "1092643323913760768": {"content_summary": "The Evolved Transformer https://t.co/FRMP8vlGki \u307b\u3089\u30fc\u3053\u3046\u3044\u3046\u306e\u304f\u308b\u3084\u3093\u3002\u8a31\u305b\u3093\u3002", "followers": "529", "datetime": "2019-02-05 04:37:23", "author": "@Tya_msp"}, "1092845155676090369": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "302", "datetime": "2019-02-05 17:59:23", "author": "@mofumofu1729"}, "1091657442038034433": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "116", "datetime": "2019-02-02 11:19:50", "author": "@pabaldonedo"}, "1095189141258555392": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "58", "datetime": "2019-02-12 05:13:33", "author": "@hash4world"}, "1095270966970728448": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "169", "datetime": "2019-02-12 10:38:42", "author": "@ashwinids89"}, "1095290433528315905": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "39", "datetime": "2019-02-12 11:56:03", "author": "@xmah_mood"}, "1140297483815227394": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "1,188", "datetime": "2019-06-16 16:38:00", "author": "@hrs1985"}, "1091918718593126401": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "52", "datetime": "2019-02-03 04:38:04", "author": "@sharad24sc"}, "1091414411544211456": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "230", "datetime": "2019-02-01 19:14:07", "author": "@tanishkasingh"}, "1091309647686823937": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "816", "datetime": "2019-02-01 12:17:50", "author": "@btcplanet"}, "1091444476067962890": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "86", "datetime": "2019-02-01 21:13:35", "author": "@tiagomluis"}, "1092417785039343617": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-04 13:41:10", "author": "@arxiv_cscl"}, "1141594954499801088": {"content_summary": "TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, leveraging the strengths of self-attention & wide convolution. A new level for NLP -evolution-based neural architecture search Read: h", "followers": "16,891", "datetime": "2019-06-20 06:33:41", "author": "@DataScienceNIG"}, "1091909793147629568": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "41", "datetime": "2019-02-03 04:02:36", "author": "@FalseAs"}, "1095640330131582976": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "306", "datetime": "2019-02-13 11:06:25", "author": "@subhobrata1"}, "1140445718290714625": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "1,101", "datetime": "2019-06-17 02:27:02", "author": "@sashimi_rawfish"}, "1092173648830648327": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "26", "datetime": "2019-02-03 21:31:04", "author": "@KlimZaporojets"}, "1096912728277565440": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "282", "datetime": "2019-02-16 23:22:28", "author": "@aldosantiago"}, "1091445473389490176": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "35", "datetime": "2019-02-01 21:17:33", "author": "@tianshi5940"}, "1091283234992734209": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "12", "datetime": "2019-02-01 10:32:52", "author": "@HitCkai"}, "1094667614838837250": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-10 18:41:11", "author": "@arxiv_cscl"}, "1096069347695517696": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "8", "datetime": "2019-02-14 15:31:11", "author": "@boo_o1"}, "1091480320946384903": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "160", "datetime": "2019-02-01 23:36:01", "author": "@tuvuumass"}, "1095180853376925696": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "487", "datetime": "2019-02-12 04:40:37", "author": "@lyomi"}, "1091286395396603905": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "2,042", "datetime": "2019-02-01 10:45:26", "author": "@imenurok"}, "1095143620003258370": {"content_summary": "Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr", "followers": "18,260", "datetime": "2019-02-12 02:12:40", "author": "@quocleix"}, "1094163505714884608": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "21", "datetime": "2019-02-09 09:18:02", "author": "@kacky24"}, "1095589539610091521": {"content_summary": "\u00ab\u062a\u0631\u0627\u0646\u0633\u0641\u0648\u0631\u0645\u0631\u0647\u0627\u00bb \u0628\u0647 \u0631\u063a\u0645 \u062a\u0644\u0627\u0634 \u0628\u0631\u0627\u06cc \u062c\u0627\u06cc\u06af\u0632\u06cc\u0646\u06cc \u0647\u0645\u0686\u0646\u0627\u0646 \u0628\u0627 \u0642\u062f\u0631\u062a \u062f\u0631 \u062d\u0631\u06a9\u062a\u0646\u062f: https://t.co/FADc03aH7I https://t.co/GF7twWtRFV", "followers": "49", "datetime": "2019-02-13 07:44:35", "author": "@vedadian"}, "1091730015769821184": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "117", "datetime": "2019-02-02 16:08:13", "author": "@d_kangin"}, "1095184296808235014": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "66", "datetime": "2019-02-12 04:54:18", "author": "@miyurud"}, "1140805414310666241": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "110", "datetime": "2019-06-18 02:16:20", "author": "@Raquel1934"}, "1091479091553886208": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "254", "datetime": "2019-02-01 23:31:08", "author": "@soulmachine"}, "1094290117525757952": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-09 17:41:09", "author": "@arxiv_cscl"}, "1091329886252343296": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "123", "datetime": "2019-02-01 13:38:15", "author": "@iantimmis"}, "1091283755950460928": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "19", "datetime": "2019-02-01 10:34:57", "author": "@raamnaathn"}, "1091421806165135361": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,171", "datetime": "2019-02-01 19:43:30", "author": "@dadabots"}, "1098304893822160896": {"content_summary": "RT @StuartReid1929: @RichmanRonald @Aerobotics_SA 100%. There has been some work in other domains. This was shared on Twitter recently: htt\u2026", "followers": "880", "datetime": "2019-02-20 19:34:26", "author": "@RichmanRonald"}, "1130649763291062272": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-05-21 01:41:24", "author": "@arxiv_cscl"}, "1091492458540613632": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "212", "datetime": "2019-02-02 00:24:15", "author": "@Nilakurinji"}, "1095472666599817217": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "215", "datetime": "2019-02-13 00:00:11", "author": "@AssistedEvolve"}, "1091272071768879104": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "361", "datetime": "2019-02-01 09:48:31", "author": "@iugoaoj"}, "1091573386759745536": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "50", "datetime": "2019-02-02 05:45:50", "author": "@__A_WADA__"}, "1091391925037326336": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "4,111", "datetime": "2019-02-01 17:44:46", "author": "@drhiot"}, "1091396979815460864": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,344", "datetime": "2019-02-01 18:04:51", "author": "@jesse_vig"}, "1091702574619725825": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "334", "datetime": "2019-02-02 14:19:11", "author": "@activatedgeek"}, "1091465133304406017": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,247", "datetime": "2019-02-01 22:35:40", "author": "@daisuzu"}, "1091574001237979142": {"content_summary": "I think architecture search and models building models are the most interesting things happening in our field. good stuff:", "followers": "1,441", "datetime": "2019-02-02 05:48:17", "author": "@mark_l_watson"}, "1091343500333993990": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "1,286", "datetime": "2019-02-01 14:32:21", "author": "@heghbalz"}, "1091980251159117824": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "2,184", "datetime": "2019-02-03 08:42:34", "author": "@TangFeihu"}, "1091402079971635200": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "547", "datetime": "2019-02-01 18:25:07", "author": "@nicolarohrseitz"}, "1095685878616186881": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "456", "datetime": "2019-02-13 14:07:24", "author": "@PerthMLGroup"}, "1095436253389041664": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "2,281", "datetime": "2019-02-12 21:35:29", "author": "@eoinbrazil"}, "1091549900532314113": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "291", "datetime": "2019-02-02 04:12:30", "author": "@Shujian_Liu"}, "1095183053289390080": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "194", "datetime": "2019-02-12 04:49:22", "author": "@jastner109"}, "1091684560599949312": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "605", "datetime": "2019-02-02 13:07:36", "author": "@rharang"}, "1091341416897236997": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "8,538", "datetime": "2019-02-01 14:24:04", "author": "@rbhar90"}, "1091697294997577729": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "372", "datetime": "2019-02-02 13:58:12", "author": "@amirfzpr"}, "1091273818734239745": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "193", "datetime": "2019-02-01 09:55:27", "author": "@hu_daa"}, "1095214144851046400": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "259", "datetime": "2019-02-12 06:52:54", "author": "@jcchinhui"}, "1093061927847157760": {"content_summary": "RT @mosko_mule: The Evolved Transformer https://t.co/uckoQz1wHL Transformer\u3092\u9032\u5316\u8a08\u7b97\u3067\u69cb\u9020\u63a2\u7d22\u3092\u3057\u305f\uff01\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u6291\u3048\u3064\u3064\u9ad8\u6027\u80fd\u306b\u3002\u30dd\u30a4\u30f3\u30c8\u306f\u5e83\u3044depth-wise sep. conv.\u3001GLU\u3001\u5206\u5c90\u2026", "followers": "36", "datetime": "2019-02-06 08:20:46", "author": "@kuz44ma69"}, "1092229482008768512": {"content_summary": "RT @arxiv_cs_cl: https://t.co/RV9gcN8uER The Evolved Transformer. (arXiv:1901.11117v1 [cs.LG]) #NLProc", "followers": "291", "datetime": "2019-02-04 01:12:55", "author": "@Shujian_Liu"}, "1092653332148162561": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "904", "datetime": "2019-02-05 05:17:09", "author": "@battlegirl_mmd"}, "1091313822881669120": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "2,071", "datetime": "2019-02-01 12:34:25", "author": "@EricSchles"}, "1095165840150933504": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "821", "datetime": "2019-02-12 03:40:58", "author": "@ErmiaBivatan"}, "1094108865086570496": {"content_summary": "The Evolved Transformer https://t.co/7eCp1BjpIw", "followers": "3,489", "datetime": "2019-02-09 05:40:55", "author": "@arxiv_cscl"}, "1091427064706674688": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "282", "datetime": "2019-02-01 20:04:24", "author": "@SPHIX_1"}, "1091489685120020480": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "29", "datetime": "2019-02-02 00:13:14", "author": "@chenlailin"}, "1091287490395287552": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "24", "datetime": "2019-02-01 10:49:47", "author": "@blissunplugged"}, "1095397599396401152": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,286", "datetime": "2019-02-12 19:01:53", "author": "@heghbalz"}, "1091276970015506433": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "52", "datetime": "2019-02-01 10:07:59", "author": "@sharad24sc"}, "1091393224617414657": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "30", "datetime": "2019-02-01 17:49:56", "author": "@CapeTownML"}, "1091269825027035136": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "8", "datetime": "2019-02-01 09:39:35", "author": "@mh_ha_soar"}, "1092681950383489025": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,286", "datetime": "2019-02-05 07:10:52", "author": "@heghbalz"}, "1095277308120838144": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "146", "datetime": "2019-02-12 11:03:54", "author": "@esvhd"}, "1091332780032696321": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,265", "datetime": "2019-02-01 13:49:45", "author": "@2pieour"}, "1092991289166049282": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "490", "datetime": "2019-02-06 03:40:04", "author": "@_davemacdonald"}, "1091417760255672321": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "2,145", "datetime": "2019-02-01 19:27:26", "author": "@iskander"}, "1091364457001828353": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "698", "datetime": "2019-02-01 15:55:37", "author": "@AINow6"}, "1091895076408549376": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "69", "datetime": "2019-02-03 03:04:07", "author": "@ku21fan"}, "1091210683138654208": {"content_summary": "RT @evolvingstuff: The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Tra\u2026", "followers": "66", "datetime": "2019-02-01 05:44:35", "author": "@shubh_300595"}, "1091553578685161472": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "18", "datetime": "2019-02-02 04:27:07", "author": "@dazing_dhillon"}, "1095367683229474816": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "196", "datetime": "2019-02-12 17:03:01", "author": "@aavella77"}, "1095169190800052225": {"content_summary": "[1901.11117] The Evolved Transformer https://t.co/ZRyLitwO72", "followers": "741", "datetime": "2019-02-12 03:54:17", "author": "@algunion"}, "1091326846526840832": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "245", "datetime": "2019-02-01 13:26:10", "author": "@__jm"}, "1092621526296186880": {"content_summary": "RT @icoxfog417: Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u2026", "followers": "1,170", "datetime": "2019-02-05 03:10:46", "author": "@zaoriku0"}, "1091403087976124416": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "6", "datetime": "2019-02-01 18:29:08", "author": "@RewaSood21"}, "1098273261962649600": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "13", "datetime": "2019-02-20 17:28:45", "author": "@xyc234"}, "1091269525922816003": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "936", "datetime": "2019-02-01 09:38:24", "author": "@n0mad_0"}, "1095177928709718016": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "179,056", "datetime": "2019-02-12 04:29:00", "author": "@Montreal_AI"}, "1091221754343841793": {"content_summary": "RT @evolvingstuff: The Evolved Transformer (from #GoogleBrain) \"the Evolved Transformer - demonstrates consistent improvement over the Tra\u2026", "followers": "15", "datetime": "2019-02-01 06:28:34", "author": "@NusWu"}, "1091620503884828672": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "", "datetime": "2019-02-02 08:53:04", "author": "@AaaLee"}, "1091389917714923523": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "613", "datetime": "2019-02-01 17:36:48", "author": "@mattmcd"}, "1095549471348289538": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "27", "datetime": "2019-02-13 05:05:22", "author": "@ctl_walt_del"}, "1091424375600017408": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "71", "datetime": "2019-02-01 19:53:43", "author": "@MirunaClinciu"}, "1141747854932873222": {"content_summary": "RT @DataScienceNIG: TRANSFORMER from @GoogleAI has EVOLVED! While the original one relied on self-attention, the Evolved is a hybrid, lev\u2026", "followers": "620", "datetime": "2019-06-20 16:41:15", "author": "@ykilcher"}, "1091743084151398400": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "357", "datetime": "2019-02-02 17:00:09", "author": "@gustavorincon29"}, "1095625864660676608": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1,100", "datetime": "2019-02-13 10:08:56", "author": "@scottedwards200"}, "1091433170019315712": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "1,213", "datetime": "2019-02-01 20:28:40", "author": "@sampathweb"}, "1095259189310418944": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "223", "datetime": "2019-02-12 09:51:54", "author": "@__haizad__"}, "1091699082131722242": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "45", "datetime": "2019-02-02 14:05:18", "author": "@yoquankara"}, "1091276101983207424": {"content_summary": "RT @Miles_Brundage: \"The Evolved Transformer,\" So et al.: https://t.co/OMbjkuxIGR", "followers": "12", "datetime": "2019-02-01 10:04:32", "author": "@HitCkai"}, "1140514397174919168": {"content_summary": "RT @yoh_okuno: LSTM\u304c\u30aa\u30ef\u30b3\u30f3\u3068\u3044\u3046\u306e\u306f\u8a00\u3044\u904e\u304e\u3067\u3001\u307e\u3060\u30c7\u30b3\u30fc\u30c0\u5074\u306fLSTM\u306e\u65b9\u304c\u5f37\u3044\u3088\u3046\u3060\u3002 https://t.co/Dk8BnAdGYV \u4e00\u65b9\u3001\u6700\u8fd1\u767a\u8868\u3055\u308c\u305fEvolved Transformer\u3067\u306fRNN\u3092\u63a2\u7d22\u7a7a\u9593\u306b\u5165\u308c\u3066\u3044\u306a\u3044\u306a\u3069\u3001Google\u793e\u5185\u2026", "followers": "550", "datetime": "2019-06-17 06:59:56", "author": "@keno_ss"}, "1095578106000228352": {"content_summary": "RT @quocleix: Evolved Transformer is now opensourced in Tensor2Tensor: https://t.co/4iLIarXywr https://t.co/4YC1VX9m65", "followers": "1", "datetime": "2019-02-13 06:59:09", "author": "@neuronavt"}, "1092278449392181249": {"content_summary": "RT @imenurok: The Evolved Transformer https://t.co/5cQQRCHgW1", "followers": "146", "datetime": "2019-02-04 04:27:30", "author": "@kz_lil_fox"}, "1095676490300252165": {"content_summary": "The Evolved Transformer https://t.co/rNmq8b4OM6 #DeepLearning #MachineLearning #AI #DataScience #NeuralNetworks #CNN #Reinforcement #Learning #DeepRL #GPU #TensorFlow #Keras #Caffe #Pytorch #Python #HPC #Robotics #AutonomousCar #Quant", "followers": "8,846", "datetime": "2019-02-13 13:30:06", "author": "@Deep_In_Depth"}, "1140844081834336256": {"content_summary": "RT @BioDecoded: Applying AutoML to Transformer Architectures | Google AI Blog https://t.co/m25RlH2h6j https://t.co/P5s0dYTRfj #NLP #DeepLe\u2026", "followers": "117", "datetime": "2019-06-18 04:49:59", "author": "@moss_dalvi"}, "1091309812141350912": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "99", "datetime": "2019-02-01 12:18:29", "author": "@greed2411"}, "1091404808555094016": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "98", "datetime": "2019-02-01 18:35:58", "author": "@justinkhup"}, "1118544681900494848": {"content_summary": "@humbnlp @hardmaru I guess (6) is this one https://t.co/lSTo3HRcFf", "followers": "14", "datetime": "2019-04-17 16:00:08", "author": "@ntanh14"}, "1091440582092902400": {"content_summary": "RT @quocleix: We used architecture search to improve Transformer architecture. Key is to use evolution and seed initial population with Tra\u2026", "followers": "165", "datetime": "2019-02-01 20:58:07", "author": "@giovenko"}, "1091398788957290497": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "1,047", "datetime": "2019-02-01 18:12:03", "author": "@loretoparisi"}, "1092611650077712384": {"content_summary": "Transformer\u306e\u69cb\u9020\u3092\u3001\u69cb\u9020\u63a2\u7d22\u3092\u7528\u3044\u3066\u6700\u9069\u5316\u3057\u305f\u3068\u3044\u3046\u7814\u7a76\u3002\u69cb\u9020\u63a2\u7d22\u306b\u306f\u9032\u5316\u6226\u7565\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u5b66\u7fd2\u7d50\u679c\u304c\u826f\u3044\u3082\u306e\u3092\u89aa\u3068\u3057\u3066\u3055\u3089\u306b\u5b50\u3092\u751f\u6210\u3059\u308b\u3002\u5b50\u306b\u3064\u3044\u3066\u306f\u3088\u308a\u826f\u3044\u3082\u306e\u306b\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u305f\u3081\u306b\u3001\u4e00\u5b9a\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u30cf\u30fc\u30c9\u30eb\u3092\u8a2d\u3051\u3066\u53b3\u9078\u3057\u3066\u3044\u308b\u3002\u5927\u5e45\u306a\u5c0f\u30b5\u30a4\u30ba\u5316\u304c\u3067\u304d\u3066\u3044\u308b", "followers": "11,436", "datetime": "2019-02-05 02:31:31", "author": "@icoxfog417"}, "1091267200818630657": {"content_summary": "RT @hardmaru: The Evolved Transformer: They perform architecture search on Transformer's stackable cells for seq2seq tasks. \u201cA much smalle\u2026", "followers": "764", "datetime": "2019-02-01 09:29:10", "author": "@cfiken"}}}