{"citation_id": "53712949", "completed": "1", "queriedAt": "2020-05-14 13:18:59", "tab": "twitter", "twitter": {"1083623558406856704": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "99,939", "datetime": "2019-01-11 07:16:03", "author": "@jeremyphoward"}, "1083456412154421248": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "179", "datetime": "2019-01-10 20:11:52", "author": "@sidbrahma"}, "1083771302098259968": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "27", "datetime": "2019-01-11 17:03:08", "author": "@amitmac22"}, "1087163131988721664": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZvtEO", "followers": "3,500", "datetime": "2019-01-21 01:41:03", "author": "@arxiv_cscl"}, "1083678865527136257": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "694", "datetime": "2019-01-11 10:55:49", "author": "@santty128"}, "1083384105507549185": {"content_summary": "REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up to 1800x faster! Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/eV4iy1kOPT TensorFlow & PyTo", "followers": "2,927", "datetime": "2019-01-10 15:24:33", "author": "@evolvingstuff"}, "1085110302381756416": {"content_summary": "RT @icoxfog417: Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u2026", "followers": "4,825", "datetime": "2019-01-15 09:43:51", "author": "@prototechno"}, "1086243626282176512": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "97", "datetime": "2019-01-18 12:47:16", "author": "@vin_mathur"}, "1083814388870598656": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "81", "datetime": "2019-01-11 19:54:21", "author": "@NWEKEHENRY1"}, "1144627098323234816": {"content_summary": "For background on Transformer-XL, see: https://t.co/1vaWHmrFsF", "followers": "505", "datetime": "2019-06-28 15:22:21", "author": "@AUEBNLPGroup"}, "1090360533180706816": {"content_summary": "\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov (Submitted on 9 Jan 2019 (v1), last revised 18 Jan 2019 (this version, v2))\" https://t.co/9", "followers": "270", "datetime": "2019-01-29 21:26:23", "author": "@aeRIsincOsmos"}, "1083800986173485056": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "912", "datetime": "2019-01-11 19:01:05", "author": "@ChikaObuah"}, "1083533576015171584": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "163,852", "datetime": "2019-01-11 01:18:30", "author": "@ceobillionaire"}, "1083648592550682625": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "257", "datetime": "2019-01-11 08:55:32", "author": "@shannky16"}, "1182602822782570498": {"content_summary": "ABST100\u30e1\u30e2\uff1a59 Transformer-XL\uff082019\uff09\u2192https://t.co/G8pLDVYQXn\u3000\uff0cTransformer\u306ffixed-length context\u306b\u3088\u308a\u5236\u9650\u3055\u308c\u308b\uff0c\u305d\u308c\u3092\u6539\u5584\u3057\u305f\u306e\u304cTransformer-XL\u3067\u300ca segment-level recurrence mechanism\u300d\u3068\u300ca novel positional encoding scheme\u300d\u3067\u69cb\u6210\u3055\u308c\u308b", "followers": "63", "datetime": "2019-10-11 10:24:19", "author": "@Ronmemo1"}, "1097825144309985281": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "57", "datetime": "2019-02-19 11:48:05", "author": "@irh86753o9"}, "1083772348816285696": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "226", "datetime": "2019-01-11 17:07:18", "author": "@sagarpath"}, "1084316583969738752": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "39", "datetime": "2019-01-13 05:09:53", "author": "@Ujjwal_1999"}, "1083509301778382848": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "0", "datetime": "2019-01-10 23:42:02", "author": "@Sam09lol"}, "1084005568065429504": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "15", "datetime": "2019-01-12 08:34:01", "author": "@Abdulla37154295"}, "1086073564447428609": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "1,348", "datetime": "2019-01-18 01:31:30", "author": "@Lidinwise"}, "1083749460612661249": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "29", "datetime": "2019-01-11 15:36:21", "author": "@Donatas36474150"}, "1083704419311648768": {"content_summary": "https://t.co/mwEbmVA9ZV", "followers": "39,742", "datetime": "2019-01-11 12:37:22", "author": "@PatWhite70"}, "1084330714957574144": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "8", "datetime": "2019-01-13 06:06:02", "author": "@dark_neurons"}, "1083695475419684864": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "126", "datetime": "2019-01-11 12:01:50", "author": "@truskovskiy"}, "1083600918350684162": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "49", "datetime": "2019-01-11 05:46:05", "author": "@etherealkatana"}, "1083579823744704512": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "906", "datetime": "2019-01-11 04:22:16", "author": "@tsuname"}, "1083426825190105088": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "274", "datetime": "2019-01-10 18:14:18", "author": "@cghosh_"}, "1083517199841517568": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "35", "datetime": "2019-01-11 00:13:25", "author": "@762810120"}, "1083768528392605698": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "11,002", "datetime": "2019-01-11 16:52:07", "author": "@data_hpz"}, "1093192797354627073": {"content_summary": "Google #AI announced details of Transformer-XL aimed at improving language understanding beyond a fixed-length context with higher self-attention. #NLP #AI https://t.co/CD4eeTHj8r https://t.co/qBGsVqJjCw", "followers": "736", "datetime": "2019-02-06 17:00:48", "author": "@TheOrbifold"}, "1086553540607311874": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "165", "datetime": "2019-01-19 09:18:45", "author": "@giovenko"}, "1083757520366243840": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "4,812", "datetime": "2019-01-11 16:08:22", "author": "@IntuitMachine"}, "1084522979613118464": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "75", "datetime": "2019-01-13 18:50:02", "author": "@yashkumaratri"}, "1085772658736648192": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "459", "datetime": "2019-01-17 05:35:49", "author": "@dolhani"}, "1083819098277056512": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "366", "datetime": "2019-01-11 20:13:04", "author": "@iugoaoj"}, "1083627021513646080": {"content_summary": "RT @nkmry_: \u518d\u5e30\u6a5f\u69cb\u3068\u76f8\u5bfe\u7684\u306a position embedding \u3092\u8003\u6848\u3057\u3066 Transformer \u3092 1800\u500d\u9ad8\u901f\u5316\uff01 TensorFlow \u3068 PyTorch \u306e\u5b9f\u88c5\u3082\u516c\u958b\u3057\u3066\u3044\u308b\u3002 https://t.co/9dv5w8pBCp", "followers": "456", "datetime": "2019-01-11 07:29:49", "author": "@trigate099"}, "1083431194749005825": {"content_summary": "#arXiv #machinelearning [cs.LG] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. (arXiv:1901.02860v1 [cs.LG]) https://t.co/GB6e85JuXQ Transformer networks have a potential of learning longer-term dependency, but are limited by a fi", "followers": "1,247", "datetime": "2019-01-10 18:31:40", "author": "@mlmemoirs"}, "1086473041553641472": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "556", "datetime": "2019-01-19 03:58:53", "author": "@puneethmishra"}, "1083535072677031947": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "179,069", "datetime": "2019-01-11 01:24:27", "author": "@Montreal_AI"}, "1085337681708474368": {"content_summary": "2019/01/09 \u6295\u7a3f 4\u4f4d LG(Machine Learning) Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/SBRm6jFCGr 2 Tweets 9 Retweets 44 Favorites", "followers": "695", "datetime": "2019-01-16 00:47:22", "author": "@arxiv_pop"}, "1083681039770251270": {"content_summary": "Well start for 2019 !", "followers": "238", "datetime": "2019-01-11 11:04:28", "author": "@frolicRishikesh"}, "1083774494915862528": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "66", "datetime": "2019-01-11 17:15:49", "author": "@diegovogeid"}, "1083926973762523137": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "293", "datetime": "2019-01-12 03:21:43", "author": "@vingovan"}, "1083777490688999425": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "21", "datetime": "2019-01-11 17:27:44", "author": "@sbmaruf"}, "1083420702869340160": {"content_summary": "Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key results: state-of-art on five language modeling benchmarks, including ppl of 21.8 on LM1B and 0.99 on enwiki8. Link: https://t.co/uPa6pR", "followers": "18,260", "datetime": "2019-01-10 17:49:59", "author": "@quocleix"}, "1086099419940995073": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "1,348", "datetime": "2019-01-18 03:14:15", "author": "@udmrzn"}, "1083680691500564480": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1,403", "datetime": "2019-01-11 11:03:05", "author": "@RTFMCelia"}, "1083608129256865792": {"content_summary": "NLP techniques are getting very interesting", "followers": "50", "datetime": "2019-01-11 06:14:45", "author": "@pa9501460"}, "1083479743834583046": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "771", "datetime": "2019-01-10 21:44:35", "author": "@aCraigPfeifer"}, "1083524424341442560": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "154", "datetime": "2019-01-11 00:42:08", "author": "@philipskokoh"}, "1085758764949491712": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "2,927", "datetime": "2019-01-17 04:40:36", "author": "@evolvingstuff"}, "1083524537222815749": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "58", "datetime": "2019-01-11 00:42:35", "author": "@ShuvenduBikash"}, "1094070025772650496": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "9", "datetime": "2019-02-09 03:06:35", "author": "@WeinroT_xc"}, "1083767194377441281": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "182", "datetime": "2019-01-11 16:46:49", "author": "@DrYogiLBear"}, "1089435600879419392": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "2,728", "datetime": "2019-01-27 08:11:02", "author": "@AImanfuture"}, "1135904434570092545": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZvtEO", "followers": "3,500", "datetime": "2019-06-04 13:41:35", "author": "@arxiv_cscl"}, "1083538079044972544": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "8,541", "datetime": "2019-01-11 01:36:23", "author": "@rbhar90"}, "1083773746941435906": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "227", "datetime": "2019-01-11 17:12:51", "author": "@J_P_Raymond"}, "1084601311331463168": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "14", "datetime": "2019-01-14 00:01:18", "author": "@pbcquoc"}, "1083788308247863296": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "4,327", "datetime": "2019-01-11 18:10:43", "author": "@jekbradbury"}, "1083718699486494721": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "145", "datetime": "2019-01-11 13:34:07", "author": "@adamajm"}, "1083753977085231104": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "239", "datetime": "2019-01-11 15:54:17", "author": "@Dneutr0n"}, "1083593725635223552": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "99", "datetime": "2019-01-11 05:17:31", "author": "@treasured_write"}, "1085976039300497408": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "306", "datetime": "2019-01-17 19:03:58", "author": "@juarelerrr"}, "1245320086031757313": {"content_summary": "#\u30e1\u30e2 #kindsoftransformer #transformer star-transformer https://t.co/rZKYfYAGlh bp-transformer https://t.co/nvDE6Dk0WU transformer-XL https://t.co/QutdaAlEyp compressive transformer https://t.co/KyfiLBMEZC", "followers": "4", "datetime": "2020-04-01 12:00:20", "author": "@joreykilig"}, "1085299460245053442": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "171", "datetime": "2019-01-15 22:15:29", "author": "@subho_mpi"}, "1083454824484499456": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "447", "datetime": "2019-01-10 20:05:34", "author": "@Allen_A_N"}, "1083859363872223232": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "57", "datetime": "2019-01-11 22:53:04", "author": "@Daisuky_I"}, "1083831419581984770": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "26", "datetime": "2019-01-11 21:02:01", "author": "@KlimZaporojets"}, "1083481141439426561": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "93", "datetime": "2019-01-10 21:50:08", "author": "@0xhexhex"}, "1084586841724964865": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "49", "datetime": "2019-01-13 23:03:48", "author": "@etherealkatana"}, "1085010316742643713": {"content_summary": "RT @icoxfog417: Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u2026", "followers": "273", "datetime": "2019-01-15 03:06:32", "author": "@morinosuke__p"}, "1088920034821488641": {"content_summary": "RT @heghbalz: Interesting idea for processing long sequences of arbitrary length for language modeling using transformer architectures. We\u2026", "followers": "189", "datetime": "2019-01-25 22:02:22", "author": "@oneinfinitezero"}, "1083549149457510405": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "1,174", "datetime": "2019-01-11 02:20:23", "author": "@EightTons"}, "1086073378560065537": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "108", "datetime": "2019-01-18 01:30:46", "author": "@MingSeow"}, "1083887288319426560": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "67", "datetime": "2019-01-12 00:44:01", "author": "@elyase"}, "1144978692461850630": {"content_summary": "Transformer-XL https://t.co/KQndN7mVDc \u306b\u3088\u3063\u3066\u3001BERT\u3092\u8d85\u3048\u305f\u3068\u3044\u3046\u5642\u306eXLNet https://t.co/zRXG66hZBd \u306ePython\u306e\u5b9f\u88c5 https://t.co/McKw6O1QJ3", "followers": "725", "datetime": "2019-06-29 14:39:27", "author": "@kazmuzik"}, "1129390185538629632": {"content_summary": "RT @AlecRad: @chipro They're pretty much the same! Transformer-XL https://t.co/7lT20UaY5u is competitive on PTB. Probably using already tun\u2026", "followers": "322", "datetime": "2019-05-17 14:16:17", "author": "@letranger14"}, "1083773086187409408": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "425", "datetime": "2019-01-11 17:10:13", "author": "@iamknighton"}, "1083579556949221376": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "739", "datetime": "2019-01-11 04:21:12", "author": "@ajinkyakale"}, "1083739911935676416": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "330", "datetime": "2019-01-11 14:58:24", "author": "@mr_ubik"}, "1085761762773868544": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "63", "datetime": "2019-01-17 04:52:31", "author": "@kvikas572"}, "1167376882829950976": {"content_summary": "[8/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 11,474 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipWRPT - \ud83d\udd17 https://t.co/7bzXyCZuaS", "followers": "222", "datetime": "2019-08-30 10:01:52", "author": "@PapersTrending"}, "1095967565824708608": {"content_summary": "\u9055\u3046\u8ad6\u6587\u304c\u30ea\u30f3\u30af\u3055\u308c\u3066\u3044\u307e\u3057\u305f\u3002\u5931\u793c\u3044\u305f\u3057\u307e\u3057\u305f\u3002 \u6b63\u3057\u304f\u306f\u3053\u3061\u3089\u3067\u3059\u3002 https://t.co/P6VBTvEdIX", "followers": "2,481", "datetime": "2019-02-14 08:46:44", "author": "@shion_honda"}, "1083772244269002753": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "21", "datetime": "2019-01-11 17:06:53", "author": "@luoyuchu"}, "1083388156995461121": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZN4wm", "followers": "3,500", "datetime": "2019-01-10 15:40:39", "author": "@arxiv_cscl"}, "1083401045038878721": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "608", "datetime": "2019-01-10 16:31:52", "author": "@1997synohro"}, "1085943221765763073": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "66", "datetime": "2019-01-17 16:53:34", "author": "@diegovogeid"}, "1083460709705428992": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "93", "datetime": "2019-01-10 20:28:57", "author": "@0xhexhex"}, "1083598805428199424": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/Sy7iOnashJ", "followers": "508", "datetime": "2019-01-11 05:37:42", "author": "@NickFisherAU"}, "1083757490662244353": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,602", "datetime": "2019-01-11 16:08:15", "author": "@alxcnwy"}, "1162303739421765635": {"content_summary": "[9/10] \ud83d\udcc8 - pytorch-transformers - 10,829 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipFgYl - \ud83d\udd17 https://t.co/7bzXyCHTjk", "followers": "222", "datetime": "2019-08-16 10:03:00", "author": "@PapersTrending"}, "1083519363934760960": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "186", "datetime": "2019-01-11 00:22:01", "author": "@nkmry_"}, "1083525662814531584": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "5", "datetime": "2019-01-11 00:47:03", "author": "@pluviophile16"}, "1083722343044825090": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "160", "datetime": "2019-01-11 13:48:35", "author": "@tuvuumass"}, "1083454447307706369": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "107", "datetime": "2019-01-10 20:04:04", "author": "@linuxpouria"}, "1083755657063530503": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "79,158", "datetime": "2019-01-11 16:00:58", "author": "@machinelearnflx"}, "1085761238687145986": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,627", "datetime": "2019-01-17 04:50:26", "author": "@chris_brockett"}, "1083597339879600129": {"content_summary": "up to 1,800+ times faster than vanilla Transformer during evaluation. Additionally, we improve the state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8, from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to 2", "followers": "1,034", "datetime": "2019-01-11 05:31:52", "author": "@ng2amarinefunk"}, "1083546562691649536": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "596", "datetime": "2019-01-11 02:10:06", "author": "@iamquadr"}, "1083646170784780288": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "732", "datetime": "2019-01-11 08:45:54", "author": "@PeleteiroAna"}, "1083620237096841216": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "657", "datetime": "2019-01-11 07:02:51", "author": "@komakusaryama"}, "1085165675834818560": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "82", "datetime": "2019-01-15 13:23:53", "author": "@mzeid4real"}, "1083721905490690049": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "45", "datetime": "2019-01-11 13:46:51", "author": "@85PercentBored"}, "1085807126725816320": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "310", "datetime": "2019-01-17 07:52:46", "author": "@fwang1987"}, "1083742313854099456": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "3,265", "datetime": "2019-01-11 15:07:57", "author": "@ymym3412"}, "1088717171558109184": {"content_summary": "Interesting idea for processing long sequences of arbitrary length for language modeling using transformer architectures. We tried attention on genomic (DeepSNP) and it worked well, but still had the length limit. Maybe this paper helps https://t.co/TWhWK", "followers": "1,288", "datetime": "2019-01-25 08:36:15", "author": "@heghbalz"}, "1084975318186680320": {"content_summary": "Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u3046\uff0b\u4f4d\u7f6e\u60c5\u5831\u306b\u3088\u308b\u8aad\u307f\u51fa\u3057\u3092\u5b66\u7fd2\u53ef\u80fd\u306b\u3057\u3066\u3044\u308b https://t.co/1fCSgJ5L2A", "followers": "11,476", "datetime": "2019-01-15 00:47:28", "author": "@icoxfog417"}, "1083802287850561536": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "279", "datetime": "2019-01-11 19:06:16", "author": "@theolivenbaum"}, "1083416089210408965": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "5", "datetime": "2019-01-10 17:31:39", "author": "@GeminiWWWG"}, "1083488639575715840": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "265", "datetime": "2019-01-10 22:19:56", "author": "@Swetava"}, "1083650792597245952": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "36", "datetime": "2019-01-11 09:04:16", "author": "@vasan_ashwin"}, "1255768873703559169": {"content_summary": "RT @mtugrull: Benim kulland\u0131\u011f\u0131m ve i\u00e7eri\u011fini sevdi\u011fim #MachineLearning Bilinmesi Gereken Ara\u015ft\u0131rma Makaleleri 1)https://t.co/QAofv0DBGm 2)h\u2026", "followers": "2,332", "datetime": "2020-04-30 08:00:05", "author": "@kyuzelif"}, "1083449547005874176": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1,602", "datetime": "2019-01-10 19:44:36", "author": "@alxcnwy"}, "1083538905993891840": {"content_summary": "\u518d\u5e30\u6a5f\u69cb\u3068\u76f8\u5bfe\u7684\u306a position embedding \u3092\u8003\u6848\u3057\u3066 Transformer \u3092 1800\u500d\u9ad8\u901f\u5316\uff01 TensorFlow \u3068 PyTorch \u306e\u5b9f\u88c5\u3082\u516c\u958b\u3057\u3066\u3044\u308b\u3002 https://t.co/9dv5w8pBCp", "followers": "186", "datetime": "2019-01-11 01:39:41", "author": "@nkmry_"}, "1085782573371064322": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,649", "datetime": "2019-01-17 06:15:13", "author": "@alexk_z"}, "1083702205515878400": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "2,043", "datetime": "2019-01-11 12:28:34", "author": "@imenurok"}, "1083685243381370880": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "109", "datetime": "2019-01-11 11:21:10", "author": "@ram_cse"}, "1083414547506053122": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "1,020", "datetime": "2019-01-10 17:25:31", "author": "@serrjoa"}, "1083479124562362380": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "218", "datetime": "2019-01-10 21:42:08", "author": "@AssistedEvolve"}, "1085016128131223552": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "33", "datetime": "2019-01-15 03:29:38", "author": "@logits_ai"}, "1148215789658476544": {"content_summary": "screenshot from https://t.co/0LRCJ4Nldl", "followers": "2,077", "datetime": "2019-07-08 13:02:31", "author": "@ojahnn"}, "1083190859397505024": {"content_summary": "RT @BrundageBot: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, William W.\u2026", "followers": "25,741", "datetime": "2019-01-10 02:36:40", "author": "@Miles_Brundage"}, "1085761416559132673": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "73", "datetime": "2019-01-17 04:51:08", "author": "@tejscript"}, "1085798181730902016": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "26", "datetime": "2019-01-17 07:17:14", "author": "@ReedRoof"}, "1083483546361114624": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "294", "datetime": "2019-01-10 21:59:42", "author": "@rose_miura"}, "1083647339284365317": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "934", "datetime": "2019-01-11 08:50:33", "author": "@pavelkordik"}, "1161578620575506434": {"content_summary": "[4/10] \ud83d\udcc8 - pytorch-transformers - 10,707 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipFgYl - \ud83d\udd17 https://t.co/RdEWpPLOJR", "followers": "222", "datetime": "2019-08-14 10:01:38", "author": "@PapersTrending"}, "1083969459876913152": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "270", "datetime": "2019-01-12 06:10:33", "author": "@kotti_sasikanth"}, "1085770511374725125": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "226", "datetime": "2019-01-17 05:27:17", "author": "@sagarpath"}, "1084615865197199360": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "10", "datetime": "2019-01-14 00:59:08", "author": "@hosjiu"}, "1085758027787976704": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "318", "datetime": "2019-01-17 04:37:40", "author": "@HorevRani"}, "1083486875879272448": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "10", "datetime": "2019-01-10 22:12:56", "author": "@MuruganRIT31996"}, "1083566536189652992": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "274", "datetime": "2019-01-11 03:29:28", "author": "@cghosh_"}, "1083559622517157889": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "445", "datetime": "2019-01-11 03:02:00", "author": "@lucasawilson"}, "1085836886835265536": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "615", "datetime": "2019-01-17 09:51:02", "author": "@KouroshMeshgi"}, "1083852131453620224": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "58", "datetime": "2019-01-11 22:24:19", "author": "@_WizDom13_"}, "1084458869261164545": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "2,671", "datetime": "2019-01-13 14:35:17", "author": "@ayirpelle"}, "1084067770361294848": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1", "datetime": "2019-01-12 12:41:12", "author": "@xuhui_zhou"}, "1083658374657515521": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "255", "datetime": "2019-01-11 09:34:24", "author": "@josipK"}, "1085822265500020736": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "4,891", "datetime": "2019-01-17 08:52:56", "author": "@IgorCarron"}, "1085860507825786882": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "207", "datetime": "2019-01-17 11:24:54", "author": "@PranjalDaga1"}, "1083814632727474177": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,058", "datetime": "2019-01-11 19:55:19", "author": "@GillesMoyse"}, "1093988840770027521": {"content_summary": "\"Transformer-XL learns dependency that is about 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformer during evaluation.\" http", "followers": "149", "datetime": "2019-02-08 21:43:59", "author": "@parker_brydon"}, "1083569385212649472": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "37", "datetime": "2019-01-11 03:40:47", "author": "@dgdaudert"}, "1085389566427779072": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "1,216", "datetime": "2019-01-16 04:13:32", "author": "@matemaz"}, "1083857096045678592": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "87", "datetime": "2019-01-11 22:44:03", "author": "@DevHunterYZ"}, "1083495678372921344": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "2,473", "datetime": "2019-01-10 22:47:54", "author": "@DontFavMe"}, "1083446074335875072": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "83", "datetime": "2019-01-10 19:30:48", "author": "@edorado93"}, "1083542043543117824": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "2,603", "datetime": "2019-01-11 01:52:09", "author": "@AdamMarblestone"}, "1084620723325349888": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "38", "datetime": "2019-01-14 01:18:26", "author": "@experiencor"}, "1083936884915458048": {"content_summary": "RT @ai_lyrn: Google\u2019s new language model - Transformer-XL - achieves SoTA results on multiple datasets (e.g. enwiki8) while being x1800 fas\u2026", "followers": "318", "datetime": "2019-01-12 04:01:06", "author": "@HorevRani"}, "1083647153191305216": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "69", "datetime": "2019-01-11 08:49:49", "author": "@MohitSinghUNIX"}, "1084007755260649472": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "16", "datetime": "2019-01-12 08:42:43", "author": "@evmatica"}, "1083471807779360768": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "277", "datetime": "2019-01-10 21:13:03", "author": "@ZimMatthias"}, "1129314871340871680": {"content_summary": "RT @AlecRad: @chipro They're pretty much the same! Transformer-XL https://t.co/7lT20UaY5u is competitive on PTB. Probably using already tun\u2026", "followers": "4,812", "datetime": "2019-05-17 09:17:01", "author": "@IntuitMachine"}, "1087359292494278661": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZvtEO", "followers": "3,500", "datetime": "2019-01-21 14:40:32", "author": "@arxiv_cscl"}, "1085801233141645313": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "26", "datetime": "2019-01-17 07:29:21", "author": "@KlimZaporojets"}, "1083444328859525120": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "50", "datetime": "2019-01-10 19:23:52", "author": "@skavulya"}, "1084999850637316097": {"content_summary": "Yay for open (and readable) code! \ud83d\ude0d Anyone tried adapting it to other data yet? Just how fast is it (yes, it's faster than Al-Rfou, but that model was downright scary) -- is it competitive to, say, @Smerity et al.'s AWD-LSTM if I don't about small gains or", "followers": "1,954", "datetime": "2019-01-15 02:24:57", "author": "@sjmielke"}, "1083595179217416193": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "511", "datetime": "2019-01-11 05:23:17", "author": "@brtknr"}, "1083742545233002497": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "365", "datetime": "2019-01-11 15:08:52", "author": "@JeanMarcJAzzi"}, "1083784164241858560": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "2,044", "datetime": "2019-01-11 17:54:15", "author": "@puromputo"}, "1083882019531087873": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "912", "datetime": "2019-01-12 00:23:05", "author": "@astro_pyotr"}, "1088976119506849792": {"content_summary": "Transformer-XL(https://t.co/w44xTtLPMQ): relative positional encoding that facilitates the reuse of the hidden states to introduce recurrence mechanism to the Transformer architecture. Improves SoTA results of multiple LM datasets. code (TF & PyTorch)", "followers": "220", "datetime": "2019-01-26 01:45:13", "author": "@ceshine_en"}, "1083479183307800582": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "77,472", "datetime": "2019-01-10 21:42:22", "author": "@rsalakhu"}, "1083444802392338433": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1,288", "datetime": "2019-01-10 19:25:44", "author": "@heghbalz"}, "1083665131895058433": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1,387", "datetime": "2019-01-11 10:01:15", "author": "@cedric_chee"}, "1083189758845730816": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov https://t.co/00cgd1w5bh", "followers": "3,881", "datetime": "2019-01-10 02:32:17", "author": "@BrundageBot"}, "1135919461783838720": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZN4wm", "followers": "3,500", "datetime": "2019-06-04 14:41:18", "author": "@arxiv_cscl"}, "1084265475968651265": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "536", "datetime": "2019-01-13 01:46:48", "author": "@sangha_deb"}, "1083410326387855360": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "161", "datetime": "2019-01-10 17:08:45", "author": "@KavehHassani"}, "1092615316209688576": {"content_summary": "RT @BioDecoded: Transformer-XL: Unleashing the Potential of Attention Models | Google AI Blog https://t.co/uZ2Cfjb4H3 https://t.co/14gvXKEl\u2026", "followers": "698", "datetime": "2019-02-05 02:46:05", "author": "@AINow6"}, "1084917338976092160": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "266", "datetime": "2019-01-14 20:57:05", "author": "@vishwa_jha"}, "1083409238846668801": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "274", "datetime": "2019-01-10 17:04:25", "author": "@cghosh_"}, "1083412796329639942": {"content_summary": "\"Transformer-XL\". The names of things in machine learning are getting goofier and goofier, and I love it.", "followers": "574", "datetime": "2019-01-10 17:18:34", "author": "@lorenlugosch"}, "1091019401946587136": {"content_summary": "Transformer - XL improves language models by taking the dependency beyond the fixed length context. Will share Google's Translation efforts https://t.co/sdX1tSjOyP", "followers": "19,016", "datetime": "2019-01-31 17:04:30", "author": "@krishnan"}, "1129292544267960321": {"content_summary": "RT @AlecRad: @chipro They're pretty much the same! Transformer-XL https://t.co/7lT20UaY5u is competitive on PTB. Probably using already tun\u2026", "followers": "818", "datetime": "2019-05-17 07:48:18", "author": "@ErmiaBivatan"}, "1085812695461330944": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "2,475", "datetime": "2019-01-17 08:14:54", "author": "@sindero"}, "1084683686753255424": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "24", "datetime": "2019-01-14 05:28:38", "author": "@rewreu"}, "1083754465625366535": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "350", "datetime": "2019-01-11 15:56:14", "author": "@0_rishabh"}, "1083412303217713152": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "198", "datetime": "2019-01-10 17:16:36", "author": "@omar_javd"}, "1212550519807283200": {"content_summary": "@theshawwn Transformer-XL is a bit like a hybrid of RNN and Transformer -- you process document sequentially and the model can use states from previous forward passes in addition to the current one -- https://t.co/AKaf2rv0XS", "followers": "2,208", "datetime": "2020-01-02 01:45:46", "author": "@yaroslavvb"}, "1083578239237292032": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "10", "datetime": "2019-01-11 04:15:58", "author": "@ImrulJubair"}, "1087967961586024450": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "303", "datetime": "2019-01-23 06:59:10", "author": "@cepera_ang"}, "1091386485217976320": {"content_summary": "Transformer-XL: Unleashing the Potential of Attention Models | Google AI Blog https://t.co/uZ2Cfjb4H3 https://t.co/14gvXKElxq #MachineLearning #NLP https://t.co/844ZrqEKM1", "followers": "1,131", "datetime": "2019-02-01 17:23:09", "author": "@BioDecoded"}, "1085831631775166464": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "198", "datetime": "2019-01-17 09:30:09", "author": "@dahlemd"}, "1083590168139841537": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "196", "datetime": "2019-01-11 05:03:22", "author": "@aavella77"}, "1083757884666724352": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "546", "datetime": "2019-01-11 16:09:49", "author": "@vdv7"}, "1083410257316114438": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "8,374", "datetime": "2019-01-10 17:08:28", "author": "@mldcmu"}, "1083531366514356224": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "38", "datetime": "2019-01-11 01:09:43", "author": "@experiencor"}, "1083745926341107718": {"content_summary": "It\u2019s always exciting to see advances in latency while preserving accuracy (hopefully). Latency is an underrated factor (certainly not the only one) that helps decide what model stays as academic research or goes into production.", "followers": "1,755", "datetime": "2019-01-11 15:22:18", "author": "@vykthur"}, "1255636138393579521": {"content_summary": "Benim kulland\u0131\u011f\u0131m ve i\u00e7eri\u011fini sevdi\u011fim #MachineLearning Bilinmesi Gereken Ara\u015ft\u0131rma Makaleleri 1)https://t.co/QAofv0DBGm 2)https://t.co/VyeLZZ1Pss 3)https://t.co/Zn4deZEuKg 4)https://t.co/TPeDZgI7Z1 5)https://t.co/UrPbzY1PVk 6)https://t.co/9rN7GM7NGs 7)ht", "followers": "551", "datetime": "2020-04-29 23:12:38", "author": "@mtugrull"}, "1083710986085900289": {"content_summary": "interesting", "followers": "89", "datetime": "2019-01-11 13:03:28", "author": "@mindhash09"}, "1085924238027079680": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "208", "datetime": "2019-01-17 15:38:08", "author": "@Don_Rubiel"}, "1083455653207654400": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "144", "datetime": "2019-01-10 20:08:52", "author": "@FelixNeutatz"}, "1085757010971308033": {"content_summary": "Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an overview of the model and key concepts such as the recurrence mechanism and the relative positional encoding scheme. https://t.co/ORv", "followers": "81,553", "datetime": "2019-01-17 04:33:38", "author": "@hardmaru"}, "1083759785416187905": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "1,647", "datetime": "2019-01-11 16:17:22", "author": "@hllo_wrld"}, "1083459343347941376": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "3,096", "datetime": "2019-01-10 20:23:31", "author": "@ivan_bezdomny"}, "1083578054277062657": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "150", "datetime": "2019-01-11 04:15:14", "author": "@rodgzilla"}, "1083176954822623238": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZvtEO", "followers": "3,500", "datetime": "2019-01-10 01:41:25", "author": "@arxiv_cscl"}, "1083850846352347136": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "299", "datetime": "2019-01-11 22:19:13", "author": "@westis96"}, "1083440453553291264": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "15,224", "datetime": "2019-01-10 19:08:28", "author": "@lmthang"}, "1083638177167011840": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "754", "datetime": "2019-01-11 08:14:09", "author": "@suneelmarthi"}, "1084976788856483840": {"content_summary": "RT @icoxfog417: Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u2026", "followers": "3,633", "datetime": "2019-01-15 00:53:19", "author": "@HirokatuKataoka"}, "1083452222866972672": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "934", "datetime": "2019-01-10 19:55:14", "author": "@pavelkordik"}, "1083580686609440773": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "93", "datetime": "2019-01-11 04:25:42", "author": "@0xhexhex"}, "1129185531957350400": {"content_summary": "@chipro They're pretty much the same! Transformer-XL https://t.co/7lT20UaY5u is competitive on PTB. Probably using already tuned hp settings for the RNN and not for the Transformer?", "followers": "33,897", "datetime": "2019-05-17 00:43:04", "author": "@AlecRad"}, "1083559199798226944": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "226", "datetime": "2019-01-11 03:00:19", "author": "@sagarpath"}, "1084053957964713985": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "145", "datetime": "2019-01-12 11:46:18", "author": "@esvhd"}, "1083632712014065664": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "16", "datetime": "2019-01-11 07:52:26", "author": "@p_kot1"}, "1083409596792872960": {"content_summary": "Adding long-term dependency to transformer networks, with code in *both* Pytorch and Tensorflow! @Emily_Alsentzer: This is right on cue after our conversation the other day.", "followers": "3,793", "datetime": "2019-01-10 17:05:51", "author": "@IAmSamFin"}, "1083452529055129600": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "122", "datetime": "2019-01-10 19:56:27", "author": "@adinagarajan"}, "1083658654295998464": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "218", "datetime": "2019-01-11 09:35:31", "author": "@AssistedEvolve"}, "1083373103458197505": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZvtEO", "followers": "3,500", "datetime": "2019-01-10 14:40:50", "author": "@arxiv_cscl"}, "1084118273485025281": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "98", "datetime": "2019-01-12 16:01:52", "author": "@kgwmath"}, "1083610283765952512": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "615", "datetime": "2019-01-11 06:23:18", "author": "@KouroshMeshgi"}, "1083914511810088963": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "109", "datetime": "2019-01-12 02:32:12", "author": "@hzmehrdad"}, "1083720601607569411": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,088", "datetime": "2019-01-11 13:41:40", "author": "@rosstaylor90"}, "1083750652143173637": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "107", "datetime": "2019-01-11 15:41:05", "author": "@LenzBelzner"}, "1083659012573253632": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "335", "datetime": "2019-01-11 09:36:56", "author": "@kobi78"}, "1083642056747175936": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "569", "datetime": "2019-01-11 08:29:34", "author": "@Klevis_Ramo"}, "1083593275913646080": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "15", "datetime": "2019-01-11 05:15:43", "author": "@vi_shall_c"}, "1085940064939589634": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "3,102", "datetime": "2019-01-17 16:41:01", "author": "@kastnerkyle"}, "1083567599189921792": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "222", "datetime": "2019-01-11 03:33:42", "author": "@MattScicluna"}, "1083189634501365760": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov https://t.co/LiNiqqycyJ", "followers": "311", "datetime": "2019-01-10 02:31:48", "author": "@arxiv_cs_LG"}, "1090360182947893249": {"content_summary": "\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov (Submitted on 9 Jan 2019 (v1), last revised 18 Jan 2019 (this version, v2))\" https://t.co/Y", "followers": "355", "datetime": "2019-01-29 21:25:00", "author": "@tSILIChtetL"}, "1085805358470197249": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "99", "datetime": "2019-01-17 07:45:45", "author": "@treasured_write"}, "1090444240583221248": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "212", "datetime": "2019-01-30 02:59:01", "author": "@nanjakorewa"}, "1166379003286102016": {"content_summary": "RT @PapersTrending: [10/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 11,218 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipF\u2026", "followers": "42", "datetime": "2019-08-27 15:56:39", "author": "@MishakinSergey"}, "1084647236468858880": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "9", "datetime": "2019-01-14 03:03:47", "author": "@onetechio"}, "1083434464745795584": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "48", "datetime": "2019-01-10 18:44:40", "author": "@ThingsReallyR"}, "1084441881805758465": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "66", "datetime": "2019-01-13 13:27:47", "author": "@shubh_300595"}, "1085374813709824000": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "51", "datetime": "2019-01-16 03:14:55", "author": "@hammad214"}, "1085226091617214464": {"content_summary": "RT @icoxfog417: Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u2026", "followers": "48", "datetime": "2019-01-15 17:23:57", "author": "@kame_123456789"}, "1085950130581319681": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "92", "datetime": "2019-01-17 17:21:01", "author": "@SentimOfficial"}, "1131862803676041216": {"content_summary": "[6/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 6,367 \u2b50 - \ud83d\udcc4 https://t.co/IlZvRq3CVg - \ud83d\udd17 https://t.co/RdEWpPLOJR", "followers": "222", "datetime": "2019-05-24 10:01:36", "author": "@PapersTrending"}, "1083465206762737666": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "55", "datetime": "2019-01-10 20:46:49", "author": "@khaledvic"}, "1083178448296837121": {"content_summary": "https://t.co/oF1cxmUw9F Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. (arXiv:1901.02860v1 [cs.LG]) #NLProc", "followers": "4,200", "datetime": "2019-01-10 01:47:21", "author": "@arxiv_cs_cl"}, "1083795279193755654": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "179,069", "datetime": "2019-01-11 18:38:25", "author": "@Montreal_AI"}, "1083511716514648065": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "85", "datetime": "2019-01-10 23:51:38", "author": "@daiquocng"}, "1083799196254330881": {"content_summary": "\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context \" by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell and @rsalakhu from @CarnegieMellon and @GoogleAI. Read the full paper at https://t.co/DYaPbCmPby https://t.co/JzRPbch36c", "followers": "2,949", "datetime": "2019-01-11 18:53:59", "author": "@Synced_Global"}, "1083670990285144064": {"content_summary": "Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely going beyond fixed-length contexts and improves inference-time speed. These improvements should be useful for any generation task (con", "followers": "419", "datetime": "2019-01-11 10:24:32", "author": "@nik0spapp"}, "1083788812566888449": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "58", "datetime": "2019-01-11 18:12:43", "author": "@aptr322"}, "1083742454770290689": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "66", "datetime": "2019-01-11 15:08:30", "author": "@diegovogeid"}, "1083623725558312960": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "10,655", "datetime": "2019-01-11 07:16:43", "author": "@kaalam_ai"}, "1083586029150453760": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "409", "datetime": "2019-01-11 04:46:56", "author": "@AiMechanic"}, "1086813585106321408": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "55", "datetime": "2019-01-20 02:32:05", "author": "@satyaprk2"}, "1083417342481031170": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "95", "datetime": "2019-01-10 17:36:38", "author": "@backpropagater"}, "1083761375871225858": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "157", "datetime": "2019-01-11 16:23:41", "author": "@Swayson"}, "1083779338011664384": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "240", "datetime": "2019-01-11 17:35:04", "author": "@ilfurio"}, "1083776355379171328": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "66", "datetime": "2019-01-11 17:23:13", "author": "@diegovogeid"}, "1083769854694428677": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "19", "datetime": "2019-01-11 16:57:23", "author": "@vonum123"}, "1083582459613044736": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "216", "datetime": "2019-01-11 04:32:45", "author": "@KSKSKSKS2"}, "1084490838036537344": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "248", "datetime": "2019-01-13 16:42:19", "author": "@gsksantosh"}, "1083708731265736709": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "245", "datetime": "2019-01-11 12:54:30", "author": "@__jm"}, "1083721884095651841": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "100", "datetime": "2019-01-11 13:46:46", "author": "@KarimiRabeeh"}, "1086043202354929665": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "12", "datetime": "2019-01-17 23:30:51", "author": "@kaushik3131993"}, "1084283576022167552": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "292", "datetime": "2019-01-13 02:58:44", "author": "@Shujian_Liu"}, "1083435896278519808": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "20", "datetime": "2019-01-10 18:50:21", "author": "@Boristream"}, "1085819245416640517": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "3,013", "datetime": "2019-01-17 08:40:56", "author": "@sarahbadr"}, "1083631178140139520": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "96", "datetime": "2019-01-11 07:46:20", "author": "@lambdaPerf"}, "1084142484320587776": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "193", "datetime": "2019-01-12 17:38:05", "author": "@alexhock"}, "1083741191009681408": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "77,472", "datetime": "2019-01-11 15:03:29", "author": "@rsalakhu"}, "1083445517453938689": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "18", "datetime": "2019-01-10 19:28:35", "author": "@vrtejus"}, "1085766452844552193": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "216", "datetime": "2019-01-17 05:11:09", "author": "@KSKSKSKS2"}, "1083667873615306752": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "2,568", "datetime": "2019-01-11 10:12:09", "author": "@mathz_aragao"}, "1083702894396878848": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "74", "datetime": "2019-01-11 12:31:18", "author": "@luisfredgs"}, "1083479702088622080": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "924", "datetime": "2019-01-10 21:44:25", "author": "@michaeldbishop"}, "1083461981791838209": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "378", "datetime": "2019-01-10 20:34:00", "author": "@RyanAEMetz"}, "1083613622067113984": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "739", "datetime": "2019-01-11 06:36:34", "author": "@philip368320"}, "1083596553036726273": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "282", "datetime": "2019-01-11 05:28:45", "author": "@vksbhandary"}, "1083612478221754368": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "279", "datetime": "2019-01-11 06:32:02", "author": "@theolivenbaum"}, "1083456141592539136": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "109", "datetime": "2019-01-10 20:10:48", "author": "@hzmehrdad"}, "1083650147836297216": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "36", "datetime": "2019-01-11 09:01:43", "author": "@vasan_ashwin"}, "1083667237154676736": {"content_summary": "Transformer-XL: attentive LMs beyond a fixed-length context: https://t.co/OPrJBT9O6V \"Due to the state reuse scheme, Transformer-XL achieves an up to 1,874 times speedup during evaluation compared to the vanilla Transformer architecture in Al-Rfou et al.", "followers": "1,387", "datetime": "2019-01-11 10:09:37", "author": "@cedric_chee"}, "1083713015617413120": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "1,288", "datetime": "2019-01-11 13:11:31", "author": "@heghbalz"}, "1083665477904359424": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,025", "datetime": "2019-01-11 10:02:38", "author": "@desertnaut"}, "1086031194238738432": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "43", "datetime": "2019-01-17 22:43:08", "author": "@jeandut14000"}, "1086109651068948489": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "46", "datetime": "2019-01-18 03:54:54", "author": "@pczzy"}, "1124977620339384320": {"content_summary": "[9/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 5,880 \u2b50 - \ud83d\udcc4 https://t.co/IlZvRq3CVg - \ud83d\udd17 https://t.co/RdEWpPLOJR", "followers": "222", "datetime": "2019-05-05 10:02:20", "author": "@PapersTrending"}, "1177523839468785664": {"content_summary": "[4/10] \ud83d\udcc8 - pytorch-transformers - 13,420 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipFgYl - \ud83d\udd17 https://t.co/7bzXyCHTjk", "followers": "222", "datetime": "2019-09-27 10:02:15", "author": "@PapersTrending"}, "1115137434063450112": {"content_summary": "RT @BioDecoded: Transformer-XL: Unleashing the Potential of Attention Models | Google AI Blog https://t.co/uZ2Cfjb4H3 https://t.co/14gvXKEl\u2026", "followers": "11,930", "datetime": "2019-04-08 06:20:57", "author": "@Rosenchild"}, "1083477019503915008": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "322", "datetime": "2019-01-10 21:33:46", "author": "@letranger14"}, "1083732718570094592": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "1,918", "datetime": "2019-01-11 14:29:49", "author": "@DocXavi"}, "1083610079125860352": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "615", "datetime": "2019-01-11 06:22:30", "author": "@KouroshMeshgi"}, "1083753568522461184": {"content_summary": "BetterTransformer ?", "followers": "1,882", "datetime": "2019-01-11 15:52:40", "author": "@ThomasScialom"}, "1083460307962552320": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "128", "datetime": "2019-01-10 20:27:21", "author": "@feedmari"}, "1084248667567996928": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "99,939", "datetime": "2019-01-13 00:40:01", "author": "@jeremyphoward"}, "1083500298558042112": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "46", "datetime": "2019-01-10 23:06:16", "author": "@pczzy"}, "1083674323695484928": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "82", "datetime": "2019-01-11 10:37:47", "author": "@Saurabh98G"}, "1083736085505757184": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "28,418", "datetime": "2019-01-11 14:43:12", "author": "@ogrisel"}, "1083431136708231169": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "411", "datetime": "2019-01-10 18:31:26", "author": "@JabaBanik"}, "1083764879713075202": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1,257", "datetime": "2019-01-11 16:37:37", "author": "@KevinClarity"}, "1086167056930738176": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "29", "datetime": "2019-01-18 07:43:01", "author": "@Donatas36474150"}, "1160896315003473920": {"content_summary": "Transformer-XL updated and released! Along with code and pretrained models. SoTA on enwik8, text8, One Billion Word, WikiText-103. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/z40rSb5nUb code https://t.co/Bt5p0CVfaU", "followers": "174", "datetime": "2019-08-12 12:50:24", "author": "@re_mahmoudi"}, "1129315939365736448": {"content_summary": "RT @AlecRad: @chipro They're pretty much the same! Transformer-XL https://t.co/7lT20UaY5u is competitive on PTB. Probably using already tun\u2026", "followers": "10", "datetime": "2019-05-17 09:21:16", "author": "@yarphs"}, "1083202335294279681": {"content_summary": "\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\", Zihang Dai, Zhilin Yang, Yiming Yang, Wi\u2026 https://t.co/QopzcGfXwN", "followers": "772", "datetime": "2019-01-10 03:22:16", "author": "@arxivml"}, "1083618260954673152": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,248", "datetime": "2019-01-11 06:55:00", "author": "@daisuzu"}, "1085854074950553602": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "366", "datetime": "2019-01-17 10:59:20", "author": "@iugoaoj"}, "1085971859512815616": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "25,311", "datetime": "2019-01-17 18:47:22", "author": "@deeplearning4j"}, "1086806046327345155": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "87", "datetime": "2019-01-20 02:02:07", "author": "@jeremimucha"}, "1083651220907089920": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "240", "datetime": "2019-01-11 09:05:58", "author": "@malcgreaves"}, "1083564803967410178": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - https://t.co/9D1COPolYq https://t.co/Te7hgsrymr", "followers": "195", "datetime": "2019-01-11 03:22:35", "author": "@hereticreader"}, "1085978903376728064": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "590", "datetime": "2019-01-17 19:15:21", "author": "@codekee"}, "1083518967824637953": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "395", "datetime": "2019-01-11 00:20:27", "author": "@linkoffate"}, "1083178057291182080": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. (arXiv:1901.02860v1 [cs.LG]) https://t.co/BSHqJ74sIF", "followers": "9,689", "datetime": "2019-01-10 01:45:47", "author": "@StatMLPapers"}, "1083678476312297472": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "25", "datetime": "2019-01-11 10:54:17", "author": "@yang_zonghan"}, "1084659101056012288": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "26", "datetime": "2019-01-14 03:50:56", "author": "@khushal2898"}, "1083482063473397767": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "13,798", "datetime": "2019-01-10 21:53:48", "author": "@mark_riedl"}, "1085935833834835968": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "2,102", "datetime": "2019-01-17 16:24:13", "author": "@datitran"}, "1083588705657356288": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "99", "datetime": "2019-01-11 04:57:34", "author": "@greed2411"}, "1083760072172421120": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "3,977", "datetime": "2019-01-11 16:18:31", "author": "@SanhEstPasMoi"}, "1083440435513708544": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "157", "datetime": "2019-01-10 19:08:23", "author": "@CodiceDD"}, "1083584522850127872": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "813", "datetime": "2019-01-11 04:40:56", "author": "@1960vd"}, "1083719977557991429": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "4,014", "datetime": "2019-01-11 13:39:11", "author": "@freitasmanuel"}, "1083523076736745473": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "141", "datetime": "2019-01-11 00:36:47", "author": "@the_onederful"}, "1090463433387057152": {"content_summary": "https://t.co/JBU59aDz6I", "followers": "1,789", "datetime": "2019-01-30 04:15:17", "author": "@steinly0"}, "1083667260181569537": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,390", "datetime": "2019-01-11 10:09:43", "author": "@elasticjava"}, "1083796227253301248": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "145", "datetime": "2019-01-11 18:42:11", "author": "@esvhd"}, "1135723172714745857": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZvtEO", "followers": "3,500", "datetime": "2019-06-04 01:41:19", "author": "@arxiv_cscl"}, "1083727903441879045": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "22", "datetime": "2019-01-11 14:10:41", "author": "@qoboty"}, "1083561418610434049": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "24", "datetime": "2019-01-11 03:09:08", "author": "@blissunplugged"}, "1085311054433869825": {"content_summary": "RT @icoxfog417: Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u2026", "followers": "141", "datetime": "2019-01-15 23:01:34", "author": "@negiyas"}, "1083518883640811520": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "291", "datetime": "2019-01-11 00:20:07", "author": "@_lpag"}, "1083700279361466368": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "302", "datetime": "2019-01-11 12:20:55", "author": "@subhobrata1"}, "1084258511423324160": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "3,772", "datetime": "2019-01-13 01:19:08", "author": "@volkuleshov"}, "1083604964205682688": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "69", "datetime": "2019-01-11 06:02:10", "author": "@ku21fan"}, "1083474397766213632": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "824", "datetime": "2019-01-10 21:23:21", "author": "@morioka"}, "1083599279338401792": {"content_summary": "RT @ng2amarinefunk: up to 1,800+ times faster than vanilla Transformer during evaluation. Additionally, we improve the state-of-the-art (So\u2026", "followers": "1,034", "datetime": "2019-01-11 05:39:35", "author": "@ng2amarinefunk"}, "1083582215164813312": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. https://t.co/h3P9irhJ5b", "followers": "9,305", "datetime": "2019-01-11 04:31:46", "author": "@sofwath"}, "1083495730399100929": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "42", "datetime": "2019-01-10 22:48:07", "author": "@A_Mairesse"}, "1083201740235853824": {"content_summary": "RT @BrundageBot: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, William W.\u2026", "followers": "124", "datetime": "2019-01-10 03:19:54", "author": "@bbc1183"}, "1083687072890667008": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "155", "datetime": "2019-01-11 11:28:26", "author": "@keylinker"}, "1083699792964837378": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "7", "datetime": "2019-01-11 12:18:59", "author": "@hi_minhoryu"}, "1083712730111115265": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "357", "datetime": "2019-01-11 13:10:23", "author": "@_florianmai"}, "1083442654237474816": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "57", "datetime": "2019-01-10 19:17:12", "author": "@DennisYTShen"}, "1083486710774796288": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "47", "datetime": "2019-01-10 22:12:16", "author": "@mariusmosbach"}, "1083728923811143681": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "134", "datetime": "2019-01-11 14:14:44", "author": "@ClouderizerT"}, "1083545335908032512": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "243", "datetime": "2019-01-11 02:05:14", "author": "@daigo_hirooka"}, "1083569105163177984": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "292", "datetime": "2019-01-11 03:39:41", "author": "@Shujian_Liu"}, "1084019876727902210": {"content_summary": "RT @ai_lyrn: Google\u2019s new language model - Transformer-XL - achieves SoTA results on multiple datasets (e.g. enwiki8) while being x1800 fas\u2026", "followers": "58", "datetime": "2019-01-12 09:30:53", "author": "@ShuvenduBikash"}, "1083720463979692034": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "250", "datetime": "2019-01-11 13:41:07", "author": "@ken_hide"}, "1083495849890594816": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "89", "datetime": "2019-01-10 22:48:35", "author": "@mgrankin"}, "1083488485397282816": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "265", "datetime": "2019-01-10 22:19:19", "author": "@Swetava"}, "1089724983880892416": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "2", "datetime": "2019-01-28 03:20:56", "author": "@allen0308"}, "1083941541087834117": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "698", "datetime": "2019-01-12 04:19:36", "author": "@AINow6"}, "1083757565165649920": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "194", "datetime": "2019-01-11 16:08:33", "author": "@jastner109"}, "1083665839180652544": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "43", "datetime": "2019-01-11 10:04:04", "author": "@jeandut14000"}, "1083452879703134208": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "10", "datetime": "2019-01-10 19:57:50", "author": "@LazyOp"}, "1089435609884647425": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "2,728", "datetime": "2019-01-27 08:11:04", "author": "@AImanfuture"}, "1083787426756284417": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "159,742", "datetime": "2019-01-11 18:07:12", "author": "@Montreal_IA"}, "1085787952179867648": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "564", "datetime": "2019-01-17 06:36:35", "author": "@gazay"}, "1085767147568033792": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,882", "datetime": "2019-01-17 05:13:55", "author": "@AtulAcharya"}, "1083581144346644480": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "15", "datetime": "2019-01-11 04:27:31", "author": "@BADRINATHJAYAK1"}, "1083439174974406661": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "340", "datetime": "2019-01-10 19:03:23", "author": "@Cedias_"}, "1083928763736322048": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "415", "datetime": "2019-01-12 03:28:50", "author": "@__nggih"}, "1085779111996321798": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "18,260", "datetime": "2019-01-17 06:01:27", "author": "@quocleix"}, "1145995858086903809": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. https://t.co/Enx0Dnsxdh (Photo by Patrick Tomasso on Unsplash). https://t.co/02t6lCIPed", "followers": "30", "datetime": "2019-07-02 10:01:18", "author": "@serendeepia"}, "1084196896229355520": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "55", "datetime": "2019-01-12 21:14:18", "author": "@satyaprk2"}, "1083621913275105280": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "61", "datetime": "2019-01-11 07:09:31", "author": "@albelwu"}, "1083421869565452295": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "51", "datetime": "2019-01-10 17:54:37", "author": "@SandeshGhi"}, "1083654024677580802": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "615", "datetime": "2019-01-11 09:17:07", "author": "@KouroshMeshgi"}, "1090418947277635585": {"content_summary": "Joint work between @googleai, @scsatcmu, @mldcmu on Transformer-XL, a novel neural net architecture that that enables natural language understanding beyond a fixed-length context. Learn more \u2192 https://t.co/DNqSwfBZap Paper \u2192 https://t.co/7Dbdqnuuro https:/", "followers": "8,374", "datetime": "2019-01-30 01:18:30", "author": "@mldcmu"}, "1083593356876247042": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "99", "datetime": "2019-01-11 05:16:03", "author": "@treasured_write"}, "1083734119194841091": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1", "datetime": "2019-01-11 14:35:23", "author": "@CarlXie0"}, "1083862126102482945": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "124", "datetime": "2019-01-11 23:04:02", "author": "@Santiag72427700"}, "1083812589031837696": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "1,058", "datetime": "2019-01-11 19:47:12", "author": "@GillesMoyse"}, "1085757742952763392": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "361", "datetime": "2019-01-17 04:36:32", "author": "@BioNLProc"}, "1083978465781506048": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "97", "datetime": "2019-01-12 06:46:20", "author": "@atakanince"}, "1083787512433336321": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "159,742", "datetime": "2019-01-11 18:07:33", "author": "@Montreal_IA"}, "1090499490946207744": {"content_summary": "Yet another language model from Google: Transformer-XL. What's great is that they make code and pretrained models available in both Tensorflow and PyTorch! #NLP #NLProc https://t.co/NfIfh3o07l", "followers": "335", "datetime": "2019-01-30 06:38:33", "author": "@AarneTalman"}, "1084250897717358593": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "34", "datetime": "2019-01-13 00:48:53", "author": "@MatRazor"}, "1083549769526624256": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "754", "datetime": "2019-01-11 02:22:51", "author": "@suneelmarthi"}, "1083478737449115648": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "218", "datetime": "2019-01-10 21:40:35", "author": "@AssistedEvolve"}, "1083442755873890304": {"content_summary": "RT @arxiv_cscl: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/P5f3OZN4wm", "followers": "1,411", "datetime": "2019-01-10 19:17:37", "author": "@RexDouglass"}, "1083595843939241984": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "163,852", "datetime": "2019-01-11 05:25:56", "author": "@ceobillionaire"}, "1083444339756462089": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "89", "datetime": "2019-01-10 19:23:54", "author": "@margusb"}, "1085830397815078912": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "1,060", "datetime": "2019-01-17 09:25:15", "author": "@loretoparisi"}, "1086029071190913025": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "22", "datetime": "2019-01-17 22:34:42", "author": "@seanie_12"}, "1085880371529474048": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "245", "datetime": "2019-01-17 12:43:49", "author": "@__jm"}, "1083583318350688256": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "59", "datetime": "2019-01-11 04:36:09", "author": "@liqiangniu"}, "1176841285400977410": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "89", "datetime": "2019-09-25 12:50:01", "author": "@saadmrb"}, "1085951922954256386": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "4,255", "datetime": "2019-01-17 17:28:09", "author": "@weballergy"}, "1083907850177368064": {"content_summary": "Google\u2019s new language model - Transformer-XL - achieves SoTA results on multiple datasets (e.g. enwiki8) while being x1800 faster. It combines their vanilla Transformer with concepts from RNN to improve long-term dependencies. https://t.co/RA5yX8gDdf Sum", "followers": "66", "datetime": "2019-01-12 02:05:44", "author": "@ai_lyrn"}, "1083719470818836481": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "274", "datetime": "2019-01-11 13:37:11", "author": "@cghosh_"}, "1083450738582253568": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "21", "datetime": "2019-01-10 19:49:20", "author": "@luoyuchu"}, "1085757297551335425": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "3,665", "datetime": "2019-01-17 04:34:46", "author": "@eigenhector"}, "1083518317388738560": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "395", "datetime": "2019-01-11 00:17:52", "author": "@linkoffate"}, "1085965767823319041": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "181", "datetime": "2019-01-17 18:23:09", "author": "@shubham_stark"}, "1083635316370690049": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "172", "datetime": "2019-01-11 08:02:47", "author": "@ggdupont"}, "1084262070231670784": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "152", "datetime": "2019-01-13 01:33:16", "author": "@ZizhaoZhang"}, "1085901263957647360": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "330", "datetime": "2019-01-17 14:06:51", "author": "@mr_ubik"}, "1083628256090157056": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "769", "datetime": "2019-01-11 07:34:43", "author": "@danyoel"}, "1085821282472595457": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "1,025", "datetime": "2019-01-17 08:49:01", "author": "@desertnaut"}, "1083758088425897991": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "47", "datetime": "2019-01-11 16:10:38", "author": "@justSravan"}, "1083772233338675200": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "95", "datetime": "2019-01-11 17:06:50", "author": "@trgokhale"}, "1085946254402772993": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "649", "datetime": "2019-01-17 17:05:37", "author": "@cortexelation"}, "1083429118207762432": {"content_summary": "#arXiv #machinelearning [cs.LG] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. (arXiv:1901.02860v1 [cs.LG]) https://t.co/Ijj7L68cvs Transformer networks have a potential of learning longer-term dependency, but are limited by a fi", "followers": "1,713", "datetime": "2019-01-10 18:23:25", "author": "@arXiv__ml"}, "1085786641212559365": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "163,852", "datetime": "2019-01-17 06:31:22", "author": "@ceobillionaire"}, "1084977056914391040": {"content_summary": "RT @icoxfog417: Transformer\u3067\u306f\u5165\u529b\u3092\u56fa\u5b9a\u9577\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f\u9650\u5b9a\u3055\u308c\u305f\u9577\u3055\u306e\u6587\u8108\u3057\u304b\u6271\u3048\u306a\u304b\u3063\u305f\u3002\u305d\u3053\u3067\u524d\u56de\u306e\u96a0\u308c\u5c64\u3092\u5f15\u304d\u7d99\u3050\u624b\u6cd5\u306e\u63d0\u6848\u3002\u524d\u56de/\u4eca\u56de\u306e\u4f4d\u7f6e\u95a2\u4fc2\u3092\u628a\u63e1\u3055\u305b\u308b\u305f\u3081\u306brelative\u306aposition encoding\u3092\u4f7f\u2026", "followers": "112", "datetime": "2019-01-15 00:54:22", "author": "@tsuchinokoyoshi"}, "1086291370296274944": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "331", "datetime": "2019-01-18 15:56:59", "author": "@sai_prasanna"}, "1084164397654859776": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "74", "datetime": "2019-01-12 19:05:09", "author": "@luisfredgs"}, "1083579401797656576": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during evaluation. New SoTA results on Wikipedia (enwik8, text8, WikiText-103), One Billion Words, and PennTree Bank. \ud83d\udd25 https://t.co/xmpTB", "followers": "81,553", "datetime": "2019-01-11 04:20:35", "author": "@hardmaru"}, "1083522839834112000": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "141", "datetime": "2019-01-11 00:35:50", "author": "@the_onederful"}, "1085218379420577794": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "44", "datetime": "2019-01-15 16:53:18", "author": "@AndreyDulub"}, "1083785767774609408": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "66", "datetime": "2019-01-11 18:00:37", "author": "@kishoreugal"}, "1167452604152057857": {"content_summary": "RT @PapersTrending: [8/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 11,474 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipWR\u2026", "followers": "42", "datetime": "2019-08-30 15:02:45", "author": "@MishakinSergey"}, "1084703997594722310": {"content_summary": "Wow \ud83d\udd25\ud83d\udd25", "followers": "207", "datetime": "2019-01-14 06:49:20", "author": "@PranjalDaga1"}, "1084053970409148416": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "145", "datetime": "2019-01-12 11:46:21", "author": "@esvhd"}, "1083456524310245379": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "1,348", "datetime": "2019-01-10 20:12:19", "author": "@udmrzn"}, "1084802545980104704": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "456", "datetime": "2019-01-14 13:20:56", "author": "@PerthMLGroup"}, "1083623096588808192": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "366", "datetime": "2019-01-11 07:14:13", "author": "@surafelml"}, "1083568948811952128": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "19", "datetime": "2019-01-11 03:39:03", "author": "@raamnaathn"}, "1083688099463475200": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "514", "datetime": "2019-01-11 11:32:31", "author": "@cathalhoran"}, "1083851054440017920": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "53", "datetime": "2019-01-11 22:20:03", "author": "@MktO740123"}, "1083492801625710593": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "456", "datetime": "2019-01-10 22:36:28", "author": "@PerthMLGroup"}, "1083593318527692801": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "99", "datetime": "2019-01-11 05:15:53", "author": "@treasured_write"}, "1083728303326924801": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,655", "datetime": "2019-01-11 14:12:16", "author": "@keunwoochoi"}, "1083630469646696450": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "564", "datetime": "2019-01-11 07:43:31", "author": "@gazay"}, "1083754092965462016": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "239", "datetime": "2019-01-11 15:54:45", "author": "@Dneutr0n"}, "1083595807281037312": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "163,852", "datetime": "2019-01-11 05:25:47", "author": "@ceobillionaire"}, "1084040024096677888": {"content_summary": "RT @ai_lyrn: Google\u2019s new language model - Transformer-XL - achieves SoTA results on multiple datasets (e.g. enwiki8) while being x1800 fas\u2026", "followers": "279", "datetime": "2019-01-12 10:50:56", "author": "@theolivenbaum"}, "1083481378073632768": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "208", "datetime": "2019-01-10 21:51:05", "author": "@masashi162"}, "1086344448152399873": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "63", "datetime": "2019-01-18 19:27:54", "author": "@fkariminejadasl"}, "1085353151496998912": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "9", "datetime": "2019-01-16 01:48:50", "author": "@WeinroT_xc"}, "1083648020216991745": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "268", "datetime": "2019-01-11 08:53:15", "author": "@corbyjerez"}, "1090755478903291911": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "30", "datetime": "2019-01-30 23:35:46", "author": "@DeclanC49019748"}, "1083437765524680710": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "339", "datetime": "2019-01-10 18:57:47", "author": "@IUILab"}, "1083402910451433472": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "175", "datetime": "2019-01-10 16:39:17", "author": "@sappy_and_sappy"}, "1083442898404663296": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "664", "datetime": "2019-01-10 19:18:11", "author": "@crude2refined"}, "1083191192211189760": {"content_summary": "RT @BrundageBot: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, William W.\u2026", "followers": "281", "datetime": "2019-01-10 02:37:59", "author": "@_agilajah"}, "1083601035522846720": {"content_summary": "RT @nkmry_: \u518d\u5e30\u6a5f\u69cb\u3068\u76f8\u5bfe\u7684\u306a position embedding \u3092\u8003\u6848\u3057\u3066 Transformer \u3092 1800\u500d\u9ad8\u901f\u5316\uff01 TensorFlow \u3068 PyTorch \u306e\u5b9f\u88c5\u3082\u516c\u958b\u3057\u3066\u3044\u308b\u3002 https://t.co/9dv5w8pBCp", "followers": "12,765", "datetime": "2019-01-11 05:46:33", "author": "@jaguring1"}, "1083739057765662725": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "194", "datetime": "2019-01-11 14:55:00", "author": "@jastner109"}, "1083191739450646529": {"content_summary": "RT @arxiv_cs_LG: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, William W.\u2026", "followers": "783", "datetime": "2019-01-10 02:40:10", "author": "@muktabh"}, "1085787703030042624": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "934", "datetime": "2019-01-17 06:35:36", "author": "@pavelkordik"}, "1083397812795039744": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u56fa\u5b9a\u9577\u306b\u5206\u5272\u3059\u308b\u305f\u3081\u4f9d\u5b58\u6027\u304c\u5207\u308c\u308b\u554f\u984c\u304c\u3042\u3063\u305f\u3002Transformer-XL\u306f\u72b6\u614b\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u3057\u5206\u5272\u5358\u4f4d\u306e\u518d\u5e30\u6027\u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067\u89e3\u6c7a\u3002\u3055\u3089\u306b1800\u500d\u9ad8\u901f\u306b\u3002", "followers": "2,673", "datetime": "2019-01-10 16:19:01", "author": "@mosko_mule"}, "1083463535320023041": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "7", "datetime": "2019-01-10 20:40:11", "author": "@dabadone2"}, "1083492762207637505": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "456", "datetime": "2019-01-10 22:36:19", "author": "@PerthMLGroup"}, "1083667529397161986": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "50", "datetime": "2019-01-11 10:10:47", "author": "@daniel_heres"}, "1083398878181740544": {"content_summary": "RT @mosko_mule: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/43AM06iwlD Transformer\u306f\u7cfb\u5217\u9577\u306b\u9650\u754c\u304c\u3042\u308a\u3001\u9577\u3044\u7cfb\u5217\u306f\u2026", "followers": "12,765", "datetime": "2019-01-10 16:23:15", "author": "@jaguring1"}, "1160853898283888640": {"content_summary": "[9/10] \ud83d\udcc8 - pytorch-transformers - 10,607 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipFgYl - \ud83d\udd17 https://t.co/RdEWpPLOJR", "followers": "222", "datetime": "2019-08-12 10:01:51", "author": "@PapersTrending"}, "1083728281264783365": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "22", "datetime": "2019-01-11 14:12:11", "author": "@qoboty"}, "1083490017664741376": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "86", "datetime": "2019-01-10 22:25:25", "author": "@tiagomluis"}, "1085773960384524288": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "310", "datetime": "2019-01-17 05:40:59", "author": "@ido87"}, "1086832444525797376": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "165", "datetime": "2019-01-20 03:47:01", "author": "@plsang"}, "1086146846269960192": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "4,643", "datetime": "2019-01-18 06:22:42", "author": "@walterdebrouwer"}, "1084465413528768513": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "16", "datetime": "2019-01-13 15:01:17", "author": "@supercoderhawk"}, "1083606764656877568": {"content_summary": "RT @nkmry_: \u518d\u5e30\u6a5f\u69cb\u3068\u76f8\u5bfe\u7684\u306a position embedding \u3092\u8003\u6848\u3057\u3066 Transformer \u3092 1800\u500d\u9ad8\u901f\u5316\uff01 TensorFlow \u3068 PyTorch \u306e\u5b9f\u88c5\u3082\u516c\u958b\u3057\u3066\u3044\u308b\u3002 https://t.co/9dv5w8pBCp", "followers": "294", "datetime": "2019-01-11 06:09:19", "author": "@rose_miura"}, "1083749540052623361": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "3", "datetime": "2019-01-11 15:36:40", "author": "@ShlinKuo"}, "1085770020939055105": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "220", "datetime": "2019-01-17 05:25:20", "author": "@ceshine_en"}, "1084159837812989952": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "506", "datetime": "2019-01-12 18:47:02", "author": "@dvilasuero"}, "1086101087399010305": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "54", "datetime": "2019-01-18 03:20:52", "author": "@jsdelfino"}, "1083470644551966721": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "224", "datetime": "2019-01-10 21:08:26", "author": "@ElectronNest"}, "1083558696410537985": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "20", "datetime": "2019-01-11 02:58:19", "author": "@darkmon96"}, "1083879152011395072": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "275", "datetime": "2019-01-12 00:11:41", "author": "@kadarakos"}, "1083389637043933184": {"content_summary": "RT @evolvingstuff: REALLY cool improvement upon Transformer networks that makes use of recurrence and a relative positional encodings! Up t\u2026", "followers": "1,649", "datetime": "2019-01-10 15:46:32", "author": "@alexk_z"}, "1083420392176242688": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "6,525", "datetime": "2019-01-10 17:48:45", "author": "@barneyp"}, "1084029765651234817": {"content_summary": "@john_pryan Still into #NLP? Fresh off the press, worth a look...", "followers": "312", "datetime": "2019-01-12 10:10:11", "author": "@EdwardDixon3"}, "1083520886852935680": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "18", "datetime": "2019-01-11 00:28:04", "author": "@caymanlee"}, "1083856271885840385": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "1,336", "datetime": "2019-01-11 22:40:46", "author": "@jigarkdoshi"}, "1085887415061962752": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "2,671", "datetime": "2019-01-17 13:11:49", "author": "@ayirpelle"}, "1166289821263904768": {"content_summary": "[10/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 11,218 \u2b50 - \ud83d\udcc4 https://t.co/a1jbipFgYl - \ud83d\udd17 https://t.co/7bzXyCHTjk", "followers": "222", "datetime": "2019-08-27 10:02:16", "author": "@PapersTrending"}, "1085848538804830208": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "697", "datetime": "2019-01-17 10:37:20", "author": "@Akanksha_Ahuja9"}, "1083801786178252803": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "63", "datetime": "2019-01-11 19:04:16", "author": "@fkariminejadasl"}, "1085908787175804931": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "15", "datetime": "2019-01-17 14:36:44", "author": "@NusWu"}, "1084377226349400064": {"content_summary": "RT @nik0spapp: Surprised to see this paper rejected from ICLR! It addresses an important limitation in Transformer architectures, namely go\u2026", "followers": "530", "datetime": "2019-01-13 09:10:52", "author": "@albertstartup"}, "1084012648033746947": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "512", "datetime": "2019-01-12 09:02:09", "author": "@tmasada"}, "1083958592808448001": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "136", "datetime": "2019-01-12 05:27:22", "author": "@bohdanome"}, "1083753787083259906": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "90,659", "datetime": "2019-01-11 15:53:32", "author": "@stanfordnlp"}, "1090810296531267584": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "301", "datetime": "2019-01-31 03:13:35", "author": "@gangeshwark"}, "1083542530749288450": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "813", "datetime": "2019-01-11 01:54:05", "author": "@SurgeBiswas"}, "1084118562027917312": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "98", "datetime": "2019-01-12 16:03:01", "author": "@kgwmath"}, "1083594417221423104": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "2,671", "datetime": "2019-01-11 05:20:15", "author": "@ayirpelle"}, "1085914521384804352": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "293", "datetime": "2019-01-17 14:59:31", "author": "@vingovan"}, "1083408430579286017": {"content_summary": "New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without disrupting temporal coherence, SOTA on 5 datasets w/t Zihang Dai, Zhilin Yang et al. https://t.co/qQa4fpjAx1 Code, pretrained models:", "followers": "77,472", "datetime": "2019-01-10 17:01:13", "author": "@rsalakhu"}, "1083625327191117824": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "487", "datetime": "2019-01-11 07:23:05", "author": "@lyomi"}, "1085806445604425729": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "354", "datetime": "2019-01-17 07:50:04", "author": "@indy9000"}, "1095724284700487685": {"content_summary": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://t.co/eICwwbMfhd @arxiv", "followers": "1,984", "datetime": "2019-02-13 16:40:01", "author": "@hmCuesta"}, "1083428282169741312": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "344", "datetime": "2019-01-10 18:20:06", "author": "@jefkine"}, "1085692897318629376": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "1,180", "datetime": "2019-01-17 00:18:52", "author": "@dadabots"}, "1144728916332023814": {"content_summary": "RT @AUEBNLPGroup: For background on Transformer-XL, see: https://t.co/1vaWHmrFsF https://t.co/cLKYoAwerk", "followers": "73", "datetime": "2019-06-28 22:06:56", "author": "@PepFriday"}, "1083755409310023681": {"content_summary": "RT @quocleix: Our work on transformer-xl which uses smart caching to improve the learning of long-term dependency in transformer. Key resul\u2026", "followers": "302", "datetime": "2019-01-11 15:59:59", "author": "@minoopy"}, "1083605010431266816": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "234", "datetime": "2019-01-11 06:02:21", "author": "@Leal2_G"}, "1084347083526672384": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "14", "datetime": "2019-01-13 07:11:05", "author": "@RumanHonza"}, "1087162488800636934": {"content_summary": "https://t.co/oF1cxmUw9F Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. (arXiv:1901.02860v2 [cs.LG] UPDATED) #NLProc", "followers": "4,200", "datetime": "2019-01-21 01:38:30", "author": "@arxiv_cs_cl"}, "1083725375383965696": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "34", "datetime": "2019-01-11 14:00:38", "author": "@MatRazor"}, "1083578165619056640": {"content_summary": "RT @rsalakhu: New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without dis\u2026", "followers": "63", "datetime": "2019-01-11 04:15:41", "author": "@hai_t_pham"}, "1083622828635815936": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "625", "datetime": "2019-01-11 07:13:09", "author": "@nova77t"}, "1131500455568265216": {"content_summary": "[8/10] \ud83d\udcc8 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - 6,321 \u2b50 - \ud83d\udcc4 https://t.co/IlZvRq3CVg - \ud83d\udd17 https://t.co/RdEWpPLOJR", "followers": "222", "datetime": "2019-05-23 10:01:45", "author": "@PapersTrending"}, "1085913662668267520": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "110", "datetime": "2019-01-17 14:56:07", "author": "@Raquel1934"}, "1086075230529806336": {"content_summary": "RT @hardmaru: Transformer-XL: Combining Transformers and RNNs Into a State-of-the-art Language Model Blog post by @HorevRani giving an ove\u2026", "followers": "38", "datetime": "2019-01-18 01:38:07", "author": "@experiencor"}, "1083876189616320512": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "4,825", "datetime": "2019-01-11 23:59:55", "author": "@prototechno"}, "1083792659829919751": {"content_summary": "RT @hardmaru: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Up to 1800x faster than vanilla Transformer during e\u2026", "followers": "736", "datetime": "2019-01-11 18:28:00", "author": "@unsorsodicorda"}}}