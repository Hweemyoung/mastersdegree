{"citation_id": "78233647", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 16:24:06", "twitter": {"1242808855316307974": {"followers": "3,500", "datetime": "2020-03-25 13:41:36", "author": "@arxiv_cscl", "content_summary": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators https://t.co/ouapviRTUd"}, "1242627410165673985": {"followers": "3,500", "datetime": "2020-03-25 01:40:36", "author": "@arxiv_cscl", "content_summary": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators https://t.co/ouapviRTUd"}, "1242950252375601152": {"followers": "1,152", "datetime": "2020-03-25 23:03:27", "author": "@jd_mashiro", "content_summary": "RT @arxiv_org: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. https://t.co/HfNedhp9CV https://t.co/4diHlXESS7"}, "1258802396479684609": {"followers": "82", "datetime": "2020-05-08 16:54:13", "author": "@BryanOffutt", "content_summary": "Link to paper here: https://t.co/VOdamkbjvw"}, "1242871131734003713": {"followers": "122", "datetime": "2020-03-25 17:49:03", "author": "@DanielAdamec5", "content_summary": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators Neat idea. Cocktail party effect approach to language masking rather than temporary sensory deprivation. https://t.co/13r6iLr2zQ"}, "1242658827364790272": {"followers": "824", "datetime": "2020-03-25 03:45:26", "author": "@morioka", "content_summary": "RT @arxiv_cscl: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators https://t.co/ouapviRTUd"}, "1243049594054901760": {"followers": "376", "datetime": "2020-03-26 05:38:12", "author": "@kammitama", "content_summary": "I'm reading this paper and I can't unsee \"MLM\" as \"Multi-level Marketing\" instead of \"Masked Language Modeling\"\ud83d\ude06\ud83d\ude06\ud83d\ude06 -> https://t.co/45Dh4seoWM"}, "1242712398911414273": {"followers": "4,200", "datetime": "2020-03-25 07:18:19", "author": "@arxiv_cs_cl", "content_summary": "https://t.co/1TjN1W5CqZ ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. (arXiv:2003.10555v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1256767160459890691": {"followers": "745", "datetime": "2020-05-03 02:06:55", "author": "@hameddaei", "content_summary": "RT @AndrewFerlitsch: Last night read recent NLP research paper: ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATOR\u2026"}, "1242942612060794880": {"followers": "12,735", "datetime": "2020-03-25 22:33:06", "author": "@arxiv_org", "content_summary": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. https://t.co/HfNedhp9CV https://t.co/4diHlXESS7"}, "1242629839124824064": {"followers": "3,881", "datetime": "2020-03-25 01:50:15", "author": "@BrundageBot", "content_summary": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning https://t.co/cH70kUiF3M"}, "1256192070059880450": {"followers": "1,185", "datetime": "2020-05-01 12:01:43", "author": "@NlpaperChalleng", "content_summary": "https://t.co/aVmcT3Z3tR"}, "1244084058998673409": {"followers": "2,195", "datetime": "2020-03-29 02:08:48", "author": "@baykenney", "content_summary": "Just read this on the arXiv ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators https://t.co/GaBDXebBA8"}, "1256229548057911298": {"followers": "223", "datetime": "2020-05-01 14:30:38", "author": "@AndrewFerlitsch", "content_summary": "Last night read recent NLP research paper: ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS (Google, Stanford, 2020) - https://t.co/bJfzislhwH"}, "1243083284029919232": {"followers": "292", "datetime": "2020-03-26 07:52:04", "author": "@Shujian_Liu", "content_summary": "Just read this on the arXiv ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators https://t.co/XZCxYAYV8S"}}}