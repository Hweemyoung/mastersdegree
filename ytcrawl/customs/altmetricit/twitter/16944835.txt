{"citation_id": "16944835", "queriedAt": "2020-05-09 14:03:40", "completed": "0", "twitter": {"1045745888805543936": {"followers": "3,382", "content_summary": "A fun thought - I still feel \"Learning Confidence for Out-of-Distribution Detection in Neural Networks\" (https://t.co/iN6hJ9Y1QF) from this year is quite cool ; I wonder if synthetic gradients (https://t.co/Ka8enDjMvF) are another way of doing confidence e", "author": "@andrey_kurenkov", "datetime": "2018-09-28 18:43:43"}, "895068107043155969": {"followers": "5,089", "content_summary": "RT @future_of_AI: Understanding Synthetic Gradients and Decoupled Neural Interfaces https://t.co/qtm3VRNxoo #ai #deeplearning #nlp via @Ali\u2026", "author": "@clairebotai", "datetime": "2017-08-08 23:44:00"}, "895068105906507776": {"followers": "2,252", "content_summary": "Understanding Synthetic Gradients and Decoupled Neural Interfaces https://t.co/qtm3VRNxoo #ai #deeplearning #nlp via @AlisonBLowndes", "author": "@future_of_AI", "datetime": "2017-08-08 23:44:00"}, "894518776573677568": {"followers": "635", "content_summary": "RT @AlisonBLowndes: Backprop not scalable. Synthetic gradients allow async training of layers & applied to RNN extends temporal extent http\u2026", "author": "@lievAnastazia", "datetime": "2017-08-07 11:21:10"}}, "tab": "twitter"}