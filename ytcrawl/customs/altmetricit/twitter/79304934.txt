{"citation_id": "79304934", "completed": "1", "queriedAt": "2020-05-14 13:01:26", "tab": "twitter", "twitter": {"1247754089124909067": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "12", "datetime": "2020-04-08 05:12:11", "author": "@RogerTarso"}, "1248229980170907649": {"content_summary": "Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried implementing it on @PyTorch. GitHub Link - https://t.co/dPXfIdFaot", "followers": "703", "datetime": "2020-04-09 12:43:12", "author": "@DigantaMisra1"}, "1248041204165439488": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "29", "datetime": "2020-04-09 00:13:05", "author": "@hardik24753v"}, "1248040481784643584": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "2,530", "datetime": "2020-04-09 00:10:12", "author": "@_329_"}, "1248078198564216832": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "2,668", "datetime": "2020-04-09 02:40:05", "author": "@ayirpelle"}, "1248583510010494976": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/efVyTOvxNs #AI #Research via @weballergy", "followers": "2,285", "datetime": "2020-04-10 12:08:01", "author": "@future_of_AI"}, "1248143820870451201": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "119", "datetime": "2020-04-09 07:00:50", "author": "@DeepBrainz"}, "1248810572012908544": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "198", "datetime": "2020-04-11 03:10:16", "author": "@zephyrzilla"}, "1247886550718791680": {"content_summary": "@TheZachMueller it seems pretty promising.", "followers": "835", "datetime": "2020-04-08 13:58:32", "author": "@albertotono3"}, "1247770894589505540": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "369", "datetime": "2020-04-08 06:18:58", "author": "@talkdatatomee"}, "1248533037161172992": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "468", "datetime": "2020-04-10 08:47:27", "author": "@SrirangaTarun"}, "1247699806161711105": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "4,526", "datetime": "2020-04-08 01:36:29", "author": "@CShorten30"}, "1248147491062677505": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "154", "datetime": "2020-04-09 07:15:25", "author": "@RadhaManisha"}, "1247708929322373122": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "28", "datetime": "2020-04-08 02:12:44", "author": "@hey_kishore"}, "1250124326784978945": {"content_summary": "#MachineLearning Evolving Normalization-Activation Layers https://t.co/957qj5Hc85", "followers": "3,413", "datetime": "2020-04-14 18:10:40", "author": "@theomitsa"}, "1248254416471207936": {"content_summary": "Evolving Normalization-Activation Layers. https://t.co/Ott7EACi7e https://t.co/rlYSKacNUC", "followers": "12,729", "datetime": "2020-04-09 14:20:18", "author": "@arxiv_org"}, "1247899820448083969": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "145,380", "datetime": "2020-04-08 14:51:16", "author": "@StartupYou"}, "1247914760340779008": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,760", "datetime": "2020-04-08 15:50:38", "author": "@akdm_bot"}, "1249361623447293954": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "193", "datetime": "2020-04-12 15:39:57", "author": "@laboage777"}, "1248128092746506240": {"content_summary": "RT @lmthang: Nice ideas of using (a) multiple architectures in the search objective for generalization & (b) a light weight proxy task on C\u2026", "followers": "161", "datetime": "2020-04-09 05:58:21", "author": "@Sid____"}, "1249504276839370756": {"content_summary": "2020/04/06 \u6295\u7a3f 4\u4f4d LG(Machine Learning) Evolving Normalization-Activation Layers https://t.co/sBgbzaP5Pw 19 Tweets 39 Retweets 169 Favorites", "followers": "694", "datetime": "2020-04-13 01:06:48", "author": "@arxiv_pop"}, "1247771373117698048": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "369", "datetime": "2020-04-08 06:20:52", "author": "@talkdatatomee"}, "1248090160362917888": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "161", "datetime": "2020-04-09 03:27:37", "author": "@rajarishis"}, "1247757826945978370": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "170", "datetime": "2020-04-08 05:27:02", "author": "@McMxciillan"}, "1247740980553494528": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "65", "datetime": "2020-04-08 04:20:06", "author": "@dave_co_dev"}, "1248206762320310272": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "3", "datetime": "2020-04-09 11:10:57", "author": "@FelipeV14702764"}, "1249253693649608704": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "554", "datetime": "2020-04-12 08:31:05", "author": "@FBWM8888"}, "1247748996996558849": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "243", "datetime": "2020-04-08 04:51:57", "author": "@vandit_25"}, "1247810060958363651": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "46", "datetime": "2020-04-08 08:54:36", "author": "@Yannlg_"}, "1248388942631559169": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "179,069", "datetime": "2020-04-09 23:14:52", "author": "@Montreal_AI"}, "1248573369353744386": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "151", "datetime": "2020-04-10 11:27:43", "author": "@gettoankit"}, "1247817928004833281": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "46", "datetime": "2020-04-08 09:25:52", "author": "@helboukkouri"}, "1248073637761974273": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "871", "datetime": "2020-04-09 02:21:57", "author": "@anpanmanmol"}, "1247817894538498048": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "29", "datetime": "2020-04-08 09:25:44", "author": "@b8horpet"}, "1247864864627986432": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "4,636", "datetime": "2020-04-08 12:32:22", "author": "@mohitban47"}, "1247809178975891460": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "4,327", "datetime": "2020-04-08 08:51:06", "author": "@jekbradbury"}, "1248956013492858881": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "12,765", "datetime": "2020-04-11 12:48:12", "author": "@jaguring1"}, "1247726957296373760": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,673", "datetime": "2020-04-08 03:24:22", "author": "@mosko_mule"}, "1247750520090931200": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "409", "datetime": "2020-04-08 04:58:00", "author": "@ulasbagci"}, "1248265116421771264": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "28", "datetime": "2020-04-09 15:02:50", "author": "@hey_kishore"}, "1247794291738415104": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "10,587", "datetime": "2020-04-08 07:51:56", "author": "@judegomila"}, "1247804159887171584": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "221", "datetime": "2020-04-08 08:31:09", "author": "@ScriptShade"}, "1248811834821361664": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "691", "datetime": "2020-04-11 03:15:17", "author": "@SythonUK"}, "1247765766415376392": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,427", "datetime": "2020-04-08 05:58:35", "author": "@imleslahdin"}, "1247786520481632257": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "534", "datetime": "2020-04-08 07:21:03", "author": "@krishnamrith12"}, "1248000032386592768": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "233", "datetime": "2020-04-08 21:29:29", "author": "@bamboo4031"}, "1247775616968974338": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "27", "datetime": "2020-04-08 06:37:44", "author": "@maxyazhbin"}, "1247765731195797504": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "63", "datetime": "2020-04-08 05:58:27", "author": "@RossBing"}, "1247987388888088576": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "493", "datetime": "2020-04-08 20:39:14", "author": "@iiSeymour"}, "1247742944603279361": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "72", "datetime": "2020-04-08 04:27:54", "author": "@chho_mailbox"}, "1247803785302306819": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "327", "datetime": "2020-04-08 08:29:40", "author": "@aditya_soni2k17"}, "1247789517626433536": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "43", "datetime": "2020-04-08 07:32:58", "author": "@justice1996V"}, "1248957076858007552": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "485", "datetime": "2020-04-11 12:52:26", "author": "@eve_yk"}, "1248055755485245441": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "1,766", "datetime": "2020-04-09 01:10:54", "author": "@jaialkdanel"}, "1249214728867627008": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "201", "datetime": "2020-04-12 05:56:15", "author": "@Singularity_43"}, "1247706489197715457": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "768", "datetime": "2020-04-08 02:03:02", "author": "@tanmingxing"}, "1247868722188034049": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "292", "datetime": "2020-04-08 12:47:42", "author": "@sam_remedios"}, "1248950574705520642": {"content_summary": "@_brohrer_ There's a new drop-in TF layer from Google Brain / DeepMind that broadly outperforms BN and has an online variant. Paper: https://t.co/85S1JtPRuy Great explainer video by @labs_henry: https://t.co/07lll8nmIr https://t.co/jYEyxuQrCv", "followers": "4,670", "datetime": "2020-04-11 12:26:36", "author": "@xsteenbrugge"}, "1247758064951808002": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "243", "datetime": "2020-04-08 05:27:59", "author": "@karthiknrao"}, "1247752185493852160": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "14", "datetime": "2020-04-08 05:04:37", "author": "@ProjectDomani"}, "1248833408744198146": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-11 04:41:01", "author": "@arxiv_cscv"}, "1247767073649909760": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "7", "datetime": "2020-04-08 06:03:47", "author": "@WeiLiu_sjtu"}, "1247786713411272706": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "508", "datetime": "2020-04-08 07:21:49", "author": "@carlosj_wb"}, "1247837506529304577": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "87", "datetime": "2020-04-08 10:43:39", "author": "@DevHunterYZ"}, "1253255811914067968": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "16", "datetime": "2020-04-23 09:34:04", "author": "@lalith1403"}, "1247970954615107584": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "4", "datetime": "2020-04-08 19:33:56", "author": "@haznaitak"}, "1247767524260732929": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "75", "datetime": "2020-04-08 06:05:34", "author": "@MozejkoMarcin"}, "1247701515474620417": {"content_summary": "Evolving Normalization-Activation Layers \u9032\u5316\u3059\u308b\u6b63\u898f\u5316\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc 2020-04-06T19:52:48+00:00 arXiv: https://t.co/SLn5PeA2AH \u82f1/\u65e5\u30b5\u30de\u30ea\u2193 https://t.co/H7qrljhoaA", "followers": "142", "datetime": "2020-04-08 01:43:17", "author": "@arXiv_reaDer"}, "1257377735942508546": {"content_summary": "=> NAS (AutoML), Google Accelerator-aware NAS, Mar 5, 2020 https://t.co/WSa0SQqKiG BigNAS, Mar 24 https://t.co/iwGQJd0C1Y EvoNorms: Evolving Normalization-Activation Layers, Apr 28 https://t.co/7N68T9Gdpq MobileDets, Apr 30 https://t.co/ROmJCzTS8V NAS 2", "followers": "2,584", "datetime": "2020-05-04 18:33:07", "author": "@ogawa_tter"}, "1247753114280546305": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,627", "datetime": "2020-04-08 05:08:19", "author": "@chris_brockett"}, "1252617485850939402": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "5", "datetime": "2020-04-21 15:17:35", "author": "@labbaali"}, "1247762750115217410": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "126", "datetime": "2020-04-08 05:46:36", "author": "@juefeix"}, "1247785809769418754": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "371", "datetime": "2020-04-08 07:18:14", "author": "@DiegoGutnisky"}, "1248088246711652353": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "3,772", "datetime": "2020-04-09 03:20:01", "author": "@volkuleshov"}, "1248172334034419717": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "26", "datetime": "2020-04-09 08:54:09", "author": "@_bam098"}, "1252815655855771648": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "1,361", "datetime": "2020-04-22 04:25:03", "author": "@Alex_Amaguaya"}, "1247748150988701703": {"content_summary": "Looks like a Swish activation function, divided by the std of the preactivations in a GroupNorm fashion. No subtracting a mean but a learned bias is applied afterwards. The real question is how many trees were killed in the making of this paper???", "followers": "572", "datetime": "2020-04-08 04:48:35", "author": "@ebetica"}, "1248963150759657473": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "578", "datetime": "2020-04-11 13:16:34", "author": "@usagisan2020"}, "1247779201848995840": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "328", "datetime": "2020-04-08 06:51:58", "author": "@Veqtor"}, "1248061838245400576": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "545", "datetime": "2020-04-09 01:35:04", "author": "@TeraBytesMemory"}, "1249118120507252736": {"content_summary": "RT @mosko_mule: Evolving Normalization-Activation Layers https://t.co/RyGwzxNgNH \u9032\u5316\u8a08\u7b97\u306b\u3088\u308bAutoML\u3092\u7528\u3044\u3066Norm+\u6d3b\u6027\u5316\u51fd\u6570\u306e\u7d44\u5408\u305b\u3092\uff0c\u8907\u6570\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6027\u80fd\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u63a2\u7d22\uff0e\u5f97\u3089\u2026", "followers": "380", "datetime": "2020-04-11 23:32:22", "author": "@harujoh"}, "1247927989951549441": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "30", "datetime": "2020-04-08 16:43:12", "author": "@gitlostmurali"}, "1248523396561784833": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "409", "datetime": "2020-04-10 08:09:08", "author": "@jainnitk"}, "1248287054796066817": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "2,070", "datetime": "2020-04-09 16:30:00", "author": "@EricSchles"}, "1247767856378286080": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "221", "datetime": "2020-04-08 06:06:54", "author": "@fdasilva59fr"}, "1247911029318676480": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "5,581", "datetime": "2020-04-08 15:35:49", "author": "@Tim_Dettmers"}, "1247760646298431494": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "567", "datetime": "2020-04-08 05:38:15", "author": "@CookieBox26"}, "1247814974870716417": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "113", "datetime": "2020-04-08 09:14:07", "author": "@yesvcan1"}, "1248167822754119683": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "23", "datetime": "2020-04-09 08:36:13", "author": "@NimanshiJha"}, "1247908626796290048": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "217", "datetime": "2020-04-08 15:26:16", "author": "@frankwwu"}, "1247764889990352903": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "65", "datetime": "2020-04-08 05:55:06", "author": "@dave_co_dev"}, "1247846179964141569": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "365", "datetime": "2020-04-08 11:18:07", "author": "@iugoaoj"}, "1252492775016325120": {"content_summary": "\u6848\u306e\u5b9a(?) EvoNorm\u306e\u5b9f\u88c5\u51fa\u307e\u3057\u305f\u306d", "followers": "2,212", "datetime": "2020-04-21 07:02:02", "author": "@nino_pira"}, "1247765232925020167": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "247", "datetime": "2020-04-08 05:56:28", "author": "@peterjansen_ai"}, "1247706907793436674": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "29", "datetime": "2020-04-08 02:04:42", "author": "@chenlailin"}, "1247774907255025664": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "10", "datetime": "2020-04-08 06:34:55", "author": "@yarphs"}, "1247928491024076800": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "36", "datetime": "2020-04-08 16:45:12", "author": "@0x9217"}, "1247777275581366281": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "66", "datetime": "2020-04-08 06:44:19", "author": "@tenzinchang"}, "1247701004537090048": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "13", "datetime": "2020-04-08 01:41:15", "author": "@Bharath09095865"}, "1247827856257945600": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "584", "datetime": "2020-04-08 10:05:19", "author": "@morgangiraud"}, "1247979723675693057": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "217", "datetime": "2020-04-08 20:08:47", "author": "@maler1ck"}, "1248119472822669312": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "208", "datetime": "2020-04-09 05:24:05", "author": "@Don_Rubiel"}, "1247857632230039553": {"content_summary": "\u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u3001GAN\u306b\u304a\u3044\u3066\u3082\u52b9\u679c\u7684\u306b\u9069\u7528\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d", "followers": "1,022", "datetime": "2020-04-08 12:03:38", "author": "@kzykmyzw"}, "1247789734564249602": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "331", "datetime": "2020-04-08 07:33:50", "author": "@Meferhau"}, "1247905941061681153": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "270", "datetime": "2020-04-08 15:15:35", "author": "@kotti_sasikanth"}, "1247795502969532418": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "116", "datetime": "2020-04-08 07:56:45", "author": "@ben_trevett"}, "1247946739329519617": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "361", "datetime": "2020-04-08 17:57:43", "author": "@BioNLProc"}, "1247734288042176517": {"content_summary": "https://t.co/d4loheYr4R \uad6c\uae00\ud45c \uc11c\uce58. normalization & activation\uc744 \ubb36\uc5b4\uc11c \ud0d0\uc0c9. mean centering\uc774 \ube60\uc9c0\uace0 swish \uac19\uc740 activation\uc774 \ub2e4\uc2dc \ub4f1\uc7a5\ud55c\ub2e4\ub294 \uac83\uc774 \ud765\ubbf8\ub85c\uc6b4 \ub4ef. https://t.co/SiZFz3cyWe", "followers": "221", "datetime": "2020-04-08 03:53:30", "author": "@daily_arXiv"}, "1248088512051556353": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "70", "datetime": "2020-04-09 03:21:04", "author": "@sumitsethy"}, "1248127320105185281": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "270", "datetime": "2020-04-09 05:55:16", "author": "@kotti_sasikanth"}, "1247834280643809280": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "303", "datetime": "2020-04-08 10:30:50", "author": "@subhobrata1"}, "1247755664421564417": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "30", "datetime": "2020-04-08 05:18:27", "author": "@flute_ud"}, "1247983355674734594": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "9", "datetime": "2020-04-08 20:23:13", "author": "@iamx9000"}, "1247751897051623424": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "82", "datetime": "2020-04-08 05:03:29", "author": "@Ariadne_Zhang"}, "1247742124092616706": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "697", "datetime": "2020-04-08 04:24:38", "author": "@Akanksha_Ahuja9"}, "1247730270800728067": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1", "datetime": "2020-04-08 03:37:32", "author": "@Kelly70057747"}, "1247743762459066370": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "123", "datetime": "2020-04-08 04:31:09", "author": "@shalinda99"}, "1247846288969719809": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "231", "datetime": "2020-04-08 11:18:33", "author": "@nutszebra"}, "1247798761725689863": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "295", "datetime": "2020-04-08 08:09:42", "author": "@alxandrecarlier"}, "1247939250235965440": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "3", "datetime": "2020-04-08 17:27:57", "author": "@namphanp"}, "1248956830752993281": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "12,765", "datetime": "2020-04-11 12:51:27", "author": "@jaguring1"}, "1247860459480338434": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "4,017", "datetime": "2020-04-08 12:14:52", "author": "@ballforest"}, "1248084535603945474": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "1,878", "datetime": "2020-04-09 03:05:16", "author": "@sesquipedale"}, "1248500119323824128": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "302", "datetime": "2020-04-10 06:36:39", "author": "@douchi_kamiya"}, "1247843060588912640": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "760", "datetime": "2020-04-08 11:05:44", "author": "@Ing_EboRobert"}, "1248006976392409089": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "34", "datetime": "2020-04-08 21:57:04", "author": "@MatRazor"}, "1248353861561180160": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "66", "datetime": "2020-04-09 20:55:28", "author": "@diegovogeid"}, "1247763876201947136": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "57", "datetime": "2020-04-08 05:51:05", "author": "@Zommiommy"}, "1247908808548106242": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "12", "datetime": "2020-04-08 15:26:59", "author": "@Jhonthanortizh"}, "1247957242218856448": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "167", "datetime": "2020-04-08 18:39:27", "author": "@shalinidemello"}, "1248264208061349888": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "16", "datetime": "2020-04-09 14:59:13", "author": "@luyiwang"}, "1250002764270354434": {"content_summary": "'Evolving normalization-activation layers in deep neural networks' by bookofjoe Read more about this story here --> https://t.co/uYcwkIXaYx", "followers": "29", "datetime": "2020-04-14 10:07:37", "author": "@TechTweetBot"}, "1248151798248022016": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "518", "datetime": "2020-04-09 07:32:32", "author": "@giannirg"}, "1247778959221075969": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "44", "datetime": "2020-04-08 06:51:01", "author": "@jetjodh"}, "1247768279050883072": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,668", "datetime": "2020-04-08 06:08:34", "author": "@ayirpelle"}, "1248745193190371335": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "92", "datetime": "2020-04-10 22:50:29", "author": "@gabrielbca15"}, "1250058762779602944": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "163", "datetime": "2020-04-14 13:50:08", "author": "@billy_gareth"}, "1247759965894275072": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "38", "datetime": "2020-04-08 05:35:32", "author": "@Rakuu13"}, "1247769077969342465": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "465", "datetime": "2020-04-08 06:11:45", "author": "@lnalborczyk"}, "1247880155680342016": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "494", "datetime": "2020-04-08 13:33:08", "author": "@jbohnslav"}, "1247872643224743937": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "19", "datetime": "2020-04-08 13:03:17", "author": "@MassBassLol"}, "1248980212290695168": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "471", "datetime": "2020-04-11 14:24:22", "author": "@daytb_twy"}, "1247769573631258625": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "6,721", "datetime": "2020-04-08 06:13:43", "author": "@paulportesi"}, "1247710945369088000": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "437", "datetime": "2020-04-08 02:20:45", "author": "@grousselle"}, "1247762394752749568": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "83", "datetime": "2020-04-08 05:45:11", "author": "@RangnekarAneesh"}, "1247817201496862721": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "628", "datetime": "2020-04-08 09:22:58", "author": "@alisher_ai"}, "1247736485140295680": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "819", "datetime": "2020-04-08 04:02:14", "author": "@deepgradient"}, "1257005291360727040": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "160", "datetime": "2020-05-03 17:53:10", "author": "@romualdosc"}, "1247892568320299009": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "824", "datetime": "2020-04-08 14:22:27", "author": "@morioka"}, "1247700641943826432": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "304", "datetime": "2020-04-08 01:39:48", "author": "@sebastien_wood"}, "1247698949215002625": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "692", "datetime": "2020-04-08 01:33:05", "author": "@luisbebop"}, "1247855057019772929": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "21,221", "datetime": "2020-04-08 11:53:24", "author": "@DataSciNews"}, "1247721083131662336": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "1", "datetime": "2020-04-08 03:01:02", "author": "@LanLe58698794"}, "1247737389998419970": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "115", "datetime": "2020-04-08 04:05:50", "author": "@viktor_m81"}, "1249205840638283779": {"content_summary": "\u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "12,765", "datetime": "2020-04-12 05:20:56", "author": "@jaguring1"}, "1249371681518424064": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "290", "datetime": "2020-04-12 16:19:55", "author": "@dannyehb"}, "1247753095368466432": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "166", "datetime": "2020-04-08 05:08:14", "author": "@YUV4R4J"}, "1257812456035848192": {"content_summary": "RT @ogawa_tter: => NAS (AutoML), Google Accelerator-aware NAS, Mar 5, 2020 https://t.co/WSa0SQqKiG BigNAS, Mar 24 https://t.co/iwGQJd0C1Y E\u2026", "followers": "163", "datetime": "2020-05-05 23:20:33", "author": "@nikonikoten"}, "1248415494924677120": {"content_summary": "Evolving Normalization-Activation Layers \u9032\u5316\u3059\u308b\u6b63\u898f\u5316\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc 2020-04-09T02:58:37+00:00 arXiv: https://t.co/72UHdCasBx \u82f1/\u65e5\u30b5\u30de\u30ea\u2193 https://t.co/xwryCaE5RH", "followers": "142", "datetime": "2020-04-10 01:00:23", "author": "@arXiv_reaDer"}, "1254275331856326656": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "58", "datetime": "2020-04-26 05:05:17", "author": "@Alima3939"}, "1248056469305491461": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "64", "datetime": "2020-04-09 01:13:44", "author": "@gpu_eater"}, "1247760291154157568": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "199", "datetime": "2020-04-08 05:36:50", "author": "@bobthemaster"}, "1247878620795682816": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "10", "datetime": "2020-04-08 13:27:02", "author": "@MinhTruonghong"}, "1247759238136381440": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "54", "datetime": "2020-04-08 05:32:39", "author": "@ttnam93"}, "1248062919780302851": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "21,221", "datetime": "2020-04-09 01:39:22", "author": "@DataSciNews"}, "1247906180661415936": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "2,475", "datetime": "2020-04-08 15:16:33", "author": "@sindero"}, "1247697627996549120": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "18,260", "datetime": "2020-04-08 01:27:50", "author": "@quocleix"}, "1247954205450465282": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "93", "datetime": "2020-04-08 18:27:23", "author": "@Singularitarian"}, "1247916233841836032": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,070", "datetime": "2020-04-08 15:56:29", "author": "@EricSchles"}, "1247890603058184209": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "824", "datetime": "2020-04-08 14:14:39", "author": "@morioka"}, "1247785261636784129": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "23", "datetime": "2020-04-08 07:16:03", "author": "@Cheng_Ching_Wen"}, "1247920348382797825": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "98", "datetime": "2020-04-08 16:12:50", "author": "@SereneBiologist"}, "1247783462540431369": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,918", "datetime": "2020-04-08 07:08:54", "author": "@DocXavi"}, "1251916061416525826": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "11", "datetime": "2020-04-19 16:50:23", "author": "@bkguo_zju"}, "1248050403624865793": {"content_summary": "Most popular computer science paper of the day: \"Evolving Normalization-Activation Layers\" https://t.co/yQ9FBzi91C https://t.co/BPlZQ6gGZc", "followers": "280", "datetime": "2020-04-09 00:49:38", "author": "@HotCompScience"}, "1247912955783581697": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "5", "datetime": "2020-04-08 15:43:28", "author": "@DaniaVera7"}, "1247689335165444096": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "99", "datetime": "2020-04-08 00:54:53", "author": "@treasured_write"}, "1247798090221805569": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "308", "datetime": "2020-04-08 08:07:02", "author": "@phiweger"}, "1257414285040459780": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "421", "datetime": "2020-05-04 20:58:21", "author": "@zacharynado"}, "1248038830101295104": {"content_summary": "RT @lmthang: Nice ideas of using (a) multiple architectures in the search objective for generalization & (b) a light weight proxy task on C\u2026", "followers": "518", "datetime": "2020-04-09 00:03:39", "author": "@pijili"}, "1247869369562120197": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "10", "datetime": "2020-04-08 12:50:16", "author": "@tassel_pierre"}, "1247717119871066113": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "164,067", "datetime": "2020-04-08 02:45:17", "author": "@ceobillionaire"}, "1248170075741466626": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "273", "datetime": "2020-04-09 08:45:10", "author": "@ghhosh"}, "1247770230408876032": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,402", "datetime": "2020-04-08 06:16:20", "author": "@RTFMCelia"}, "1248118148076695552": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "282", "datetime": "2020-04-09 05:18:50", "author": "@dunce_ml"}, "1247713970707980291": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "59", "datetime": "2020-04-08 02:32:46", "author": "@liqiangniu"}, "1258442750405509134": {"content_summary": "Thanks @ldvcapital! This is from my most recent ML4Sci newsletter issue on AutoML and Photonic Inverse Design: we need more benchmark CV datasets to evaluate autoML (perhaps #ML4Sci datasets can help?) https://t.co/LSKYiyOeCL", "followers": "3", "datetime": "2020-05-07 17:05:07", "author": "@charlesxjyang21"}, "1248110609008185344": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "625", "datetime": "2020-04-09 04:48:52", "author": "@Eseshinpu"}, "1249262409232945153": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "9", "datetime": "2020-04-12 09:05:43", "author": "@WeinroT_xc"}, "1248973202929270785": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "554", "datetime": "2020-04-11 13:56:31", "author": "@FBWM8888"}, "1248129434961489920": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1", "datetime": "2020-04-09 06:03:41", "author": "@mikel_zhobro"}, "1247724342932365314": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "71", "datetime": "2020-04-08 03:13:59", "author": "@92HsChoi"}, "1247888329011388417": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "2,668", "datetime": "2020-04-08 14:05:36", "author": "@ayirpelle"}, "1247896142282555394": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "10", "datetime": "2020-04-08 14:36:39", "author": "@RUppaal"}, "1257378031531888640": {"content_summary": "RT @ogawa_tter: => NAS (AutoML), Google Accelerator-aware NAS, Mar 5, 2020 https://t.co/WSa0SQqKiG BigNAS, Mar 24 https://t.co/iwGQJd0C1Y E\u2026", "followers": "0", "datetime": "2020-05-04 18:34:18", "author": "@Torries_ICML"}, "1247744669477289984": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "801", "datetime": "2020-04-08 04:34:45", "author": "@KoyO_JakaNeEs"}, "1248146499470364673": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "119", "datetime": "2020-04-09 07:11:29", "author": "@DeepBrainz"}, "1247721779251924992": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "425", "datetime": "2020-04-08 03:03:48", "author": "@iamknighton"}, "1247946034296369155": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "48", "datetime": "2020-04-08 17:54:54", "author": "@mzadrogaPL"}, "1248635991453241345": {"content_summary": "RT @fabtar: Excellent video from @ykilcher on\"Evolving Normalization-Activation Layers\" https://t.co/MHUmTRRjDw. This is about this paper h\u2026", "followers": "620", "datetime": "2020-04-10 15:36:33", "author": "@ykilcher"}, "1249532617650851842": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "0", "datetime": "2020-04-13 02:59:25", "author": "@Sam09lol"}, "1247750433973530628": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "56", "datetime": "2020-04-08 04:57:40", "author": "@MelroLeandro"}, "1247765508935405569": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "31", "datetime": "2020-04-08 05:57:34", "author": "@sanchithira76"}, "1254254256476975106": {"content_summary": "Interesting paper about EvoNorms. Just learnt about BatchNorm from https://t.co/8hlxOIeDNL course and now this. People say JS moves fast. Looks like the same can be said for Machine Learning ;)", "followers": "286", "datetime": "2020-04-26 03:41:32", "author": "@max_natho"}, "1248806001161064448": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "478", "datetime": "2020-04-11 02:52:07", "author": "@yasuokajihei"}, "1247698015613116416": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "0", "datetime": "2020-04-08 01:29:22", "author": "@Sam09lol"}, "1248988030834106369": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/RyGwzxNgNH \u9032\u5316\u8a08\u7b97\u306b\u3088\u308bAutoML\u3092\u7528\u3044\u3066Norm+\u6d3b\u6027\u5316\u51fd\u6570\u306e\u7d44\u5408\u305b\u3092\uff0c\u8907\u6570\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6027\u80fd\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u63a2\u7d22\uff0e\u5f97\u3089\u308c\u305fEvoNorm\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u3088\u3089\u305a\u6027\u80fd\u304c\u3088\u304f\uff0cIS\u3084GAN\u3067\u3082\u9ad8\u3044\u6027\u80fd\u3092\u767a\u63ee\uff0e https://t.co/thHfXxQw9X", "followers": "2,673", "datetime": "2020-04-11 14:55:26", "author": "@mosko_mule"}, "1247768724771180545": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "354", "datetime": "2020-04-08 06:10:21", "author": "@indy9000"}, "1247857200652906496": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "111", "datetime": "2020-04-08 12:01:55", "author": "@enricesena"}, "1247756443857473545": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "118", "datetime": "2020-04-08 05:21:33", "author": "@datamase"}, "1247923743722278915": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "588", "datetime": "2020-04-08 16:26:20", "author": "@usmanahmed189"}, "1252486060623396865": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "2,873", "datetime": "2020-04-21 06:35:21", "author": "@sicara_fr"}, "1247775484236025866": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "42", "datetime": "2020-04-08 06:37:12", "author": "@MishakinSergey"}, "1247700797858725889": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-08 01:40:26", "author": "@arxiv_cscv"}, "1247859000689020928": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,103", "datetime": "2020-04-08 12:09:04", "author": "@kagglingdieter"}, "1249302640745463808": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "3,374", "datetime": "2020-04-12 11:45:35", "author": "@renato_umeton"}, "1247702877239312384": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,294", "datetime": "2020-04-08 01:48:41", "author": "@tak_yamm"}, "1252295307217432577": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "33,505", "datetime": "2020-04-20 17:57:22", "author": "@DynamicWebPaige"}, "1247932766806818816": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "103", "datetime": "2020-04-08 17:02:11", "author": "@urosn"}, "1247785553237389322": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "3,013", "datetime": "2020-04-08 07:17:13", "author": "@sarahbadr"}, "1247771688030232576": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "192", "datetime": "2020-04-08 06:22:07", "author": "@tao_bio"}, "1247761501936467969": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "167", "datetime": "2020-04-08 05:41:39", "author": "@Tatha93tj"}, "1248393223359225856": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "52", "datetime": "2020-04-09 23:31:53", "author": "@Wind_Xiaoli"}, "1247749019457077249": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "2,102", "datetime": "2020-04-08 04:52:02", "author": "@datitran"}, "1248804626696044544": {"content_summary": "NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068Swish\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u8fd1\u3044\u3002https://t.co/5qtnsJc11a", "followers": "18,231", "datetime": "2020-04-11 02:46:39", "author": "@hillbig"}, "1247742140722982913": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "357", "datetime": "2020-04-08 04:24:42", "author": "@JFernandoGRE"}, "1248834440647725057": {"content_summary": "RT @hillbig: Optimal normalization-activation layers are searched with multi-objective evolution. Found EvoNorm-B0 uses the normalization b\u2026", "followers": "99", "datetime": "2020-04-11 04:45:07", "author": "@treasured_write"}, "1247994754459197442": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "145", "datetime": "2020-04-08 21:08:30", "author": "@esvhd"}, "1252419113214816261": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "202", "datetime": "2020-04-21 02:09:20", "author": "@AngryEgyptianX"}, "1247755413203734531": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "199", "datetime": "2020-04-08 05:17:27", "author": "@SyedMoinudeen"}, "1248637213862178817": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-10 15:41:24", "author": "@arxiv_cscv"}, "1247738648092504064": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "40", "datetime": "2020-04-08 04:10:50", "author": "@linbo_pythoner"}, "1252434076637618176": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "431", "datetime": "2020-04-21 03:08:47", "author": "@kelmoujahid"}, "1247771926396731398": {"content_summary": "Hmm....", "followers": "118", "datetime": "2020-04-08 06:23:04", "author": "@skr1125"}, "1247805821683023873": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "258", "datetime": "2020-04-08 08:37:45", "author": "@mohomran"}, "1249151607868178432": {"content_summary": "RT @mosko_mule: Evolving Normalization-Activation Layers https://t.co/RyGwzxNgNH \u9032\u5316\u8a08\u7b97\u306b\u3088\u308bAutoML\u3092\u7528\u3044\u3066Norm+\u6d3b\u6027\u5316\u51fd\u6570\u306e\u7d44\u5408\u305b\u3092\uff0c\u8907\u6570\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6027\u80fd\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u63a2\u7d22\uff0e\u5f97\u3089\u2026", "followers": "2,043", "datetime": "2020-04-12 01:45:26", "author": "@imenurok"}, "1249301714764693506": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "21,221", "datetime": "2020-04-12 11:41:54", "author": "@DataSciNews"}, "1253463138004955136": {"content_summary": "\ud83d\udcdc The Evolving Normalization-Activation Layers paper by @Hanxiao_6 et all \u2013 https://t.co/rWFm1N0gSt \ud83d\udc69\u200d\ud83d\udd2c Interactive @weights_biases report with results \u2013 https://t.co/1Q8qytxDfM \ud83d\udc69\u200d\ud83d\udcbb Github repo to reproduce results \u2013 https://t.co/i5NpEdNRcG", "followers": "6,012", "datetime": "2020-04-23 23:17:54", "author": "@lavanyaai"}, "1248158004249153539": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "105", "datetime": "2020-04-09 07:57:12", "author": "@alfo_512"}, "1247917242152607750": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "13", "datetime": "2020-04-08 16:00:30", "author": "@maksym_del"}, "1248240423979671553": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "168", "datetime": "2020-04-09 13:24:42", "author": "@dr_levan"}, "1247720517605490691": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "740", "datetime": "2020-04-08 02:58:47", "author": "@JoaoVictor_AC"}, "1248028667197956096": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,813", "datetime": "2020-04-08 23:23:16", "author": "@CSProfKGD"}, "1247727257549819906": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "119", "datetime": "2020-04-08 03:25:34", "author": "@snrrrub"}, "1247985107241222151": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "442", "datetime": "2020-04-08 20:30:10", "author": "@AdrianB82"}, "1252706691369623553": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "9", "datetime": "2020-04-21 21:12:04", "author": "@RubeenVSanchez"}, "1259738744355090432": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "339", "datetime": "2020-05-11 06:54:56", "author": "@dir0417"}, "1247794014092324864": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "177", "datetime": "2020-04-08 07:50:50", "author": "@gerard_sanroma"}, "1249471234632380416": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "42", "datetime": "2020-04-12 22:55:31", "author": "@dkmsuzuki"}, "1247766480273334272": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "75", "datetime": "2020-04-08 06:01:25", "author": "@MozejkoMarcin"}, "1247835891848134658": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "121", "datetime": "2020-04-08 10:37:14", "author": "@throwawayndelet"}, "1247891145595576322": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1", "datetime": "2020-04-08 14:16:48", "author": "@LanLe58698794"}, "1247795363391537153": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "694", "datetime": "2020-04-08 07:56:12", "author": "@santty128"}, "1252257583504732162": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "819", "datetime": "2020-04-20 15:27:28", "author": "@deepgradient"}, "1248109084697485312": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "13", "datetime": "2020-04-09 04:42:49", "author": "@GiangTr24025714"}, "1255626616824197120": {"content_summary": "Evolving Normalization-Activation Layers Authors: @Hanxiao_6, Andrew Brock, Karen Simonyan, Quoc V. Le Paper Link: https://t.co/65AgsXJEEI Featured in PTK Newsletter #33: https://t.co/cWXakNTsNK https://t.co/uuYfjVw7eG", "followers": "1,762", "datetime": "2020-04-29 22:34:48", "author": "@PTKDaily"}, "1248026999203016704": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "428", "datetime": "2020-04-08 23:16:38", "author": "@el_pachocamacho"}, "1247938767467528193": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "347", "datetime": "2020-04-08 17:26:02", "author": "@MAMUGI1492"}, "1247818367437864960": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "139", "datetime": "2020-04-08 09:27:36", "author": "@kitty_racha"}, "1248994213997371392": {"content_summary": "RT @xsteenbrugge: @_brohrer_ There's a new drop-in TF layer from Google Brain / DeepMind that broadly outperforms BN and has an online vari\u2026", "followers": "3,632", "datetime": "2020-04-11 15:20:00", "author": "@HamelHusain"}, "1247846478493519872": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "226", "datetime": "2020-04-08 11:19:18", "author": "@sagarpath"}, "1247966121233268736": {"content_summary": "RT @lmthang: Nice ideas of using (a) multiple architectures in the search objective for generalization & (b) a light weight proxy task on C\u2026", "followers": "669", "datetime": "2020-04-08 19:14:44", "author": "@arunkumar_bvr"}, "1247741758957486081": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "268", "datetime": "2020-04-08 04:23:11", "author": "@ReginaMeszlenyi"}, "1248136153602985984": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "9", "datetime": "2020-04-09 06:30:22", "author": "@XuanyiDong"}, "1247852167404433408": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,544", "datetime": "2020-04-08 11:41:55", "author": "@nicholdav"}, "1247768266153447425": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "2,668", "datetime": "2020-04-08 06:08:31", "author": "@ayirpelle"}, "1247851825505562625": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "415", "datetime": "2020-04-08 11:40:33", "author": "@__nggih"}, "1247779340323934209": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "46", "datetime": "2020-04-08 06:52:32", "author": "@minhman_ho"}, "1248146843202138113": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "306", "datetime": "2020-04-09 07:12:51", "author": "@juarelerrr"}, "1247701374692986880": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "3,304", "datetime": "2020-04-08 01:42:43", "author": "@mr_tommz"}, "1247757097476882435": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,213", "datetime": "2020-04-08 05:24:08", "author": "@sampathweb"}, "1247988621925285889": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "2,895", "datetime": "2020-04-08 20:44:08", "author": "@KissiSteve"}, "1247739326433116169": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "65", "datetime": "2020-04-08 04:13:31", "author": "@russell_lliu"}, "1248002012987318273": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "233", "datetime": "2020-04-08 21:37:21", "author": "@atlanticailabs"}, "1247809549970493450": {"content_summary": "RT @daily_arXiv: https://t.co/d4loheYr4R \uad6c\uae00\ud45c \uc11c\uce58. normalization & activation\uc744 \ubb36\uc5b4\uc11c \ud0d0\uc0c9. mean centering\uc774 \ube60\uc9c0\uace0 swish \uac19\uc740 activation\uc774 \ub2e4\uc2dc \ub4f1\uc7a5\ud55c\ub2e4\ub294 \uac83\uc774\u2026", "followers": "13", "datetime": "2020-04-08 08:52:34", "author": "@EricKuy"}, "1247748141928968192": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "140", "datetime": "2020-04-08 04:48:33", "author": "@OlliNiemitalo"}, "1247921589942804480": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "148", "datetime": "2020-04-08 16:17:46", "author": "@Tata_oac"}, "1248963339654434818": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,978", "datetime": "2020-04-11 13:17:19", "author": "@thalesians"}, "1250385633497034754": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,558", "datetime": "2020-04-15 11:29:00", "author": "@patri_vaquero_"}, "1247882143902175232": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-08 13:41:02", "author": "@arxiv_cscv"}, "1247753969541111809": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "395", "datetime": "2020-04-08 05:11:43", "author": "@wdddy"}, "1248365756783505408": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "1,585", "datetime": "2020-04-09 21:42:44", "author": "@jakubzavrel"}, "1247745590995238919": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "217", "datetime": "2020-04-08 04:38:25", "author": "@sambyalAbhi"}, "1247755729043197955": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "7,135", "datetime": "2020-04-08 05:18:42", "author": "@anshulkundaje"}, "1247771331715715072": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "322", "datetime": "2020-04-08 06:20:42", "author": "@countrsignal"}, "1247995982526078977": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "41", "datetime": "2020-04-08 21:13:23", "author": "@b_prasad26"}, "1248343114730721281": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "68", "datetime": "2020-04-09 20:12:46", "author": "@gireeshkbogu"}, "1247774095397122056": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "42", "datetime": "2020-04-08 06:31:41", "author": "@MishakinSergey"}, "1248037437802045440": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "228", "datetime": "2020-04-08 23:58:07", "author": "@hengcherkeng"}, "1247761624716341248": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "746", "datetime": "2020-04-08 05:42:08", "author": "@rheza_h"}, "1247781132411625473": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "43", "datetime": "2020-04-08 06:59:39", "author": "@jeandut14000"}, "1248096322655141890": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "554", "datetime": "2020-04-09 03:52:06", "author": "@FBWM8888"}, "1248207779644329984": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,412", "datetime": "2020-04-09 11:14:59", "author": "@cournape"}, "1247815535598977024": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "41", "datetime": "2020-04-08 09:16:21", "author": "@Mz_Hentai"}, "1248061864661127175": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,504", "datetime": "2020-04-09 01:35:11", "author": "@tawatawara"}, "1247771000210522113": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "14", "datetime": "2020-04-08 06:19:23", "author": "@nholmber_"}, "1247789652922130442": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "78", "datetime": "2020-04-08 07:33:30", "author": "@ilyes_baali07"}, "1247967355650506753": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "669", "datetime": "2020-04-08 19:19:38", "author": "@arunkumar_bvr"}, "1247763128265273344": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "82", "datetime": "2020-04-08 05:48:06", "author": "@junjungoal"}, "1247752995883761670": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,434", "datetime": "2020-04-08 05:07:51", "author": "@damianborth"}, "1247749804286828545": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "129", "datetime": "2020-04-08 04:55:10", "author": "@nodefff"}, "1247732393928163329": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "256", "datetime": "2020-04-08 03:45:59", "author": "@jcchinhui"}, "1247798347726868481": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,438", "datetime": "2020-04-08 08:08:03", "author": "@naohisashoji"}, "1247817577100800000": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "11", "datetime": "2020-04-08 09:24:28", "author": "@AbhayGa22429023"}, "1248542980677369857": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "57", "datetime": "2020-04-10 09:26:58", "author": "@Ithinkphysics"}, "1249152038086344706": {"content_summary": "RT @mosko_mule: Evolving Normalization-Activation Layers https://t.co/RyGwzxNgNH \u9032\u5316\u8a08\u7b97\u306b\u3088\u308bAutoML\u3092\u7528\u3044\u3066Norm+\u6d3b\u6027\u5316\u51fd\u6570\u306e\u7d44\u5408\u305b\u3092\uff0c\u8907\u6570\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6027\u80fd\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u63a2\u7d22\uff0e\u5f97\u3089\u2026", "followers": "291", "datetime": "2020-04-12 01:47:08", "author": "@ZQ875328"}, "1248973058020278272": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "554", "datetime": "2020-04-11 13:55:56", "author": "@FBWM8888"}, "1248057996497702913": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "7", "datetime": "2020-04-09 01:19:48", "author": "@IMG0909"}, "1247702902505795584": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "237", "datetime": "2020-04-08 01:48:47", "author": "@veydpz_public"}, "1247769425014484992": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "167", "datetime": "2020-04-08 06:13:08", "author": "@Tatha93tj"}, "1247781480153006080": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "2", "datetime": "2020-04-08 07:01:02", "author": "@tk_tomk10tk"}, "1247886359538229251": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,725", "datetime": "2020-04-08 13:57:47", "author": "@iamstarnord"}, "1247745703578755074": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "152", "datetime": "2020-04-08 04:38:52", "author": "@iZachTo"}, "1247754820158500864": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,020", "datetime": "2020-04-08 05:15:05", "author": "@serrjoa"}, "1247904801268137984": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "73", "datetime": "2020-04-08 15:11:04", "author": "@anujdutt92"}, "1247766747869876224": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "156", "datetime": "2020-04-08 06:02:29", "author": "@tsauri_eecs"}, "1247697812868845568": {"content_summary": "Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-ReLU. Excited with the potential use of AutoML for discovering novel ML concepts from low level primitives.", "followers": "18,260", "datetime": "2020-04-08 01:28:34", "author": "@quocleix"}, "1248096387641663489": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "554", "datetime": "2020-04-09 03:52:21", "author": "@FBWM8888"}, "1247779486923231234": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "528", "datetime": "2020-04-08 06:53:06", "author": "@hirotomusiker"}, "1247782218354659330": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "29", "datetime": "2020-04-08 07:03:58", "author": "@VasquezLouella"}, "1248017941964046336": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09C9NM", "followers": "4,099", "datetime": "2020-04-08 22:40:39", "author": "@arxiv_cscv"}, "1249146269446045697": {"content_summary": "RT @xsteenbrugge: @_brohrer_ There's a new drop-in TF layer from Google Brain / DeepMind that broadly outperforms BN and has an online vari\u2026", "followers": "217", "datetime": "2020-04-12 01:24:13", "author": "@AssistedEvolve"}, "1248214149684850690": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "824", "datetime": "2020-04-09 11:40:18", "author": "@morioka"}, "1247759107928424450": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "218", "datetime": "2020-04-08 05:32:08", "author": "@mpaepper"}, "1248067755711983616": {"content_summary": "\u3053\u3046\u3044\u3046\u306e\u3069\u3046\u306a\u3093\u3067\u3057\u3087\u3002\u67af\u308c\u3066\u304f\u308c\u308b\u307e\u3067\u30d1\u30c3\u3068\u98db\u3073\u3064\u304f\u6c17\u306b\u306f\u306a\u308c\u306a\u3044\u306a\u3093\u3067\u3059\u304c https://t.co/LYdvG7pj5T", "followers": "187", "datetime": "2020-04-09 01:58:35", "author": "@tokumini_ss"}, "1247900839160602634": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "560", "datetime": "2020-04-08 14:55:19", "author": "@yuvalmarton"}, "1247760404085792768": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "379", "datetime": "2020-04-08 05:37:17", "author": "@loeweh"}, "1247747856825384961": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "2", "datetime": "2020-04-08 04:47:25", "author": "@BsZNJbkXzy091Wb"}, "1248440725416611841": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-10 02:40:38", "author": "@arxiv_cscv"}, "1247906757545885699": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "15", "datetime": "2020-04-08 15:18:50", "author": "@uthamkamath"}, "1253496767900815360": {"content_summary": "RT @lavanyaai: \ud83d\udcdc The Evolving Normalization-Activation Layers paper by @Hanxiao_6 et all \u2013 https://t.co/rWFm1N0gSt \ud83d\udc69\u200d\ud83d\udd2c Interactive @weight\u2026", "followers": "0", "datetime": "2020-04-24 01:31:32", "author": "@Sam09lol"}, "1247813636267094016": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "3,623", "datetime": "2020-04-08 09:08:48", "author": "@tarantulae"}, "1247840454135013376": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "783", "datetime": "2020-04-08 10:55:22", "author": "@muktabh"}, "1252440608419065861": {"content_summary": "RT @sqcai: Nice use of the new Graphs support of https://t.co/YEhElejmjf to show the computation graph underlying EvoNorm! https://t.co/GZf\u2026", "followers": "165", "datetime": "2020-04-21 03:34:44", "author": "@denta_delta"}, "1248235949240094720": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "516", "datetime": "2020-04-09 13:06:56", "author": "@risounomoewaifu"}, "1247826662261166081": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "3,261", "datetime": "2020-04-08 10:00:34", "author": "@sosinformatico"}, "1248260266011381760": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "1,288", "datetime": "2020-04-09 14:43:33", "author": "@heghbalz"}, "1247991112607977478": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "159,742", "datetime": "2020-04-08 20:54:02", "author": "@Montreal_IA"}, "1247838241207857152": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "882", "datetime": "2020-04-08 10:46:35", "author": "@vadimkantorov"}, "1247700403140976640": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "51", "datetime": "2020-04-08 01:38:51", "author": "@phanhuutho92"}, "1247776027884961792": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "16", "datetime": "2020-04-08 06:39:22", "author": "@mxxtsai"}, "1247769692782997504": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "87", "datetime": "2020-04-08 06:14:11", "author": "@JrKibs"}, "1247963935535280129": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "669", "datetime": "2020-04-08 19:06:02", "author": "@arunkumar_bvr"}, "1248092069047857153": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "838", "datetime": "2020-04-09 03:35:12", "author": "@inoichan"}, "1247745259993305090": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "266", "datetime": "2020-04-08 04:37:06", "author": "@brandonkindred"}, "1247812000920584192": {"content_summary": "The most popular ArXiv tweet in the last 24h: https://t.co/KjTT3yCwoP", "followers": "60", "datetime": "2020-04-08 09:02:18", "author": "@popular_ML"}, "1247780231936536579": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "425", "datetime": "2020-04-08 06:56:04", "author": "@ryo_masumura"}, "1247856516037828608": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "332", "datetime": "2020-04-08 11:59:12", "author": "@vsinghalus"}, "1247818659596242947": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "628", "datetime": "2020-04-08 09:28:46", "author": "@alisher_ai"}, "1247741188653731841": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "4,531", "datetime": "2020-04-08 04:20:55", "author": "@ChrSzegedy"}, "1247741200674643968": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "808", "datetime": "2020-04-08 04:20:58", "author": "@BlkHwk0ps"}, "1247798634373984257": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "161", "datetime": "2020-04-08 08:09:12", "author": "@Sid____"}, "1248118115054940160": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "282", "datetime": "2020-04-09 05:18:42", "author": "@dunce_ml"}, "1247949946898112512": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "578", "datetime": "2020-04-08 18:10:27", "author": "@usagisan2020"}, "1247790915436957697": {"content_summary": "Evolving Normalization-Activation Layers. (arXiv:2004.02967v1 [cs.LG]) https://t.co/apetSu6O7M", "followers": "50", "datetime": "2020-04-08 07:38:31", "author": "@CSarXiv"}, "1247953067850547201": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "3,096", "datetime": "2020-04-08 18:22:51", "author": "@ivan_bezdomny"}, "1247788848169381897": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "0", "datetime": "2020-04-08 07:30:18", "author": "@Sam09lol"}, "1248709795613298688": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "137", "datetime": "2020-04-10 20:29:49", "author": "@LeonardMosescu"}, "1247770985811468288": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "322", "datetime": "2020-04-08 06:19:20", "author": "@countrsignal"}, "1247775100343963649": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "361", "datetime": "2020-04-08 06:35:41", "author": "@mundoplano"}, "1247850350872330241": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "164,067", "datetime": "2020-04-08 11:34:42", "author": "@ceobillionaire"}, "1250142302145785857": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "1,130", "datetime": "2020-04-14 19:22:06", "author": "@aigirlcoppelia"}, "1248772757493616640": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "819", "datetime": "2020-04-11 00:40:01", "author": "@deepgradient"}, "1247949842694803458": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "578", "datetime": "2020-04-08 18:10:02", "author": "@usagisan2020"}, "1248621957563609089": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09C9NM", "followers": "4,099", "datetime": "2020-04-10 14:40:47", "author": "@arxiv_cscv"}, "1248814123124547589": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "903", "datetime": "2020-04-11 03:24:23", "author": "@wk77"}, "1247705697183244288": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "26", "datetime": "2020-04-08 01:59:54", "author": "@zakariarguibi05"}, "1252791639153786881": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "39", "datetime": "2020-04-22 02:49:37", "author": "@arinzechukwu_c"}, "1248486184763797504": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09C9NM", "followers": "4,099", "datetime": "2020-04-10 05:41:16", "author": "@arxiv_cscv"}, "1247732663202373632": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "824", "datetime": "2020-04-08 03:47:03", "author": "@morioka"}, "1248927386625687552": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "625", "datetime": "2020-04-11 10:54:27", "author": "@Eseshinpu"}, "1247959812794531840": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "64", "datetime": "2020-04-08 18:49:39", "author": "@MaruHiroReiwa"}, "1247988501758369797": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "66", "datetime": "2020-04-08 20:43:39", "author": "@panchovie"}, "1252613703943372800": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "638", "datetime": "2020-04-21 15:02:34", "author": "@Soumya32"}, "1258389371776045057": {"content_summary": "Great thought, @charlesxjyang21! Another #autoML paper evaluated on the same set of benchmarks \u2013 #ImageNet & #CIFAR is acceptable as long they still pose a difficult/ relevant problem https://t.co/g4lq1iioJs", "followers": "3,982", "datetime": "2020-05-07 13:33:00", "author": "@ldvcapital"}, "1253715660263378945": {"content_summary": "Google Reaserch and DeepMind join forces and unify normalization and activation layers. EvoNorms, the new structure, was designed to evolve form low-level primitives and not rely on building blocks. Very interesting read from @Hanxiao_6 https://t.co/ps1V", "followers": "117", "datetime": "2020-04-24 16:01:20", "author": "@Marek_Bardonski"}, "1247731192398405632": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "250", "datetime": "2020-04-08 03:41:12", "author": "@takka__Q"}, "1252838356628549634": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "47", "datetime": "2020-04-22 05:55:15", "author": "@mbilaliam"}, "1248021806616678401": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "93", "datetime": "2020-04-08 22:56:00", "author": "@0xhexhex"}, "1247966641213730817": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "113", "datetime": "2020-04-08 19:16:48", "author": "@_moto86"}, "1249151731751149570": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "2,043", "datetime": "2020-04-12 01:45:55", "author": "@imenurok"}, "1247729867719835649": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "315", "datetime": "2020-04-08 03:35:56", "author": "@anshumanmishra"}, "1248410547034284034": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09C9NM", "followers": "4,099", "datetime": "2020-04-10 00:40:43", "author": "@arxiv_cscv"}, "1248989368850628610": {"content_summary": "RT @mosko_mule: Evolving Normalization-Activation Layers https://t.co/RyGwzxNgNH \u9032\u5316\u8a08\u7b97\u306b\u3088\u308bAutoML\u3092\u7528\u3044\u3066Norm+\u6d3b\u6027\u5316\u51fd\u6570\u306e\u7d44\u5408\u305b\u3092\uff0c\u8907\u6570\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6027\u80fd\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u63a2\u7d22\uff0e\u5f97\u3089\u2026", "followers": "2,077", "datetime": "2020-04-11 15:00:45", "author": "@yutakama"}, "1252613079986311168": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "65", "datetime": "2020-04-21 15:00:05", "author": "@dave_co_dev"}, "1251696099485605888": {"content_summary": "RT @RisingSayak: Thanks to @CShorten30 for his awesome video on the paper and I definitely recommend checking it out: https://t.co/egFt12gV\u2026", "followers": "217", "datetime": "2020-04-19 02:16:20", "author": "@AssistedEvolve"}, "1248060552703660032": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "164", "datetime": "2020-04-09 01:29:58", "author": "@ZachBessinger"}, "1252612346511364097": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "192,230", "datetime": "2020-04-21 14:57:10", "author": "@fchollet"}, "1252615734968176640": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "43", "datetime": "2020-04-21 15:10:38", "author": "@jeandut14000"}, "1248041525663027200": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "44", "datetime": "2020-04-09 00:14:21", "author": "@jetjodh"}, "1247803880764653568": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "252", "datetime": "2020-04-08 08:30:02", "author": "@inigo_zgz"}, "1247880207068880898": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "8", "datetime": "2020-04-08 13:33:20", "author": "@meditech57"}, "1247751489130336257": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "39", "datetime": "2020-04-08 05:01:51", "author": "@Ann_Qnn"}, "1247809903109869574": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "13", "datetime": "2020-04-08 08:53:58", "author": "@EricKuy"}, "1251493666008395777": {"content_summary": "Thanks to @CShorten30 for his awesome video on the paper and I definitely recommend checking it out: https://t.co/egFt12gVyo. Link to the original paper: https://t.co/e3wc0jymvf. @GoogleAI @GoogleDevsIN @GoogleDevExpert 5/5", "followers": "1,703", "datetime": "2020-04-18 12:51:56", "author": "@RisingSayak"}, "1248993193418358787": {"content_summary": "RT @mosko_mule: Evolving Normalization-Activation Layers https://t.co/RyGwzxNgNH \u9032\u5316\u8a08\u7b97\u306b\u3088\u308bAutoML\u3092\u7528\u3044\u3066Norm+\u6d3b\u6027\u5316\u51fd\u6570\u306e\u7d44\u5408\u305b\u3092\uff0c\u8907\u6570\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6027\u80fd\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u63a2\u7d22\uff0e\u5f97\u3089\u2026", "followers": "2,456", "datetime": "2020-04-11 15:15:57", "author": "@jinbeizame007"}, "1247711526326329346": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "65", "datetime": "2020-04-08 02:23:03", "author": "@kli_nlpr"}, "1247744841892507651": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "168", "datetime": "2020-04-08 04:35:26", "author": "@dr_levan"}, "1248806772686520322": {"content_summary": "Optimal normalization-activation layers are searched with multi-objective evolution. Found EvoNorm-B0 uses the normalization by the max of batch/instance variances and no activation. EvoNorm-S0 (no batch dependencies) is similar to GN+Swish. https://t.co/", "followers": "18,231", "datetime": "2020-04-11 02:55:10", "author": "@hillbig"}, "1250385744977412096": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "808", "datetime": "2020-04-15 11:29:27", "author": "@BlkHwk0ps"}, "1248019206387322881": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,168", "datetime": "2020-04-08 22:45:40", "author": "@mark_cummins"}, "1247747707755618305": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "150", "datetime": "2020-04-08 04:46:50", "author": "@y_yammt"}, "1247744848045559809": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "18", "datetime": "2020-04-08 04:35:28", "author": "@pitbull_wang"}, "1247806713295601665": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "3,414", "datetime": "2020-04-08 08:41:18", "author": "@alxndrkalinin"}, "1247748340831277057": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "8,940", "datetime": "2020-04-08 04:49:21", "author": "@danbri"}, "1247777326126882816": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "354", "datetime": "2020-04-08 06:44:31", "author": "@indy9000"}, "1247810703055962112": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "125", "datetime": "2020-04-08 08:57:09", "author": "@sti4KkS5E0y1qul"}, "1247967438358163457": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "414", "datetime": "2020-04-08 19:19:58", "author": "@evan_cofer"}, "1247697430285447168": {"content_summary": "New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperform BatchNorm-ReLU on many tasks. A promising use of AutoML to discover fundamental ML building blocks. https://t.co/qFfqGzPO6G Join", "followers": "18", "datetime": "2020-04-08 01:27:03", "author": "@Hanxiao_6"}, "1247710622911004680": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "356", "datetime": "2020-04-08 02:19:28", "author": "@saruftw"}, "1249259390697271297": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "2,681", "datetime": "2020-04-12 08:53:43", "author": "@tacmasi"}, "1248000659976097792": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "590", "datetime": "2020-04-08 21:31:58", "author": "@codekee"}, "1248775823869358087": {"content_summary": "RT @fabtar: Excellent video from @ykilcher on\"Evolving Normalization-Activation Layers\" https://t.co/MHUmTRRjDw. This is about this paper h\u2026", "followers": "819", "datetime": "2020-04-11 00:52:12", "author": "@deepgradient"}, "1249322094837600259": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "877", "datetime": "2020-04-12 13:02:53", "author": "@boriel"}, "1247968474464477184": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "837", "datetime": "2020-04-08 19:24:05", "author": "@MarklDouthwaite"}, "1247703151307718656": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "4,531", "datetime": "2020-04-08 01:49:47", "author": "@ChrSzegedy"}, "1247701717552005120": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "155", "datetime": "2020-04-08 01:44:05", "author": "@keylinker"}, "1247776932042686464": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "114", "datetime": "2020-04-08 06:42:57", "author": "@MPetrochuk"}, "1248135282915827712": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "570", "datetime": "2020-04-09 06:26:55", "author": "@mhough"}, "1247868046766755841": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "23", "datetime": "2020-04-08 12:45:01", "author": "@braneloop"}, "1247767080792801280": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "170", "datetime": "2020-04-08 06:03:49", "author": "@razielag"}, "1248970241692110848": {"content_summary": "RT @xsteenbrugge: @_brohrer_ There's a new drop-in TF layer from Google Brain / DeepMind that broadly outperforms BN and has an online vari\u2026", "followers": "365", "datetime": "2020-04-11 13:44:45", "author": "@iugoaoj"}, "1248239157266116610": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "871", "datetime": "2020-04-09 13:19:40", "author": "@mihail_eric"}, "1247774333407129603": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "28,418", "datetime": "2020-04-08 06:32:38", "author": "@ogrisel"}, "1248621039904407558": {"content_summary": "Excellent video from @ykilcher on\"Evolving Normalization-Activation Layers\" https://t.co/MHUmTRRjDw. This is about this paper https://t.co/19I5JRWLvx by @Hanxiao_6, Andrew Brock, Karen Simonyan and Quoc V. Le", "followers": "3,137", "datetime": "2020-04-10 14:37:08", "author": "@fabtar"}, "1247702485914898432": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "3,665", "datetime": "2020-04-08 01:47:08", "author": "@eigenhector"}, "1247762240679239681": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "6,105", "datetime": "2020-04-08 05:44:35", "author": "@counterattack9"}, "1248023959725133825": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "210", "datetime": "2020-04-08 23:04:33", "author": "@Feldman1Michael"}, "1252418566084685825": {"content_summary": "Nice use of the new Graphs support of https://t.co/YEhElejmjf to show the computation graph underlying EvoNorm!", "followers": "2,399", "datetime": "2020-04-21 02:07:09", "author": "@sqcai"}, "1247743101654851587": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,151", "datetime": "2020-04-08 04:28:32", "author": "@jd_mashiro"}, "1247836387371433986": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,043", "datetime": "2020-04-08 10:39:13", "author": "@imenurok"}, "1248119672412598272": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "712", "datetime": "2020-04-09 05:24:53", "author": "@kinako1183"}, "1248056661303914496": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,023", "datetime": "2020-04-09 01:14:30", "author": "@NmaViv"}, "1249262025982664705": {"content_summary": "RT @jaguring1: \u6b63\u898f\u5316\u5c64 https://t.co/nOS9d2Ze8H", "followers": "1,878", "datetime": "2020-04-12 09:04:11", "author": "@sesquipedale"}, "1248876345590874112": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "217", "datetime": "2020-04-11 07:31:38", "author": "@AssistedEvolve"}, "1247834196040544259": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "53", "datetime": "2020-04-08 10:30:30", "author": "@MktO740123"}, "1248280184635064320": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "26", "datetime": "2020-04-09 16:02:42", "author": "@wangzhangup"}, "1247699305454080000": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "344", "datetime": "2020-04-08 01:34:30", "author": "@jefkine"}, "1249195864230412293": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-12 04:41:17", "author": "@arxiv_cscv"}, "1247864802736832512": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "274", "datetime": "2020-04-08 12:32:07", "author": "@JanLauGe"}, "1247743804737638406": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "395", "datetime": "2020-04-08 04:31:19", "author": "@linkoffate"}, "1247961581788573696": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "6", "datetime": "2020-04-08 18:56:41", "author": "@siddhadev"}, "1247914739721621505": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "301", "datetime": "2020-04-08 15:50:33", "author": "@NLP_niko"}, "1247808302563516418": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "18", "datetime": "2020-04-08 08:47:37", "author": "@saint_armadillo"}, "1248859953302892544": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "233", "datetime": "2020-04-11 06:26:30", "author": "@bamboo4031"}, "1247750760638472197": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "25", "datetime": "2020-04-08 04:58:58", "author": "@RainbowMcDreamy"}, "1247707738060328961": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "67", "datetime": "2020-04-08 02:08:00", "author": "@dksdc"}, "1249256611899555840": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "150", "datetime": "2020-04-12 08:42:40", "author": "@mrecjaipur"}, "1247760313497161728": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "47", "datetime": "2020-04-08 05:36:55", "author": "@mbilaliam"}, "1247708878642589696": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "2,875", "datetime": "2020-04-08 02:12:32", "author": "@sschoenholz"}, "1247742150940545024": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "52", "datetime": "2020-04-08 04:24:45", "author": "@Wind_Xiaoli"}, "1247803842575515648": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "327", "datetime": "2020-04-08 08:29:53", "author": "@aditya_soni2k17"}, "1248145441511931905": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "4,255", "datetime": "2020-04-09 07:07:17", "author": "@weballergy"}, "1247905036392214529": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "1,024", "datetime": "2020-04-08 15:12:00", "author": "@desertnaut"}, "1247861995237330944": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "19", "datetime": "2020-04-08 12:20:58", "author": "@_nibuiro"}, "1247799923266834432": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "2,193", "datetime": "2020-04-08 08:14:19", "author": "@calabi_and_yau"}, "1255507407960264704": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-29 14:41:06", "author": "@arxiv_cscv"}, "1248005264172711937": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "962", "datetime": "2020-04-08 21:50:16", "author": "@jnhwkim"}, "1247912171545055232": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "21", "datetime": "2020-04-08 15:40:21", "author": "@miCroPlatinum"}, "1247810446771597312": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "252", "datetime": "2020-04-08 08:56:08", "author": "@krystoferivanov"}, "1249391949531971587": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-12 17:40:28", "author": "@arxiv_cscv"}, "1248581494009778182": {"content_summary": "[DeepLearning] Evolving Normalization-Activation Layers (@Hanxiao_6 @DeepMind) : Une couche pour remplacer BatchNorm-ReLU ! Architecture d\u00e9voil\u00e9e par une strat\u00e9gie d'\u00e9volution. La Neural Architecture Search continue \u00e0 relever le niveau du SOTA. https://t.", "followers": "1", "datetime": "2020-04-10 12:00:00", "author": "@CodeWithData"}, "1247970547708784640": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,706", "datetime": "2020-04-08 19:32:19", "author": "@koshian2"}, "1247989651396165632": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "697", "datetime": "2020-04-08 20:48:14", "author": "@takatoh1"}, "1247782111966146561": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "282", "datetime": "2020-04-08 07:03:32", "author": "@Ar_Douillard"}, "1247757020662390788": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "54", "datetime": "2020-04-08 05:23:50", "author": "@Surreabral"}, "1247917058525777920": {"content_summary": "Nice ideas of using (a) multiple architectures in the search objective for generalization & (b) a light weight proxy task on CIFAR-10 but rerank final candidates with ImageNet. EvoNorm seems to work pretty well across batch sizes! by @Hanxiao_6, @quocl", "followers": "15,224", "datetime": "2020-04-08 15:59:46", "author": "@lmthang"}, "1247752849582243841": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "615", "datetime": "2020-04-08 05:07:16", "author": "@mattmcd"}, "1247898002141769729": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "625", "datetime": "2020-04-08 14:44:03", "author": "@ampanmdagaba"}, "1248272884067266561": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "281", "datetime": "2020-04-09 15:33:42", "author": "@manideep2510"}, "1253494983539056641": {"content_summary": "RT @lmthang: Nice ideas of using (a) multiple architectures in the search objective for generalization & (b) a light weight proxy task on C\u2026", "followers": "2", "datetime": "2020-04-24 01:24:27", "author": "@Sheuancts"}, "1248620368132059145": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "3,137", "datetime": "2020-04-10 14:34:28", "author": "@fabtar"}, "1247740850760593409": {"content_summary": "List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better activation functions \u2022 better learning rules than sgd/adam \u2022 better data augmentation strategies \u2022 better loss functions \u2022 better no", "followers": "81,490", "datetime": "2020-04-08 04:19:35", "author": "@hardmaru"}, "1247806783139135489": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,934", "datetime": "2020-04-08 08:41:34", "author": "@mandubian"}, "1247790938807603201": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "228", "datetime": "2020-04-08 07:38:37", "author": "@hengcherkeng"}, "1247887695608606726": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "217", "datetime": "2020-04-08 14:03:05", "author": "@AssistedEvolve"}, "1247948561364463616": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "5", "datetime": "2020-04-08 18:04:57", "author": "@elvcastelo"}, "1247823543762444289": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "130", "datetime": "2020-04-08 09:48:10", "author": "@rhira2016"}, "1248062552153772033": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "14", "datetime": "2020-04-09 01:37:54", "author": "@MagicYu1"}, "1252295449068793857": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "93", "datetime": "2020-04-20 17:57:56", "author": "@0xhexhex"}, "1249262480771043335": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "9", "datetime": "2020-04-12 09:06:00", "author": "@WeinroT_xc"}, "1247746095091867650": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09C9NM", "followers": "4,099", "datetime": "2020-04-08 04:40:25", "author": "@arxiv_cscv"}, "1247785237733470209": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,288", "datetime": "2020-04-08 07:15:58", "author": "@heghbalz"}, "1247866307862654977": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "250", "datetime": "2020-04-08 12:38:06", "author": "@takka__Q"}, "1247699750146789377": {"content_summary": "Evolving Normalization-Activation Layers. Hanxiao Liu, Andrew Brock, Karen Simonyan, and Quoc V. Le https://t.co/bep1SYPBeS", "followers": "3,882", "datetime": "2020-04-08 01:36:16", "author": "@BrundageBot"}, "1247909048214679554": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "130", "datetime": "2020-04-08 15:27:56", "author": "@manujosephv"}, "1247690512947019776": {"content_summary": "RT @ak92501: Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "53", "datetime": "2020-04-08 00:59:33", "author": "@hana_jeetsu"}, "1247795022453334016": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,371", "datetime": "2020-04-08 07:54:50", "author": "@Dresdenboy"}, "1247717899189522433": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "7,556", "datetime": "2020-04-08 02:48:23", "author": "@brandondamos"}, "1247792970759208964": {"content_summary": "Automating #DeepLearning https://t.co/TPX7ISzPVx", "followers": "4,314", "datetime": "2020-04-08 07:46:41", "author": "@srisatish"}, "1248235337891090433": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "340", "datetime": "2020-04-09 13:04:30", "author": "@mcgenergy"}, "1247828322496548865": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "425", "datetime": "2020-04-08 10:07:10", "author": "@iamknighton"}, "1248309860334055424": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "272", "datetime": "2020-04-09 18:00:37", "author": "@nicholashcain"}, "1248166379473326082": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "266", "datetime": "2020-04-09 08:30:29", "author": "@annaearwen"}, "1247732857872801793": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "523", "datetime": "2020-04-08 03:47:49", "author": "@langdon"}, "1252962361243308032": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/efVyTOvxNs #AI #Research via @fchollet", "followers": "2,285", "datetime": "2020-04-22 14:08:00", "author": "@future_of_AI"}, "1247840977320136704": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "900", "datetime": "2020-04-08 10:57:27", "author": "@marknabil"}, "1255313989959061505": {"content_summary": "Evolving Normalization-Activation Layers \u9032\u5316\u3059\u308b\u6b63\u898f\u5316\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc 2020-04-28T16:29:08+00:00 arXiv: https://t.co/r9Q0jIZqhJ \u82f1/\u65e5\u30b5\u30de\u30ea\u2193 https://t.co/7glvnUva2K", "followers": "142", "datetime": "2020-04-29 01:52:32", "author": "@arXiv_reaDer"}, "1247963907668373506": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "669", "datetime": "2020-04-08 19:05:56", "author": "@arunkumar_bvr"}, "1252738343697178624": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "183", "datetime": "2020-04-21 23:17:50", "author": "@piyush04k"}, "1247707614189957121": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "275", "datetime": "2020-04-08 02:07:31", "author": "@kadarakos"}, "1248968713958608896": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "31", "datetime": "2020-04-11 13:38:40", "author": "@kenasnet"}, "1247722356664979456": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "127", "datetime": "2020-04-08 03:06:06", "author": "@shimopino"}, "1247784399233400833": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "3,883", "datetime": "2020-04-08 07:12:38", "author": "@ajmooch"}, "1247727173844094978": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "4,017", "datetime": "2020-04-08 03:25:14", "author": "@ballforest"}, "1248144109036023808": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "57", "datetime": "2020-04-09 07:01:59", "author": "@Kweku_Sofo"}, "1248044531548696577": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "1,151", "datetime": "2020-04-09 00:26:18", "author": "@jd_mashiro"}, "1247919484800622592": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "93", "datetime": "2020-04-08 16:09:25", "author": "@0xhexhex"}, "1247796681208299526": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1", "datetime": "2020-04-08 08:01:26", "author": "@DevMontag"}, "1247794781431844865": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "4,891", "datetime": "2020-04-08 07:53:53", "author": "@IgorCarron"}, "1252613205441929221": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "2,119", "datetime": "2020-04-21 15:00:35", "author": "@fullNam35087976"}, "1252801116019896320": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "615", "datetime": "2020-04-22 03:27:16", "author": "@KouroshMeshgi"}, "1247871034709114882": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "19", "datetime": "2020-04-08 12:56:53", "author": "@MassBassLol"}, "1252961045078880257": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "217", "datetime": "2020-04-22 14:02:46", "author": "@AssistedEvolve"}, "1252440585337700352": {"content_summary": "RT @sqcai: Nice use of the new Graphs support of https://t.co/YEhElejmjf to show the computation graph underlying EvoNorm!", "followers": "439", "datetime": "2020-04-21 03:34:39", "author": "@loraxorg"}, "1252619864407187456": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "365", "datetime": "2020-04-21 15:27:02", "author": "@iugoaoj"}, "1250366856231337985": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "103", "datetime": "2020-04-15 10:14:23", "author": "@avmoldovan"}, "1248106690546810883": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "135,925", "datetime": "2020-04-09 04:33:18", "author": "@hiconcep"}, "1248018853965123584": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "26", "datetime": "2020-04-08 22:44:16", "author": "@milk_away123"}, "1247717089730859013": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "164,067", "datetime": "2020-04-08 02:45:10", "author": "@ceobillionaire"}, "1248807510649311232": {"content_summary": "RT @hillbig: Optimal normalization-activation layers are searched with multi-objective evolution. Found EvoNorm-B0 uses the normalization b\u2026", "followers": "249", "datetime": "2020-04-11 02:58:06", "author": "@mshero_y"}, "1247711633062977536": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "31", "datetime": "2020-04-08 02:23:29", "author": "@chinmayrane16"}, "1247770690696007682": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "0", "datetime": "2020-04-08 06:18:09", "author": "@hogier4"}, "1248283751601209346": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "0", "datetime": "2020-04-09 16:16:53", "author": "@Sam09lol"}, "1247741130009010176": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "150", "datetime": "2020-04-08 04:20:41", "author": "@BedabrataChoud1"}, "1247876544346947589": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "191", "datetime": "2020-04-08 13:18:47", "author": "@Kaszanas"}, "1247906933119447040": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "397", "datetime": "2020-04-08 15:19:32", "author": "@badarahmed"}, "1247970246360616960": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "828", "datetime": "2020-04-08 19:31:07", "author": "@kuritateppei"}, "1248041495325601792": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "2", "datetime": "2020-04-09 00:14:14", "author": "@hoboroots"}, "1248807088584675328": {"content_summary": "RT @hillbig: Optimal normalization-activation layers are searched with multi-objective evolution. Found EvoNorm-B0 uses the normalization b\u2026", "followers": "478", "datetime": "2020-04-11 02:56:26", "author": "@yasuokajihei"}, "1247752559122497538": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "107", "datetime": "2020-04-08 05:06:06", "author": "@agispof"}, "1248475077282123779": {"content_summary": "RT @DigantaMisra1: Recently @GoogleAI and @DeepMind released a paper shortly called EvoNorm (Paper Link - https://t.co/RhFs6LzlLN). I tried\u2026", "followers": "89", "datetime": "2020-04-10 04:57:08", "author": "@mgrankin"}, "1252698951800754182": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "245", "datetime": "2020-04-21 20:41:18", "author": "@__jm"}, "1253491329323106304": {"content_summary": "RT @lavanyaai: \ud83d\udcdc The Evolving Normalization-Activation Layers paper by @Hanxiao_6 et all \u2013 https://t.co/rWFm1N0gSt \ud83d\udc69\u200d\ud83d\udd2c Interactive @weight\u2026", "followers": "2,070", "datetime": "2020-04-24 01:09:56", "author": "@EricSchles"}, "1253096828414935042": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "32", "datetime": "2020-04-22 23:02:19", "author": "@MonitaurAI"}, "1247863409581928451": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,084", "datetime": "2020-04-08 12:26:35", "author": "@Atabey_Kaygun"}, "1247871834198093824": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "242", "datetime": "2020-04-08 13:00:04", "author": "@blauigris"}, "1247919540039643137": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "93", "datetime": "2020-04-08 16:09:38", "author": "@0xhexhex"}, "1247843741228060677": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "198", "datetime": "2020-04-08 11:08:26", "author": "@UrScienceFriend"}, "1248109646004588547": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "96", "datetime": "2020-04-09 04:45:03", "author": "@MsCheikh"}, "1248033793480347648": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "155", "datetime": "2020-04-08 23:43:38", "author": "@hitsgub"}, "1248001135320764417": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "2,927", "datetime": "2020-04-08 21:33:52", "author": "@evolvingstuff"}, "1248003381643239425": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "1,082", "datetime": "2020-04-08 21:42:47", "author": "@shinmura0"}, "1247781659832799232": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "99", "datetime": "2020-04-08 07:01:45", "author": "@treasured_write"}, "1247872409442582529": {"content_summary": "RT @kzykmyzw: \u6b63\u898f\u5316\u3068\u6d3b\u6027\u5316\u3092\u500b\u5225\u306b\u6271\u3046\u306e\u3067\u306f\u306a\u304f\u5358\u4e00\u306e\u30ec\u30a4\u30e4\u3068\u3057\u3066\u8a2d\u8a08\u3002\u5358\u7d14\u306a\u30aa\u30da\u30ec\u30fc\u30bf\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u30b5\u30fc\u30c1\u3059\u308b\u3053\u3068\u3067EvoNorm\u30d5\u30a1\u30df\u30ea\u30fc\u3068\u547c\u3076\u65b0\u305f\u306a\u30ec\u30a4\u30e4\u3092\u7372\u5f97\u3002Res/Mobile/EfficientNet\u306e\u3044\u305a\u308c\u3067\u3082\u6027\u80fd\u6539\u5584\u304c\u898b\u3089\u308c\u3001\u3055\u3089\u306b\u7269\u4f53\u691c\u51fa\u3084\u30bb\u2026", "followers": "1,151", "datetime": "2020-04-08 13:02:21", "author": "@jd_mashiro"}, "1247857304323551232": {"content_summary": "@Yannlg_ next step", "followers": "111", "datetime": "2020-04-08 12:02:20", "author": "@enricesena"}, "1248691336267563008": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "808", "datetime": "2020-04-10 19:16:28", "author": "@BlkHwk0ps"}, "1247853414496813056": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "18", "datetime": "2020-04-08 11:46:52", "author": "@CyberStoic"}, "1249014658079563777": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-11 16:41:14", "author": "@arxiv_cscv"}, "1247789850998128641": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "69", "datetime": "2020-04-08 07:34:17", "author": "@davidm_ller"}, "1247730491362553856": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "292", "datetime": "2020-04-08 03:38:25", "author": "@PrasathLab"}, "1247808274834964481": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "26", "datetime": "2020-04-08 08:47:30", "author": "@6963616e74646f7"}, "1248064888058118156": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "3,374", "datetime": "2020-04-09 01:47:11", "author": "@renato_umeton"}, "1247753041001861120": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "197", "datetime": "2020-04-08 05:08:01", "author": "@bAbAHAdAd"}, "1255311116957253633": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-29 01:41:07", "author": "@arxiv_cscv"}, "1247861939138711557": {"content_summary": "is this #ArtificialIntelligence evolution Darwinian? If so, we have our answer about man's ability to oversee and govern #AI.", "followers": "1,626", "datetime": "2020-04-08 12:20:45", "author": "@sceeto"}, "1247822060803567626": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "118", "datetime": "2020-04-08 09:42:17", "author": "@matthewopala"}, "1255553087290343426": {"content_summary": "Evolving Normalization-Activation Layers https://t.co/PUHt09kyWe", "followers": "4,099", "datetime": "2020-04-29 17:42:37", "author": "@arxiv_cscv"}, "1247686613326577664": {"content_summary": "Evolving Normalization-Activation Layers pdf: https://t.co/ndDCccbsIn abs: https://t.co/BFV7qoyeli https://t.co/pGcuisODw1", "followers": "8,314", "datetime": "2020-04-08 00:44:04", "author": "@ak92501"}, "1247747425537642497": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "6,480", "datetime": "2020-04-08 04:45:42", "author": "@sebprovencher"}, "1247963582110842881": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "3,413", "datetime": "2020-04-08 19:04:38", "author": "@theomitsa"}, "1252247125557161991": {"content_summary": "Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR 10 & 100. \ud83d\udcbb Code available here : https://t.co/EYNxiISxwP \ud83d\udcc8 TensorBoard (w/ and w/o data aug) https://t.co/Pnkb8GC7Hb \ud83d\udcdd Paper: ht", "followers": "263", "datetime": "2020-04-20 14:45:55", "author": "@raphaelmeudec"}, "1247967551625129984": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "669", "datetime": "2020-04-08 19:20:25", "author": "@arunkumar_bvr"}, "1249275894885212160": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "43", "datetime": "2020-04-12 09:59:18", "author": "@jeandut14000"}, "1247777752666669066": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "703", "datetime": "2020-04-08 06:46:13", "author": "@DigantaMisra1"}, "1247884012011454465": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "14", "datetime": "2020-04-08 13:48:27", "author": "@e7mul"}, "1247780847765188611": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "356", "datetime": "2020-04-08 06:58:31", "author": "@dj_olas"}, "1248897430105370626": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "4", "datetime": "2020-04-11 08:55:25", "author": "@Amor5lIZZY"}, "1247832264089767938": {"content_summary": "Potential powerful AutoML usecase.", "followers": "912", "datetime": "2020-04-08 10:22:50", "author": "@ChikaObuah"}, "1247777293658763265": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "15", "datetime": "2020-04-08 06:44:24", "author": "@hahnz_"}, "1247800343351574528": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "561", "datetime": "2020-04-08 08:15:59", "author": "@josepemanzano"}, "1247814551271133184": {"content_summary": "Evolving Normalization-Activation Layers. (arXiv:2004.02967v1 [cs.LG]) https://t.co/2Lit282544 Normalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing th", "followers": "48", "datetime": "2020-04-08 09:12:26", "author": "@yapp1e"}, "1248007499606720512": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "291", "datetime": "2020-04-08 21:59:09", "author": "@sculpepper"}, "1247825510597525504": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "138", "datetime": "2020-04-08 09:55:59", "author": "@InfoOrganon"}, "1247795091323805696": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,371", "datetime": "2020-04-08 07:55:07", "author": "@Dresdenboy"}, "1248861914412670979": {"content_summary": "RT @hillbig: NN\u3067\u6700\u9069\u306a\u6b63\u898f\u5316\u64cd\u4f5c\u3068\u6d3b\u6027\u5316\u95a2\u6570\u3068\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u9032\u5316\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u63a2\u7d22\u3002\u5f97\u3089\u308c\u305fEvoNorm-B0\u306f\u30d0\u30c3\u30c1\u5185\u5206\u6563\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5185\u5206\u6563\u306e\u5927\u304d\u3044\u65b9\u3067\u5272\u308a\u3001\u305d\u306e\u5f8c\u975e\u7dda\u5f62\u64cd\u4f5c\u306f\u4f7f\u308f\u306a\u3044\u3002BN-Relu\u3092\u5927\u304d\u304f\u4e0a\u56de\u308b\u3002\u30d0\u30c3\u30c1\u7d71\u8a08\u91cf\u3092\u4f7f\u308f\u306a\u3044S0\u306fGN\u3068S\u2026", "followers": "1,384", "datetime": "2020-04-11 06:34:17", "author": "@oltanathiv"}, "1247744486903386116": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "0", "datetime": "2020-04-08 04:34:02", "author": "@Sam09lol"}, "1247968592450093057": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "493", "datetime": "2020-04-08 19:24:33", "author": "@sivaram2k10"}, "1247856460501041152": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "332", "datetime": "2020-04-08 11:58:58", "author": "@vsinghalus"}, "1248272809387675648": {"content_summary": "RT @lmthang: Nice ideas of using (a) multiple architectures in the search objective for generalization & (b) a light weight proxy task on C\u2026", "followers": "281", "datetime": "2020-04-09 15:33:24", "author": "@manideep2510"}, "1247745626013454345": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "168", "datetime": "2020-04-08 04:38:33", "author": "@dr_levan"}, "1248691224258711554": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "414", "datetime": "2020-04-10 19:16:02", "author": "@evan_cofer"}, "1247705581927923713": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "1,150", "datetime": "2020-04-08 01:59:26", "author": "@wightmanr"}, "1247844754349588480": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "201", "datetime": "2020-04-08 11:12:27", "author": "@vlatsis"}, "1252616203706630144": {"content_summary": "RT @raphaelmeudec: Spent the weekend on implementing EvoNorm S0 and B0 with @TensorFlow 2.0 and running some ResNet18 trainings over CIFAR\u2026", "followers": "7", "datetime": "2020-04-21 15:12:30", "author": "@LakshwinShrees1"}, "1248195391998345216": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "96", "datetime": "2020-04-09 10:25:46", "author": "@Delfox29"}, "1247711822007992322": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "39", "datetime": "2020-04-08 02:24:14", "author": "@asitkumarmishra"}, "1248242891622760449": {"content_summary": "RT @jaguring1: AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u2026", "followers": "842", "datetime": "2020-04-09 13:34:31", "author": "@kazanagisora"}, "1247742594135625729": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "145", "datetime": "2020-04-08 04:26:31", "author": "@vigi99"}, "1247933819593936896": {"content_summary": "RT @milesboard: Evolving Normalization-Activation Layers #AI #deeplearning https://t.co/czjJOctKQd https://t.co/vwbDNxe44S", "followers": "271", "datetime": "2020-04-08 17:06:22", "author": "@LazyBot6"}, "1247783109711368197": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "1,020", "datetime": "2020-04-08 07:07:30", "author": "@serrjoa"}, "1247855629936492544": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "3,374", "datetime": "2020-04-08 11:55:40", "author": "@renato_umeton"}, "1248510078208086016": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "14", "datetime": "2020-04-10 07:16:13", "author": "@rjpatel117"}, "1247698131602395137": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "132", "datetime": "2020-04-08 01:29:50", "author": "@aquadude1231"}, "1247933222463422468": {"content_summary": "Evolving Normalization-Activation Layers #AI #deeplearning https://t.co/czjJOctKQd https://t.co/vwbDNxe44S", "followers": "1,380", "datetime": "2020-04-08 17:04:00", "author": "@milesboard"}, "1248162926348951563": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "3,918", "datetime": "2020-04-09 08:16:46", "author": "@andreas_madsen"}, "1251580538885832705": {"content_summary": "RT @RisingSayak: Thanks to @CShorten30 for his awesome video on the paper and I definitely recommend checking it out: https://t.co/egFt12gV\u2026", "followers": "0", "datetime": "2020-04-18 18:37:08", "author": "@Sam09lol"}, "1247732182530965506": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "415", "datetime": "2020-04-08 03:45:08", "author": "@__nggih"}, "1247765415159197697": {"content_summary": "RT @JeffDean: Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evol\u2026", "followers": "362", "datetime": "2020-04-08 05:57:11", "author": "@atulskulkarni"}, "1251527403354161158": {"content_summary": "RT @RisingSayak: Thanks to @CShorten30 for his awesome video on the paper and I definitely recommend checking it out: https://t.co/egFt12gV\u2026", "followers": "4,526", "datetime": "2020-04-18 15:05:59", "author": "@CShorten30"}, "1247703479717519362": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "403", "datetime": "2020-04-08 01:51:05", "author": "@DSPonFPGA"}, "1248196137489555456": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "87", "datetime": "2020-04-09 10:28:44", "author": "@badripatro_iitb"}, "1247932618957611010": {"content_summary": "RT @quocleix: Cool results from our collaboration with colleagues at @DeepMind on searching for new layers as alternatives for BatchNorm-Re\u2026", "followers": "103", "datetime": "2020-04-08 17:01:36", "author": "@urosn"}, "1249320141734711297": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "12", "datetime": "2020-04-12 12:55:07", "author": "@lotharrats"}, "1248052521274183682": {"content_summary": "AutoML\u3059\u3054\u3002Google Brain\u3068DeepMind\u306e\u7814\u7a76\u3002\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306e\u6709\u671b\u306a\u4f7f\u3044\u65b9\u3002\u65e2\u5b58\u306e\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u3092\u8d85\u3048\u305f\u65b0\u3057\u3044\u30ec\u30a4\u30e4\u30fc\u300cEvoNorms\u300d\u3092\u9032\u5316\u3067\u63a2\u7d22\u3002\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067BatchNorm-ReLU\u3092\u4e0a\u56de\u308b\u3002AutoML-Zero\u306b\u4f3c\u305f\u4f4e\u30ec\u30d9\u30eb\u306e\u30d7\u30ea\u30df\u30c6\u30a3\u30d6\u304b\u3089\u59cb\u3081\u308b https://t.co/I64JIXVA0p", "followers": "12,765", "datetime": "2020-04-09 00:58:03", "author": "@jaguring1"}, "1247890345259487233": {"content_summary": "RT @Hanxiao_6: New paper: Evolving Normalization-Activation Layers. We use evolution to design new layers called EvoNorms, which outperfor\u2026", "followers": "31", "datetime": "2020-04-08 14:13:37", "author": "@khanig_online"}, "1247752121367146499": {"content_summary": "RT @hardmaru: List of papers about automating improvements for deep learning: \u2022 better architectures from known building blocks \u2022 better a\u2026", "followers": "14", "datetime": "2020-04-08 05:04:22", "author": "@ProjectDomani"}, "1247764021815554048": {"content_summary": "Some nice work from @Hanxiao_6 Andrew Brock Karen Simonyan and @quocleix (joint work between @GoogleAI and @DeepMind) on evolving new normalization techniques that outperform batchnorm on a variety of tasks. Evolution is the new norm!", "followers": "151,963", "datetime": "2020-04-08 05:51:39", "author": "@JeffDean"}}}