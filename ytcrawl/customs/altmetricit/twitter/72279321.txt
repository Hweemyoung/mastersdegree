{"citation_id": "72279321", "queriedAt": "2020-05-09 13:19:43", "completed": "0", "twitter": {"1247618343386746882": {"followers": "5", "content_summary": "When more data/training hurts: Deep Double Descent from OpenAI. https://t.co/BpUSLKzMwZ", "author": "@AWTotty", "datetime": "2020-04-07 20:12:47"}, "1258503727918518273": {"followers": "3,407", "content_summary": "@GiorgioPatrini See this https://t.co/w4dn9Pahta for empirical work, and this https://t.co/TRllmQuJuF and section 7 of this https://t.co/H1LLezLode", "author": "@_onionesque", "datetime": "2020-05-07 21:07:25"}, "1244120592338440195": {"followers": "70", "content_summary": "\u305d\u305d \u3046\u3059\u3046\u3059\u611f\u3058\u3066\u3044\u307e\u3057\u305f https://t.co/824xEWi4eM", "author": "@Shinnosuke0529_", "datetime": "2020-03-29 04:33:58"}, "1243132506519601156": {"followers": "1,566", "content_summary": "@ultrazool @SK53onOSM @EdinburghNLP @inaturalist Ha. Thanks for the kind words! I found the work on (deep) double descent quite fascinating and still try to make sense of it. Basically more labelled data isn't always better and at some points hurts model", "author": "@JesperDramsch", "datetime": "2020-03-26 11:07:40"}}, "tab": "twitter"}