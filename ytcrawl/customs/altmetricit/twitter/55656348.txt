{"tab": "twitter", "completed": "1", "twitter": {"1097312346664566784": {"author": "@StatMLPapers", "followers": "9,689", "datetime": "2019-02-18 01:50:25", "content_summary": "CrossNorm: Normalization for Off-Policy TD Reinforcement Learning. (arXiv:1902.05605v1 [cs.LG]) https://t.co/3gLr0EJ9T7"}, "1097417933184290817": {"author": "@aditya_bhatt", "followers": "582", "datetime": "2019-02-18 08:49:58", "content_summary": "My first paper ever is now on arXiv! With CrossNorm, you can finally train off-policy Deep RL methods (like DDPG etc) without using target networks. https://t.co/PLujmjmLjZ"}, "1098028138771169289": {"author": "@arxiv_in_review", "followers": "1,309", "datetime": "2019-02-20 01:14:43", "content_summary": "#ICML2019 CrossNorm: Normalization for Off-Policy TD Reinforcement Learning. (arXiv:1902.05605v1 [cs\\.LG]) https://t.co/rot73oafcZ"}, "1103385234295476224": {"author": "@aditya_bhatt", "followers": "582", "datetime": "2019-03-06 20:01:54", "content_summary": "@svlevine @CsabaSzepesvari If you're interested, we found a very \"simple\" trick can make off-policy TD learning converge: data standardization! In Deep RL (like with DDPG), it lets us train faster without target networks. Here's my thesis: https://t.co/IN"}, "1097373093218189314": {"author": "@arxivml", "followers": "773", "datetime": "2019-02-18 05:51:48", "content_summary": "\"CrossNorm: Normalization for Off-Policy TD Reinforcement Learning\", Aditya Bhatt, Max Argus, Artemij Amiranashvili\u2026 https://t.co/YeJVXq6aH9"}, "1195028578880110592": {"author": "@MasterScrat", "followers": "675", "datetime": "2019-11-14 17:19:50", "content_summary": "RT @aditya_bhatt: My first paper ever is now on arXiv! With CrossNorm, you can finally train off-policy Deep RL methods (like DDPG etc) w\u2026"}, "1109044042250289152": {"author": "@camiladotcodes", "followers": "190", "datetime": "2019-03-22 10:47:59", "content_summary": "You got your dragon\ud83d\udc09! \ud83d\udc4f\ud83d\udc4f"}, "1097327996514504704": {"author": "@yapp1e", "followers": "48", "datetime": "2019-02-18 02:52:36", "content_summary": "CrossNorm: Normalization for Off-Policy TD Reinforcement Learning. (arXiv:1902.05605v1 [cs.LG]) https://t.co/WxoLFAIl3Q Off-policy Temporal Difference (TD) learning methods, when combined with function approximators, suffer from the risk of divergence, a"}, "1195288332571418624": {"author": "@MoTaylor95", "followers": "82", "datetime": "2019-11-15 10:32:00", "content_summary": "RT @aditya_bhatt: My first paper ever is now on arXiv! With CrossNorm, you can finally train off-policy Deep RL methods (like DDPG etc) w\u2026"}, "1097326746301616128": {"author": "@BrundageBot", "followers": "3,882", "datetime": "2019-02-18 02:47:38", "content_summary": "CrossNorm: Normalization for Off-Policy TD Reinforcement Learning. Aditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox https://t.co/JPWjrl2HAp"}, "1109028038325518336": {"author": "@aditya_bhatt", "followers": "582", "datetime": "2019-03-22 09:44:23", "content_summary": "RT @aditya_bhatt: My first paper ever is now on arXiv! With CrossNorm, you can finally train off-policy Deep RL methods (like DDPG etc) w\u2026"}}, "citation_id": "55656348", "queriedAt": "2020-06-03 23:31:49"}