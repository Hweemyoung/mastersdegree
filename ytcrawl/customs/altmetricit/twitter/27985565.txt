{"citation_id": "27985565", "completed": "1", "queriedAt": "2020-05-14 12:50:38", "tab": "twitter", "twitter": {"925306850924523520": {"content_summary": "RT @hardmaru: Link to the paper: https://t.co/pXSwZERBjc By using this data augmentation technique, the achieve SOTA on ImageNet 2012, CIFA\u2026", "followers": "4,894", "datetime": "2017-10-31 10:21:59", "author": "@IgorCarron"}, "1137298490260885504": {"content_summary": "[7/10] \ud83d\udcc8 - pytorch-image-models - 689 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-08 10:01:04", "author": "@PapersTrending"}, "1093646066337992704": {"content_summary": "Relevant to the \u201cmixup: Beyond Empirical Risk Minimization\u201d paper:\u2026 \"TensorFlow Speech Recognition Challenge: Can you build an algorithm that understands simple speech commands?\" https://t.co/T4JUghEvfX H/T @brucesharpe https://t.co/pHXMVvuELP", "followers": "3,446", "datetime": "2019-02-07 23:01:55", "author": "@reiver"}, "935890142358773760": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "997", "datetime": "2017-11-29 15:16:12", "author": "@AkiraIsaka88"}, "925650096699650048": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "4,894", "datetime": "2017-11-01 09:05:55", "author": "@IgorCarron"}, "923724186639618048": {"content_summary": "mixup: Beyond Empirical Risk Minimization. Zhang, Cisse, Dauphin, and Lopez-Paz https://t.co/zBjErPP3pH", "followers": "3,858", "datetime": "2017-10-27 01:33:02", "author": "@BrundageBot"}, "1044746346186592256": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "197", "datetime": "2018-09-26 00:31:54", "author": "@MatthewTeschke"}, "925793540038201344": {"content_summary": "This is super interesting. Some sort of convexity assumption? Forcing the search into an easy-to-optimize landscape? https://t.co/1sUbVe9AjV", "followers": "2,026", "datetime": "2017-11-01 18:35:55", "author": "@geoffreyirving"}, "1134761789873410049": {"content_summary": "[5/10] \ud83d\udcc8 - pytorch-image-models - 338 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-01 10:01:08", "author": "@PapersTrending"}, "1044132460789821440": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "270", "datetime": "2018-09-24 07:52:32", "author": "@kotti_sasikanth"}, "1109967125668519937": {"content_summary": "@karpathy This whole line of work where they learn a linear interpolation of the representation. Original Idea of Mixup (https://t.co/wluANKblkr), followed by plenty of extensions similar ideas For eg: https://t.co/hez8E6yddz or https://t.co/dZDQci27x8", "followers": "1,336", "datetime": "2019-03-24 23:55:59", "author": "@jigarkdoshi"}, "1044305197277810689": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "1,187", "datetime": "2018-09-24 19:18:55", "author": "@ivanprado"}, "925778451956674561": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "313", "datetime": "2017-11-01 17:35:57", "author": "@jrbtaylor"}, "1201990331094441985": {"content_summary": "mixup \ud83d\ude0d https://t.co/QiV8uec5ig https://t.co/NNFGHzufsg", "followers": "207", "datetime": "2019-12-03 22:23:21", "author": "@eram1205"}, "925412963867086848": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "242", "datetime": "2017-10-31 17:23:38", "author": "@tjskhot"}, "1044843829403287553": {"content_summary": "This is amazing work!! Truly. Such a simple idea and to see it's impact. #Mixup #DeepLearning #Optimization", "followers": "341", "datetime": "2018-09-26 06:59:15", "author": "@mohankarthik"}, "924889252101545985": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "141", "datetime": "2017-10-30 06:42:35", "author": "@the_onederful"}, "1044055476936343552": {"content_summary": "A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fastai_v1 (one GPU, 94% accuracy, 6 minutes): https://t.co/USgm0Ev08y", "followers": "7,658", "datetime": "2018-09-24 02:46:37", "author": "@GuggerSylvain"}, "925701888577765376": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "1,896", "datetime": "2017-11-01 12:31:43", "author": "@aasensior"}, "925292184152264704": {"content_summary": "The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "4,418", "datetime": "2017-10-31 09:23:42", "author": "@abursuc"}, "1044193091765264384": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "65", "datetime": "2018-09-24 11:53:27", "author": "@russell_lliu"}, "1044099212630081536": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "217", "datetime": "2018-09-24 05:40:25", "author": "@AssistedEvolve"}, "935877259671494656": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "2,042", "datetime": "2017-11-29 14:25:01", "author": "@dosei_sanga"}, "1033707409309491200": {"content_summary": "1\u4ef6\u306e\u30b3\u30e1\u30f3\u30c8 https://t.co/4pc3oQDC1P \u201c[1710.09412] mixup: Beyond Empirical Risk Minimization\u201d https://t.co/ReTpWW4feb", "followers": "474", "datetime": "2018-08-26 13:27:06", "author": "@nrtkbb"}, "924471979532468225": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "2,042", "datetime": "2017-10-29 03:04:30", "author": "@dosei_sanga"}, "1091399159205158912": {"content_summary": "\"mixup: Beyond Empirical Risk Minimization\" by Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz https://t.co/iSTUw89Xyh (machine learning)", "followers": "3,446", "datetime": "2019-02-01 18:13:31", "author": "@reiver"}, "1132233317024239617": {"content_summary": "Interesting and resultative type of training/regularization https://t.co/A8YnhGCeWJ Demo code here: https://t.co/Hiof1pCR5u https://t.co/A8YnhGCeWJ", "followers": "83", "datetime": "2019-05-25 10:33:53", "author": "@mlwcommunity"}, "924054947229925378": {"content_summary": "mixup: Beyond Empirical Risk Minimization. https://t.co/Vqqx8rllJI https://t.co/SDwbXbvSyE", "followers": "12,718", "datetime": "2017-10-27 23:27:22", "author": "@arxiv_org"}, "926868572193816576": {"content_summary": "RT @hardmaru: Link to the paper: https://t.co/pXSwZERBjc By using this data augmentation technique, the achieve SOTA on ImageNet 2012, CIFA\u2026", "followers": "52", "datetime": "2017-11-04 17:47:42", "author": "@HengjianJia"}, "925410417916219392": {"content_summary": "RT @arxiv_org: mixup: Beyond Empirical Risk Minimization. https://t.co/Vqqx8rllJI https://t.co/SDwbXbvSyE", "followers": "1,286", "datetime": "2017-10-31 17:13:31", "author": "@heghbalz"}, "935850592378937344": {"content_summary": "Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u3089\u3057\u3044\uff0eCNN\u306f\u753b\u50cf\u3092\u6ce2\u5f62\u3067\u3082\u6271\u3063\u3066\u3044\u308b\u306e\u3067\u4f7f\u3048\u308b\uff0c\u306a\u3069\u7406\u8ad6\u7684\u306a\u88cf\u4ed8\u3051\u3082\u5145\u5b9f\u3057\u3066\u3044\u308b\uff0e https://t.co/FTOt9hlMSI", "followers": "2,653", "datetime": "2017-11-29 12:39:03", "author": "@mosko_mule"}, "1044138568598220801": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "416", "datetime": "2018-09-24 08:16:48", "author": "@__nggih"}, "935874849402101760": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "1,762", "datetime": "2017-11-29 14:15:26", "author": "@jaialkdanel"}, "935871653292130304": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "10,129", "datetime": "2017-11-29 14:02:44", "author": "@sylvan5"}, "925425103835467776": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "16", "datetime": "2017-10-31 18:11:52", "author": "@p_kot1"}, "925551965810999296": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "41", "datetime": "2017-11-01 02:35:59", "author": "@aiton5"}, "936008129279492096": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "380", "datetime": "2017-11-29 23:05:02", "author": "@harujoh"}, "925405009004474368": {"content_summary": "personally most surprising line from https://t.co/v8FZ0U93BT https://t.co/swVb8z5wRw", "followers": "266", "datetime": "2017-10-31 16:52:01", "author": "@duncanswilson"}, "925297329560866816": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "575", "datetime": "2017-10-31 09:44:09", "author": "@hniemeye"}, "972114388516618240": {"content_summary": "@cab938 Would using SMOTE preclude also using mixup? ( https://t.co/XpeUW6r21c )", "followers": "60", "datetime": "2018-03-09 14:18:25", "author": "@TedInRealLife"}, "1044528507722829824": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "242", "datetime": "2018-09-25 10:06:17", "author": "@crieramolina"}, "1044209028929376256": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "255", "datetime": "2018-09-24 12:56:47", "author": "@christian_hudon"}, "925405868203454464": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "4,327", "datetime": "2017-10-31 16:55:26", "author": "@jekbradbury"}, "1136936150277529601": {"content_summary": "[7/10] \ud83d\udcc8 - pytorch-image-models - 683 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-07 10:01:16", "author": "@PapersTrending"}, "925409376327864320": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "28,418", "datetime": "2017-10-31 17:09:23", "author": "@ogrisel"}, "925366798727258112": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "2,709", "datetime": "2017-10-31 14:20:11", "author": "@adnothing"}, "1044249508933246977": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "89", "datetime": "2018-09-24 15:37:38", "author": "@singularpattern"}, "935850816153444352": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "256", "datetime": "2017-11-29 12:39:56", "author": "@guguchi_yama"}, "924804231910932481": {"content_summary": "\u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l2fvS85W4E", "followers": "18,217", "datetime": "2017-10-30 01:04:45", "author": "@hillbig"}, "925790331810979840": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "537", "datetime": "2017-11-01 18:23:10", "author": "@thehiphopswami"}, "925453622732886018": {"content_summary": "mixup: Beyond Empirical Risk Minimization https://t.co/2nY03iJtkW #ai #deeplearning #nlp via @abursuc", "followers": "2,266", "datetime": "2017-10-31 20:05:12", "author": "@future_of_AI"}, "925242923314176002": {"content_summary": "RT @hardmaru: Link to the paper: https://t.co/pXSwZERBjc By using this data augmentation technique, the achieve SOTA on ImageNet 2012, CIFA\u2026", "followers": "29", "datetime": "2017-10-31 06:07:57", "author": "@_ranjodh_singh"}, "925159328096460800": {"content_summary": "\u30c7\u30fc\u30bf\u62e1\u5f35\u306e\u65b0\u624b\u6cd5\u3002\u3042\u3068\u3067\u8aad\u3080\u3000mixup: BEYOND EMPIRICAL RISK MINIMIZATION https://t.co/kU71geBNHl", "followers": "203", "datetime": "2017-10-31 00:35:47", "author": "@wayama_ryousuke"}, "924440650833043456": {"content_summary": "mixup: Beyond Empirical Risk Minimization https://t.co/GcYWWYBXuF #machinelearning", "followers": "30,496", "datetime": "2017-10-29 01:00:01", "author": "@v_vashishta"}, "926062990348210176": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "1,146", "datetime": "2017-11-02 12:26:36", "author": "@sudeeppillai"}, "1044426570406850560": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "109", "datetime": "2018-09-25 03:21:13", "author": "@ram_cse"}, "1044057923532386304": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "1,585", "datetime": "2018-09-24 02:56:21", "author": "@aneesha"}, "931006293145538560": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "711", "datetime": "2017-11-16 03:49:32", "author": "@Babak_Taati"}, "1044418035006754816": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "331", "datetime": "2018-09-25 02:47:18", "author": "@philomate"}, "925427026785431552": {"content_summary": "Deep learning is so silly sometimes https://t.co/QGiEF5SQWr", "followers": "789", "datetime": "2017-10-31 18:19:31", "author": "@eprosenthal"}, "929691242589446144": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "561", "datetime": "2017-11-12 12:43:59", "author": "@yasunori1978"}, "1040305152496873474": {"content_summary": "mixup\u3063\u3066\u305d\u3046\u3044\u3046\u306e\u304b\u30fb\u30fb\u30fb\u30e9\u30d9\u30eb\u3082\u6df7\u305c\u308b\u306e\u304b\u3002 https://t.co/4Q3U0D1kYf", "followers": "11,061", "datetime": "2018-09-13 18:24:10", "author": "@tdualdir"}, "1205005812361416705": {"content_summary": "For regularization, we use weight decay and MixUp (https://t.co/CijMNHzJhk) across both labeled and unlabeled examples. This is super important to get good performance. 8/11", "followers": "12,556", "datetime": "2019-12-12 06:05:48", "author": "@colinraffel"}, "1135848973602828288": {"content_summary": "[2/10] \ud83d\udcc8 - pytorch-image-models - 588 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-04 10:01:13", "author": "@PapersTrending"}, "1044217130114306048": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "1,882", "datetime": "2018-09-24 13:28:59", "author": "@meltemiatay_AI"}, "1129326113107390465": {"content_summary": "[2/10] \ud83d\udcc8 - mixup: Beyond Empirical Risk Minimization - 181 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/PkYLcYUQLu", "followers": "221", "datetime": "2019-05-17 10:01:41", "author": "@PapersTrending"}, "924813607916859393": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "62", "datetime": "2017-10-30 01:42:00", "author": "@panpu333"}, "923708572176859136": {"content_summary": "mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v1 [cs.LG]) https://t.co/kIKyxqPHsl", "followers": "9,669", "datetime": "2017-10-27 00:30:59", "author": "@StatMLPapers"}, "925871081860354048": {"content_summary": "mixup: Beyond Empirical Risk Minimization https://t.co/2nY03iJtkW #ai #deeplearning #nlp via @ogrisel", "followers": "2,266", "datetime": "2017-11-01 23:44:02", "author": "@future_of_AI"}, "1044701779357814786": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "1", "datetime": "2018-09-25 21:34:48", "author": "@junweima2"}, "1044712433372319744": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "26", "datetime": "2018-09-25 22:17:08", "author": "@BogdanGliga"}, "923724516735451136": {"content_summary": "mixup: Beyond Empirical Risk Minimization - Hongyi Zhang https://t.co/bxoYB0kLgu", "followers": "857", "datetime": "2017-10-27 01:34:21", "author": "@deep_rl"}, "1137660909902602240": {"content_summary": "[7/10] \ud83d\udcc8 - pytorch-image-models - 699 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-09 10:01:12", "author": "@PapersTrending"}, "926265628570390529": {"content_summary": "mixup: Beyond Empirical Risk Minimization https://t.co/3MqjXqk0F9 https://t.co/U1Yr8O01cN", "followers": "263", "datetime": "2017-11-03 01:51:49", "author": "@beedotkiran"}, "991120181651947520": {"content_summary": "mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v2 [cs.LG] UPDATED) https://t.co/1dBehVLDK1 Large deep neural networks are power", "followers": "759", "datetime": "2018-05-01 01:00:40", "author": "@M157q_News_RSS"}, "1044616225479958529": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "65", "datetime": "2018-09-25 15:54:50", "author": "@savchynt"}, "924831304302059520": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "674", "datetime": "2017-10-30 02:52:20", "author": "@hidemotoNakada"}, "935855617524109312": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "2,050", "datetime": "2017-11-29 12:59:01", "author": "@shunk031"}, "1136211266635214848": {"content_summary": "[2/10] \ud83d\udcc8 - pytorch-image-models - 636 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-05 10:00:50", "author": "@PapersTrending"}, "1044128959103819777": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "456", "datetime": "2018-09-24 07:38:37", "author": "@PerthMLGroup"}, "952053137946193920": {"content_summary": "mixup: Beyond Empirical Risk Minimization https://t.co/4JHeQFrxhS", "followers": "602", "datetime": "2018-01-13 05:42:11", "author": "@Swall0wTech"}, "935871726319046657": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "1,323", "datetime": "2017-11-29 14:03:01", "author": "@ararabo"}, "924562187179274240": {"content_summary": "RT @mosko_mule: naive\u306a\u5b9f\u88c5\u3067\u3082\u6570%\u3042\u304c\u308b\uff0e\u65e9\u304f\u516c\u5f0f\u306e\u5b9f\u88c5\u304c\u516c\u958b\u3055\u308c\u3066\u6b32\u3057\u3044\uff0e\uff0e https://t.co/FTOt9hlMSI", "followers": "45", "datetime": "2017-10-29 09:02:57", "author": "@nullbytep"}, "936092329701556224": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "119", "datetime": "2017-11-30 04:39:37", "author": "@toto_toilet"}, "924843227366035456": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "68", "datetime": "2017-10-30 03:39:42", "author": "@shinryu_rk"}, "1044692963484360705": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "52", "datetime": "2018-09-25 20:59:46", "author": "@thao_nguyen26"}, "1044112049603825664": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "1,020", "datetime": "2018-09-24 06:31:25", "author": "@serrjoa"}, "1024774707113906176": {"content_summary": "mixup\u3063\u3066\u753b\u50cf\u5411\u3051\u304b\u3068\u601d\u3063\u3066\u305f\u3089\u3001\u8ad6\u6587\u3067\u666e\u901a\u306b\u6ce2\u5f62\u3068\u30b9\u30da\u30af\u30c8\u30b0\u30e9\u30e0\u306b\u5bfe\u3057\u3066\u3082\u52b9\u679c\u3067\u3066\u308b\u3057\u3001\u6642\u7cfb\u5217\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u7cfb\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u5c11\u306a\u3044\u30b1\u30fc\u30b9\u306e\u5bfe\u5fdc\u306b\u3082\u4f7f\u3048\u305d\u3046 / [1710.09412] mixup: Beyond Empirical Risk Minimization https://t.co/rC7oB4KJju", "followers": "198", "datetime": "2018-08-01 21:51:44", "author": "@ndruger"}, "927672699970715648": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "256", "datetime": "2017-11-06 23:03:01", "author": "@dizzy_my_future"}, "926152407817641985": {"content_summary": "@iangoodfellow seen this? https://t.co/16yjK3ufHz sounds like it helps against adv examples bc it makes NN oscillate less in neighborhood?", "followers": "733", "datetime": "2017-11-02 18:21:55", "author": "@unsorsodicorda"}, "1141285002723938304": {"content_summary": "[8/10] \ud83d\udcc8 - mixup: Beyond Empirical Risk Minimization - 50 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-19 10:02:03", "author": "@PapersTrending"}, "924811808128483328": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "1,762", "datetime": "2017-10-30 01:34:51", "author": "@jaialkdanel"}, "927495261420531712": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "433", "datetime": "2017-11-06 11:17:57", "author": "@hs_heddy"}, "924571280317210625": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "125", "datetime": "2017-10-29 09:39:05", "author": "@WahWah_wa"}, "935863724757225473": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "822", "datetime": "2017-11-29 13:31:14", "author": "@morioka"}, "924959276442853376": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "216", "datetime": "2017-10-30 11:20:51", "author": "@KSKSKSKS2"}, "926128536901079041": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "733", "datetime": "2017-11-02 16:47:04", "author": "@unsorsodicorda"}, "1224045786838814720": {"content_summary": "\u6065\u305a\u304b\u3057\u306a\u304c\u3089\u5f8c\u8005\u304c\u672c\u5bb6\u3063\u307d\u3044(https://t.co/zDlm0NBlV4)\u3002\u500b\u4eba\u7684\u306b\u306f\u524d\u8005\u306e\u307b\u3046\u304c\u6c4e\u7528\u6027\u9ad8\u3044\u6c17\u304c\u3059\u308b\u3093\u3060\u3051\u3069\u3001\u3088\u304f\u308f\u304b\u3089\u306a\u304f\u306a\u3063\u3066\u304d\u305f\u306a\u30fb\u30fb\u30fb", "followers": "865", "datetime": "2020-02-02 19:03:51", "author": "@mlaass1"}, "925759502506516481": {"content_summary": "mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https://t.co/NFkJnPKSiZ https://t.co/X5vv43Ebgl", "followers": "5,797", "datetime": "2017-11-01 16:20:39", "author": "@AndrewLBeam"}, "924813732533780480": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "254", "datetime": "2017-10-30 01:42:30", "author": "@mokemokechicken"}, "924957624767995904": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "1,708", "datetime": "2017-10-30 11:14:17", "author": "@superbradyon"}, "924263944184721409": {"content_summary": "[1710.09412] mixup: Beyond Empirical Risk Minimization https://t.co/xA3gCwGkxa", "followers": "275", "datetime": "2017-10-28 13:17:50", "author": "@22114w"}, "925718822660853761": {"content_summary": "mixup: Beyond Empirical Risk Minimization https://t.co/2nY03iJtkW #ai #deeplearning #nlp via @abursuc", "followers": "2,266", "datetime": "2017-11-01 13:39:01", "author": "@future_of_AI"}, "926793898776653825": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "1,248", "datetime": "2017-11-04 12:50:59", "author": "@daisuzu"}, "1129688537304109056": {"content_summary": "[5/10] \ud83d\udcc8 - mixup: Beyond Empirical Risk Minimization - 186 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/PkYLcYUQLu", "followers": "221", "datetime": "2019-05-18 10:01:50", "author": "@PapersTrending"}, "1139164377058369539": {"content_summary": "RT @jigarkdoshi: @karpathy This whole line of work where they learn a linear interpolation of the representation. Original Idea of Mixup (h\u2026", "followers": "3", "datetime": "2019-06-13 13:35:26", "author": "@flyingchickenAI"}, "1128963807034912769": {"content_summary": "[1/10] \ud83d\udcc8 - mixup: Beyond Empirical Risk Minimization - 158 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/PkYLcYUQLu", "followers": "221", "datetime": "2019-05-16 10:02:01", "author": "@PapersTrending"}, "1044303022837256192": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "1,164", "datetime": "2018-09-24 19:10:17", "author": "@Even_Oldridge"}, "924808845393772546": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "119", "datetime": "2017-10-30 01:23:05", "author": "@toto_toilet"}, "935966698837917696": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "3,642", "datetime": "2017-11-29 20:20:25", "author": "@losnuevetoros"}, "925801688707026952": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "120", "datetime": "2017-11-01 19:08:17", "author": "@iMelekhov"}, "1044732907930226694": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "251", "datetime": "2018-09-25 23:38:30", "author": "@mcanan"}, "1044108524689616897": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "98", "datetime": "2018-09-24 06:17:25", "author": "@treasured_write"}, "923791740884185089": {"content_summary": "\"mixup: Beyond Empirical Risk Minimization\", Hongyi Zhang, Moustapha Cisse, Yann N\uff0e Dauphin, David Lopez-Paz https://t.co/ERq3VekTzw", "followers": "767", "datetime": "2017-10-27 06:01:28", "author": "@arxivml"}, "936186870207168512": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "6,491", "datetime": "2017-11-30 10:55:18", "author": "@jingbay"}, "1044491089594527745": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "16", "datetime": "2018-09-25 07:37:36", "author": "@p_kot1"}, "926173421872009216": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "302", "datetime": "2017-11-02 19:45:25", "author": "@cepera_ang"}, "924886130499117057": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "561", "datetime": "2017-10-30 06:30:11", "author": "@ocaokgbu"}, "1044409411991670784": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "99,889", "datetime": "2018-09-25 02:13:02", "author": "@jeremyphoward"}, "925797195357360128": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "859", "datetime": "2017-11-01 18:50:26", "author": "@cosminnegruseri"}, "1044732476763983872": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "67,413", "datetime": "2018-09-25 23:36:47", "author": "@math_rachel"}, "924838889830805504": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "114", "datetime": "2017-10-30 03:22:28", "author": "@vanTateno"}, "924559830882193410": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "4,615", "datetime": "2017-10-29 08:53:35", "author": "@HITStales"}, "927330730488639489": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "14", "datetime": "2017-11-06 00:24:09", "author": "@TsuguoMogami"}, "925145517545287680": {"content_summary": "Link to the paper: https://t.co/pXSwZERBjc By using this data augmentation technique, the achieve SOTA on ImageNet 2012, CIFAR-10/100.", "followers": "81,077", "datetime": "2017-10-30 23:40:54", "author": "@hardmaru"}, "924815620666933248": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "145", "datetime": "2017-10-30 01:50:00", "author": "@mi141"}, "935863638849548288": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "822", "datetime": "2017-11-29 13:30:53", "author": "@morioka"}, "1110435353456893952": {"content_summary": "A dead simple trick that greatly improves generalization. https://t.co/AGsrBTGybk", "followers": "83", "datetime": "2019-03-26 06:56:33", "author": "@dome_cs"}, "1032440231730053120": {"content_summary": "\"God should not allow this to work!\"", "followers": "1", "datetime": "2018-08-23 01:31:47", "author": "@junweima2"}, "924493389168492544": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "256", "datetime": "2017-10-29 04:29:34", "author": "@guguchi_yama"}, "1044737717194485760": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "720", "datetime": "2018-09-25 23:57:36", "author": "@ErikaVaris"}, "935910104045461504": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "1,558", "datetime": "2017-11-29 16:35:31", "author": "@aras_ko"}, "923711612451635200": {"content_summary": "mixup: Beyond Empirical Risk Minimization. https://t.co/BZGSO0RwVW", "followers": "5,454", "datetime": "2017-10-27 00:43:04", "author": "@StatsPapers"}, "925478058953445380": {"content_summary": "good recent DL reads: - Growing GANs: https://t.co/6aa4MDiL73 - MixUp : https://t.co/AIoOl4870o - Color Transfer: https://t.co/pgzsb7n9ln", "followers": "506", "datetime": "2017-10-31 21:42:18", "author": "@erogol"}, "1044488658022207493": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "34", "datetime": "2018-09-25 07:27:56", "author": "@therealaseifert"}, "929512757891698689": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "83", "datetime": "2017-11-12 00:54:45", "author": "@fqz7c3"}, "925392339329093633": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "615", "datetime": "2017-10-31 16:01:41", "author": "@KouroshMeshgi"}, "927494753276518400": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "1,188", "datetime": "2017-11-06 11:15:55", "author": "@ken_demu"}, "925335783795343360": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "54", "datetime": "2017-10-31 12:16:57", "author": "@SutanshuRaj"}, "1044891767881883648": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "58", "datetime": "2018-09-26 10:09:45", "author": "@gshashank84"}, "1244927915671904257": {"content_summary": "[6/10] \ud83d\udcc8 - pytorch-image-models - 3,002 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2020-03-31 10:01:59", "author": "@PapersTrending"}, "1008350045748383744": {"content_summary": "RT @SpaceAnubis: love the simplicity of this paper (data augmentation by taking linear combination of training samples): https://t.co/81rLy\u2026", "followers": "2,675", "datetime": "2018-06-17 14:05:59", "author": "@ayirpelle"}, "978444871290646528": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "1,145", "datetime": "2018-03-27 01:33:30", "author": "@niku9Tenhou"}, "1044764957181714433": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "743", "datetime": "2018-09-26 01:45:51", "author": "@richardtomsett"}, "935863774153646091": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "3,710", "datetime": "2017-11-29 13:31:25", "author": "@hiroharu_kato"}, "991116051793563648": {"content_summary": "mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v2 [cs.LG] UPDATED) https://t.co/ZcmtQgJi30", "followers": "9,669", "datetime": "2018-05-01 00:44:15", "author": "@StatMLPapers"}, "935890150122389504": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "997", "datetime": "2017-11-29 15:16:14", "author": "@AkiraIsaka88"}, "925151357538988032": {"content_summary": "RT @hardmaru: Link to the paper: https://t.co/pXSwZERBjc By using this data augmentation technique, the achieve SOTA on ImageNet 2012, CIFA\u2026", "followers": "2,387", "datetime": "2017-10-31 00:04:06", "author": "@kamalikac"}, "924812862756761600": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "232", "datetime": "2017-10-30 01:39:03", "author": "@bamboo4031"}, "925006757524488193": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "471", "datetime": "2017-10-30 14:29:31", "author": "@daytb_twy"}, "1090649448886132736": {"content_summary": "mixup\u3068\u3044\u3046data augmentation\uff0c\u753b\u50cf\u3060\u3051\u3058\u3083\u306a\u304f\u97f3\u58f0\u3084\u30c6\u30fc\u30d6\u30eb\u30c7\u30fc\u30bf\u3067\u3082\u4f7f\u3048\u308b\u3089\u3057\u3044\uff0c\u975e\u5e38\u306b\u4f7f\u3063\u3066\u307f\u305f\u3044 \u4ee5\u4e0b\u306e\u5f0f\u30672\u7a2e\u985e\u306e\u30e9\u30d9\u30eb\u3092\u6df7\u305c\u308b\u3060\u3051\u3068\u3044\u3046\u7c21\u5358\u5b9f\u88c5 x_hat = \u03bbx_i + (1-\u03bb)x_j y_hat = \u03bby_i + (1-\u03bb)y_j \u8ad6\u6587\u306f\u3053\u308c https://t.co/ZkwyBkQomd", "followers": "394", "datetime": "2019-01-30 16:34:26", "author": "@whey_yooguruto"}, "927941332928102400": {"content_summary": "\u4eca\u65e5\u306fmixup\u8ad6\u6587\u5468\u308a\u3092\u8aad\u3093\u3067\u3044\u305f\u306e\u3067ACL\u306f\u304a\u4f11\u307f [mixup: Data-Dependent Data Augmentation] https://t.co/FoF9q6B0aV", "followers": "3,258", "datetime": "2017-11-07 16:50:28", "author": "@ymym3412"}, "1044216690798727169": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "261", "datetime": "2018-09-24 13:27:14", "author": "@GuptaRajat033"}, "925263651505815554": {"content_summary": "RT @Umut1Eser: Trains a neural network on convex combinations of pairs of examples and their labels. An intriguing idea: https://t.co/LSZNG\u2026", "followers": "7,122", "datetime": "2017-10-31 07:30:19", "author": "@anshulkundaje"}, "923873441463365632": {"content_summary": "New #deeplearning paper https://t.co/BcI2xedV9i https://t.co/yigSnUD5yo", "followers": "98", "datetime": "2017-10-27 11:26:07", "author": "@DeepLearningNow"}, "925671586899857408": {"content_summary": "This morning's research catchup : reading the mixup paper which has some buzz this morning : https://t.co/3YFsyUaTZN", "followers": "1,063", "datetime": "2017-11-01 10:31:19", "author": "@rosstaylor90"}, "949606512372080640": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "81", "datetime": "2018-01-06 11:40:10", "author": "@llx_shunny_xll"}, "1140922524710178816": {"content_summary": "[7/10] \ud83d\udcc8 - mixup: Beyond Empirical Risk Minimization - 39 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-18 10:01:41", "author": "@PapersTrending"}, "1115255594405879808": {"content_summary": "mixup \u306e\u6b63\u5f53\u6027\u304c\u7406\u89e3\u3067\u304d\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u539f\u8ad6\u6587\u3092\u8aad\u307f\u307e\u3059\u30fb\u30fb\u30fb\u3002 \u2193 https://t.co/KCQqjzHhHI", "followers": "3,186", "datetime": "2019-04-08 14:10:28", "author": "@Maxwell_110"}, "923709474841419776": {"content_summary": "#arXiv #stat_ML \"mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v1 [cs.LG])\" https://t.co/TfjVacl8r8", "followers": "626", "datetime": "2017-10-27 00:34:35", "author": "@helioRocha_"}, "935880410348658689": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "21,151", "datetime": "2017-11-29 14:37:32", "author": "@tokoroten"}, "935855865457811456": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "376", "datetime": "2017-11-29 13:00:00", "author": "@katsunori_ohnis"}, "1044192778333433856": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "75", "datetime": "2018-09-24 11:52:13", "author": "@MozejkoMarcin"}, "1044220194443448320": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "126", "datetime": "2018-09-24 13:41:09", "author": "@alexginsca"}, "1039148818904899584": {"content_summary": "Found it https://t.co/A4DCUDndbo \ud83d\udc4d", "followers": "627", "datetime": "2018-09-10 13:49:19", "author": "@Shuyin_ben"}, "924559212100837376": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "45", "datetime": "2017-10-29 08:51:08", "author": "@nullbytep"}, "1044194069189210112": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "289", "datetime": "2018-09-24 11:57:20", "author": "@itsrainingdata"}, "925871200189939712": {"content_summary": "RT @future_of_AI: mixup: Beyond Empirical Risk Minimization https://t.co/2nY03iJtkW #ai #deeplearning #nlp via @ogrisel", "followers": "7,977", "datetime": "2017-11-01 23:44:30", "author": "@Calcaware"}, "1100859449776828416": {"content_summary": "@GuggerSylvain @davidwbressler @jeremyphoward mixup looks interesting too from that same bag of tricks paper and referenced across to mixup: BEYOND EMPIRICAL RISK MINIMIZATION https://t.co/yL2QQiyuv7. Augmentation as regularization.", "followers": "991", "datetime": "2019-02-27 20:45:20", "author": "@LunchWithALens"}, "935880814126030848": {"content_summary": "RT @mosko_mule: Between-class Learning for Image Classification https://t.co/0anAUYprmG \u539f\u7530\u30fb\u725b\u4e45\u7814\u304b\u3089\uff0emixup\u3068\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u306f\u540c\u3058\u3060\u304c\u3053\u3061\u3089\u306e\u65b9\u304c\u5148\u306bILSVRC\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u305f\u2026", "followers": "1,667", "datetime": "2017-11-29 14:39:08", "author": "@Scaled_Wurm"}, "925274799710056449": {"content_summary": "RT @arxiv_org: mixup: Beyond Empirical Risk Minimization. https://t.co/Vqqx8rllJI https://t.co/SDwbXbvSyE", "followers": "255", "datetime": "2017-10-31 08:14:37", "author": "@josipK"}, "1091399370379948033": {"content_summary": "\u201cLarge deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues.\u201d https://t.co/pHXMVvuELP", "followers": "3,446", "datetime": "2019-02-01 18:14:21", "author": "@reiver"}, "923763865208291328": {"content_summary": "mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u9811\u5065\u306b\uff0e\u5358\u7d14\u3060\u304c\u5f37\u529b\uff0e", "followers": "2,653", "datetime": "2017-10-27 04:10:42", "author": "@mosko_mule"}, "1044432998702206978": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "406", "datetime": "2018-09-25 03:46:46", "author": "@mattpetersen_ai"}, "932614832511242241": {"content_summary": "@iamstarnord @jegpeek @chrislintott @Tom_Donaldson @astrocrawford @DainaBouquin I tried some more regularization (including mixup, https://t.co/F2o4j7iDzU, which I think could be done in a physically meaningful way with this data), but could never get vali", "followers": "611", "datetime": "2017-11-20 14:21:17", "author": "@thouis"}, "936008081770553344": {"content_summary": "RT @mosko_mule: mixup: Beyond ERM https://t.co/7cmcoGAFxs \u7d4c\u9a13\u8aa4\u5dee\u6700\u5c0f\u5316\u3067\u306f\u306a\u304f\uff0c\u30a4\u30f3\u30d7\u30c3\u30c8\u03bbx+(1-\u03bb)X\uff0c\u30bf\u30fc\u30b2\u30c3\u30c8\u03bby+(1-\u03bb)Y\u3068\u3059\u308bmixup\u306b\u3088\u3063\u3066CIFAR100\u30a8\u30e9\u30fc\u3067-4%\u7a0b\u5ea6\uff0c\u6575\u5bfe\u7684\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u2026", "followers": "380", "datetime": "2017-11-29 23:04:51", "author": "@harujoh"}, "924836374208851968": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "822", "datetime": "2017-10-30 03:12:28", "author": "@morioka"}, "1008255420039757824": {"content_summary": "love the simplicity of this paper (data augmentation by taking linear combination of training samples): https://t.co/81rLyuAqjW", "followers": "913", "datetime": "2018-06-17 07:49:59", "author": "@SpaceAnubis"}, "1044245996304257024": {"content_summary": "Nice data augmentation approach, looks like it can applied to other problems too", "followers": "45", "datetime": "2018-09-24 15:23:41", "author": "@NumericalAlex"}, "929656567619731456": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "903", "datetime": "2017-11-12 10:26:12", "author": "@wk77"}, "925156096448196608": {"content_summary": "RT @hillbig: \u8a13\u7df4\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3057\u305f\u5834\u5408\u3001\u30e1\u30e2\u5316\u304c\u8d77\u304d\u305f\u308a\u3001\u6575\u5bfe\u7684\u306a\u30b5\u30f3\u30d7\u30eb\u306b\u5f31\u3044\u554f\u984c\u304c\u3042\u308b\u3002\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5165\u51fa\u529b\u4e21\u65b9\u306e\u51f8\u7d50\u5408\uff08\u5b9f\u9a13\u306e\u591a\u304f\u306f\u7dda\u5f62\u7d50\u5408\uff09\u306e\u8a13\u7df4\u4e8b\u4f8b\u3092\u4f5c\u308a\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3053\u308c\u3089\u3092\u9632\u3050\u3002\u753b\u50cf\u3060\u3051\u3067\u306a\u304f\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u6b63\u5247\u5316\u3068\u3057\u3066\u6709\u52b9 https://t.co/l\u2026", "followers": "166", "datetime": "2017-10-31 00:22:56", "author": "@kawamuramasahar"}, "925642419194028032": {"content_summary": "RT @abursuc: The mixup paper, doing data augmentation by interpolating pairs training samples and labels is baffling https://t.co/qX7N8hH0xJ", "followers": "1,101", "datetime": "2017-11-01 08:35:24", "author": "@permutans"}, "1044734797862424576": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "216", "datetime": "2018-09-25 23:46:00", "author": "@KSKSKSKS2"}, "1044059715120979968": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "33", "datetime": "2018-09-24 03:03:28", "author": "@PGajjewar"}, "925394394328518657": {"content_summary": "A new data-agnostic data augmentation routine reports improved generalization in the of state-of-the-art networks https://t.co/GlBhwkIhlA https://t.co/XU8Tb5naJk", "followers": "163", "datetime": "2017-10-31 16:09:51", "author": "@iugax"}, "1044288251056533507": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "76", "datetime": "2018-09-24 18:11:35", "author": "@JhairGallardo"}, "925283456430592000": {"content_summary": "RT @hardmaru: Link to the paper: https://t.co/pXSwZERBjc By using this data augmentation technique, the achieve SOTA on ImageNet 2012, CIFA\u2026", "followers": "189", "datetime": "2017-10-31 08:49:01", "author": "@AndreuSancho"}, "924007207703740416": {"content_summary": "mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v1 [cs.LG]) https://t.co/1dBehVLDK1 Large deep neural networks are powerful, but", "followers": "759", "datetime": "2017-10-27 20:17:40", "author": "@M157q_News_RSS"}, "1135486577608708096": {"content_summary": "[2/10] \ud83d\udcc8 - pytorch-image-models - 536 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-03 10:01:11", "author": "@PapersTrending"}, "1140560085971849217": {"content_summary": "[5/10] \ud83d\udcc8 - mixup: Beyond Empirical Risk Minimization - 22 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-17 10:01:29", "author": "@PapersTrending"}, "1044087326572523520": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "2,675", "datetime": "2018-09-24 04:53:11", "author": "@ayirpelle"}, "1136573758167293952": {"content_summary": "[7/10] \ud83d\udcc8 - pytorch-image-models - 654 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-06 10:01:15", "author": "@PapersTrending"}, "1044443635989860352": {"content_summary": "RT @GuggerSylvain: A little post about mixup (https://t.co/YyOoM9ACAS) and how it allowed us to better our training time on CIFAR-10 in fas\u2026", "followers": "193", "datetime": "2018-09-25 04:29:02", "author": "@alexhock"}, "1135124197490012162": {"content_summary": "[5/10] \ud83d\udcc8 - pytorch-image-models - 481 \u2b50 - \ud83d\udcc4 https://t.co/CoJG66fnYS - \ud83d\udd17 https://t.co/ZjVwsQ0Ylj", "followers": "221", "datetime": "2019-06-02 10:01:12", "author": "@PapersTrending"}, "925200583937740800": {"content_summary": "Trains a neural network on convex combinations of pairs of examples and their labels. An intriguing idea: https://t.co/LSZNGiN65M", "followers": "974", "datetime": "2017-10-31 03:19:43", "author": "@Umut1Eser"}, "1090162177707659264": {"content_summary": "RT @AndrewLBeam: mixup is the most counter-intuitive data augmentation method I've ever seen. Hard to understand how/why this works: https:\u2026", "followers": "1,416", "datetime": "2019-01-29 08:18:12", "author": "@RexDouglass"}, "1145254961522982912": {"content_summary": "https://t.co/Ql87Nnxj4u mixup: Beyond Empirical Risk Minimization \u306b GAN \u306e\u5b66\u7fd2\u3092 stabilize \u3059\u308b\u3063\u3066\u66f8\u304b\u308c\u3068\u3063\u305f\u30fc\u3002\u3061\u3083\u3093\u3068\u8aad\u307e\u306d\u3070\u3002", "followers": "58", "datetime": "2019-06-30 08:57:15", "author": "@Ab_ten"}}}