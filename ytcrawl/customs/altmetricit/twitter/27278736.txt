{"citation_id": "27278736", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 15:16:28", "twitter": {"965399092347244545": {"followers": "626", "datetime": "2018-02-19 01:34:14", "author": "@helioRocha_", "content_summary": "\"Mixed Precision Training. (arXiv:1710.03740v3 [https://t.co/jqZ5dwubSV] UPDATED)\" #arXiv https://t.co/BuscbA67fI"}, "919443265555922945": {"followers": "12,547", "datetime": "2017-10-15 06:02:11", "author": "@ml_review", "content_summary": "\u201cMixed Precision Training\u201d by @BaiduResearch with @NvidiaAI (2.x reduced memory usage for #DeepLearning) https://t.co/GsKn2kdE3n #ML https://t.co/EorTdP3161"}, "940637124067495941": {"followers": "2,266", "datetime": "2017-12-12 17:39:01", "author": "@future_of_AI", "content_summary": "Mixed Precision Training https://t.co/iLwZ1pIl89 #ai #deeplearning #nlp via @Smerity"}, "918079736328458241": {"followers": "2,266", "datetime": "2017-10-11 11:44:00", "author": "@future_of_AI", "content_summary": "Mixed Precision Training https://t.co/vxW8uoQUWE #ai #deeplearning #nlp via @Miles_Brundage"}, "917926555602620416": {"followers": "862", "datetime": "2017-10-11 01:35:19", "author": "@deep_rl", "content_summary": "Mixed Precision Training - Paulius Micikevicius https://t.co/4pTITt6anm"}, "917911938411192320": {"followers": "61", "datetime": "2017-10-11 00:37:14", "author": "@SoEngineering", "content_summary": "#NewPaper: #arXiv cs.AI https://t.co/TGPtFPtGRY Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI])"}, "920227698789797888": {"followers": "1,810", "datetime": "2017-10-17 09:59:15", "author": "@cfregly", "content_summary": "This Baidu, Inc. paper has it all!  Half-Precision Matrix Math, GPUs, TensorCores, Batch Normalization, Neural Nets https://t.co/kTuQtJ6DgB"}, "965405682450153472": {"followers": "759", "datetime": "2018-02-19 02:00:25", "author": "@M157q_News_RSS", "content_summary": "Mixed Precision Training. (arXiv:1710.03740v3 [https://t.co/C8CkWQFMOP] UPDATED) https://t.co/c63JN8dZ93 Deep neural networks have enabled progress in a wide"}, "918447651208691712": {"followers": "798", "datetime": "2017-10-12 12:05:58", "author": "@schaumberg_a", "content_summary": "#Nvidia #Volta #GPU architecture ~2x memory savings claimed, tho no tables w/ exact numbers https://t.co/ieQpwbEhpn by @sharan0909 Exciting https://t.co/l67LyHg1mW"}, "918322611565707264": {"followers": "2,582", "datetime": "2017-10-12 03:49:06", "author": "@ogawa_tter", "content_summary": "=> Baidu Sheds Precision Without Paying Deep Learning Accuracy Cost, Oct 11 2017 https://t.co/4PhVkOP9bm Oct 10 2017 https://t.co/CbbpCvcU4A https://t.co/yhVsRcTO50"}, "1046094525981237248": {"followers": "20", "datetime": "2018-09-29 17:49:05", "author": "@Master_Yoda_1", "content_summary": "@Tim_Dettmers Mixed precision needs tuning scaling factor S & skips/overflow N. With wrong values model diverges. Only with correct values you reach FP32 accuracy. Does 2080ti 1.65x speedup vanish if you may need to tune these? Paper https://t.co/XABQn"}, "918176711602331648": {"followers": "492", "datetime": "2017-10-11 18:09:21", "author": "@PythonLoop", "content_summary": "Mixed Precision Training for Deep Learning Models https://t.co/NDeXQyJUX4"}, "965399359545278464": {"followers": "660", "datetime": "2018-02-19 01:35:18", "author": "@dbworld_", "content_summary": "https://t.co/TWJ0JEn3lS Mixed Precision Training. (arXiv:1710.03740v3 [https://t.co/w36LjRyh6A] UPDATED) #ai"}, "919979410312138753": {"followers": "5,082", "datetime": "2017-10-16 17:32:38", "author": "@aneel", "content_summary": "https://t.co/h8njYIkPw9 https://t.co/HM3X7VhZrr"}, "918241553096806406": {"followers": "2,266", "datetime": "2017-10-11 22:27:01", "author": "@future_of_AI", "content_summary": "Mixed Precision Training https://t.co/gJWbXHAxwM #ai #deeplearning #nlp via @Miles_Brundage"}, "1001610688219959296": {"followers": "833", "datetime": "2018-05-29 23:46:11", "author": "@fabinger", "content_summary": "@jwangARK You probably know these papers, but I found them useful. Mixed precision training: https://t.co/QhipaZVKXN Nvidia Volta: https://t.co/PVECb5F2YY Google TPU: https://t.co/K0Fmbnro1K"}, "917911480586076165": {"followers": "626", "datetime": "2017-10-11 00:35:25", "author": "@helioRocha_", "content_summary": "#arXiv #cs_AI \"Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI])\" https://t.co/BuscbA67fI"}, "920398199063502855": {"followers": "3,321", "datetime": "2017-10-17 21:16:45", "author": "@juantomas", "content_summary": "RT @cfregly: This Baidu, Inc. paper has it all!  Half-Precision Matrix Math, GPUs, TensorCores, Batch Normalization, Neural Nets https://t.\u2026"}, "918614007346495488": {"followers": "2,582", "datetime": "2017-10-12 23:07:01", "author": "@ogawa_tter", "content_summary": "=> Mixed-Precision (FP32 & FP16) Training of Deep Neural Networks Oct 11 2017 https://t.co/b2FO2CV8of Baidu & NVIDIA https://t.co/7kzRs3wVDv"}, "917912257778069504": {"followers": "660", "datetime": "2017-10-11 00:38:30", "author": "@dbworld_", "content_summary": "https://t.co/TWJ0JEn3lS Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI]) #ai"}, "918193089835098112": {"followers": "3,878", "datetime": "2017-10-11 19:14:26", "author": "@DL_arXiv", "content_summary": "Mixed Precision Training. Micikevicius et al. https://t.co/AL2bQQJNa8"}, "992163453115580421": {"followers": "50", "datetime": "2018-05-03 22:06:15", "author": "@kuchaev", "content_summary": "Our #ICLR2018 Poster and Paper https://t.co/WMHZgbhFSj on mixed precision training with Volta GPUs https://t.co/2kdO9jaT0R"}, "917970988821876736": {"followers": "770", "datetime": "2017-10-11 04:31:53", "author": "@arxivml", "content_summary": "\"Mixed Precision Training\", Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Ga\u2026 https://t.co/kfrLtIQrU8"}, "917910200258060288": {"followers": "9,685", "datetime": "2017-10-11 00:30:20", "author": "@StatMLPapers", "content_summary": "Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI]) https://t.co/Ze9SP3vB7A"}, "985891299164172288": {"followers": "18", "datetime": "2018-04-16 14:42:57", "author": "@ShinobuKinjo1", "content_summary": "## With Pascal: MIXED PRECISION TRAINING: https://t.co/ioWvspong1"}, "918914237765902336": {"followers": "30,496", "datetime": "2017-10-13 19:00:01", "author": "@v_vashishta", "content_summary": "Mixed Precision Training https://t.co/Sj8rZfpF8V #MachineLearning #DeepLearning"}, "922862204923383809": {"followers": "34", "datetime": "2017-10-24 16:27:50", "author": "@dnl0x00", "content_summary": "Summary of paper \"mixed precision training\" https://t.co/fz3RijGjme"}, "918175947286859776": {"followers": "136", "datetime": "2017-10-11 18:06:19", "author": "@FtrRetailNews", "content_summary": "Mixed Precision Training for Deep Learning Models https://t.co/OrHlbIWsKa"}, "918164692111187969": {"followers": "861", "datetime": "2017-10-11 17:21:35", "author": "@ryf_feed", "content_summary": "Mixed Precision Training for Deep Learning Models https://t.co/PzW919cFjX"}, "917944410469367811": {"followers": "5,454", "datetime": "2017-10-11 02:46:16", "author": "@StatsPapers", "content_summary": "Mixed Precision Training. https://t.co/EbcKuIpPJs"}, "919716931049000960": {"followers": "862", "datetime": "2017-10-16 00:09:38", "author": "@deep_rl", "content_summary": "Mixed Precision Training - Paulius Micikevicius https://t.co/iRUmxVR5C0"}, "1000238582072684545": {"followers": "162", "datetime": "2018-05-26 04:53:56", "author": "@argv_sat184", "content_summary": "Adaptive Quantization for Deep Neural Network https://t.co/HQm0HaV7sR Mixed Precision Training https://t.co/DyAV59ieZw Mixed Precision Training of Convolutional Neural Networks using Integer Operations https://t.co/Da2ca1Ovyq #iclr18yomikai"}, "921371428284522496": {"followers": "59", "datetime": "2017-10-20 13:44:01", "author": "@luminothai", "content_summary": "RT @cfregly: This Baidu, Inc. paper has it all!  Half-Precision Matrix Math, GPUs, TensorCores, Batch Normalization, Neural Nets https://t.\u2026"}, "918183933094842368": {"followers": "2,266", "datetime": "2017-10-11 18:38:03", "author": "@future_of_AI", "content_summary": "Mixed Precision Training https://t.co/NrUaVqBaLP #ai #machinelearning #artificialintelligence via @Miles_Brundage"}, "918915477602123777": {"followers": "40,603", "datetime": "2017-10-13 19:04:57", "author": "@meisshaily", "content_summary": "@v_vashishta: Mixed Precision Training https://t.co/f6n3AT2nZr #MachineLearning #DeepLearning"}, "917915086479601664": {"followers": "25,578", "datetime": "2017-10-11 00:49:45", "author": "@Miles_Brundage", "content_summary": "\"Mixed Precision Training,\" Narang and Micikevicius et al., Baidu/NVIDIA: https://t.co/hqe6xbFDuC"}, "918618659307905024": {"followers": "21", "datetime": "2017-10-12 23:25:30", "author": "@ryo_takata", "content_summary": "RT @ogawa_tter: => Mixed-Precision (FP32 & FP16) Training of Deep Neural Networks Oct 11 2017 https://t.co/b2FO2CV8of Baidu & NVIDIA https:\u2026"}, "917911084815798272": {"followers": "626", "datetime": "2017-10-11 00:33:51", "author": "@helioRocha_", "content_summary": "#arXiv #stat_ML \"Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI])\" https://t.co/BuscbA67fI"}, "919444878630834176": {"followers": "78,940", "datetime": "2017-10-15 06:08:36", "author": "@machinelearnbot", "content_summary": "RT @ml_review: \u201cMixed Precision Training\u201d by @BaiduResearch with @NvidiaAI (2.x reduced memory usage for #DeepLearning) https://t.co/GsKn\u2026"}, "918021866211696641": {"followers": "2,266", "datetime": "2017-10-11 07:54:03", "author": "@future_of_AI", "content_summary": "Mixed Precision Training https://t.co/TiMxHx4B8V #ai #machinelearning #artificialintelligence via @Miles_Brundage"}, "918295521956990976": {"followers": "4,825", "datetime": "2017-10-12 02:01:28", "author": "@prototechno", "content_summary": "\u201c[1710.03740] Mixed Precision Training\u201d https://t.co/0Lh37JtdjN"}, "918150943279079426": {"followers": "1,303", "datetime": "2017-10-11 16:26:57", "author": "@arxiv_flying", "content_summary": "#ICLR2018 Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI]) https://t.co/AeLLu1pFUb"}, "986661641457577984": {"followers": "955", "datetime": "2018-04-18 17:44:01", "author": "@Kingwulf", "content_summary": "Mixed precision training https://t.co/4JOLzpF8mX"}, "939647050844553216": {"followers": "31,357", "datetime": "2017-12-10 00:04:49", "author": "@Smerity", "content_summary": "@semiDL @jackclarkSF @JeffDean @scottgray76 There certainly are tactics but they still aren't trivial and they're a good deal of complexity and potential complication. Mixed Precision Training was a beautiful concise encapsulation of the tactics: https://t"}, "965398918598098944": {"followers": "61", "datetime": "2018-02-19 01:33:33", "author": "@SoEngineering", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/TGPtFPtGRY Mixed Precision Training. (arXiv:1710.03740v3 [https://t.co/8kHVi9UcuF] UPDATED)"}, "965456879072706560": {"followers": "98", "datetime": "2018-02-19 05:23:51", "author": "@DeepLearningNow", "content_summary": "35/49 New #deeplearning paper https://t.co/XbTaxRFaVx https://t.co/A8Q4brHZ0o"}, "1144023625969745922": {"followers": "184", "datetime": "2019-06-26 23:24:22", "author": "@SiavashSakhavi", "content_summary": "RT @remykarem: Automatic Mixed Precision training in @TensorFlow using @NvidiaAI\u2019s Tensor Cores. Simple idea \ud83d\udc4d\ud83c\udffc ^ float32 * float16 \u2207 grad\u2026"}, "1125355066464989185": {"followers": "375", "datetime": "2019-05-06 11:02:10", "author": "@remykarem", "content_summary": "Automatic Mixed Precision training in @TensorFlow using @NvidiaAI\u2019s Tensor Cores. Simple idea \ud83d\udc4d\ud83c\udffc ^ float32 * float16 \u2207 gradient w weights Forward: w* = float16(w^) y* = model(w*, x*) Backward: w^ = w^ - \u03b1*\u2022\u2207* Paper: https://t.co/y10ijRkyGt Article: htt"}, "918072555554181120": {"followers": "10", "datetime": "2017-10-11 11:15:28", "author": "@allegro_smart", "content_summary": "Mixed Precision Training \u6df7\u5408\u7cbe\u5bc6\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0https://https://t.co/kpIEO2iJOr \u8981\u7d04\uff1a\u6df1\u3044\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u3001\u69d8\u3005\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u9032\u6b69\u3092\u53ef\u80fd\u306b\u3057\u307e\u3057\u305f\u3002\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b5\u30a4\u30ba\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u3001\u901a\u5e38\u3001\u7cbe\u5ea6\u2026"}, "918026628072132610": {"followers": "98", "datetime": "2017-10-11 08:12:58", "author": "@DeepLearningNow", "content_summary": "New #deeplearning paper https://t.co/XbTaxRFaVx https://t.co/dgAEPtL2ue"}, "917912452851032064": {"followers": "1,388", "datetime": "2017-10-11 00:39:17", "author": "@gastronomy", "content_summary": "[arXiv] Mixed Precision Training. (arXiv:1710.03740v1 [cs.AI]) --> Deep neural networks have enabled progress in \u2026 https://t.co/dgsYY47AUE"}}}