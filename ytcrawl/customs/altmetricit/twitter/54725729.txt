{"twitter": {"1142164344236584960": {"author": "@AliceICecile", "datetime": "2019-06-21 20:16:14", "content_summary": "RT @iraphas13: @LakeBrenden Hey Brenden, we'd love to understand how this fits into the context of a couple of recent works: Ford et al (h\u2026", "followers": "75"}, "1142141525658181632": {"author": "@iraphas13", "datetime": "2019-06-21 18:45:34", "content_summary": "@LakeBrenden Hey Brenden, we'd love to understand how this fits into the context of a couple of recent works: Ford et al (https://t.co/g2jHskOFV6) shows that without distributional robustness you can't obtain generally useful robustness (because of the wa", "followers": "1,449"}, "1090998294757736449": {"author": "@arxiv_cscv", "datetime": "2019-01-31 15:40:37", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise https://t.co/iZPXpMnUu6", "followers": "4,115"}, "1090913058501750784": {"author": "@MhRohban", "datetime": "2019-01-31 10:01:55", "content_summary": "RT @StatMLPapers: Adversarial Examples Are a Natural Consequence of Test Error in Noise. (arXiv:1901.10513v1 [cs.LG]) https://t.co/ARmiTRu6\u2026", "followers": "80"}, "1142502847277404162": {"author": "@unsorsodicorda", "datetime": "2019-06-22 18:41:20", "content_summary": "RT @iraphas13: @LakeBrenden Hey Brenden, we'd love to understand how this fits into the context of a couple of recent works: Ford et al (h\u2026", "followers": "740"}, "1145012196780519425": {"author": "@skornblith", "datetime": "2019-06-29 16:52:35", "content_summary": "@recursus @kendmil @marius10p @behrenstimb @KriegeskorteLab @computingnature They are not actually so robust. Adversarial vulnerability is predictable and guaranteed given neural nets' sensitivity to Gaussian noise perturbations, as shown in https://t.co/L", "followers": "1,429"}, "1114232991985827842": {"author": "@blaine_bateman", "datetime": "2019-04-05 18:27:01", "content_summary": "Google Brain: Adversarial Examples Are a Natural Consequence of Test Error in Noise https://t.co/iJPO7xY2mM https://t.co/2MPuSIqCzD", "followers": "125"}, "1090800714996203520": {"author": "@MLandDL_papers", "datetime": "2019-01-31 02:35:31", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise. (arXiv:1901.10513v1 [cs.LG]) https://t.co/u3fyLtWxIG", "followers": "380"}, "1090908083168124928": {"author": "@arxivml", "datetime": "2019-01-31 09:42:09", "content_summary": "\"Adversarial Examples Are a Natural Consequence of Test Error in Noise\", Nic Ford, Justin Gilmer, Nicolas Carlini, \u2026 https://t.co/A0ciNqNhvF", "followers": "780"}, "1152134708731531265": {"author": "@permutans", "datetime": "2019-07-19 08:34:54", "content_summary": "Distinction of natural adversarial example vs. error: IG @ ICLR 2019 https://t.co/JcfbSKcF9h Google Brain, 2018 https://t.co/ACA94u14qQ \"From one perspective, there isn't much of a difference between most types of model errors and most types of adv. ex's:\"", "followers": "1,116"}, "1090790223569657857": {"author": "@StatMLPapers", "datetime": "2019-01-31 01:53:49", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise. (arXiv:1901.10513v1 [cs.LG]) https://t.co/ARmiTRu6Qq", "followers": "9,721"}, "1091039106681827328": {"author": "@arxiv_org", "datetime": "2019-01-31 18:22:48", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise. https://t.co/AIue4ZKqzv https://t.co/bLQ1nnH7XB", "followers": "12,763"}, "1090804764487630854": {"author": "@yapp1e", "datetime": "2019-01-31 02:51:36", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise. (arXiv:1901.10513v1 [cs.LG]) https://t.co/pAqk2fJl4O Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learni", "followers": "50"}, "1090802114429882368": {"author": "@arxiv_cscv", "datetime": "2019-01-31 02:41:04", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise https://t.co/iZPXpMnUu6", "followers": "4,115"}, "1230617431069970432": {"author": "@ekindogus", "datetime": "2020-02-20 22:17:13", "content_summary": "@TheGregYang @bucketofkets @quocleix we think gaussian, corruption, and adversarial robustness are all closely related. (see https://t.co/Np4lhous2I). Here are examples where increased accuracy leads to better adversarial and general robustness: https://t.", "followers": "1,840"}, "1142141805179199489": {"author": "@poolio", "datetime": "2019-06-21 18:46:40", "content_summary": "RT @iraphas13: @LakeBrenden Hey Brenden, we'd love to understand how this fits into the context of a couple of recent works: Ford et al (h\u2026", "followers": "7,637"}, "1090804177826123777": {"author": "@BrundageBot", "datetime": "2019-01-31 02:49:16", "content_summary": "Adversarial Examples Are a Natural Consequence of Test Error in Noise. Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk https://t.co/IOsFnVkh3g", "followers": "3,891"}}, "queriedAt": "2020-05-21 20:39:49", "completed": "1", "citation_id": "54725729", "tab": "twitter"}