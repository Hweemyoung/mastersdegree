{"citation_id": "56155705", "tab": "twitter", "twitter": {"1100987047752327168": {"author": "@yarphs", "followers": "11", "datetime": "2019-02-28 05:12:22", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1113657027832631296": {"author": "@komakusaryama", "followers": "659", "datetime": "2019-04-04 04:18:20", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1106429321504124928": {"author": "@copasta_", "followers": "637", "datetime": "2019-03-15 05:38:01", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1102480706108907520": {"author": "@GeileHirnbude", "followers": "47", "datetime": "2019-03-04 08:07:38", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101155665949675521": {"author": "@marypcbuk", "followers": "11,314", "datetime": "2019-02-28 16:22:23", "content_summary": "RT @mark_riedl: Yes! Thank you. https://t.co/GFoKOp6GuR"}, "1107145290954334208": {"author": "@atsuc", "followers": "60", "datetime": "2019-03-17 05:03:01", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101145256614117377": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-02-28 15:41:02", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1179583196826128384": {"author": "@zhuzining", "followers": "45", "datetime": "2019-10-03 02:25:24", "content_summary": "Attention is not explaination (Jain and Wallace, NAACL '19) https://t.co/P7zbo36eL3 Attention is not not explaination (Wiegreffe and Pinter, EMNLP '19) https://t.co/jKNK4S9JJy"}, "1257713830408286208": {"author": "@soldni", "followers": "668", "datetime": "2020-05-05 16:48:39", "content_summary": "I'm also puzzled by the Synthesizer https://t.co/jk0gvQcnCu (@ytay017 et al) I guess being able to replace self attention is not 100% unexpected given counterfactual attention results by @successar_nlp and @byron_c_wallace? (https://t.co/qNAz5QxcKO)"}, "1106470173786005504": {"author": "@KSKSKSKS2", "followers": "216", "datetime": "2019-03-15 08:20:21", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1113749855904489472": {"author": "@morioka", "followers": "824", "datetime": "2019-04-04 10:27:12", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1101130213335941121": {"author": "@Dadang_Ewp", "followers": "370", "datetime": "2019-02-28 14:41:15", "content_summary": "So, deep learning would never be explainable? \ud83e\udd14"}, "1101216418245292032": {"author": "@brdskggs", "followers": "259", "datetime": "2019-02-28 20:23:48", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1103630432363560960": {"author": "@adlucem9", "followers": "7", "datetime": "2019-03-07 12:16:14", "content_summary": "RT @arxiv_cs_cl: https://t.co/GHFiutBoGS Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1101296900370644993": {"author": "@krishnamrith12", "followers": "534", "datetime": "2019-03-01 01:43:36", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101112408641277953": {"author": "@johnjnay", "followers": "297", "datetime": "2019-02-28 13:30:30", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1110016531692281857": {"author": "@kacky24", "followers": "21", "datetime": "2019-03-25 03:12:18", "content_summary": "RT @Bollegala: https://t.co/7ibu4KcZS0 This paper shows that attention (in the case of classification and induced by an biLSTM) does not co\u2026"}, "1223805904572817414": {"author": "@karma_Aleph", "followers": "30", "datetime": "2020-02-02 03:10:39", "content_summary": "https://t.co/viAd9eVogJ reasoning paradigm changed with nlp explanation, and this paper... is great"}, "1264864907477934082": {"author": "@AssistedEvolve", "followers": "216", "datetime": "2020-05-25 10:24:28", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1101130564516675585": {"author": "@gregd_nlp", "followers": "2,230", "datetime": "2019-02-28 14:42:39", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113745520189562881": {"author": "@yasutomo57jp", "followers": "2,649", "datetime": "2019-04-04 10:09:59", "content_summary": "\u753b\u50cf\u8a8d\u8b58\u3067\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3082\uff0c\u753b\u50cf\u306e\u8a00\u8a9e\u8868\u73fe\uff08\u30ad\u30e3\u30d7\u30b7\u30e7\u30cb\u30f3\u30b0\uff09\u3082\u89e3\u91c8\u3063\u3066\u30db\u30f3\u30c8\u304b\uff1f\u3063\u3066\u6628\u65e5\u67d0\u61c7\u89aa\u4f1a\u3067\u8a71\u3057\u3066\u305f\u3068\u3053\u308d"}, "1103021169878450176": {"author": "@shubh_300595", "followers": "66", "datetime": "2019-03-05 19:55:14", "content_summary": "RT @arxiv_cs_cl: https://t.co/GHFiutBoGS Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1101814213226377216": {"author": "@subhobrata1", "followers": "310", "datetime": "2019-03-02 11:59:13", "content_summary": "RT @arxiv_org: Attention is not Explanation. https://t.co/hmjCCYjjHL https://t.co/ASmlpYA8LC"}, "1101244853894168577": {"author": "@Shujian_Liu", "followers": "297", "datetime": "2019-02-28 22:16:47", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1102410624196325376": {"author": "@chaitjo", "followers": "733", "datetime": "2019-03-04 03:29:09", "content_summary": "Myth busting for DL+NLP: my favorite thing about attention was this feeling of understanding the logic/reasoning behind the predictions of a deep net. Maybe not!"}, "1101592850322649089": {"author": "@adityachivu", "followers": "79", "datetime": "2019-03-01 21:19:36", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113633070802821120": {"author": "@shigerufujita", "followers": "2,174", "datetime": "2019-04-04 02:43:09", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1101102870303330305": {"author": "@byron_c_wallace", "followers": "712", "datetime": "2019-02-28 12:52:36", "content_summary": "Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, at least not consistently. New #naacl2019 paper (w/PhD student Sarthak Jain): https://t.co/4r49zzQYyr"}, "1113631603308163079": {"author": "@U_M_V_U_E", "followers": "546", "datetime": "2019-04-04 02:37:19", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1106209369299968001": {"author": "@TristanNaumann", "followers": "171", "datetime": "2019-03-14 15:04:00", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1100938806264516614": {"author": "@arxiv_cs_cl", "followers": "4,223", "datetime": "2019-02-28 02:00:40", "content_summary": "https://t.co/GHFiutBoGS Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1106551984155353088": {"author": "@toshihiro_yama", "followers": "146", "datetime": "2019-03-15 13:45:26", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1113630658851557376": {"author": "@odashi_t", "followers": "9,452", "datetime": "2019-04-04 02:33:33", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1127282254227460097": {"author": "@RexDouglass", "followers": "1,402", "datetime": "2019-05-11 18:40:07", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1117986067121147910": {"author": "@nasrinmmm", "followers": "2,574", "datetime": "2019-04-16 03:00:24", "content_summary": "So glad that someone actually wrote this paper, confirming many people\u2019s old skepticism towards posing attention as explanation \u201cOur findings show that standard attention doesn\u2019t provide meaningful explanations and should not be treated as though they do.\u201d"}, "1161723956220420101": {"author": "@0xhexhex", "followers": "96", "datetime": "2019-08-14 19:39:09", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1114976305999560705": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-07 19:40:41", "content_summary": "Attention is not Explanation https://t.co/XaUyOHCPkf"}, "1101283130013495296": {"author": "@iamknighton", "followers": "425", "datetime": "2019-03-01 00:48:53", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113841353803218944": {"author": "@copasta_", "followers": "637", "datetime": "2019-04-04 16:30:47", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1101200399392882689": {"author": "@ajinkyakale", "followers": "739", "datetime": "2019-02-28 19:20:09", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1161635761919315968": {"author": "@mlatgt", "followers": "2,963", "datetime": "2019-08-14 13:48:42", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1100948909940187137": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-02-28 02:40:49", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1102243899723128832": {"author": "@joostbastings", "followers": "879", "datetime": "2019-03-03 16:26:39", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106427773155860480": {"author": "@kinetock", "followers": "153", "datetime": "2019-03-15 05:31:52", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1161558526491275269": {"author": "@coastalcph", "followers": "1,848", "datetime": "2019-08-14 08:41:48", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101220107672535040": {"author": "@NirmalSinghania", "followers": "36", "datetime": "2019-02-28 20:38:28", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106460249504931840": {"author": "@ElectronNest", "followers": "231", "datetime": "2019-03-15 07:40:55", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1161503754644000769": {"author": "@ayirpelle", "followers": "2,677", "datetime": "2019-08-14 05:04:09", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1177882039942443009": {"author": "@copasta_", "followers": "637", "datetime": "2019-09-28 09:45:36", "content_summary": "RT @atsuyakoba: Attention is not Explanation...?? https://t.co/OzU2wyKx6J"}, "1106450203182157824": {"author": "@tommy19970714", "followers": "1,687", "datetime": "2019-03-15 07:00:59", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1161467384588328960": {"author": "@HenryWolfAI", "followers": "586", "datetime": "2019-08-14 02:39:38", "content_summary": "Awaiting: All you need is not not Explanation"}, "1161511973919559680": {"author": "@Sam09lol", "followers": "0", "datetime": "2019-08-14 05:36:49", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101502574874820608": {"author": "@udmrzn", "followers": "1,347", "datetime": "2019-03-01 15:20:53", "content_summary": "RT @arxiv_org: Attention is not Explanation. https://t.co/hmjCCYjjHL https://t.co/ASmlpYA8LC"}, "1101199843354124293": {"author": "@DocXavi", "followers": "1,922", "datetime": "2019-02-28 19:17:56", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1100982098645712896": {"author": "@NafiseSadat", "followers": "266", "datetime": "2019-02-28 04:52:42", "content_summary": "RT @arxiv_cs_cl: https://t.co/GHFiutBoGS Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1101113042249703424": {"author": "@dasmiq", "followers": "815", "datetime": "2019-02-28 13:33:01", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1114437979173351424": {"author": "@jojonki", "followers": "1,157", "datetime": "2019-04-06 08:01:34", "content_summary": "@momoaiaiai \u3053\u308c\u3063\u3059\u306d\uff0e\u660e\u65e5\u53ce\u9332\u3059\u308b\u305f\u3081\u306b\u8aad\u3093\u3067\u308b\u3051\u3069\u9593\u306b\u5408\u308f\u306a\u305d\u3046\uff0e Attention is not Explanation Sarthak Jain, Byron C. Wallace Accepted as NAACL 2019 Long Paper. Draft Version https://t.co/Mn4UM5TgI3"}, "1264104053677711361": {"author": "@gettoankit", "followers": "149", "datetime": "2020-05-23 08:01:06", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1255493662043181056": {"author": "@FBWM8888", "followers": "557", "datetime": "2020-04-29 13:46:29", "content_summary": "RT @icoxfog417: \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u306b\u4f7f\u308f\u308c\u308b\u4e8b\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3053\u306e\u8ad6\u6587\u306e\u6570\u30ab\u6708\u524d\u306bAttention is not Explanation(https://t.co/TmCtku8uO5)\u304c\u51fa\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002BERT\u4ee5\u5f8c\u306eAttention\u89e3\u6790\u304c\u6df1\u307e\u308b\u624b\u524d\u306e\u30b5\u30fc\u2026"}, "1116935545035169792": {"author": "@THsama2", "followers": "114", "datetime": "2019-04-13 05:26:00", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101192831471775744": {"author": "@dtsbourg", "followers": "538", "datetime": "2019-02-28 18:50:04", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113770316931534848": {"author": "@MLDon1Choume", "followers": "108", "datetime": "2019-04-04 11:48:31", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1102336126973468673": {"author": "@himakotsu", "followers": "144", "datetime": "2019-03-03 22:33:07", "content_summary": "Attention is not Explanation https://t.co/msso05xZmx"}, "1101353062898196480": {"author": "@musharna000", "followers": "634", "datetime": "2019-03-01 05:26:47", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113469500773392384": {"author": "@shunk031", "followers": "2,079", "datetime": "2019-04-03 15:53:10", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1263910769730732033": {"author": "@jeremyphoward", "followers": "100,527", "datetime": "2020-05-22 19:13:04", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1177815090814316546": {"author": "@atsuyakoba", "followers": "626", "datetime": "2019-09-28 05:19:35", "content_summary": "Attention is not Explanation...?? https://t.co/OzU2wyKx6J"}, "1101129379369881606": {"author": "@BioNLProc", "followers": "372", "datetime": "2019-02-28 14:37:56", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101101938165366786": {"author": "@byron_c_wallace", "followers": "712", "datetime": "2019-02-28 12:48:54", "content_summary": "RT @benbenhh: Attention is not Explanation: It's possible to adversarially construct different attention distributions which still give the\u2026"}, "1106423888433471488": {"author": "@t_01201", "followers": "363", "datetime": "2019-03-15 05:16:26", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1106423739351130113": {"author": "@icoxfog417", "followers": "11,575", "datetime": "2019-03-15 05:15:50", "content_summary": "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u3082\u4e88\u6e2c\u7d50\u679c\u3092\u7dad\u6301\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d https://t.co/TmCtku8uO5"}, "1100938043916079104": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-02-28 01:57:38", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9CB65 https://t.co/1vmvxbfZTK Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/HHSAgzt2MA])"}, "1113600307890540544": {"author": "@shunk031", "followers": "2,079", "datetime": "2019-04-04 00:32:57", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1107965454851596289": {"author": "@Scaled_Wurm", "followers": "1,668", "datetime": "2019-03-19 11:22:04", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101115374853152769": {"author": "@parag_jain", "followers": "75", "datetime": "2019-02-28 13:42:17", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1107963969904340992": {"author": "@housecat442", "followers": "1,569", "datetime": "2019-03-19 11:16:10", "content_summary": "RT @Bollegala: https://t.co/7ibu4KcZS0 This paper shows that attention (in the case of classification and induced by an biLSTM) does not co\u2026"}, "1126663556919955457": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-10 01:41:39", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1102732355582648320": {"author": "@arxiv_pop", "followers": "714", "datetime": "2019-03-05 00:47:36", "content_summary": "2019/02/26 \u6295\u7a3f 4\u4f4d CL(Computation and Language) Attention is not Explanation https://t.co/jqEpExiRYm 10 Tweets 10 Retweets 47 Favorites"}, "1106443225651306497": {"author": "@morioka", "followers": "824", "datetime": "2019-03-15 06:33:16", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101233566875348992": {"author": "@theolivenbaum", "followers": "279", "datetime": "2019-02-28 21:31:56", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113554137302437888": {"author": "@katsuhitosudoh", "followers": "1,868", "datetime": "2019-04-03 21:29:29", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1101142258492076032": {"author": "@ionandrou", "followers": "650", "datetime": "2019-02-28 15:29:07", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1107928749104861185": {"author": "@Bollegala", "followers": "3,173", "datetime": "2019-03-19 08:56:12", "content_summary": "https://t.co/7ibu4KcZS0 This paper shows that attention (in the case of classification and induced by an biLSTM) does not correlate highly with feature weights (computed via gradient methods). (1)"}, "1113462449284407298": {"author": "@omitawo", "followers": "129", "datetime": "2019-04-03 15:25:09", "content_summary": "RT @shunk031: [1902.10186] Attention is not Explanation https://t.co/1yNCrdOstb"}, "1239929272023666688": {"author": "@nino_pira", "followers": "2,311", "datetime": "2020-03-17 14:59:09", "content_summary": "attention\u306e\u8aac\u660e\u6027\u306d\u3047\u2026 \u65e5\u672c\u8a9e\u30bf\u30b9\u30af\u3060\u3068\u306a\u304a\u5206\u304b\u3089\u3093\u2026 https://t.co/lG1IG9ZbfX"}, "1255499595192455173": {"author": "@rkakamilan", "followers": "642", "datetime": "2020-04-29 14:10:04", "content_summary": "RT @icoxfog417: \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u306b\u4f7f\u308f\u308c\u308b\u4e8b\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3053\u306e\u8ad6\u6587\u306e\u6570\u30ab\u6708\u524d\u306bAttention is not Explanation(https://t.co/TmCtku8uO5)\u304c\u51fa\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002BERT\u4ee5\u5f8c\u306eAttention\u89e3\u6790\u304c\u6df1\u307e\u308b\u624b\u524d\u306e\u30b5\u30fc\u2026"}, "1101123216981008385": {"author": "@MimansaJ", "followers": "405", "datetime": "2019-02-28 14:13:27", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101139731180445698": {"author": "@iamyehai", "followers": "10", "datetime": "2019-02-28 15:19:04", "content_summary": "really\uff1f"}, "1106770974588850176": {"author": "@nononono713", "followers": "13", "datetime": "2019-03-16 04:15:37", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101203924466520064": {"author": "@yumo_xu", "followers": "48", "datetime": "2019-02-28 19:34:09", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101149098478989313": {"author": "@shfaithy", "followers": "4", "datetime": "2019-02-28 15:56:18", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1114985252672290818": {"author": "@RexDouglass", "followers": "1,402", "datetime": "2019-04-07 20:16:14", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHCPkf"}, "1100937480533086208": {"author": "@helioRocha_", "followers": "630", "datetime": "2019-02-28 01:55:24", "content_summary": "\"Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/CE87fflQud])\" #arXiv https://t.co/skNebJQpwB"}, "1101126529910824961": {"author": "@nsaphra", "followers": "2,912", "datetime": "2019-02-28 14:26:37", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101483223710720000": {"author": "@vanessa_murdock", "followers": "1,119", "datetime": "2019-03-01 14:03:59", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1161616159889920002": {"author": "@dannyehb", "followers": "307", "datetime": "2019-08-14 12:30:48", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1120406193456001024": {"author": "@successar_nlp", "followers": "164", "datetime": "2019-04-22 19:17:07", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101132935854542848": {"author": "@mark_riedl", "followers": "14,054", "datetime": "2019-02-28 14:52:04", "content_summary": "Yes! Thank you."}, "1100958457245646849": {"author": "@BrundageBot", "followers": "3,913", "datetime": "2019-02-28 03:18:45", "content_summary": "Attention is not Explanation. Sarthak Jain and Byron C. Wallace https://t.co/ICF0KMFNkK"}, "1177856341076873217": {"author": "@inoudayo", "followers": "983", "datetime": "2019-09-28 08:03:29", "content_summary": "RT @atsuyakoba: Attention is not Explanation...?? https://t.co/OzU2wyKx6J"}, "1101104398057394176": {"author": "@__jm", "followers": "245", "datetime": "2019-02-28 12:58:40", "content_summary": "RT @BrundageBot: Attention is not Explanation. Sarthak Jain and Byron C. Wallace https://t.co/ICF0KMFNkK"}, "1114029447059886080": {"author": "@take_mlstudy", "followers": "29", "datetime": "2019-04-05 04:58:12", "content_summary": "RT @odashi_t: \u307e\u3042attention\u304c\u6697\u9ed9\u306b\u6c7a\u307e\u3089\u306a\u3044\u2192\u9006\u306b\u640d\u5931\u304b\u3051\u308b\u3053\u3068\u3067\u8ffd\u52a0\u60c5\u5831\u3092\u57cb\u3081\u8fbc\u3081\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002\u5b9f\u969b\u306b\u305d\u3046\u3044\u3046\u8ad6\u6587\u306f\u3042\u308b\u306e\u3067\u3001\u524d\u63d0\u90e8\u5206\u306e\u8a71\u304c\u3088\u3046\u3084\u304f\u51fa\u305f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002Transformer\u306f\u305d\u3082\u305d\u3082\u8907\u6570attemtion\u306a\u306e\u3067\u5358\u4f53\u306f\u6700\u521d\u304b\u3089\u4fe1\u2026"}, "1127433397687803904": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-12 04:40:43", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1101038205128847360": {"author": "@evanmiltenburg", "followers": "1,533", "datetime": "2019-02-28 08:35:39", "content_summary": "Sounds like an interesting #NLProc paper! \u201cIn this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful 'explanations' for predictions. We find that they largely d"}, "1101275457570271232": {"author": "@ukyo_ho", "followers": "73", "datetime": "2019-03-01 00:18:24", "content_summary": "RT @arxiv_in_review: #NAACL2019 Attention is not Explanation. (arXiv:1902.10186v1 [cs\\.CL]) https://t.co/hkA9IBo05Q"}, "1113710924207026176": {"author": "@ougai_quantum", "followers": "915", "datetime": "2019-04-04 07:52:30", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1107599642735239168": {"author": "@wisteria0gps", "followers": "118", "datetime": "2019-03-18 11:08:27", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1113467187564470272": {"author": "@mosko_mule", "followers": "2,704", "datetime": "2019-04-03 15:43:59", "content_summary": "Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u308b\u4ed6\u306eattention\u5206\u5e03\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3059\u3089\u3067\u304d\u308b\u3002\u7ffb\u8a33\u30bf\u30b9\u30af\u3084Transformer\u7cfb\u306e\u30e2\u30c7\u30eb\u3067\u306e\u7d50\u679c\u3082\u77e5\u308a\u305f\u3044\u2026 https://t.co/VhyJcXrDM0"}, "1113450651244068864": {"author": "@shunk031", "followers": "2,079", "datetime": "2019-04-03 14:38:16", "content_summary": "[1902.10186] Attention is not Explanation https://t.co/1yNCrdOstb"}, "1101344407800954881": {"author": "@nzw0301", "followers": "1,445", "datetime": "2019-03-01 04:52:23", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1126859762082177024": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-10 14:41:17", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1113968296364265474": {"author": "@helioRocha_", "followers": "630", "datetime": "2019-04-05 00:55:13", "content_summary": "\"Attention is not Explanation. (arXiv:1902.10186v2 [https://t.co/CE87fflQud] UPDATED)\" #arXiv https://t.co/skNebJQpwB"}, "1102672069215563781": {"author": "@0_rishabh", "followers": "355", "datetime": "2019-03-04 20:48:02", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1263958643789176833": {"author": "@Sohilnewa", "followers": "166", "datetime": "2020-05-22 22:23:18", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1263877102224445450": {"author": "@TheZachMueller", "followers": "780", "datetime": "2020-05-22 16:59:17", "content_summary": "Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do explanatory analysis for NLP-based models (such as with ULM-FiT)? cc @ChristianFJung @jeremyphoward @seb_ruder"}, "1101084231269261312": {"author": "@anmarasovic", "followers": "543", "datetime": "2019-02-28 11:38:32", "content_summary": "RT @benbenhh: Attention is not Explanation: It's possible to adversarially construct different attention distributions which still give the\u2026"}, "1101231223446425600": {"author": "@DebjitPaul2", "followers": "63", "datetime": "2019-02-28 21:22:38", "content_summary": "RT @benbenhh: Attention is not Explanation: It's possible to adversarially construct different attention distributions which still give the\u2026"}, "1106590836232351744": {"author": "@bunkei_ac_math", "followers": "166", "datetime": "2019-03-15 16:19:49", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1106685860357898241": {"author": "@8kazu3", "followers": "467", "datetime": "2019-03-15 22:37:24", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101123868218003456": {"author": "@ElisaFerracane", "followers": "63", "datetime": "2019-02-28 14:16:02", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1125574224301654016": {"author": "@SanhEstPasMoi", "followers": "4,567", "datetime": "2019-05-07 01:33:01", "content_summary": "Arxiv link to the paper: https://t.co/4WpBXSRRX5"}, "1113643284306358272": {"author": "@mamoruk", "followers": "8,966", "datetime": "2019-04-04 03:23:44", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1161637746739466240": {"author": "@mvaldenegro", "followers": "1,074", "datetime": "2019-08-14 13:56:35", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101198478103982080": {"author": "@mjp39", "followers": "1,132", "datetime": "2019-02-28 19:12:31", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101445026872061952": {"author": "@arxiv_org", "followers": "12,795", "datetime": "2019-03-01 11:32:12", "content_summary": "Attention is not Explanation. https://t.co/hmjCCYjjHL https://t.co/ASmlpYA8LC"}, "1106424508154417152": {"author": "@__tuxi__", "followers": "162", "datetime": "2019-03-15 05:18:53", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1113638620139679744": {"author": "@morioka", "followers": "824", "datetime": "2019-04-04 03:05:12", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1255496206958166017": {"author": "@jd_mashiro", "followers": "1,159", "datetime": "2020-04-29 13:56:36", "content_summary": "RT @icoxfog417: \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u306b\u4f7f\u308f\u308c\u308b\u4e8b\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3053\u306e\u8ad6\u6587\u306e\u6570\u30ab\u6708\u524d\u306bAttention is not Explanation(https://t.co/TmCtku8uO5)\u304c\u51fa\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002BERT\u4ee5\u5f8c\u306eAttention\u89e3\u6790\u304c\u6df1\u307e\u308b\u624b\u524d\u306e\u30b5\u30fc\u2026"}, "1118298691851309056": {"author": "@rajasuresh", "followers": "17", "datetime": "2019-04-16 23:42:39", "content_summary": "RT @nasrinmmm: So glad that someone actually wrote this paper, confirming many people\u2019s old skepticism towards posing attention as explanat\u2026"}, "1101207601008631808": {"author": "@kylewadegrove", "followers": "914", "datetime": "2019-02-28 19:48:46", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106429946195402753": {"author": "@ymym3412", "followers": "3,333", "datetime": "2019-03-15 05:40:30", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1114734699019153409": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-07 03:40:37", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1127040989380657152": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-11 02:41:25", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1113634952388562944": {"author": "@1997synohro", "followers": "605", "datetime": "2019-04-04 02:50:37", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1113631836381388800": {"author": "@yo_ehara", "followers": "2,113", "datetime": "2019-04-04 02:38:14", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1101122989578313729": {"author": "@ChenhaoTan", "followers": "1,393", "datetime": "2019-02-28 14:12:33", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113594263361613825": {"author": "@fukuma_tomoki", "followers": "358", "datetime": "2019-04-04 00:08:56", "content_summary": "Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u3063\u3061\u306e\u7d50\u679c\u3082\u6b32\u3057\u3044\u3002 https://t.co/acqouRDNv6"}, "1101225028295819265": {"author": "@josipK", "followers": "256", "datetime": "2019-02-28 20:58:01", "content_summary": "RT @benbenhh: Attention is not Explanation: It's possible to adversarially construct different attention distributions which still give the\u2026"}, "1125582106086735875": {"author": "@julien_c", "followers": "9,786", "datetime": "2019-05-07 02:04:21", "content_summary": "RT @SanhEstPasMoi: Arxiv link to the paper: https://t.co/4WpBXSRRX5"}, "1101133141207642113": {"author": "@aeronlaffere", "followers": "722", "datetime": "2019-02-28 14:52:53", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106513551890350082": {"author": "@morinosuke__p", "followers": "276", "datetime": "2019-03-15 11:12:43", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1255491143745196032": {"author": "@icoxfog417", "followers": "11,575", "datetime": "2020-04-29 13:36:29", "content_summary": "\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u306b\u4f7f\u308f\u308c\u308b\u4e8b\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3053\u306e\u8ad6\u6587\u306e\u6570\u30ab\u6708\u524d\u306bAttention is not Explanation(https://t.co/TmCtku8uO5)\u304c\u51fa\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002BERT\u4ee5\u5f8c\u306eAttention\u89e3\u6790\u304c\u6df1\u307e\u308b\u624b\u524d\u306e\u30b5\u30fc\u30d9\u30a4\u3068\u3044\u3046\u5370\u8c61\u3067\u3001\u3061\u3087\u3063\u3068\u4e2d\u9014\u534a\u7aef\u306a\u611f\u3058\u306f\u3042\u308b\u3002"}, "1101274522542657537": {"author": "@arxiv_in_review", "followers": "1,344", "datetime": "2019-03-01 00:14:41", "content_summary": "#NAACL2019 Attention is not Explanation. (arXiv:1902.10186v1 [cs\\.CL]) https://t.co/hkA9IBo05Q"}, "1161583886352044032": {"author": "@alessandroleite", "followers": "376", "datetime": "2019-08-14 10:22:34", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1106511197258444801": {"author": "@kz311", "followers": "311", "datetime": "2019-03-15 11:03:22", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101226552073834498": {"author": "@kaleidic", "followers": "1,780", "datetime": "2019-02-28 21:04:04", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1224794575392428034": {"author": "@JakeCowton", "followers": "228", "datetime": "2020-02-04 20:39:17", "content_summary": "Titles in #MachineLearning can be pretty great! \"Attention is not Explanation\" https://t.co/XfXCI9c5SP \"Attention is not not Explanation\" https://t.co/mj2T1sjmAd"}, "1101171661162741760": {"author": "@ayirpelle", "followers": "2,677", "datetime": "2019-02-28 17:25:57", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1264122624516460550": {"author": "@popular_ML", "followers": "63", "datetime": "2020-05-23 09:14:54", "content_summary": "The most popular ArXiv tweet in the last 24h: https://t.co/SrH0iLnvUn"}, "1113770373051375616": {"author": "@Don1Choume", "followers": "605", "datetime": "2019-04-04 11:48:44", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1126968522746163200": {"author": "@ElectronNest", "followers": "231", "datetime": "2019-05-10 21:53:28", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1101205943218511874": {"author": "@thomasrdavidson", "followers": "554", "datetime": "2019-02-28 19:42:10", "content_summary": "Thursday: https://t.co/8NXSlBESUL https://t.co/wn40OrrEjI"}, "1263885776972132352": {"author": "@ChristianFJung", "followers": "20", "datetime": "2020-05-22 17:33:45", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1106538932198502405": {"author": "@fukkaa1225", "followers": "236", "datetime": "2019-03-15 12:53:34", "content_summary": "Attention\u3067\u3044\u3064\u3082\u8aac\u660e\u3092\u8a66\u307f\u3088\u3046\u3068\u3057\u3066\u3054\u3081\u3093\u306a\u3055\u3044 https://t.co/t3gdlkopin"}, "1113847269025955840": {"author": "@odashi_t", "followers": "9,452", "datetime": "2019-04-04 16:54:17", "content_summary": "\u307e\u3042attention\u304c\u6697\u9ed9\u306b\u6c7a\u307e\u3089\u306a\u3044\u2192\u9006\u306b\u640d\u5931\u304b\u3051\u308b\u3053\u3068\u3067\u8ffd\u52a0\u60c5\u5831\u3092\u57cb\u3081\u8fbc\u3081\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002\u5b9f\u969b\u306b\u305d\u3046\u3044\u3046\u8ad6\u6587\u306f\u3042\u308b\u306e\u3067\u3001\u524d\u63d0\u90e8\u5206\u306e\u8a71\u304c\u3088\u3046\u3084\u304f\u51fa\u305f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002Transformer\u306f\u305d\u3082\u305d\u3082\u8907\u6570attemtion\u306a\u306e\u3067\u5358\u4f53\u306f\u6700\u521d\u304b\u3089\u4fe1\u7528\u306a\u308a\u307e\u305b\u3093\u3002 https://t.co/mZJTpcnnz3"}, "1101079632194035713": {"author": "@benbenhh", "followers": "222", "datetime": "2019-02-28 11:20:16", "content_summary": "Attention is not Explanation: It's possible to adversarially construct different attention distributions which still give the same model output and there is only weak correlation between attention weights and other feature importance measures. https://t.co"}, "1210035169769312259": {"author": "@Akira_Mat5", "followers": "876", "datetime": "2019-12-26 03:10:40", "content_summary": "Attention\u3068\u3044\u3048\u3070\"Attention is not Explanation\"\u3068\"Attention is not not Explanation\"\u8aad\u3093\u3067\u304a\u3082\u3057\u308d\u304b\u3063\u305f\u3002 https://t.co/AQ9bFEeR7W https://t.co/ZmZ6IDPwOd"}, "1161644680343425024": {"author": "@keviv9", "followers": "165", "datetime": "2019-08-14 14:24:08", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1113733410399313920": {"author": "@ebiflyyyyyyyy", "followers": "170", "datetime": "2019-04-04 09:21:51", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1176397538250772485": {"author": "@akanyaani", "followers": "29", "datetime": "2019-09-24 07:26:44", "content_summary": "An interesting paper on Attention Mechanism. https://t.co/kcbGgwC12o https://t.co/mwuqLDcYmX"}, "1101164345604608002": {"author": "@muktabh", "followers": "798", "datetime": "2019-02-28 16:56:53", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101084961539588096": {"author": "@anmarasovic", "followers": "543", "datetime": "2019-02-28 11:41:26", "content_summary": "RT @benbenhh: Attention is not Explanation: It's possible to adversarially construct different attention distributions which still give the\u2026"}, "1113752783700643841": {"author": "@Trtd6Trtd", "followers": "157", "datetime": "2019-04-04 10:38:50", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1113656334493278208": {"author": "@Ksk_Szk8424", "followers": "761", "datetime": "2019-04-04 04:15:35", "content_summary": "Attention is not Explanation \u6ce8\u610f\u6a5f\u69cb\u306e\u6ce8\u610f\u72b6\u614b\u3067\u30e2\u30c7\u30eb\u306e\u8aac\u660e\u529b\u3092\u793a\u3059\u306e\u306f\u5272\u3068\u30d9\u30fc\u30b7\u30c3\u30af\u306a\u4eee\u8aac\u3060\u3063\u305f\u304b\u3089\u3053\u308c\u306f\u58ee\u5927\u306a\u30a2\u30f3\u30c1\u30c6\u30fc\u30bc https://t.co/RSQO5JWXab https://t.co/8mvVQCBvdb"}, "1101133774044254209": {"author": "@tallinzen", "followers": "6,382", "datetime": "2019-02-28 14:55:24", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106968265182638080": {"author": "@AUEBNLPGroup", "followers": "511", "datetime": "2019-03-16 17:19:35", "content_summary": "Next AUEB NLP Group meeting, Tue. *26* March, 17:15-19:00: Discussion of \"Attention is not Explanation\", S. Jain and B.C. Wallace (NAACL 2019). Paper: https://t.co/8hc0ABdtRd. Central AUEB buildings, room A36. All welcome. Study the paper before the meetin"}, "1207879822191366145": {"author": "@MednlpS", "followers": "119", "datetime": "2019-12-20 04:26:05", "content_summary": "1. Attention Is Not Explanation. \u300cAttention=\u5224\u65ad\u6839\u62e0\u300d\u3092\u7591\u554f\u8996\u3057\u305f\u8ad6\u6587\u3002 \u6587\u66f8\u5206\u985e, QA, NLI\u3092BiLSTM+Attention\u3067\u89e3\u304f\u3068\uff0c \u30fbAttention\u306fGBT\u306efeature importance\u3068\u4e00\u81f4\u3057\u306a\u304b\u3063\u305f\u3002 \u30fbmodel output\u3092\u307b\u307c\u5909\u3048\u305a\u306bAttention\u3092\u5927\u304d\u304f\u6539\u5909\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u3063\u305f\u3002 https://t.co/Cr5lII6e1B"}, "1101108190203297792": {"author": "@zacharylipton", "followers": "30,203", "datetime": "2019-02-28 13:13:44", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101266502207049735": {"author": "@karlhigley", "followers": "785", "datetime": "2019-02-28 23:42:49", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1169553983083446272": {"author": "@cnerg", "followers": "421", "datetime": "2019-09-05 10:12:53", "content_summary": "Reading Group : Attention is not Explanation (NAACL 2019) | CSE Seminar Room(107), 6PM| https://t.co/upJXvJdqcJ"}, "1114742766154596353": {"author": "@ElectronNest", "followers": "231", "datetime": "2019-04-07 04:12:41", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1161463819266416641": {"author": "@mark_riedl", "followers": "14,054", "datetime": "2019-08-14 02:25:28", "content_summary": "\"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi and @sarahwiegreffe (appearing at #emnlp2019) https://t.co/WDiEf9mkGo"}, "1101131062456000512": {"author": "@texttheater", "followers": "624", "datetime": "2019-02-28 14:44:37", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113967269825142789": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-04-05 00:51:08", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/1vmvxaYp2c Attention is not Explanation. (arXiv:1902.10186v2 [https://t.co/HHSAgzbrV2] UPDATED)"}, "1113752222553083904": {"author": "@Trtd6Trtd", "followers": "157", "datetime": "2019-04-04 10:36:37", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1161537520649342977": {"author": "@Bouh___", "followers": "449", "datetime": "2019-08-14 07:18:19", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1127629651025723392": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-12 17:40:33", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1101862779810705408": {"author": "@cocoweixu", "followers": "3,350", "datetime": "2019-03-02 15:12:12", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1161464388785561600": {"author": "@jeffbigham", "followers": "11,188", "datetime": "2019-08-14 02:27:43", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1113979861410963456": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-05 01:41:10", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1113663132755316736": {"author": "@sleeping_michi", "followers": "413", "datetime": "2019-04-04 04:42:36", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1113644261688221697": {"author": "@olanleed", "followers": "1,655", "datetime": "2019-04-04 03:27:37", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1263993958750806016": {"author": "@92HsChoi", "followers": "90", "datetime": "2020-05-23 00:43:38", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1101266137545887744": {"author": "@dongng", "followers": "1,086", "datetime": "2019-02-28 23:41:22", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101331012389203970": {"author": "@KouroshMeshgi", "followers": "622", "datetime": "2019-03-01 03:59:09", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1161544721220739073": {"author": "@sarahwiegreffe", "followers": "528", "datetime": "2019-08-14 07:46:56", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101337483151720448": {"author": "@kuanchen22", "followers": "69", "datetime": "2019-03-01 04:24:52", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101199626097455105": {"author": "@anshulkundaje", "followers": "7,253", "datetime": "2019-02-28 19:17:04", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1255494630612795395": {"author": "@oregakitaworld", "followers": "144", "datetime": "2020-04-29 13:50:20", "content_summary": "RT @icoxfog417: \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u306b\u4f7f\u308f\u308c\u308b\u4e8b\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3053\u306e\u8ad6\u6587\u306e\u6570\u30ab\u6708\u524d\u306bAttention is not Explanation(https://t.co/TmCtku8uO5)\u304c\u51fa\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002BERT\u4ee5\u5f8c\u306eAttention\u89e3\u6790\u304c\u6df1\u307e\u308b\u624b\u524d\u306e\u30b5\u30fc\u2026"}, "1161470982495690752": {"author": "@morlikow", "followers": "265", "datetime": "2019-08-14 02:53:55", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1162692238062653440": {"author": "@NonLocalityGuy", "followers": "419", "datetime": "2019-08-17 11:46:45", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1114745314160730113": {"author": "@rtanaka_lab", "followers": "146", "datetime": "2019-04-07 04:22:48", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1106423990497628160": {"author": "@shunk031", "followers": "2,079", "datetime": "2019-03-15 05:16:50", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101139277612756992": {"author": "@Shujian_Liu", "followers": "297", "datetime": "2019-02-28 15:17:16", "content_summary": "RT @arxiv_cs_cl: https://t.co/GHFiutBoGS Attention is not Explanation. (arXiv:1902.10186v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1100974674077470720": {"author": "@udmrzn", "followers": "1,347", "datetime": "2019-02-28 04:23:12", "content_summary": "RT @arxiv_cscl: Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1101272666059825152": {"author": "@georgepar_91", "followers": "46", "datetime": "2019-03-01 00:07:18", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1110051149283909632": {"author": "@kacky24", "followers": "21", "datetime": "2019-03-25 05:29:52", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101108646107381760": {"author": "@dkaushik96", "followers": "878", "datetime": "2019-02-28 13:15:33", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1255493598868635654": {"author": "@morioka", "followers": "824", "datetime": "2020-04-29 13:46:14", "content_summary": "RT @icoxfog417: \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u306b\u4f7f\u308f\u308c\u308b\u4e8b\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3053\u306e\u8ad6\u6587\u306e\u6570\u30ab\u6708\u524d\u306bAttention is not Explanation(https://t.co/TmCtku8uO5)\u304c\u51fa\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002BERT\u4ee5\u5f8c\u306eAttention\u89e3\u6790\u304c\u6df1\u307e\u308b\u624b\u524d\u306e\u30b5\u30fc\u2026"}, "1101404636462759937": {"author": "@jnhwkim", "followers": "964", "datetime": "2019-03-01 08:51:43", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113733381668261890": {"author": "@katsuhitosudoh", "followers": "1,868", "datetime": "2019-04-04 09:21:45", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1179584353803702272": {"author": "@yuco49574109", "followers": "0", "datetime": "2019-10-03 02:30:00", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101510328704065536": {"author": "@letranger14", "followers": "329", "datetime": "2019-03-01 15:51:42", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101161644346896384": {"author": "@Sunghyo_Chung", "followers": "21", "datetime": "2019-02-28 16:46:09", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106427159768252416": {"author": "@ohtaman", "followers": "558", "datetime": "2019-03-15 05:29:25", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101445803854356480": {"author": "@ElectronNest", "followers": "231", "datetime": "2019-03-01 11:35:18", "content_summary": "RT @arxiv_org: Attention is not Explanation. https://t.co/hmjCCYjjHL https://t.co/ASmlpYA8LC"}, "1161513066145701897": {"author": "@heghbalz", "followers": "1,314", "datetime": "2019-08-14 05:41:09", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1113650606374174720": {"author": "@miorgash", "followers": "82", "datetime": "2019-04-04 03:52:49", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1106506200193990656": {"author": "@nishinojunji", "followers": "953", "datetime": "2019-03-15 10:43:30", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101353867671937025": {"author": "@negri_teo", "followers": "267", "datetime": "2019-03-01 05:29:58", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1264228985136975873": {"author": "@EricSchles", "followers": "2,138", "datetime": "2020-05-23 16:17:32", "content_summary": "RT @popular_ML: The most popular ArXiv tweet in the last 24h: https://t.co/SrH0iLnvUn"}, "1161978601174179841": {"author": "@AISC_TO", "followers": "1,038", "datetime": "2019-08-15 12:31:01", "content_summary": "Attention is not Explanation https://t.co/OTf2F8FuwN https://t.co/SuIIJBkinE"}, "1101306243602415616": {"author": "@vijayant_k", "followers": "54", "datetime": "2019-03-01 02:20:44", "content_summary": "Cool. I am going to be reading this like a zealot."}, "1126797713687945222": {"author": "@byron_c_wallace", "followers": "712", "datetime": "2019-05-10 10:34:44", "content_summary": "We've posted a new version w/more results and discussion: https://t.co/z6h4zN6wGp. Thanks @yuvalpi and @sarahwiegreffe for the feedback; we hope integrating this has improved the work! @successar_nlp"}, "1106435275041832960": {"author": "@ougai_quantum", "followers": "915", "datetime": "2019-03-15 06:01:40", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101865572608831488": {"author": "@Tanaygahlot", "followers": "255", "datetime": "2019-03-02 15:23:18", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101141899258355712": {"author": "@__isaac_s__", "followers": "59", "datetime": "2019-02-28 15:27:41", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1161954318360043521": {"author": "@inmoonlight_kr", "followers": "17", "datetime": "2019-08-15 10:54:32", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101457693720612864": {"author": "@mauriciociprian", "followers": "214", "datetime": "2019-03-01 12:22:32", "content_summary": "RT @arxiv_org: Attention is not Explanation. https://t.co/hmjCCYjjHL https://t.co/ASmlpYA8LC"}, "1161620026790539266": {"author": "@KouroshMeshgi", "followers": "622", "datetime": "2019-08-14 12:46:10", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1101160769096425474": {"author": "@avineshpvs", "followers": "342", "datetime": "2019-02-28 16:42:40", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1263914693229457410": {"author": "@Wind_Xiaoli", "followers": "51", "datetime": "2020-05-22 19:28:39", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1114357297072017408": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-06 02:40:58", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1114538576707035138": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-06 14:41:18", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1113605751350497281": {"author": "@sugi3_34", "followers": "1,345", "datetime": "2019-04-04 00:54:35", "content_summary": "RT @fukuma_tomoki: Attention is Not Explanation attention\u3092\u89e3\u91c8\u6027\u3068\u3059\u308b\u306e\u306f\u5371\u967a\u3063\u3066\u8a71\u3060\u3051\u3069\u3001ELMO\u3068\u304bBERT\u307f\u305f\u3044\u306a\u5b66\u7fd2\u6e08\u307fEmbedding\u56fa\u5b9a\u3057\u3066Attention\u3064\u3051\u305f\u3089\u500b\u4eba\u7684\u306b\u306f\u7d50\u679c\u304b\u306a\u308a\u9055\u3063\u3066\u304f\u308b\u6c17\u304c\u3057\u3066\u305d\u2026"}, "1101483534970048513": {"author": "@cghosh_", "followers": "281", "datetime": "2019-03-01 14:05:13", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101155395597410307": {"author": "@arxivml", "followers": "787", "datetime": "2019-02-28 16:21:19", "content_summary": "\"Attention is not Explanation\", Sarthak Jain, Byron C\uff0e Wallace https://t.co/ip5jlHruqv"}, "1101185155446857730": {"author": "@MohitIyyer", "followers": "2,714", "datetime": "2019-02-28 18:19:34", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1106427233235668992": {"author": "@ainewsantenna", "followers": "55", "datetime": "2019-03-15 05:29:43", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1101134646048448512": {"author": "@kroscoo", "followers": "685", "datetime": "2019-02-28 14:58:52", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1114176085648916480": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-05 14:40:53", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1113714671809208323": {"author": "@ArxivSanityHype", "followers": "7", "datetime": "2019-04-04 08:07:24", "content_summary": "Attention is not Explanation https://t.co/EBCYS9cMjQ https://t.co/uJf7WUdrlj"}, "1113560088180998144": {"author": "@keeeeei0315", "followers": "3,608", "datetime": "2019-04-03 21:53:08", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1127237218945372160": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-11 15:41:10", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1101412145756073984": {"author": "@fblain", "followers": "577", "datetime": "2019-03-01 09:21:33", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1161589326364454912": {"author": "@sara_hlt", "followers": "1,370", "datetime": "2019-08-14 10:44:11", "content_summary": "RT @mark_riedl: \"Attention is not Explanation\" by @byron_c_wallace https://t.co/uhdeITXa38 \"Attention is not not Explanation\u201d by @yuvalpi\u2026"}, "1112722077448978433": {"author": "@himakotsu", "followers": "144", "datetime": "2019-04-01 14:23:11", "content_summary": "[1902.10186] Attention is not Explanation https://t.co/DCetzJVtP3"}, "1101135799893008384": {"author": "@bgalbraith", "followers": "958", "datetime": "2019-02-28 15:03:27", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101152964843065347": {"author": "@Emily_Alsentzer", "followers": "638", "datetime": "2019-02-28 16:11:39", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101377849636868097": {"author": "@DH_FBK", "followers": "1,860", "datetime": "2019-03-01 07:05:16", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1114915933561262080": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-04-07 15:40:47", "content_summary": "Attention is not Explanation https://t.co/XaUyOHUqIP"}, "1101379098151174144": {"author": "@KlimZaporojets", "followers": "26", "datetime": "2019-03-01 07:10:14", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1126651926739922944": {"author": "@arxiv_cs_cl", "followers": "4,223", "datetime": "2019-05-10 00:55:26", "content_summary": "https://t.co/Ql1XxEocRi Attention is not Explanation. (arXiv:1902.10186v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc"}, "1115858197359861772": {"author": "@kacky24", "followers": "21", "datetime": "2019-04-10 06:05:00", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1113567241352515585": {"author": "@rkakamilan", "followers": "642", "datetime": "2019-04-03 22:21:34", "content_summary": "RT @shunk031: [1902.10186] Attention is not Explanation https://t.co/1yNCrdOstb"}, "1106504553027895296": {"author": "@miyayou", "followers": "18,356", "datetime": "2019-03-15 10:36:57", "content_summary": "RT @icoxfog417: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u304a\u3044\u3066Attention\u306f\u3088\u304f\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u304c\u3001\u672c\u5f53\u306b\u8aac\u660e\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u691c\u8a3c\u3057\u305f\u7814\u7a76\u3002\u7d50\u679c\u3068\u3057\u3066\u3001Gradient\u30d9\u30fc\u30b9\u306e\u30b9\u30b3\u30a2\u3068Attention\u306f\u4e56\u96e2\u304c\u3042\u308a\u3001\u307e\u305fAttention\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u3088\u3046\u5909\u66f4\u3057\u3066\u2026"}, "1113631733335740416": {"author": "@watchdog20xx", "followers": "78", "datetime": "2019-04-04 02:37:50", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1101272543619538944": {"author": "@dhruvkr1993", "followers": "27", "datetime": "2019-03-01 00:06:49", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1263956208404312071": {"author": "@_bam098", "followers": "33", "datetime": "2020-05-22 22:13:37", "content_summary": "RT @TheZachMueller: Re: Attention is not Explanation for NLP based models (https://t.co/vJ5w7KDLZl), what is the accepted approach to do ex\u2026"}, "1101140944248860672": {"author": "@RichmanRonald", "followers": "913", "datetime": "2019-02-28 15:23:53", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1101111154494095362": {"author": "@mtutek", "followers": "121", "datetime": "2019-02-28 13:25:31", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1116574205934325761": {"author": "@YoshifumiSeki", "followers": "3,805", "datetime": "2019-04-12 05:30:10", "content_summary": "\u8ad6\u6587\u306f\u3053\u308c https://t.co/cGAkWWYne5"}, "1113570605339516928": {"author": "@harujoh", "followers": "382", "datetime": "2019-04-03 22:34:56", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1117287285450346497": {"author": "@HirokiT57858674", "followers": "376", "datetime": "2019-04-14 04:43:41", "content_summary": "RT @YoshifumiSeki: \u8ad6\u6587\u306f\u3053\u308c https://t.co/cGAkWWYne5"}, "1113477340992929792": {"author": "@kame_123456789", "followers": "48", "datetime": "2019-04-03 16:24:20", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1101110133449461763": {"author": "@ryanjgallag", "followers": "2,193", "datetime": "2019-02-28 13:21:28", "content_summary": "RT @byron_c_wallace: Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, a\u2026"}, "1113537285822144513": {"author": "@omitawo", "followers": "129", "datetime": "2019-04-03 20:22:32", "content_summary": "RT @mosko_mule: Attention is not Explanation https://t.co/aVerjUo4OS NLP\u306e\u30bf\u30b9\u30af\u3067attention\u306e\u5206\u5e03\u304c\u4e88\u6e2c\u306e\u8aac\u660e\u306b\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\u3001attention\u306f\u4ed6\u306e\u7279\u5fb4\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u4e00\u81f4\u305b\u305a\u3001\u540c\u3058\u4e88\u6e2c\u3092\u3059\u2026"}, "1126693634990936070": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-05-10 03:41:10", "content_summary": "Attention is not Explanation https://t.co/XaUyOHCPkf"}}, "completed": "1", "queriedAt": "2020-06-03 01:40:59"}