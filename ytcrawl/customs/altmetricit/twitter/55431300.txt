{"tab": "twitter", "completed": "1", "twitter": {"1095868831782879233": {"author": "@yapp1e", "followers": "48", "datetime": "2019-02-14 02:14:24", "content_summary": "Extreme Tensoring for Low-Memory Preconditioning. (arXiv:1902.04620v1 [cs.LG]) https://t.co/nsaEdTkJWH State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent"}, "1096453727068868608": {"author": "@AmalFeriani", "followers": "127", "datetime": "2019-02-15 16:58:34", "content_summary": "RT @HazanPrinceton: Courtesy of your local Google AI Lab part 1: Should you use a larger model or more memory in your optimizer? A techniqu\u2026"}, "1095963829551448064": {"author": "@arxivml", "followers": "767", "datetime": "2019-02-14 08:31:53", "content_summary": "\"Extreme Tensoring for Low-Memory Preconditioning\", Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, Yi Zhang https://t.co/7yPtkzca8O"}, "1096067518282772481": {"author": "@HazanPrinceton", "followers": "3,036", "datetime": "2019-02-14 15:23:54", "content_summary": "Courtesy of your local Google AI Lab part 1: Should you use a larger model or more memory in your optimizer? A technique for reducing memory in preconditioning, and study of the tradeoffs involved: https://t.co/03Y8scIOYq"}, "1095864402253881344": {"author": "@StatMLPapers", "followers": "9,669", "datetime": "2019-02-14 01:56:48", "content_summary": "Extreme Tensoring for Low-Memory Preconditioning. (arXiv:1902.04620v1 [cs.LG]) https://t.co/v4VkRWwKjG"}}, "citation_id": "55431300", "queriedAt": "2020-06-03 23:17:03"}