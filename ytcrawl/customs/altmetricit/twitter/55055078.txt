{"citation_id": "55055078", "tab": "twitter", "twitter": {"1093442221263777793": {"author": "@arxivml", "followers": "787", "datetime": "2019-02-07 09:31:55", "content_summary": "\"Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing\", Andrea Galassi, M\u2026 https://t.co/X0blka67hq"}, "1093692847302168577": {"author": "@owltrainlab", "followers": "45", "datetime": "2019-02-08 02:07:49", "content_summary": "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing. (arXiv:1902.02181v1 [https://t.co/7dRtN4TFpi]) https://t.co/lCbdtX2ho1"}, "1239938447881560064": {"author": "@norihitoishida", "followers": "794", "datetime": "2020-03-17 15:35:37", "content_summary": "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/wYaEFyQCeb \u5225\u306eAttention\u306e\u30b5\u30fc\u30d9\u30a4\u3092\u8aad\u3093\u3060 \u4e0b\u6d41\u30bf\u30b9\u30af\u6bce\u306e\u5206\u985e\u3068\u304b\u304c\u3042\u3063\u3066\u826f\u3055\u307f"}, "1093577512775344129": {"author": "@KouroshMeshgi", "followers": "622", "datetime": "2019-02-07 18:29:31", "content_summary": "RT @arxiv_cscl: Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/JncsXAvOrd"}, "1093535154646204417": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-02-07 15:41:12", "content_summary": "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/JncsXAvOrd"}, "1094170258288578560": {"author": "@arnicas", "followers": "13,279", "datetime": "2019-02-09 09:44:52", "content_summary": "RT @bgoncalves: Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/8brhoDdhqg"}, "1093324887157030913": {"author": "@arxiv_cs_cl", "followers": "4,224", "datetime": "2019-02-07 01:45:40", "content_summary": "https://t.co/ciDGuJEG8H Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing. (arXiv:1902.02181v1 [https://t.co/HW5RVw4UkE]) #NLProc"}, "1122111164433850368": {"author": "@morioka", "followers": "825", "datetime": "2019-04-27 12:12:04", "content_summary": "RT @sei_shinagawa: \u3053\u308c\u65e2\u51fa\u3067\u3057\u305f\u304b\u306d\uff1f\u4e0d\u899a\u306b\u3082\u7b11\u3063\u3066\u3057\u307e\u3063\u305f Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing And\u2026"}, "1261258767179476992": {"author": "@ymym3412", "followers": "3,334", "datetime": "2020-05-15 11:34:57", "content_summary": "RT @norihitoishida: BERT\u5fdc\u7528\u52c9\u5f37\u4f1a\u3001 @ymym3412 \u3055\u3093\u306e\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u306e\u8a71\u3001\u9762\u767d\u3044\uff01 https://t.co/wYaEFyQCeb https://t.co/tkDfXMTGqu \u3042\u305f\u308a\u3092\u3061\u3083\u3093\u3068\u8aad\u307f\u76f4\u3057\u3066\u3001\u8272\u3093\u306aattention\u6a5f\u69cb\u3092\u52c9\u5f37\u2026"}, "1093325556207243265": {"author": "@SoEngineering", "followers": "61", "datetime": "2019-02-07 01:48:20", "content_summary": "#NewPaper: #arXiv https://t.co/8kHVi9UcuF https://t.co/7MRLCnpoS9 Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing. (arXiv:1902.02181v1 [https://t.co/HHSAgzbrV2])"}, "1266301256747773952": {"author": "@ayshaan98", "followers": "13", "datetime": "2020-05-29 09:32:01", "content_summary": "RT @arxiv_cscl: Attention in Natural Language Processing https://t.co/JncsXAed2D"}, "1267119826457296897": {"author": "@jmsl", "followers": "489", "datetime": "2020-05-31 15:44:43", "content_summary": "RT @arxiv_cscl: Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1266601684475580418": {"author": "@puneethmishra", "followers": "565", "datetime": "2020-05-30 05:25:48", "content_summary": "RT @arxiv_cscl: Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1266575174461259777": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-30 03:40:28", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1266264064088354819": {"author": "@pm_girl", "followers": "2,173", "datetime": "2020-05-29 07:04:13", "content_summary": "#Attention in Natural Language Processing. (arXiv:1902.02181v2 [https://t.co/JDgvDeZyeq] UPDATED) https://t.co/9dpX9DeEfq #artificialintelligence #ai"}, "1176998312697720832": {"author": "@reiver", "followers": "3,443", "datetime": "2019-09-25 23:13:59", "content_summary": "\"Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing\" by Andrea Galassi, Marco Lippi, Paolo Torroni https://t.co/e2zqLIy0EG (machine learning)"}, "1266243220855336963": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-29 05:41:24", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAed2D"}, "1267135964020789248": {"author": "@muktabh", "followers": "798", "datetime": "2020-05-31 16:48:50", "content_summary": "RT @arxiv_cscl: Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1266182811737784320": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-29 01:41:21", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1122104933052866560": {"author": "@sei_shinagawa", "followers": "2,136", "datetime": "2019-04-27 11:47:18", "content_summary": "\u3053\u308c\u65e2\u51fa\u3067\u3057\u305f\u304b\u306d\uff1f\u4e0d\u899a\u306b\u3082\u7b11\u3063\u3066\u3057\u307e\u3063\u305f Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing Andrea Galassi, Marco Lippi, Paolo Torroni https://t.co/eAknzocn1X"}, "1122131179266641920": {"author": "@Catechine0125", "followers": "598", "datetime": "2019-04-27 13:31:35", "content_summary": "RT @sei_shinagawa: \u3053\u308c\u65e2\u51fa\u3067\u3057\u305f\u304b\u306d\uff1f\u4e0d\u899a\u306b\u3082\u7b11\u3063\u3066\u3057\u307e\u3063\u305f Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing And\u2026"}, "1266271354581442561": {"author": "@arxiv_cs_cl", "followers": "4,224", "datetime": "2020-05-29 07:33:11", "content_summary": "https://t.co/LIJrLBaFmK Attention in Natural Language Processing. (arXiv:1902.02181v2 [https://t.co/HW5RVvNiW4] UPDATED) #NLProc"}, "1266741394418384902": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-30 14:40:58", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1267118886157246465": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-31 15:40:59", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1266378939133501440": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-29 14:40:41", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1093338810304397312": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2019-02-07 02:41:00", "content_summary": "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/JncsXAvOrd"}, "1266277644569477123": {"author": "@helioRocha_", "followers": "631", "datetime": "2020-05-29 07:58:11", "content_summary": "\"Attention in Natural Language Processing. (arXiv:1902.02181v2 [https://t.co/CE87ffDrlL] UPDATED)\" #arXiv https://t.co/S9TOVj7kYu"}, "1267125050458558469": {"author": "@peratham", "followers": "83", "datetime": "2020-05-31 16:05:28", "content_summary": "RT @arxiv_cscl: Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1093861316689842178": {"author": "@aGalaxy42", "followers": "0", "datetime": "2019-02-08 13:17:15", "content_summary": "What can be used to improve neural networks and improve their result? A simple yet effective tool is Neural Attention: https://t.co/ElTgxvakfs"}, "1093325289206267906": {"author": "@helioRocha_", "followers": "631", "datetime": "2019-02-07 01:47:16", "content_summary": "\"Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing. (arXiv:1902.02181v1 [https://t.co/CE87fflQud])\" #arXiv https://t.co/pT4ofWC7qq"}, "1093978040961519616": {"author": "@bgoncalves", "followers": "4,096", "datetime": "2019-02-08 21:01:04", "content_summary": "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/8brhoDdhqg"}, "1261254266410299393": {"author": "@norihitoishida", "followers": "794", "datetime": "2020-05-15 11:17:04", "content_summary": "BERT\u5fdc\u7528\u52c9\u5f37\u4f1a\u3001 @ymym3412 \u3055\u3093\u306e\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u306e\u8a71\u3001\u9762\u767d\u3044\uff01 https://t.co/wYaEFyQCeb https://t.co/tkDfXMTGqu \u3042\u305f\u308a\u3092\u3061\u3083\u3093\u3068\u8aad\u307f\u76f4\u3057\u3066\u3001\u8272\u3093\u306aattention\u6a5f\u69cb\u3092\u52c9\u5f37\u3057\u76f4\u3055\u306a\u304d\u3083\u306a\u30fc"}, "1266922707326775296": {"author": "@arxiv_cscl", "followers": "3,538", "datetime": "2020-05-31 02:41:26", "content_summary": "Attention in Natural Language Processing https://t.co/JncsXAvOrd"}, "1094017032855515136": {"author": "@sigitpurnomo", "followers": "2,668", "datetime": "2019-02-08 23:36:01", "content_summary": "RT @bgoncalves: Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing https://t.co/8brhoDdhqg"}, "1122107365149384705": {"author": "@SythonUK", "followers": "691", "datetime": "2019-04-27 11:56:58", "content_summary": "RT @sei_shinagawa: \u3053\u308c\u65e2\u51fa\u3067\u3057\u305f\u304b\u306d\uff1f\u4e0d\u899a\u306b\u3082\u7b11\u3063\u3066\u3057\u307e\u3063\u305f Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing And\u2026"}, "1093391489822973952": {"author": "@sigitpurnomo", "followers": "2,668", "datetime": "2019-02-07 06:10:20", "content_summary": "RT @arxiv_cs_cl: https://t.co/ciDGuJEG8H Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing. (a\u2026"}, "1266249200351633415": {"author": "@muktabh", "followers": "798", "datetime": "2020-05-29 06:05:09", "content_summary": "RT @arxiv_cscl: Attention in Natural Language Processing https://t.co/JncsXAed2D"}}, "completed": "1", "queriedAt": "2020-06-03 00:53:29"}