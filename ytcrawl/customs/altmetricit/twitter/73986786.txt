{"citation_id": "73986786", "completed": "1", "queriedAt": "2020-05-14 13:54:28", "tab": "twitter", "twitter": {"1226305520782364677": {"content_summary": "RT @fabtar: Great video explaining the paper \"Reformer: The Efficient Transformer\" by @ykilcher https://t.co/s3TSF3aXVO. Here the link of t\u2026", "followers": "29", "datetime": "2020-02-09 00:43:14", "author": "@chenlailin"}, "1255227870923886594": {"content_summary": "RT @LightOnIO: More than half of LightOn is attending the virtual @iclr_conf 2020. Impressive presentations from the #MachineLearning Commu\u2026", "followers": "121", "datetime": "2020-04-28 20:10:20", "author": "@Ch_Charis"}, "1217224834645622784": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "361", "datetime": "2020-01-14 23:19:50", "author": "@BioNLProc"}, "1230809769843286016": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,068 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-21 11:01:31", "author": "@PapersTrending"}, "1217957989555916800": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "150", "datetime": "2020-01-16 23:53:07", "author": "@y_yammt"}, "1231172230375231488": {"content_summary": "[5/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,130 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-22 11:01:48", "author": "@PapersTrending"}, "1219194570656952322": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "9", "datetime": "2020-01-20 09:46:51", "author": "@Amine01983926"}, "1235353190943649793": {"content_summary": "#Reformer performs on par with #Transformer models while being much more memory-efficient and much faster - https://t.co/XRQsN4NjZr - #DeepLearning #NLProc", "followers": "1,401", "datetime": "2020-03-04 23:55:27", "author": "@andreaesseci"}, "1229722579650347010": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 2,686 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-18 11:01:24", "author": "@PapersTrending"}, "1219260484844580865": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "615", "datetime": "2020-01-20 14:08:47", "author": "@KouroshMeshgi"}, "1217116401888112648": {"content_summary": "Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the sequence. Nikita Kitaev, @lukaszkaiser, and Anselm Levskaya show how to decrease memory footprint. Very promising for biological data!", "followers": "1,165", "datetime": "2020-01-14 16:08:57", "author": "@KevinKaichuang"}, "1217145599734272000": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "740", "datetime": "2020-01-14 18:04:59", "author": "@JoaoVictor_AC"}, "1224857748338364416": {"content_summary": "Reformer: An efficient transformer model. #NLProc #MachineLearning Paper:https://t.co/AULxW4gxmH", "followers": "712", "datetime": "2020-02-05 00:50:18", "author": "@Seanku"}, "1219188531811233792": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "100", "datetime": "2020-01-20 09:22:52", "author": "@Pedro18_Neto"}, "1217954662558486529": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "143", "datetime": "2020-01-16 23:39:54", "author": "@n_kats_"}, "1237666974856957952": {"content_summary": "\u8efd\u91cf\u30d0\u30fc\u30b8\u30e7\u30f3\u306eTransformer\u3002Google Research\u3002 / https://t.co/GH1jo2S6sp", "followers": "145", "datetime": "2020-03-11 09:09:36", "author": "@meta_takoroy"}, "1238510790153580544": {"content_summary": "The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing those. Try our OPUs out @ https://t.co/qIVUt7THD2 Great video on the Reformer algorithm https://t.co/D6SJ7cBTCl by @ykilcher https", "followers": "1,282", "datetime": "2020-03-13 17:02:37", "author": "@LightOnIO"}, "1219372732455976961": {"content_summary": "\u201cInformer\u201d is inevitable, right? A licky boom boom down", "followers": "336", "datetime": "2020-01-20 21:34:48", "author": "@TheLeanAcademic"}, "1219402946267492352": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "24", "datetime": "2020-01-20 23:34:52", "author": "@ncooper57"}, "1221750253642272768": {"content_summary": "[4/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 429 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-27 11:02:14", "author": "@PapersTrending"}, "1255212765331030017": {"content_summary": "RT @LightOnIO: More than half of LightOn is attending the virtual @iclr_conf 2020. Impressive presentations from the #MachineLearning Commu\u2026", "followers": "412", "datetime": "2020-04-28 19:10:18", "author": "@iacopo_poli"}, "1217221531761287168": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "5,808", "datetime": "2020-01-14 23:06:42", "author": "@AndrewLBeam"}, "1255163516656508930": {"content_summary": "More than half of LightOn is attending the virtual @iclr_conf 2020. Impressive presentations from the #MachineLearning Community. Great talk by Nikita Kitaev on his paper \"Reformer: the Efficient Transformer\" written with @lukaszkaiser and @anselmlevskaya.", "followers": "1,282", "datetime": "2020-04-28 15:54:36", "author": "@LightOnIO"}, "1236814213122682880": {"content_summary": "\u6642\u9593\u3042\u308b\u3068\u304d\u306b\u8aad\u3082\u3046 https://t.co/aBxa33NF30", "followers": "304", "datetime": "2020-03-09 00:41:01", "author": "@stkt_KU"}, "1220611191279480833": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "13", "datetime": "2020-01-24 07:36:00", "author": "@Bharath09095865"}, "1217924950159347712": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,212", "datetime": "2020-01-16 21:41:50", "author": "@hiroaki_tanaka"}, "1240052052639846405": {"content_summary": "RT @theomitsa: BY GOOGLE RESEARCH REFORMER: THE EFFICIENT TRANSFORMER https://t.co/WT6GZiilqz", "followers": "25", "datetime": "2020-03-17 23:07:03", "author": "@davidrivera_94"}, "1219400969902886912": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "50", "datetime": "2020-01-20 23:27:01", "author": "@Dmitry77162374"}, "1238519326212308995": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "1,282", "datetime": "2020-03-13 17:36:32", "author": "@LightOnIO"}, "1223573897355776000": {"content_summary": "\u5927\u4f53\u306f\u7406\u89e3\u3002 attention\u306e\u512a\u5148\u9806\u4f4d\u4ed8\u3051\u3068RevNet\u306b\u3088\u308b\u5b66\u7fd2\u6642\u306e\u30e1\u30e2\u30ea\u306e\u52b9\u7387\u5316\u3063\u3066\u3053\u3068\u306e\u3088\u3046\u3002 \u307e\u3068\u3081\u306a\u304c\u3089\u3082\u3046\u5c11\u3057\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u78ba\u8a8d\u3057\u307e\u3059\u30fc https://t.co/VECqM9whzy", "followers": "145", "datetime": "2020-02-01 11:48:44", "author": "@arts_lib"}, "1219863345131663360": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "442", "datetime": "2020-01-22 06:04:20", "author": "@AdrianB82"}, "1219309881066512386": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "2,808", "datetime": "2020-01-20 17:25:03", "author": "@Al_Grigor"}, "1219218216444018689": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "302", "datetime": "2020-01-20 11:20:49", "author": "@subhobrata1"}, "1222837277870710785": {"content_summary": "[4/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 493 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-30 11:01:40", "author": "@PapersTrending"}, "1226307510933454848": {"content_summary": "Reformer: Transformer with hashing-based attention & reversible layers for long sequences / less compute (Kitaev, Kaiser, Levskaya): https://t.co/62JQj2hz8U", "followers": "219", "datetime": "2020-02-09 00:51:08", "author": "@erik_nijkamp"}, "1238529305514332160": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "716", "datetime": "2020-03-13 18:16:11", "author": "@Laurent_Daudet"}, "1219466641848713216": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "23", "datetime": "2020-01-21 03:47:58", "author": "@deehzee"}, "1219309091190796290": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "18", "datetime": "2020-01-20 17:21:55", "author": "@shrey_yay"}, "1219166576617885697": {"content_summary": "From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens using only 16GB of memory! It leverages locality-sensitive hashing (LSH) & reversible residual layers to use memory efficiently", "followers": "17,007", "datetime": "2020-01-20 07:55:37", "author": "@DataScienceNIG"}, "1226225501083037696": {"content_summary": "RT @fabtar: Great video explaining the paper \"Reformer: The Efficient Transformer\" by @ykilcher https://t.co/s3TSF3aXVO. Here the link of t\u2026", "followers": "12", "datetime": "2020-02-08 19:25:16", "author": "@2000Qiu"}, "1217946416875622400": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "824", "datetime": "2020-01-16 23:07:08", "author": "@morioka"}, "1219323681475846146": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "111", "datetime": "2020-01-20 18:19:54", "author": "@AIplus_Akure"}, "1220090702811496449": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "9", "datetime": "2020-01-22 21:07:46", "author": "@limitless_ai"}, "1221921174789263365": {"content_summary": "RT @realitybuck: RT please: Are you a Learner? #Trax \u2014 your path to advanced deep learning Trax helps you understand and explore advanced\u2026", "followers": "659", "datetime": "2020-01-27 22:21:24", "author": "@critic_sam"}, "1217890150698471424": {"content_summary": "@davidpfahler Was taken there from: Reformer: The Efficient Transformer. https://t.co/ymoqJOzZqH", "followers": "2,195", "datetime": "2020-01-16 19:23:33", "author": "@baykenney"}, "1219938077201833986": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 1,393 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-22 11:01:17", "author": "@PapersTrending"}, "1218951700108759040": {"content_summary": "Improving the time and memory efficiency of Transformer models when dealing with large context windows #image #completion #document #summarization https://t.co/sIjhCfH30M / https://t.co/kOJArOQgRT / https://t.co/qpgdU6r3ny", "followers": "201", "datetime": "2020-01-19 17:41:46", "author": "@francescospiu"}, "1219623440564379649": {"content_summary": "\u5927\u898f\u6a21\u306aTransformer\u30e2\u30c7\u30eb\u306f\u3001\u591a\u304f\u306e\u30bf\u30b9\u30af\u3067state-of-the-art\u3092\u9054\u6210\u3059\u308b\u304c\u3001\u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306f\u7279\u306b\u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u5834\u5408\u3001\u975e\u5e38\u306b\u30b3\u30b9\u30c8\u304c\u304b\u304b\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u52b9\u7387\u3092\u6539\u5584\u3059\u308b2\u3064\u306e\u624b\u6cd5\u3092\u7d39\u4ecb\u3002 https://t.co/Y64HjFD7eO", "followers": "10", "datetime": "2020-01-21 14:11:02", "author": "@38_76"}, "1230447415544688640": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,031 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-20 11:01:39", "author": "@PapersTrending"}, "1219231990039269376": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "218", "datetime": "2020-01-20 12:15:33", "author": "@AssistedEvolve"}, "1222179345390800897": {"content_summary": "Reformer: The Efficient Transformer https://t.co/RIQ5LsUP0I", "followers": "52", "datetime": "2020-01-28 15:27:17", "author": "@agesmundo"}, "1217225409353244673": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "3,147", "datetime": "2020-01-14 23:22:07", "author": "@adamgdunn"}, "1217956038537707520": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "3", "datetime": "2020-01-16 23:45:22", "author": "@luda_green"}, "1216930865902690304": {"content_summary": "\"Reformer: The Efficient Transformer\", Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya https://t.co/n5DEmTJATk", "followers": "772", "datetime": "2020-01-14 03:51:42", "author": "@arxivml"}, "1219316962154729478": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "194", "datetime": "2020-01-20 17:53:12", "author": "@jastner109"}, "1220300484134801408": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 1,455 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-23 11:01:22", "author": "@PapersTrending"}, "1218463398578573314": {"content_summary": "\"Reformer um\u00ed zpracovat cel\u00e9 rom\u00e1ny najednou a na jedin\u00e9m za\u0159\u00edzen\u00ed...\" https://t.co/iq02y4Slt9 #pdf https://t.co/SyKp8kpuSe", "followers": "88", "datetime": "2020-01-18 09:21:26", "author": "@z45VuM03LhWsYwB"}, "1220112304693547008": {"content_summary": "Cool new #AI paper: \"Reformer: The Efficient Transformer\" \ud83d\udc49 Makes the transformer more memory efficient with little performance loss & enables longer sequences. Key ingredients: Local sensitive hashing, chunking & reversible layers https://t.co/", "followers": "2,314", "datetime": "2020-01-22 22:33:36", "author": "@Standerwahre"}, "1218751835407835137": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "5,314", "datetime": "2020-01-19 04:27:35", "author": "@HubBucket"}, "1226135217288728578": {"content_summary": "RT @fabtar: Great video explaining the paper \"Reformer: The Efficient Transformer\" by @ykilcher https://t.co/s3TSF3aXVO. Here the link of t\u2026", "followers": "818", "datetime": "2020-02-08 13:26:30", "author": "@deepgradient"}, "1219245733171298307": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "290", "datetime": "2020-01-20 13:10:09", "author": "@dannyehb"}, "1219477394802823169": {"content_summary": "RT @SOdaibo: Interesting. At a glance, they take Transformer from quadratic to N*logN time by approximating the attention step: i.e. the so\u2026", "followers": "1,406", "datetime": "2020-01-21 04:30:42", "author": "@CharlesXavieer_"}, "1235559212953915397": {"content_summary": "Reformer : The Efficient Transformer which performs on par with Transformer has 2 major improvements. It uses locality-sensitive hashing instead of dot-product attention and reversible residual layers instead of standard residuals. #NLP #DeepLearning ht", "followers": "59", "datetime": "2020-03-05 13:34:06", "author": "@rahulbakshee"}, "1218668923878617096": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "11,930", "datetime": "2020-01-18 22:58:07", "author": "@Rosenchild"}, "1246752860462710784": {"content_summary": "Deep Learning\u5206\u91ce\u306f\u73fe\u5728\u3001Attention\u3068LSH\u304c\u71b1\u3044\u3089\u3057\u3044\u3002 Reformer: The efficient Transformer https://t.co/wQKoxwJ4Py SLIDE: Sub-LInear Deep learning Engine https://t.co/FCvhJqyOsx", "followers": "117", "datetime": "2020-04-05 10:53:40", "author": "@CollapsedBass"}, "1249848506275303424": {"content_summary": "Reformer: The Efficient Transformer https://t.co/5Qvs1DyOM1", "followers": "4", "datetime": "2020-04-13 23:54:39", "author": "@MarkoIvanov11"}, "1219591564239855617": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "456", "datetime": "2020-01-21 12:04:22", "author": "@PerthMLGroup"}, "1217945338289348608": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,091", "datetime": "2020-01-16 23:02:51", "author": "@tatatrade"}, "1229958987682131968": {"content_summary": "Reformer: The Efficient Transformer https://t.co/kjE1lzBTDY", "followers": "3,500", "datetime": "2020-02-19 02:40:48", "author": "@arxiv_cscl"}, "1219169263799492608": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "300", "datetime": "2020-01-20 08:06:18", "author": "@ukpai_best"}, "1220566926943932416": {"content_summary": "[2001.04451] Reformer: The Efficient Transformer https://t.co/y07mAf0gzX", "followers": "1,724", "datetime": "2020-01-24 04:40:07", "author": "@whisponchan"}, "1223216280284663809": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "4,841", "datetime": "2020-01-31 12:07:42", "author": "@ouledamor"}, "1219242201294622720": {"content_summary": "Read this paper... Reformer: The Efficient Transformer https://t.co/yKFEQsEHK6 Very interesting #ML method for working with understanding more than a paragraph at a time... #MachineLearning #AI #ArtificialIntelligence", "followers": "318", "datetime": "2020-01-20 12:56:07", "author": "@nelslindahl"}, "1219058616327499779": {"content_summary": "2020/01/13 \u6295\u7a3f 4\u4f4d LG(Machine Learning) Reformer: The Efficient Transformer https://t.co/PS0TVWxhzJ 12 Tweets 34 Retweets 93 Favorites", "followers": "695", "datetime": "2020-01-20 00:46:37", "author": "@arxiv_pop"}, "1237333008694247424": {"content_summary": "[10/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,784 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-03-10 11:02:32", "author": "@PapersTrending"}, "1229489574168203265": {"content_summary": "A student pointed out this cool paper to me: \"Reformer: The Efficient Transformer\", an architecture that can handle much longer input sequences than standard transformers. https://t.co/Ouow2scU20 (Nikita Kitaev, @lukaszkaiser, @anselmlevskaya -- @GoogleAI", "followers": "1,944", "datetime": "2020-02-17 19:35:31", "author": "@suzan"}, "1216924862091948033": {"content_summary": "Reformer: The Efficient Transformer. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya https://t.co/N6eczVahwB", "followers": "311", "datetime": "2020-01-14 03:27:51", "author": "@arxiv_cs_LG"}, "1217925886592212992": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "190", "datetime": "2020-01-16 21:45:33", "author": "@nskm_m"}, "1232986446367272961": {"content_summary": "RT @PapersTrending: [10/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,326 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "292", "datetime": "2020-02-27 11:10:51", "author": "@Shujian_Liu"}, "1218105941369008128": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "405", "datetime": "2020-01-17 09:41:02", "author": "@HinkleyPoint1"}, "1218050835235557377": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "962", "datetime": "2020-01-17 06:02:04", "author": "@jnhwkim"}, "1220662898206396417": {"content_summary": "[3/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 1,497 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-24 11:01:28", "author": "@PapersTrending"}, "1219437383717769217": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "590", "datetime": "2020-01-21 01:51:42", "author": "@codekee"}, "1226125662240591878": {"content_summary": "Great video explaining the paper \"Reformer: The Efficient Transformer\" by @ykilcher https://t.co/s3TSF3aXVO. Here the link of the paper: https://t.co/MgJZhI9Ba2", "followers": "3,137", "datetime": "2020-02-08 12:48:32", "author": "@fabtar"}, "1216913220071567361": {"content_summary": "Reformer: The Efficient Transformer https://t.co/kjE1lzBTDY", "followers": "3,500", "datetime": "2020-01-14 02:41:35", "author": "@arxiv_cscl"}, "1217116970258173953": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "2,071", "datetime": "2020-01-14 16:11:13", "author": "@EricSchles"}, "1230085068854562817": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 2,889 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-19 11:01:48", "author": "@PapersTrending"}, "1216907355776081920": {"content_summary": "https://t.co/fUPEv8M334 Reformer: The Efficient Transformer. (arXiv:2001.04451v1 [cs.LG]) #NLProc", "followers": "4,200", "datetime": "2020-01-14 02:18:17", "author": "@arxiv_cs_cl"}, "1240048097146867712": {"content_summary": "BY GOOGLE RESEARCH REFORMER: THE EFFICIENT TRANSFORMER https://t.co/WT6GZiilqz", "followers": "3,413", "datetime": "2020-03-17 22:51:19", "author": "@theomitsa"}, "1221387712353140736": {"content_summary": "[3/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 398 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-26 11:01:37", "author": "@PapersTrending"}, "1221621954504990720": {"content_summary": "RT please: Are you a Learner? #Trax \u2014 your path to advanced deep learning Trax helps you understand and explore advanced #DeepLearning. https://t.co/eap4myBPuw Paper Reformer: The Efficient Transformer: https://t.co/6YIc3qGtea", "followers": "2,670", "datetime": "2020-01-27 02:32:25", "author": "@realitybuck"}, "1233050003758821376": {"content_summary": "Next AUEB NLP Group meeting, Tue March 3, 17:15-19:00, IPLab (https://t.co/ZetbrvHKuB): \"Reformer: The Efficient Transformer\", Kitaev et al. (ICLR 2020). Study the paper before the meeting. All welcome. https://t.co/SupG7oWEgZ https://t.co/RIaw0gsm3b", "followers": "505", "datetime": "2020-02-27 15:23:24", "author": "@AUEBNLPGroup"}, "1218065161749237760": {"content_summary": "Holy sheet https://t.co/YuruFoBoxo", "followers": "51", "datetime": "2020-01-17 06:58:59", "author": "@OrixAY"}, "1219282842838085634": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "27", "datetime": "2020-01-20 15:37:37", "author": "@mahesh21aug"}, "1217012176126468096": {"content_summary": "#ICLR2020 Reformer: The Efficient Transformer. (arXiv:2001.04451v1 [cs\\.LG]) https://t.co/PSlfeVejLF", "followers": "1,310", "datetime": "2020-01-14 09:14:48", "author": "@arxiv_in_review"}, "1218545575290757120": {"content_summary": "Reformer an improvement over transformer by replacing a) dot-product attention with locality-sensitive hashing and b) residual layers with reversible residual layers #NLProc https://t.co/j7zTXSJ6WN https://t.co/3ulrOpJGQc", "followers": "21", "datetime": "2020-01-18 14:47:59", "author": "@kalyan_kpl"}, "1217925150638624768": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,333", "datetime": "2020-01-16 21:42:38", "author": "@alfredplpl"}, "1218488623902920705": {"content_summary": "[5/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 732 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-18 11:01:41", "author": "@PapersTrending"}, "1219420004484972544": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "361", "datetime": "2020-01-21 00:42:39", "author": "@sazi27"}, "1222112542207741952": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 459 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-28 11:01:50", "author": "@PapersTrending"}, "1255222907875266560": {"content_summary": "RT @LightOnIO: More than half of LightOn is attending the virtual @iclr_conf 2020. Impressive presentations from the #MachineLearning Commu\u2026", "followers": "4,891", "datetime": "2020-04-28 19:50:36", "author": "@IgorCarron"}, "1221023021625856006": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "117", "datetime": "2020-01-25 10:52:28", "author": "@CollapsedBass"}, "1231534665045041157": {"content_summary": "[9/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,155 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-23 11:01:59", "author": "@PapersTrending"}, "1223189361400766464": {"content_summary": "Abstract\u8aad\u3093\u3060\u611f\u3058\u3060\u3068Reformer\u3067\u306fTransformer\u306e\u52b9\u7387\u5316\u306b\u3042\u305f\u3063\u3066\u3001locality-sensitive hashing\u3068reversible residual layers\u3092\u7528\u3044\u3066\u3044\u308b\u3089\u3057\u3044\u3002 \u8a73\u3057\u304f\u306f\u4eca\u304b\u3089\u8aad\u3080\u3002 https://t.co/FceXo5GFVV", "followers": "145", "datetime": "2020-01-31 10:20:44", "author": "@arts_lib"}, "1219156163637309440": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "0", "datetime": "2020-01-20 07:14:14", "author": "@Sam09lol"}, "1223037087424733185": {"content_summary": "RT @AkiraTOSEI: https://t.co/rDTHiG68zC Propose self-attention layer Reformer that only calculate important attention by sorting similar va\u2026", "followers": "554", "datetime": "2020-01-31 00:15:39", "author": "@FBWM8888"}, "1219475597094600704": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "783", "datetime": "2020-01-21 04:23:33", "author": "@muktabh"}, "1238524283355365376": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "4,891", "datetime": "2020-03-13 17:56:14", "author": "@IgorCarron"}, "1217992190900572160": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "294", "datetime": "2020-01-17 02:09:02", "author": "@rose_miura"}, "1219369981768556544": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "522", "datetime": "2020-01-20 21:23:53", "author": "@langdon"}, "1219122765418024961": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "99", "datetime": "2020-01-20 05:01:32", "author": "@treasured_write"}, "1217921763431993344": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,847", "datetime": "2020-01-16 21:29:10", "author": "@katsuhitosudoh"}, "1219179394951974913": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "344", "datetime": "2020-01-20 08:46:33", "author": "@davlanade"}, "1219363222211395587": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "1,912", "datetime": "2020-01-20 20:57:01", "author": "@OluwafemiAlo"}, "1236328170967744512": {"content_summary": "#localsensitivehashing iclassical algorithmic trick used to reduce complexity in search problems such as clustering. Good to see it applied to #deeplearning . Found 2 recent papers with this trick 1/2 #reformers optimised #tranformers for #nlp https://t.", "followers": "511", "datetime": "2020-03-07 16:29:40", "author": "@antoniogulli"}, "1216938156634841089": {"content_summary": "Just read this on the arXiv Reformer: The Efficient Transformer https://t.co/O8B02S1oWW", "followers": "2,195", "datetime": "2020-01-14 04:20:40", "author": "@baykenney"}, "1219221148233273345": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "419", "datetime": "2020-01-20 11:32:28", "author": "@flasisi"}, "1222474990517440513": {"content_summary": "[2/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 485 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-29 11:02:04", "author": "@PapersTrending"}, "1238779665478815745": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "1,025", "datetime": "2020-03-14 10:51:02", "author": "@desertnaut"}, "1238639569630683136": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "0", "datetime": "2020-03-14 01:34:20", "author": "@Sam09lol"}, "1219250448705753089": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "392", "datetime": "2020-01-20 13:28:54", "author": "@OyAttia"}, "1234433817353605121": {"content_summary": "[4/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,577 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-03-02 11:02:11", "author": "@PapersTrending"}, "1218531269992230917": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "165", "datetime": "2020-01-18 13:51:08", "author": "@ReformReality"}, "1223033881361649665": {"content_summary": "https://t.co/rDTHiG68zC Propose self-attention layer Reformer that only calculate important attention by sorting similar value by locality sensitive hash (LSH) and making layers reversible. The calculation amount and memory amount were reduced to L logL w", "followers": "1,309", "datetime": "2020-01-31 00:02:54", "author": "@AkiraTOSEI"}, "1219389608145956865": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "6", "datetime": "2020-01-20 22:41:52", "author": "@stefon369"}, "1238529555930939399": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "1,624", "datetime": "2020-03-13 18:17:11", "author": "@sylvaingigan"}, "1238520162359410688": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "121", "datetime": "2020-03-13 17:39:51", "author": "@Ch_Charis"}, "1226138414149300224": {"content_summary": "RT @fabtar: Great video explaining the paper \"Reformer: The Efficient Transformer\" by @ykilcher https://t.co/s3TSF3aXVO. Here the link of t\u2026", "followers": "409", "datetime": "2020-02-08 13:39:13", "author": "@jainnitk"}, "1218408939026448384": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "201", "datetime": "2020-01-18 05:45:02", "author": "@tksmml"}, "1238518659955527680": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "117", "datetime": "2020-03-13 17:33:53", "author": "@VictoireLouisC"}, "1217920313620434944": {"content_summary": "Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u91cf\u3092O(L^2)\u2192O(L log L)\uff0e(2) \u5404\u5c64\u306e\u51fa\u529b\u3092\u8a18\u61b6\u305b\u305a\u9006\u4f1d\u64ad\u6642\u306b\u518d\u8a08\u7b97\u3057\u3066\u7701\u30e1\u30e2\u30ea\u5316\uff0e\u30d6\u30ed\u30b0\u3067L=1M\u306e\u7cfb\u5217\u309216GB(1GPU)\u3067\u51e6\u7406\u53ef\u80fd\u3068\u5831\u544a\uff0eICLR20 https://t.co/Y16HxzKRCB", "followers": "2,439", "datetime": "2020-01-16 21:23:25", "author": "@kyoun"}, "1219322063976050688": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "4,891", "datetime": "2020-01-20 18:13:28", "author": "@IgorCarron"}, "1218821788500709376": {"content_summary": "#NLP @Google #Reformer instead of #Transformer @plevy @IEML_ @LouisvanBeurden first impression? https://t.co/8luLObhEZc", "followers": "1,783", "datetime": "2020-01-19 09:05:33", "author": "@kmesch1"}, "1217936658206605312": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1", "datetime": "2020-01-16 22:28:22", "author": "@haradatm"}, "1232984321067245568": {"content_summary": "[10/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,326 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-27 11:02:24", "author": "@PapersTrending"}, "1238688851356291072": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "1,857", "datetime": "2020-03-14 04:50:10", "author": "@MattChallacombe"}, "1233346500391993344": {"content_summary": "[5/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,442 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-28 11:01:34", "author": "@PapersTrending"}, "1219191653421998081": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "68", "datetime": "2020-01-20 09:35:16", "author": "@L_A_Scott_"}, "1255171360772337671": {"content_summary": "RT @LightOnIO: More than half of LightOn is attending the virtual @iclr_conf 2020. Impressive presentations from the #MachineLearning Commu\u2026", "followers": "0", "datetime": "2020-04-28 16:25:47", "author": "@Sam09lol"}, "1217109370435260418": {"content_summary": "Reformer: The Efficient Transformer https://t.co/kjE1lzBTDY", "followers": "3,500", "datetime": "2020-01-14 15:41:01", "author": "@arxiv_cscl"}, "1217938671967469568": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "4,289", "datetime": "2020-01-16 22:36:22", "author": "@Nakada_itpro"}, "1217079301574516736": {"content_summary": "RT @arxiv_in_review: #ICLR2020 Reformer: The Efficient Transformer. (arXiv:2001.04451v1 [cs\\.LG]) https://t.co/PSlfeVejLF", "followers": "6", "datetime": "2020-01-14 13:41:32", "author": "@northShepherd_"}, "1217953013970563072": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,152", "datetime": "2020-01-16 23:33:21", "author": "@jd_mashiro"}, "1219362921064599553": {"content_summary": "Transformers on steroids \ud83d\udcaa", "followers": "159", "datetime": "2020-01-20 20:55:49", "author": "@HamzatOluwaseun"}, "1236970436405141504": {"content_summary": "[7/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,784 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-03-09 11:01:48", "author": "@PapersTrending"}, "1217886985467187202": {"content_summary": "Check out Google's new transformer, the Reformer. #AI #ArtificialIntelligence #NLProc #DeepLearning #MachineLearning Paper: https://t.co/bBsSFryL2q Colab (text generation): https://t.co/UVXUhXV8YS", "followers": "928", "datetime": "2020-01-16 19:10:59", "author": "@Quantum_Stat"}, "1217138323900137473": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "782", "datetime": "2020-01-14 17:36:04", "author": "@seeliglab"}, "1217951851284287488": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,055", "datetime": "2020-01-16 23:28:44", "author": "@stealthinu"}, "1219449280525676544": {"content_summary": "Interesting. At a glance, they take Transformer from quadratic to N*logN time by approximating the attention step: i.e. the softmax{<Q,K>} step instead of calculating it fully. This works since most values in the resulting weight distribution are ~0.", "followers": "1,484", "datetime": "2020-01-21 02:38:59", "author": "@SOdaibo"}, "1217929693967441922": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "17", "datetime": "2020-01-16 22:00:41", "author": "@ryokkie"}, "1217171220392828928": {"content_summary": "RT @KevinKaichuang: Transformers are great at modeling sequences, but the attention layer requires L^2 memory, where L is the length of the\u2026", "followers": "1,658", "datetime": "2020-01-14 19:46:47", "author": "@biochemistries"}, "1217962280509300736": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "1,223", "datetime": "2020-01-17 00:10:10", "author": "@knok"}, "1233708995526955009": {"content_summary": "[2/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,503 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-02-29 11:02:00", "author": "@PapersTrending"}, "1234796200727019520": {"content_summary": "[6/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,624 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-03-03 11:02:10", "author": "@PapersTrending"}, "1226333803599163392": {"content_summary": "RT @erik_nijkamp: Reformer: Transformer with hashing-based attention & reversible layers for long sequences / less compute (Kitaev, Kaiser,\u2026", "followers": "104", "datetime": "2020-02-09 02:35:37", "author": "@alfo_512"}, "1219431975506927616": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "889", "datetime": "2020-01-21 01:30:13", "author": "@bargava"}, "1221025408868548609": {"content_summary": "[5/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 1,513 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-25 11:01:57", "author": "@PapersTrending"}, "1219401152455831552": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "201", "datetime": "2020-01-20 23:27:44", "author": "@tksmml"}, "1216983520029003777": {"content_summary": "Reformer: The Efficient Transformer. https://t.co/0g8LkU1qzP", "followers": "5,454", "datetime": "2020-01-14 07:20:56", "author": "@StatsPapers"}, "1219049518982787072": {"content_summary": "Reformer: The Efficient Transformer https://t.co/fOpzWY9yHm https://t.co/9KjWRhDSnR", "followers": "43", "datetime": "2020-01-20 00:10:28", "author": "@PcaNews"}, "1219213310496735232": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 1,099 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-20 11:01:19", "author": "@PapersTrending"}, "1217949307539808258": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "783", "datetime": "2020-01-16 23:18:37", "author": "@K_Ryuichirou"}, "1238532152180060174": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "412", "datetime": "2020-03-13 18:27:30", "author": "@iacopo_poli"}, "1219285256433307653": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "44", "datetime": "2020-01-20 15:47:13", "author": "@_kanokkorn_"}, "1219575638425788417": {"content_summary": "[1/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 1,325 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-21 11:01:05", "author": "@PapersTrending"}, "1219189996197838848": {"content_summary": "RT @DataScienceNIG: From Transformer to Reformer. @GoogleAI & @BerkeleyNLP take NLP to next level! Reformer can process millions of tokens\u2026", "followers": "423", "datetime": "2020-01-20 09:28:41", "author": "@FunminiyiOlada1"}, "1239056374518222848": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "57", "datetime": "2020-03-15 05:10:34", "author": "@jonathdong"}, "1219437703135158274": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "174", "datetime": "2020-01-21 01:52:59", "author": "@suren_h"}, "1238531121228533760": {"content_summary": "RT @LightOnIO: The new Reformer algorithm in NLP features random projections based LSH. Guess what? LightOn OPUs are very good at computing\u2026", "followers": "150", "datetime": "2020-03-13 18:23:24", "author": "@BonnyArmelle"}, "1220270838752120832": {"content_summary": "RT @Standerwahre: Cool new #AI paper: \"Reformer: The Efficient Transformer\" \ud83d\udc49 Makes the transformer more memory efficient with little perf\u2026", "followers": "2,314", "datetime": "2020-01-23 09:03:34", "author": "@Standerwahre"}, "1236734101798809600": {"content_summary": "@gwern Not that I know of, but I think someone will soon. You have to get around the memory constraints, but people are working on stuff like that. See e.g. @giannis_daras 's paper: https://t.co/OgB8euelH3, or the reformer paper: https://t.co/EcKQPa6G5y", "followers": "5,919", "datetime": "2020-03-08 19:22:41", "author": "@gstsdn"}, "1220581719687847936": {"content_summary": "Megatron / Bert\u307f\u305f\u3044\u306a\u5727\u5012\u7684\u306b\u3067\u304b\u3044\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u30e2\u30c7\u30eb\u306e\u30b5\u30a4\u30ba\u3092\u5c0f\u3055\u304f\u3059\u308b\u65b9\u6cd5\u3000https\">#Transformer\">https://t.co/7bmEiUK7bS\u3000#Transformer #\u6a5f\u68b0\u5b66\u7fd2 #BERT", "followers": "156", "datetime": "2020-01-24 05:38:53", "author": "@kedbreak136"}, "1217958556139261952": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "99", "datetime": "2020-01-16 23:55:23", "author": "@KamuiRoeru"}, "1219337010323820544": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "265", "datetime": "2020-01-20 19:12:52", "author": "@eaxitect"}, "1228067803535728654": {"content_summary": "@owulveryck @arxiv \u201cGame changer\u201d is quite subjective, and \u201cfor months\u201d rather vague \ud83d\ude09 Here are a few papers with ideas I found interesting last year: _ https://t.co/ccD4jBmDOy Reformer: The Efficient Transformer - https://t.co/VWxLwoU8Sp Generative Teac", "followers": "221", "datetime": "2020-02-13 21:25:55", "author": "@fdasilva59fr"}, "1219277174244626435": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "4,526", "datetime": "2020-01-20 15:15:06", "author": "@CShorten30"}, "1218291803461472261": {"content_summary": "From Transformer to Reformer News: https://t.co/7tHypqeNEh Github: https://t.co/AofFxp75xL Colab: https://t.co/Blpwsyd88X Paper: https://t.co/aTqw85d0zu", "followers": "112", "datetime": "2020-01-17 21:59:35", "author": "@9meo"}, "1218376237883777024": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "41,142", "datetime": "2020-01-18 03:35:06", "author": "@prakashadvani"}, "1219138724132507648": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "5,437", "datetime": "2020-01-20 06:04:56", "author": "@Daniel_Lerch"}, "1240049426435096578": {"content_summary": "RT @theomitsa: BY GOOGLE RESEARCH REFORMER: THE EFFICIENT TRANSFORMER https://t.co/WT6GZiilqz", "followers": "123", "datetime": "2020-03-17 22:56:36", "author": "@adarshbhandaryp"}, "1218850977580748800": {"content_summary": "[2/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 996 \u2b50 - \ud83d\udcc4 https://t.co/ZNVdRolEMZ - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-01-19 11:01:32", "author": "@PapersTrending"}, "1230155434339831808": {"content_summary": "Reformer: The Efficient Transformer https://t.co/kjE1lzBTDY", "followers": "3,500", "datetime": "2020-02-19 15:41:25", "author": "@arxiv_cscl"}, "1219092413454536704": {"content_summary": "Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L^2) to O(L log L), where L is the length of the sequence\" paper: https://t.co/3o1scnoCCT code: https://t.c", "followers": "2,927", "datetime": "2020-01-20 03:00:55", "author": "@evolvingstuff"}, "1219314078692200448": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "141", "datetime": "2020-01-20 17:41:44", "author": "@waydegilliam"}, "1219369546340958210": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "1,215", "datetime": "2020-01-20 21:22:09", "author": "@devnag"}, "1221285905727705088": {"content_summary": "RT @kyoun: Reformer: The Efficient Transformer (Google) https://t.co/V2yRUEAhYS https://t.co/dnTmdZKufs \u52b9\u7387\u7684\u306aTransformer\uff0e(1) \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092LSH\u306b\u3057\u3066\u8a08\u7b97\u2026", "followers": "394", "datetime": "2020-01-26 04:17:04", "author": "@DomLachinger"}, "1229489664840609797": {"content_summary": "RT @suzan: A student pointed out this cool paper to me: \"Reformer: The Efficient Transformer\", an architecture that can handle much longer\u2026", "followers": "579", "datetime": "2020-02-17 19:35:53", "author": "@TMLeiden"}, "1234071414480740352": {"content_summary": "[2/10] \ud83d\udcc8 - Reformer: The Efficient Transformer - 3,555 \u2b50 - \ud83d\udcc4 https://t.co/jU1Pmtbmtx - \ud83d\udd17 https://t.co/RYjBtNYZJk", "followers": "222", "datetime": "2020-03-01 11:02:07", "author": "@PapersTrending"}, "1219278309198090240": {"content_summary": "RT @evolvingstuff: Reformer: The Efficient Transformer \"we replace dot-product attention by one that uses locality-sensitive hashing, chan\u2026", "followers": "274", "datetime": "2020-01-20 15:19:36", "author": "@ghhosh"}, "1230039588162412545": {"content_summary": "https://t.co/fUPEv8M334 Reformer: The Efficient Transformer. (arXiv:2001.04451v2 [cs.LG] UPDATED) #NLProc", "followers": "4,200", "datetime": "2020-02-19 08:01:05", "author": "@arxiv_cs_cl"}}}