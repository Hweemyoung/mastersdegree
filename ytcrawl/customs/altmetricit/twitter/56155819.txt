{"tab": "twitter", "completed": "1", "twitter": {"1101006917449715713": {"author": "@arxivml", "followers": "793", "datetime": "2019-02-28 06:31:19", "content_summary": "\"Distributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Data\", Richeng Jin, Xiaofan He, Huaiy\u2026 https://t.co/aGVH14B9Gu"}, "1101211137935237120": {"author": "@StatsPapers", "followers": "5,454", "datetime": "2019-02-28 20:02:49", "content_summary": "Distributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Data. https://t.co/mVCrDY3apQ"}, "1100938030490234882": {"author": "@StatMLPapers", "followers": "9,763", "datetime": "2019-02-28 01:57:35", "content_summary": "Distributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Data. (arXiv:1902.10336v1 [cs.LG]) https://t.co/4gMDPR0u3N"}, "1100960095968677888": {"author": "@arxiv_cs_LG", "followers": "324", "datetime": "2019-02-28 03:25:16", "content_summary": "Distributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Data. Richeng Jin, Xiaofan He, and Huaiyu Dai https://t.co/gV7b0txQTY"}}, "citation_id": "56155819", "queriedAt": "2020-06-04 00:41:31"}