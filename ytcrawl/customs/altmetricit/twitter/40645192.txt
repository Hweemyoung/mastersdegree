{"citation_id": "40645192", "tab": "twitter", "completed": "1", "queriedAt": "2020-05-14 17:10:47", "twitter": {"1046442181093937153": {"followers": "4,750", "datetime": "2018-09-30 16:50:32", "author": "@vcheplygina", "content_summary": "@goodfellow_ian Not sure which of the authors are on Twitter but @lvdmaaten recently used a 3 billion dataset scraped from Instagram https://t.co/g3wVd2wrPY"}, "991952310245801984": {"followers": "613", "datetime": "2018-05-03 08:07:15", "author": "@ComputerPapers", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining. https://t.co/xvDc9RTWrf"}, "1030329566324314112": {"followers": "255", "datetime": "2018-08-17 05:44:45", "author": "@Mpanga96", "content_summary": "RT @Grananqvist: #100DaysOfMLCode Day 14 - Reading \"Exploring the Limits of Weakly Supervised Pretraining\" by @Facebook. Great paper on tra\u2026"}, "1035671031124393985": {"followers": "1,501", "datetime": "2018-08-31 23:29:50", "author": "@DrZeeshanZia", "content_summary": "RT @Grananqvist: #100DaysOfMLCode Day 14 - Reading \"Exploring the Limits of Weakly Supervised Pretraining\" by @Facebook. Great paper on tra\u2026"}, "1065009940866187264": {"followers": "516", "datetime": "2018-11-20 22:32:11", "author": "@martin_schrimpf", "content_summary": "@quocleix since the abstract claims \"achieve a new state-of-the-art 84.3% top-1\" -- didn't Mahajan et al. already report 85.4% (https://t.co/LpPdC9JmSj)?"}, "1068280600270786560": {"followers": "1,903", "datetime": "2018-11-29 23:08:37", "author": "@ak11", "content_summary": "very small\u306fhttps://t.co/yILGzvRnmJ\u3001small\u306fhttps://t.co/Cg5IwZ9C1f\u3088\u308a\u3002Facebook\u2026\u3002"}, "1067263832496594944": {"followers": "12,756", "datetime": "2018-11-27 03:48:21", "author": "@jaguring1", "content_summary": "RT @imenurok: Exploring the Limits of Weakly Supervised Pretraining https://t.co/SRxXYsdilA \u30a4\u30f3\u30b9\u30bf\u304b\u3089\u96c6\u3081\u305f17k\u30bf\u30b0\u4ed8\u304d\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4e8b\u524d\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3067\u3001ImageNet\u30675%\u2026"}, "991970831612887040": {"followers": "49", "datetime": "2018-05-03 09:20:50", "author": "@kzmssk1599", "content_summary": "\uff13\uff15\u5104 https://t.co/J5uhoyOB4p"}, "1030036916878622721": {"followers": "5,338", "datetime": "2018-08-16 10:21:52", "author": "@100DaysOfMLCode", "content_summary": "RT @Grananqvist: #100DaysOfMLCode Day 14 - Reading \"Exploring the Limits of Weakly Supervised Pretraining\" by @Facebook. Great paper on tra\u2026"}, "1049382761063165953": {"followers": "108", "datetime": "2018-10-08 19:35:21", "author": "@ErickMuzart", "content_summary": "Transfer learning, sem limites! https://t.co/urDrXjvQnL https://t.co/cW5LGG7DvX"}, "1049429879043719168": {"followers": "231", "datetime": "2018-10-08 22:42:35", "author": "@cmoryah", "content_summary": "meodeus @_@"}, "992156916926287872": {"followers": "4,080", "datetime": "2018-05-03 21:40:17", "author": "@arxiv_cscv", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3JV1Xb"}, "1145552888669847552": {"followers": "5,115", "datetime": "2019-07-01 04:41:06", "author": "@omarsar0", "content_summary": "I like the idea of leveraging weakly supervised training. I used a similar technique as part of my dissertation (applied to text). SoTA on Top-1 ImageNet. Would love to see something similar in NLP. PyTorch code is available: https://t.co/TKu4XJKTCk http"}, "992166378856566784": {"followers": "369", "datetime": "2018-05-03 22:17:52", "author": "@ms3rdzone", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "995007787905245186": {"followers": "1,286", "datetime": "2018-05-11 18:28:37", "author": "@heghbalz", "content_summary": "RT @cosminnegruseri: The cost of a large scale training deep learning paper would be >100k on Google Compute Cloud (3.5B instagram pics, 33\u2026"}, "993669172818206720": {"followers": "38", "datetime": "2018-05-08 01:49:26", "author": "@dyama1813", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "992659194145857536": {"followers": "376", "datetime": "2018-05-05 06:56:09", "author": "@learnlinksfeed", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/rZAMMZeztp"}, "1143821432260104193": {"followers": "220", "datetime": "2019-06-26 10:00:55", "author": "@PapersTrending", "content_summary": "[3/10] \ud83d\udcc8 - Exploring the Limits of Weakly Supervised Pretraining - 103 \u2b50 - \ud83d\udcc4 https://t.co/iK34ZF6snO - \ud83d\udd17 https://t.co/uD7KgRumwg"}, "1070079916606140417": {"followers": "411", "datetime": "2018-12-04 22:18:27", "author": "@michalwols", "content_summary": "@srchvrs In that case you want billions of images like google and facebook as shown in papers like https://t.co/LrDtilOn5R"}, "1004855759618834432": {"followers": "1,522", "datetime": "2018-06-07 22:40:57", "author": "@datapriestess", "content_summary": "Even if you are not a techie this is an interesting (and important) read. Bottom line: every time we add a hashtag to a photo (on Facebook or Instagram) we are labeling the data for the company. For free. For them to use however they want... https://t.co/Y"}, "1145560704566603777": {"followers": "880", "datetime": "2019-07-01 05:12:10", "author": "@RichmanRonald", "content_summary": "RT @omarsar0: I like the idea of leveraging weakly supervised training. I used a similar technique as part of my dissertation (applied to t\u2026"}, "992236124545339392": {"followers": "2,042", "datetime": "2018-05-04 02:55:01", "author": "@dosei_sanga", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "1145410294417678336": {"followers": "75", "datetime": "2019-06-30 19:14:29", "author": "@gevero", "content_summary": "RT @CVCND: Paper: Exploring the Limits of Weakly Supervised Pretraining https://t.co/o063osWpkV"}, "1143184810430611456": {"followers": "382", "datetime": "2019-06-24 15:51:12", "author": "@adantro", "content_summary": "A \"hard\" example on why our social media data is so valuable... https://t.co/A77XaOfdIw A difficult aspect to training most #SOTA recognition systems is the requirement for labelled data. Humans need to pre-label massive training data sets for algorithms t"}, "991965879670730752": {"followers": "526", "datetime": "2018-05-03 09:01:10", "author": "@hirotomusiker", "content_summary": "https://t.co/UNL5OBhka5 Exploring the Limits of Weakly Supervised Pretraining, from Facebook Research. Is COCO AP: 45.2% state-of-the-art? (COCO challenge '17 winner: 52.6%) Is the test dataset different, or is it the bells-and-whistles thing?"}, "1054370545444184066": {"followers": "16,954", "datetime": "2018-10-22 13:55:01", "author": "@kazoo04", "content_summary": "RT @imenurok: 32x48d \u2192https://t.co/SRxXYsdilA"}, "1143047981047996416": {"followers": "5,710", "datetime": "2019-06-24 06:47:30", "author": "@rschu", "content_summary": "Facebook uses Instagram photos for AI training.\ud83d\udca1 They automate #DeepLearning training using Insta posts and their hashtags and achieve the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%. \ud83d\udc4f Paper: https://t.co/dRstOxVfm0 GitHub: https://t."}, "1239598678869852163": {"followers": "233", "datetime": "2020-03-16 17:05:30", "author": "@astellon_music", "content_summary": "\u5148\u884c\u7814\u7a76\u3068\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30b0\u30e9\u30e0\u306e\u30bf\u30b0\u3092\u5229\u7528\u3057\u305f\u4f8b\u304c\u3042\u308a\u307e\u3059 https://t.co/bMLaLACsVO"}, "1067270958757568512": {"followers": "84", "datetime": "2018-11-27 04:16:40", "author": "@ashi__no", "content_summary": "RT @imenurok: Exploring the Limits of Weakly Supervised Pretraining https://t.co/SRxXYsdilA \u30a4\u30f3\u30b9\u30bf\u304b\u3089\u96c6\u3081\u305f17k\u30bf\u30b0\u4ed8\u304d\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4e8b\u524d\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3067\u3001ImageNet\u30675%\u2026"}, "991855084005937153": {"followers": "4,080", "datetime": "2018-05-03 01:40:54", "author": "@arxiv_cscv", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3JV1Xb"}, "991981026300911616": {"followers": "768", "datetime": "2018-05-03 10:01:21", "author": "@arxivml", "content_summary": "\"Exploring the Limits of Weakly Supervised Pretraining\", Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming \u2026 https://t.co/fxm7EhFtff"}, "1067259664486719488": {"followers": "2,042", "datetime": "2018-11-27 03:31:47", "author": "@imenurok", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/SRxXYsdilA \u30a4\u30f3\u30b9\u30bf\u304b\u3089\u96c6\u3081\u305f17k\u30bf\u30b0\u4ed8\u304d\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4e8b\u524d\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3067\u3001ImageNet\u30675%\u8fd1\u3044\u6539\u5584\u53e9\u304d\u51fa\u3057\u3066\u308bResNeXt101 32-48d\u307b\u3093\u3068\u5bb9\u8d66\u306a\u3044\u304b\u3089\u2026"}, "1236268100657590273": {"followers": "1,503", "datetime": "2020-03-07 12:30:58", "author": "@NEUBIAS_COST", "content_summary": "RT @martinjones78: .@Lastu21 from @FIMM_UH on using #WeaklySupervisedLearning for learning representations of microscopy images (Following\u2026"}, "1145237055493947393": {"followers": "129", "datetime": "2019-06-30 07:46:06", "author": "@KrishaMehta2", "content_summary": "RT @CVCND: Paper: Exploring the Limits of Weakly Supervised Pretraining https://t.co/o063osWpkV"}, "996373150152601600": {"followers": "2,042", "datetime": "2018-05-15 12:54:05", "author": "@dosei_sanga", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/SRxXYsdilA"}, "1054359993284083713": {"followers": "2,042", "datetime": "2018-10-22 13:13:06", "author": "@imenurok", "content_summary": "32x48d \u2192https://t.co/SRxXYsdilA"}, "995098287412834304": {"followers": "3,733", "datetime": "2018-05-12 00:28:14", "author": "@SinaBahram", "content_summary": "RT @cosminnegruseri: The cost of a large scale training deep learning paper would be >100k on Google Compute Cloud (3.5B instagram pics, 33\u2026"}, "991970432621277184": {"followers": "18,214", "datetime": "2018-05-03 09:19:15", "author": "@hillbig", "content_summary": "SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u306f\u4f4d\u7f6e\u691c\u51fa\u306b\u60aa\u5f71\u97ff https://t.co/BBlyYNRx5Y"}, "991995070197841921": {"followers": "470", "datetime": "2018-05-03 10:57:09", "author": "@yasuokajihei", "content_summary": "RT @hillbig: Pretraining by predicting a hashtag from an image using up to 3.5 billion images showed large improvements in image recognitio\u2026"}, "1235899437538476034": {"followers": "429", "datetime": "2020-03-06 12:06:02", "author": "@Nicolas26538817", "content_summary": "RT @martinjones78: .@Lastu21 from @FIMM_UH on using #WeaklySupervisedLearning for learning representations of microscopy images (Following\u2026"}, "1035666638585114624": {"followers": "5,338", "datetime": "2018-08-31 23:12:23", "author": "@100DaysOfMLCode", "content_summary": "RT @Grananqvist: #100DaysOfMLCode Day 14 - Reading \"Exploring the Limits of Weakly Supervised Pretraining\" by @Facebook. Great paper on tra\u2026"}, "994980871382843393": {"followers": "859", "datetime": "2018-05-11 16:41:40", "author": "@cosminnegruseri", "content_summary": "The cost of a large scale training deep learning paper would be >100k on Google Compute Cloud (3.5B instagram pics, 336 GPUs in 22days) https://t.co/U4YJxLc4Yh via https://t.co/benvoqtkek https://t.co/6HTn3HGCKn"}, "1163329412466483200": {"followers": "309", "datetime": "2019-08-19 05:58:40", "author": "@vivnat", "content_summary": "This is in direct contrast to work like \"Exploring the limits of weakly supervised pretraining\" - https://t.co/4v88irE0Yg which showed imagenet accuracy plateau with increasing data and model capacity. Suggests for vision we need richer pretraining tasks t"}, "1208121710353903616": {"followers": "223", "datetime": "2019-12-20 20:27:16", "author": "@AndrewFerlitsch", "content_summary": "Catching up on previous research on weakly supervised learning. Read paper: Exploring the Limits of Weakly Supervised Pretraining (FB, 2018) - https://t.co/zPVwCDHeK3"}, "991972516653494272": {"followers": "18,214", "datetime": "2018-05-03 09:27:32", "author": "@hillbig", "content_summary": "Pretraining by predicting a hashtag from an image using up to 3.5 billion images showed large improvements in image recognition tasks. findings: 1) label engineering is needed 2) observed underfitting 3) visual variety is important https://t.co/BBlyYNRx5Y"}, "991991588581658627": {"followers": "189", "datetime": "2018-05-03 10:43:19", "author": "@nskm_m", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "996741217798049792": {"followers": "126", "datetime": "2018-05-16 13:16:39", "author": "@alexginsca", "content_summary": "RT @cosminnegruseri: The cost of a large scale training deep learning paper would be >100k on Google Compute Cloud (3.5B instagram pics, 33\u2026"}, "992051246331654154": {"followers": "4,080", "datetime": "2018-05-03 14:40:23", "author": "@arxiv_cscv", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3KcCOJ"}, "1143459025943781377": {"followers": "220", "datetime": "2019-06-25 10:00:50", "author": "@PapersTrending", "content_summary": "[3/10] \ud83d\udcc8 - Exploring the Limits of Weakly Supervised Pretraining - 87 \u2b50 - \ud83d\udcc4 https://t.co/iK34ZF6snO - \ud83d\udd17 https://t.co/uD7KgRumwg"}, "991994759953575936": {"followers": "470", "datetime": "2018-05-03 10:55:55", "author": "@yasuokajihei", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "995009148755562496": {"followers": "421", "datetime": "2018-05-11 18:34:02", "author": "@zacharynado", "content_summary": "RT @cosminnegruseri: The cost of a large scale training deep learning paper would be >100k on Google Compute Cloud (3.5B instagram pics, 33\u2026"}, "991853228848570369": {"followers": "3,856", "datetime": "2018-05-03 01:33:32", "author": "@BrundageBot", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten https://t.co/lsl5bVQnfU"}, "991844988102193152": {"followers": "759", "datetime": "2018-05-03 01:00:47", "author": "@M157q_News_RSS", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining. (arXiv:1805.00932v1 [https://t.co/2Nuv3dYG1c]) https://t.co/R0rHQnUuBX State-of-the-art visual percepti"}, "1144183902896082945": {"followers": "220", "datetime": "2019-06-27 10:01:15", "author": "@PapersTrending", "content_summary": "[6/10] \ud83d\udcc8 - Exploring the Limits of Weakly Supervised Pretraining - 130 \u2b50 - \ud83d\udcc4 https://t.co/iK34ZF6snO - \ud83d\udd17 https://t.co/uD7KgRumwg"}, "991981209616924673": {"followers": "305", "datetime": "2018-05-03 10:02:05", "author": "@subhobrata1", "content_summary": "RT @arxiv_cscv: Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3JV1Xb"}, "992042282013659136": {"followers": "1,269", "datetime": "2018-05-03 14:04:45", "author": "@FelixGoldberg1", "content_summary": "RT @arxiv_cscv: Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3JV1Xb"}, "1030036914475282432": {"followers": "961", "datetime": "2018-08-16 10:21:52", "author": "@Grananqvist", "content_summary": "#100DaysOfMLCode Day 14 - Reading \"Exploring the Limits of Weakly Supervised Pretraining\" by @Facebook. Great paper on transfer learning! Transfer learning should not be done with ImageNet anymore, but with 3.5 Billion @Instagram images \ud83d\ude02\ud83d\ude02\ud83d\ude02 https://t.co/"}, "1008068131120668672": {"followers": "472", "datetime": "2018-06-16 19:25:46", "author": "@fusionconfusion", "content_summary": "RT @datapriestess: Even if you are not a techie this is an interesting (and important) read. Bottom line: every time we add a hashtag to a\u2026"}, "1024500292275064832": {"followers": "3,775", "datetime": "2018-08-01 03:41:18", "author": "@asset25", "content_summary": "RT @curtlanglotz: Large datasets with noisy labels can produce AI algorithms with high accuracy. Empiric evidence: https://t.co/mnb1jazZTS\u2026"}, "994263885749235712": {"followers": "236", "datetime": "2018-05-09 17:12:37", "author": "@rui_mashita", "content_summary": "\u624b\u52d5\u3067\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3084\u30c7\u30fc\u30bf\u306e\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\u3092\u3057\u3066\u3044\u306a\u3044\u3001\u91ce\u826fInstagram\u306e\u753b\u50cf35\u5104\u679a\u3068hashtag\u304b\u3089336GPU\u3092\u4f7f\u3063\u3066\u5f31\u6559\u5e2b\u5b66\u7fd2\u3002 \u305d\u306e\u5f8cImageNet\u3067finetune\u3057\u3066\u7cbe\u5ea6\u304c85.4% Exploring the Limits of Weakly Supervised Pretraining. https://t.co/F3dW3VRGUD"}, "991972580335611904": {"followers": "433", "datetime": "2018-05-03 09:27:47", "author": "@hs_heddy", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "1024249757231923202": {"followers": "1,034", "datetime": "2018-07-31 11:05:46", "author": "@CCIPD_Case", "content_summary": "RT @curtlanglotz: Large datasets with noisy labels can produce AI algorithms with high accuracy. Empiric evidence: https://t.co/mnb1jazZTS\u2026"}, "1235891805494939648": {"followers": "1,569", "datetime": "2020-03-06 11:35:42", "author": "@martinjones78", "content_summary": ".@Lastu21 from @FIMM_UH on using #WeaklySupervisedLearning for learning representations of microscopy images (Following from https://t.co/2kNUiR0BUr) #neubiasBordeaux #Neubias https://t.co/BbmWxe9jCA"}, "1145137294497714179": {"followers": "13,627", "datetime": "2019-06-30 01:09:41", "author": "@CVCND", "content_summary": "Paper: Exploring the Limits of Weakly Supervised Pretraining https://t.co/o063osWpkV"}, "992076986204831744": {"followers": "392", "datetime": "2018-05-03 16:22:40", "author": "@tez600", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "994375839402766336": {"followers": "903", "datetime": "2018-05-10 00:37:29", "author": "@wk77", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "991998996150345728": {"followers": "232", "datetime": "2018-05-03 11:12:45", "author": "@bamboo4031", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "1030067626356162560": {"followers": "651", "datetime": "2018-08-16 12:23:54", "author": "@C_Glastonbury", "content_summary": "The recent Instagram pretraining paper from Facebook: https://t.co/QXAkNu4oJI Great work, but is this a case in which mixture of softmaxes would work better? Is the number of hashtags predicted (through softmax) greater than the last FC layer, resulting in"}, "1145186086555021312": {"followers": "67", "datetime": "2019-06-30 04:23:34", "author": "@dksdc", "content_summary": "RT @CVCND: Paper: Exploring the Limits of Weakly Supervised Pretraining https://t.co/o063osWpkV"}, "1145877276531265536": {"followers": "13", "datetime": "2019-07-02 02:10:06", "author": "@xyc234", "content_summary": "RT @omarsar0: I like the idea of leveraging weakly supervised training. I used a similar technique as part of my dissertation (applied to t\u2026"}, "1035666637289123840": {"followers": "1,286", "datetime": "2018-08-31 23:12:22", "author": "@heghbalz", "content_summary": "RT @Grananqvist: #100DaysOfMLCode Day 14 - Reading \"Exploring the Limits of Weakly Supervised Pretraining\" by @Facebook. Great paper on tra\u2026"}, "1024370525949308928": {"followers": "23", "datetime": "2018-07-31 19:05:39", "author": "@tiqerlu", "content_summary": "RT @curtlanglotz: Large datasets with noisy labels can produce AI algorithms with high accuracy. Empiric evidence: https://t.co/mnb1jazZTS\u2026"}, "1046481209445044224": {"followers": "76,673", "datetime": "2018-09-30 19:25:37", "author": "@NandoDF", "content_summary": "RT @vcheplygina: @goodfellow_ian Not sure which of the authors are on Twitter but @lvdmaaten recently used a 3 billion dataset scraped from\u2026"}, "995090499961868289": {"followers": "207", "datetime": "2018-05-11 23:57:17", "author": "@TechRonic9876", "content_summary": "RT @cosminnegruseri: The cost of a large scale training deep learning paper would be >100k on Google Compute Cloud (3.5B instagram pics, 33\u2026"}, "991976563599015936": {"followers": "3,338", "datetime": "2018-05-03 09:43:37", "author": "@Dmiag2008", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "1151093875982790656": {"followers": "1,490", "datetime": "2019-07-16 11:39:00", "author": "@tawatawara", "content_summary": "\u898b\u8fd4\u3059\u3068 \"Exploring the Limits of Weakly Supervised Pretraining\" \u3060\u304b\u3089\u30bf\u30a4\u30c8\u30eb\u305d\u3093\u306a\u306b\u4f3c\u3066\u306a\u3044\ud83d\ude05 \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u4e8b\u524d\u5b66\u7fd2\u3068\u3044\u3046\u70b9\u304c\u540c\u3058\u3067\u3001\u65b0\u3057\u3044\u65b9\u306e\u8ad6\u6587\u306f\u3088\u308a\u30bf\u30a4\u30c8\u30eb\u306b\u73fe\u308f\u308c\u3066\u308b\u3002 https://t.co/5Qwx3g6V5N"}, "1236311186121134081": {"followers": "704", "datetime": "2020-03-07 15:22:10", "author": "@fab_cordelieres", "content_summary": "RT @martinjones78: .@Lastu21 from @FIMM_UH on using #WeaklySupervisedLearning for learning representations of microscopy images (Following\u2026"}, "1145671550936522752": {"followers": "14", "datetime": "2019-07-01 12:32:37", "author": "@StefanGerlachSK", "content_summary": "RT @CVCND: Paper: Exploring the Limits of Weakly Supervised Pretraining https://t.co/o063osWpkV"}, "992039044518264832": {"followers": "512", "datetime": "2018-05-03 13:51:54", "author": "@tn0bu", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "991971482371698689": {"followers": "2,904", "datetime": "2018-05-03 09:23:25", "author": "@raven_38_", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "1024128135489413122": {"followers": "3,956", "datetime": "2018-07-31 03:02:29", "author": "@curtlanglotz", "content_summary": "Large datasets with noisy labels can produce AI algorithms with high accuracy. Empiric evidence: https://t.co/mnb1jazZTS. Figure 3 shows 10% label noise causes only 1% decrease in accuracy. 50% label noise causes only 6% decrease in accuracy. #CMIMI18 h"}, "992400186726023168": {"followers": "195", "datetime": "2018-05-04 13:46:57", "author": "@AmirRosenfeld", "content_summary": "https://t.co/YGFdVQsAg1 \"...This data source has the advantage of being large and continuously growing, as well as \u201cfree\u201d from an annotation perspective since no manual labeling is required...\" - I disagree - this *is* manually labeled by the people who pr"}, "1143048079144366081": {"followers": "8,280", "datetime": "2019-06-24 06:47:53", "author": "@SantchiWeb", "content_summary": "RT @rschu: Facebook uses Instagram photos for AI training.\ud83d\udca1 They automate #DeepLearning training using Insta posts and their hashtags and\u2026"}, "992284691632828416": {"followers": "79", "datetime": "2018-05-04 06:08:00", "author": "@junjungoal", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}, "992036305205977090": {"followers": "4,080", "datetime": "2018-05-03 13:41:00", "author": "@arxiv_cscv", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3JV1Xb"}, "991839959035338752": {"followers": "4,080", "datetime": "2018-05-03 00:40:48", "author": "@arxiv_cscv", "content_summary": "Exploring the Limits of Weakly Supervised Pretraining https://t.co/ykoi3JV1Xb"}, "992015781553618946": {"followers": "119", "datetime": "2018-05-03 12:19:27", "author": "@toto_toilet", "content_summary": "RT @hillbig: SNS\u4e0a\u306e\u6700\u592735\u5104\u679a\u306e\u753b\u50cf\u3092\u4f7f\u3063\u3066\u753b\u50cf\u304b\u3089\u30cf\u30c3\u30b7\u30e5\u30bf\u30b0\u3092\u4e88\u6e2c\u30bf\u30b9\u30af\u3092\u4e8b\u524d\u5b66\u7fd2\u3068\u3057\u3001\u753b\u50cf\u5206\u985e\u306b\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7cbe\u5ea6\u306f\u9806\u5f53\u306b\u6539\u5584\u3055\u308c\u305f 1)\u30bf\u30b0\u3068\u30e9\u30d9\u30eb\u306e\u4e00\u81f4\u5ea6\u304c\u91cd\u8981 2)\u5b66\u7fd2\u30c7\u30fc\u30bf\u304c\u591a\u304f\u30e2\u30c7\u30eb\u306f\u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c8 3)\u753b\u8cea\u306e\u591a\u69d8\u6027\u304c\u91cd\u8981 4)\u30bf\u30b0\u4e88\u6e2c\u3060\u3051\u3067\u2026"}}}