[{"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "wKzNfjSpXRk", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "9", "viewCount": "934", "favoriteCount": "0"}, "contentDetails": {"duration": "PT18M49S"}, "snippet": {"channelId": "UCUp6Pd9fx8_UX7S38Ih_JqA", "description": "October 11, 2015 - ENCODE and the Common Fund-supported Roadmap Epigenomics Mapping Centers (REMC) held a workshop at the American Society of Human Genetics (ASHG) annual meeting. More: http://www.genome.gov/27563006", "defaultAudioLanguage": "en", "channelTitle": "National Human Genome Research Institute", "publishedAt": "2015-10-24T14:23:10Z", "title": "The ENCODE Encyclopedia and Variant Annotation Using RegulomeDB and HaploReg - Jill E. Moore"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "LIrjLfdvLGE", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "6", "viewCount": "367", "favoriteCount": "0"}, "contentDetails": {"duration": "PT41M49S"}, "snippet": {"channelId": "UCdDzrluRaryED8SAHgJ0l9Q", "description": "Adam Blattler Ph.D., Research Scientist at Active Motif, discusses the variety of tools and services we have developed to overcome many of these challenges.", "channelTitle": "Active Motif", "publishedAt": "2015-07-10T17:00:57Z", "title": "[WEBINAR] Advances in ChIP-Based Technologies for Profiling Epigenomic Landscapes", "tags": ["ChIP", "Epigenetics", "Epigenomics", "Technology", "Gene Regulatory Network", "Gene", "Biotechnology", "Active Motif", "MiRNA", "Gene Regulation", "Antibodies", "Assays", "Kits", "Research", "Scientist", "Profiling", "Next Generation Sequencing", "Chromatin", "Chromatin Immunoprecipitation", "ChIP webinar", "ChIP-Seq webinar", "science webinar"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "5virhl152rM", "statistics": {"dislikeCount": "2", "commentCount": "4", "likeCount": "54", "viewCount": "3795", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H14M21S"}, "snippet": {"channelId": "UCTWjRu1gC9kvmlwPaM9HLkQ", "description": "Ricardo N Ramirez, Harvard Medical School\nSTATegra NGS & Data Integration Summer School \nNavarrabiomed, 25th September 2018", "defaultAudioLanguage": "en", "channelTitle": "STATegra Channel", "publishedAt": "2018-10-25T00:54:27Z", "title": "NGS & Data Integration School 2018 Introduction to ATAC-seq", "tags": ["NGS", "STATegra", "Ricardo Ramirez", "ATAC-seq", "Tutorial", "Summer School"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "Mo2154QzhY4", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "3", "viewCount": "169", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H53M23S"}, "snippet": {"channelId": "UCKbkfKk65PZyRCzUwXOJung", "description": "This is the second module of the Pathway and Network Analysis of -omics Data 2013 workshop hosted by the Canadian Bioinformatics Workshops in Toronto. This session is by Wyeth Wasserman.", "defaultAudioLanguage": "en", "channelTitle": "Bioinformatics DotCa", "publishedAt": "2018-07-20T19:53:27Z", "title": "Regulatory Network Analysis", "tags": ["CBW", "Network Analysis"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "acnjajGILl0", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "2", "viewCount": "144", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H19M43S"}, "snippet": {"channelId": "UCVqWctrxf5-oBIM1lqOIt-A", "description": "Computational & Comparative Genomics 2015\nAttend a CSHL meeting: http://bit.ly/cshlmtg\nTrain at a CSHL course: http://bit.ly/cshlcourses\nSubscribe to receive new video notifications: http://bit.ly/2aVrXaM\n\nCONNECT WITH US\nBlog: http://bit.ly/cshlcxblog\nTwitter: http://bit.ly/cshlmctwitter\nInstagram: http://bit.ly/cshlmcinsta\nFacebook: http://bit.ly/cshlmcfb\nWebsite: http://bit.ly/cshlmtgcrs", "channelTitle": "CSHL Leading Strand", "publishedAt": "2016-09-27T16:46:01Z", "title": "CCG 2015 Hawkins ChromatinState1 AnalysisOf HistoneModifications", "tags": ["CSHL", "Computational & Comparative Genomics 2015"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "l6Hy0GE1RdM", "statistics": {"dislikeCount": "0", "likeCount": "1", "viewCount": "19", "favoriteCount": "0"}, "contentDetails": {"duration": "PT43M37S"}, "snippet": {"channelId": "UCN9kqT7pfOzZddPJHqWSuyA", "description": "Keynote: Combinatorial Regulation of Enhancers during Drosophila Development - Julia Zeitlinger - RECOMB/RSG 2015", "channelTitle": "ISCB", "publishedAt": "2020-01-16T20:39:08Z", "title": "Keynote: Combinatorial Regulation of Enhancers during... - Julia Zeitlinger - RECOMB/RSG 2015", "tags": ["Computational Biology", "ISCB", "Biology", "Computer Science"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "hAQUadYWNDo", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "6", "viewCount": "861", "favoriteCount": "0"}, "contentDetails": {"duration": "PT50M56S"}, "snippet": {"channelId": "UCW1C2xOfXsIzPgjXyuhkw9g", "description": "Jonathan Pritchard, Stanford University\nComputation-Intensive Probabilistic and Statistical Methods for Large-Scale Population Genomics\nhttp://simons.berkeley.edu/talks/jonathan-pritchard-2014-02-18", "channelTitle": "Simons Institute", "publishedAt": "2014-02-26T18:04:06Z", "title": "Genetic Variation in Gene Regulation", "tags": ["Simons Institute", "Evolutionary Biology", "UC Berkeley", "computer science", "theory of computing", "Jonathan Pritchard"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "3JBa00ZIUpo", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "6", "viewCount": "448", "favoriteCount": "0"}, "contentDetails": {"duration": "PT41M"}, "snippet": {"channelId": "UCGzuiiLdQZu9wxDNJHO_JnA", "description": "Computational Genomics Summer Institute 2017\n\nTutorial: \"Computational approaches for deciphering the non-coding human genome\"\nJason Ernst, University of California, Los Angeles\n\nInstitute for Pure and Applied Mathematics, UCLA\nJuly 14, 2017\n\nFor more information: http://computationalgenomics.bioinformatics.ucla.edu/", "defaultLanguage": "en", "channelTitle": "Institute for Pure & Applied Mathematics (IPAM)", "publishedAt": "2017-08-04T17:30:01Z", "title": "Jason Ernst: \"Computational approaches for deciphering the non-coding human genome\"", "tags": ["computational genomics", "genomics", "bioinformatics", "cgsi", "ucla", "ipam", "jason ernst", "computation", "non-coding human genome", "human genome", "genetics", "genome"], "defaultAudioLanguage": "en"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "wDjLQk3hvFk", "statistics": {"dislikeCount": "5", "commentCount": "20", "likeCount": "227", "viewCount": "9170", "favoriteCount": "0"}, "contentDetails": {"duration": "PT39M30S"}, "snippet": {"channelId": "UCsvqEZBO-kNmwuDBbKbfL6A", "description": "https://www.ibiology.org/genetics-and-gene-regulation/epigenomics-lncrnas\n\nIn this talk, Dr. Howard Chang describes epigenomic approaches pioneered by his lab and the role of long-noncoding RNAs (lncRNAs) in regulating gene expression.\n\nIn Part 1 of this series, Dr. Howard Chang introduces epigenomics, the study of DNA regulatory mechanisms that determine which genes are turned on or off in cells at specific times. The epigenome integrates signals from the environment to modify expression of the DNA blueprint inherited from an individual\u2019s parents. Chang\u2019s lab has pioneered techniques to map the landscape of chromatin, the complex of DNA, RNA and protein that organizes the genome and regulates gene expression. One example is ATAC, the Assay of Transposase Accessible Chromatin, which uses a bacterial transposase to mark open chromatin and identify genes that are likely turned \u201con\u201d.\n\nIn his Part 2, Chang introduces long noncoding RNAs, or lncRNAs. As their name suggests, lncRNAs are not translated into proteins, and initially their functions were poorly understood. Chang\u2019s group has developed technologies to better understand the function of lncRNAs. For example, his lab characterized the protein partners that interact with Xist, a canonical lncRNA that mediates X chromosome inactivation. They found that the protein Spen is necessary for X chromosome silencing. Interestingly, Spen has likely been co-opted by mammalian cells to inactivate the X chromosome via viral mimicry.\n\nIn his Part 3, Chang reminds us that every lncRNA gene has its own set of DNA regulatory elements, such as enhancers and promoters. These regulatory elements can confer functionality to lncRNA genes. Chang shares the research story of a mysterious lncRNA known as PVT1, which is frequently co-amplified with the proto-oncogene MYC in human cancers. His group found that PVT1 promoter activity is inversely correlated with MYC expression - when one is up, the other is down. Finally, Chang shows that the PVT1 and MYC promoters compete for four enhancers located within the PVT1 gene locus.\n\nSpeaker Biography:\nHoward Chang is the Virginia and D. K. Ludwig Professor of Cancer Genomics and a professor of dermatology and genetics at Stanford University. He is an Investigator of the Howard Hughes Medical Institute. He studied biochemistry at Harvard University and completed a doctorate in biology at the Massachusetts Institute of Technology and medical degree at Harvard Medical School. The Chang lab pioneers new technologies for probing the function of the non-coding genome.\n \nhttps://med.stanford.edu/changlab.html", "defaultAudioLanguage": "en", "channelTitle": "iBiology", "publishedAt": "2020-01-28T22:22:50Z", "title": "Howard Chang (Stanford, HHMI) 1: Epigenomic Technologies", "tags": ["epigenomics", "genomics", "long noncoding RNAs", "lncRNAs", "PVT1", "MYC", "Xist", "X chromosome inactivation", "Spen", "bioinformatics", "ATAC-seq", "Assay of Transposase Accessible Chromatin", "cancer", "promoter competition"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "eXrIfzS0BRA", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "18", "viewCount": "1833", "favoriteCount": "0"}, "contentDetails": {"duration": "PT59M47S"}, "snippet": {"channelTitle": "LabRoots", "channelId": "UCP7xodi1WBjs2hNvEjow6ig", "publishedAt": "2016-05-15T20:40:54Z", "title": "Paul Datlinger - DNA methylation analysis by multiplexed reduced representation bisulfite sequencing", "description": "Watch on LabRoots at http://labroots.com/webcast/dna-methylation-analysis-by-multiplexed-reduced-representation-bisulfite-sequencing\nDNA methylation is an essential mechanism of epigenetic gene regulation with broad relevance in development and disease. Its localization on genomic DNA and general stability make this epigenetic mark an attractive target for large-scale studies in cancer research, developmental biology and ecology. In recent years, microarray-based technologies have been gradually replaced by more robust, accurate and versatile next generation sequencing-based methods.\u00a0\n\u00a0\u00a0 \u00a0\nThis presentation will introduce highly-multiplexed reduced representation bisulfite sequencing as a cost-effective means for large DNA methylation studies. The protocol provides a single-base resolution read-out of 5-methylcytosine, while avoiding the cost of whole-genome sequencing. Based on an enzymatic enrichment step, the method is cost-effective, but provides excellent coverage of promoter regions, CpG islands, and other genomic elements such as enhancers and CpG island shores. At the same time, the protocol supports high-throughput applications, is suitable for any vertebrate species and has been optimized specifically for formalin-fixed, paraffin-embedded samples.\n\nIn our lab, we have successfully used RRBS on over 2000 samples comprising many different vertebrate species, various cancers, FFPE and low-input samples, and are happy to share our experience with the epigenomics community.\n\nLearning objectives:\nthe talk will introduce highly multiplexed reduced representation bisulfite sequencing as a cost-effective means for large DNA methylation studies\nget an overview of the required bioinformatic workflow, for an easy transition from microarrays to next generation sequencing-based analyses"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "51CDgElMgKc", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "3", "viewCount": "155", "favoriteCount": "0"}, "contentDetails": {"duration": "PT30M48S"}, "snippet": {"channelId": "UCCUr096WDp86n62CXBeHlQw", "description": "Subject: Biophysics        \nPaper: Cellullar And Molecular Biophysics", "defaultAudioLanguage": "en", "channelTitle": "Vidya-mitra", "publishedAt": "2018-05-15T12:21:40Z", "title": "DNA Methylation and human disease"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "aoLr1gxBYNc", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "29", "favoriteCount": "0"}, "contentDetails": {"duration": "PT47M25S"}, "snippet": {"channelId": "UCEkpcbCuXCHSUD3oMarGd4g", "description": "Curated and processed human/mouse ChIP/DNase-seq datasets in GEO, allowing users to search, browse, download ChIP-seq data signals, peaks, QC, motifs, target genes, and similar datasets.", "defaultLanguage": "en", "channelTitle": "NCIwebinars", "publishedAt": "2020-06-10T18:15:35Z", "title": "Introduction to Cistrome", "tags": ["ITCR", "Cistrome", "ChIP-seq", "chromatin", "omics"], "defaultAudioLanguage": "en"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "SmornBbZngg", "statistics": {"viewCount": "203", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H4M22S"}, "snippet": {"channelId": "UC_BXuG-5qYaAKOQqcLXhrig", "description": "Genome architecture and transcriptional regulation in C. elegans\nUW-Madison Biochemistry Department\nJune 29, 2016", "defaultLanguage": "en", "channelTitle": "UW-Madison Department of Biochemistry", "publishedAt": "2016-07-11T19:02:15Z", "title": "Everson Lecture - Julie Ahringer", "tags": ["Biochemistry", "UW-Madison", "Science", "c. elegans", "transcription"], "defaultAudioLanguage": "en"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "7ZoXIYzSa4w", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "1", "viewCount": "126", "favoriteCount": "0"}, "contentDetails": {"duration": "PT34M23S"}, "snippet": {"channelTitle": "Collaborative on Health and the Environment", "channelId": "UCdY7qeUM25ENUzgjWp5zDDQ", "publishedAt": "2017-12-12T21:48:48Z", "title": "CHE Webinar: Examining Environmental Contributors to Autism", "description": "CHE EDC Strategies Partnership Webinar: Examining Environmental Contributors to Autism. Dr. Valerie Hu, Professor of Biochemistry and Molecular Medicine at George Washington University\u2019s School of Medicine and Health Sciences, presents her laboratory\u2019s research on the influence of environmental factors on RORA, the \u201cmaster regulator\u201d of many autism risk genes. Dr. Hu specifically discusses the effects of exposure to atrazine, one of the most widely used herbicides in the world, and how alterations caused by environmental exposures may be transmitted across generations. December 12, 2017."}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "4jKfbHy714Y", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "6", "viewCount": "475", "favoriteCount": "0"}, "contentDetails": {"duration": "PT33M36S"}, "snippet": {"channelId": "UCUp6Pd9fx8_UX7S38Ih_JqA", "description": "June 29 - July 1, 2015 -- ENCODE 2015: Research Applications and Users Meeting\nMore: http://www.genome.gov/27561910", "defaultAudioLanguage": "en", "channelTitle": "National Human Genome Research Institute", "publishedAt": "2015-08-21T20:48:00Z", "title": "Integrative Analysis of Human and Mouse Regulomes - John Stamatoyannopoulos"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "OAK_wUDHx2U", "statistics": {"dislikeCount": "0", "likeCount": "0", "viewCount": "7", "favoriteCount": "0"}, "contentDetails": {"duration": "PT46M12S"}, "snippet": {"channelId": "UCN9kqT7pfOzZddPJHqWSuyA", "description": "Keynote: Expanding the ENCODE Encyclopedia: Applications for Annotating Non-coding Variants - Zhiping Weng - RECOMB/RSG 2016", "channelTitle": "ISCB", "publishedAt": "2020-01-17T20:09:36Z", "title": "Keynote: Expanding the ENCODE Encyclopedia: Applications for... - Zhiping Weng - RECOMB/RSG 2016", "tags": ["Computational Biology", "ISCB", "Biology", "Computer Science"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "v4HuhZ4NUjg", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "0", "viewCount": "400", "favoriteCount": "0"}, "contentDetails": {"duration": "PT31M59S"}, "snippet": {"channelId": "UCUp6Pd9fx8_UX7S38Ih_JqA", "description": "June 9, 2016 - ENCODE 2016: Research Applications and Users Meeting\nMore: https://www.genome.gov/27566810", "defaultLanguage": "en", "channelTitle": "National Human Genome Research Institute", "publishedAt": "2016-11-06T14:04:08Z", "title": "Tools for analyzing cancer variation - Ekta Khurana", "defaultAudioLanguage": "en"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "F5i1p4CDQ6I", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "59", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H5M44S"}, "snippet": {"channelTitle": "Tushar Kundu", "channelId": "UCpwuOWUVasa168eWFhX-48A", "publishedAt": "2017-09-12T18:36:46Z", "title": "Fleur Garton: Molecular Genetics IV\u2014Animal Models, Intellectual Disability", "description": ""}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "ycRcLqscnuQ", "statistics": {"dislikeCount": "5", "commentCount": "4", "likeCount": "51", "viewCount": "11168", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H13M5S"}, "snippet": {"channelId": "UCUp6Pd9fx8_UX7S38Ih_JqA", "description": "February 15, 2012 - Current Topics in Genome Analysis 2012\nMore: http://www.genome.gov/COURSE2012", "channelTitle": "National Human Genome Research Institute", "publishedAt": "2012-02-17T20:40:58Z", "title": "Regulatory and Epigenetic Landscapes of Mammalian Genomes -  Laura Elnitski (2012)", "tags": ["Current Topics", "2012", "February", "Laura Elnitski", "Mammalian Genomes", "Training"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "L68hFeK5Jn0", "statistics": {"dislikeCount": "0", "likeCount": "10", "viewCount": "1264", "favoriteCount": "0"}, "contentDetails": {"duration": "PT54M54S"}, "snippet": {"channelId": "UCv4IbnP9j9RC_aZAs8wqdeQ", "description": "April 5, 2017 \n\nJesse Engreitz\nLander Lab, Broad Institute\n\nGrand Challenge: Mapping the regulatory wiring of the genome\n\nAbstract:  Our cells are controlled by complex molecular instructions encoded in the \"noncoding\" sequences of our genome, and alterations to these noncoding sequences underlie many common human diseases. The grammar of these noncoding sequences has been difficult to study, but the recent confluence of methods for both high-throughput measurement and high-throughput perturbation offers new opportunities to understand these sequences at a systems level. In this talk, I will highlight outstanding challenges in gene regulation where applying computational approaches in combination with emerging genomics datasets may allow us to build integrated maps that describe the regulatory wiring of the genome. As an example, I will present our efforts to experimentally and computationally map the functional connections between promoters and distal enhancers and use this information to understand human genetic variation in the noncoding genome.\n\nFor more information on the Broad Institute and MIA visit: \nhttp://www.broadinstitute.org/MIA\n\nCopyright Broad Institute, 2017. All rights reserved.", "defaultAudioLanguage": "en", "channelTitle": "Broad Institute", "publishedAt": "2017-04-15T15:17:38Z", "title": "MIA: Jesse Engreitz, Grand Challenge: Mapping the regulatory wiring of the genome", "tags": ["Broad Institute", "Broad", "Science", "Institute", "of", "MIT", "and", "Harvard"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "3H29rbtVeFs", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "5", "viewCount": "163", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H17M56S"}, "snippet": {"channelId": "UCP7xodi1WBjs2hNvEjow6ig", "description": "Watch this webinar on LabRoots at: https://www.labroots.com/virtual-event/neuroscience-2017/agenda\n\nEpigenetics refers to the study of nuclear architecture and gene regulation. Epigenetic mechanisms govern many physiological processes such as cell differentiation, x-inactivation, and genomic imprinting, and pathophysiological processes such as loss of imprinting. Epigenetic mechanisms are especially relevant in psychiatry and neuroscience, as exposure to stress, trauma, or toxicants are thought to alter brain function through epigenetic mechanisms. However, there are substantial challenges to studying epigenetics in the brain, as the brain is a highly heterogeneous organ whose constituent cells and circuits perform diverse functions such as storage of memory, behavioral and emotional response, and regulation of homeostasis among many others.\n\nIn this presentation, Dr. Lee will first provide a brief overview of epigenetics and several commonly used approaches to study epigenetic mechanisms in gene regulation. In particular, he will focus on the implementation of an unbiased, genome-wide platform called Methyl-Seq to study DNA methylation, which is one of the more commonly studied epigenetic modifications, to identify targets of stress and glucocorticoid exposure in the brain. Dr. Lee will then discuss some of the challenges associated with implementing epigenetic tools to answer questions in psychiatry and neuroscience, such as limiting starting material, cellular heterogeneity, and neuronal subtypes. Dr. Lee will also focus on enrichment methods using genetic labeling and flow cytometry to isolate relatively homogenous populations of cells for downstream epigenetic applications. Epigenetic tools, when combined with concepts and powerful analytical tools in neuroscience, have the potential to elucidate the molecular underpinnings of neuronal function.", "channelTitle": "LabRoots", "publishedAt": "2017-06-27T15:26:44Z", "title": "Richard Lee - Refinement of epigenetic approaches in neuroscience", "tags": ["epigenetics", "pathophysiological", "methylation", "glucocorticoid"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "NKuLrfA256Y", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "9", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H16M38S"}, "snippet": {"channelId": "UCjvLXvfPupVHtGU_ckTDPwA", "description": "\"Multi-Dimensional Analysis of Gene Regulation in Mammalian Cells\"\n\nPresented by Bing Ren, PhD (University of California San Diego)\n\nSponsored by The University of Michigan Department of Computational Medicine and Bioinformatics\n\nAbstract\n\nCell-type specific gene regulation is under the purview of enhancers. Great strides have been made recently to characterize and identify enhancers both genetically and epigenetically for multiple cell types and species, but efforts have just begun to functionally characterize these long-range control elements. Mapping interactions between enhancers and promoters, and understanding how the 3D landscape of the genome constrains such interactions is fundamental to our understanding of enhancer function. I will present recent findings related to 3D genome organization in mammalian cells, with a particular focus on how chromatin organization contributes to enhancer-mediated transcriptional regulation. I will describe higher-order organizational features that are observed at the level of both the whole chromosome and individual loci. I will highlight changes in genome organization that occur during the course of differentiation, and discuss the functional relationship between chromatin architecture and gene regulation. Taken together, mounting evidence now shows that the genome organization plays an essential role in orchestrating the lineage-specific gene expression programs through modulating long-range interactions between enhancers and target genes.", "channelTitle": "University of Michigan Computational Medicine and Bioinformatics", "publishedAt": "2019-10-21T15:31:17Z", "title": "CCMB Seminar 04/05/2017 - Bing Ren, PhD", "tags": ["Gene Regulation", "3D genome organization", "bioinformatics", "ccmb", "dcmb"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "T14L2J7Pw0I", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "2", "viewCount": "172", "favoriteCount": "0"}, "contentDetails": {"duration": "PT36M11S"}, "snippet": {"channelId": "UCUp6Pd9fx8_UX7S38Ih_JqA", "description": "June 9, 2016 - ENCODE 2016: Research Applications and Users Meeting\nMore: https://www.genome.gov/27566810", "defaultLanguage": "en", "channelTitle": "National Human Genome Research Institute", "publishedAt": "2016-11-06T14:04:09Z", "title": "Using ENCODE to interpret mutational patterns in cancer genomes - Shamil Sunyaev", "defaultAudioLanguage": "en"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "PRGqFhZOtQs", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "4", "favoriteCount": "0"}, "contentDetails": {"duration": "PT59M49S"}, "snippet": {"channelId": "UCjvLXvfPupVHtGU_ckTDPwA", "description": "\u201cAn integrative systems approach to understanding cell type-specific gene regulatory networks, and its application to macrophages in atherosclerosis\u201d Presented by Stephen Ramsey, Ph.D. (Institute for Systems Biology)\n\nSponsored by The University of Michigan Department of Computational Medicine and Bioinformatics", "channelTitle": "University of Michigan Computational Medicine and Bioinformatics", "publishedAt": "2019-12-10T14:23:45Z", "title": "CCMB SEMINAR 05/08/2013 - Stephen Ramsey", "tags": ["bioinformatics", "dcmb", "ccmb", "gene regulatory networks", "genetics", "atherosclerosis"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "io2iQ6EahIU", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "1", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H18M59S"}, "snippet": {"channelId": "UCTE2wOWEBLDUUXUJQ7wh7DQ", "description": "", "defaultLanguage": "en", "channelTitle": "mafchal kumuch", "publishedAt": "2020-03-01T21:20:14Z", "title": "11", "defaultAudioLanguage": "en"}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "T7MiVV4pQew", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "17", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H24M48S"}, "snippet": {"channelId": "UCjvLXvfPupVHtGU_ckTDPwA", "description": "\"Mechanical control and modeling of human pluripotent stem cells\"\n\nPresented by Jianping Fu, PhD (University of Michigan)\n\nSponsored by The University of Michigan Department of Computational Medicine and Bioinformatics", "channelTitle": "University of Michigan Computational Medicine and Bioinformatics", "publishedAt": "2019-10-23T18:09:00Z", "title": "CCMB Seminar 04/13/2016 - Jianping Fu, PhD", "tags": ["pluripotent stem cells", "stem cells", "ccmb", "dcmb", "bioinformatics"]}}]}, {"q": "detection of active transcription factor binding sites with the combination of dnase hypersensitivity and histone modifications", "idx_paper": "10.1093/bioinformatics/btu519", "items": [{"id": "FVO-MF-vho0", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "0", "viewCount": "19", "favoriteCount": "0"}, "contentDetails": {"duration": "PT47M32S"}, "snippet": {"channelId": "UCGoNozP_2TZV5hVciGW1y6Q", "description": "This is an audio version of the Wikipedia Article:\r\nhttps://en.wikipedia.org/wiki/Human_genome\r\n\r\n\r\n00:03:57 1 Molecular organization and gene content\n00:08:02 1.1 Completeness of the human genome sequence\n00:08:41 1.2 Information content\n00:10:04 2 Coding vs. noncoding DNA\n00:11:07 3 Coding sequences (protein-coding genes)\n00:13:36 4 Noncoding DNA (ncDNA)\n00:14:30 4.1 Pseudogenes\n00:15:30 4.2 Genes for noncoding RNA (ncRNA)\n00:15:52 4.3 Introns and untranslated regions of mRNA\n00:15:59 4.4 Regulatory DNA sequences\n00:19:20 4.5 Repetitive DNA sequences\n00:20:34 4.6 Mobile genetic elements (transposons) and their relics\n00:21:38 5 Genomic variation in humans\n00:22:16 5.1 Human reference genome\n00:24:01 5.2 Measuring human genetic variation\n00:25:31 5.2.1 Mapping human genomic variation\n00:26:54 5.3 SNP frequency across the human genome\n00:27:05 5.4 Personal genomes\n00:28:12 5.5 Human knockouts\n00:30:13 6 Human genetic disorders\n00:31:30 7 Evolution\n00:32:55 8 Mitochondrial DNA\n00:35:35 9 Epigenome\n00:37:34 10 See also\n00:41:24 11 References\n00:43:58 12 External links\n00:45:42 Epigenome\n\r\n\r\n\r\nListening is a more natural way of learning, when compared to reading. Written language only began at around 3200 BC, but spoken language has existed long ago.\r\n\r\nLearning by listening is a great way to:\r\n- increases imagination and understanding\r\n- improves your listening skills\r\n- improves your own spoken accent\r\n- learn while on the move\r\n- reduce eye strain\r\n\r\nNow learn the vast amount of general knowledge available on Wikipedia through audio (audio article). You could even learn subconsciously by playing the audio while you are sleeping! If you are planning to listen a lot, you could try using a bone conduction headphone, or a standard speaker instead of an earphone.\r\n\r\nListen on Google Assistant through Extra Audio:\r\nhttps://assistant.google.com/services/invoke/uid/0000001a130b3f91\r\nOther Wikipedia audio articles at:\r\nhttps://www.youtube.com/results?search_query=wikipedia+tts\r\nUpload your own Wikipedia articles through:\r\nhttps://github.com/nodef/wikipedia-tts\r\nSpeaking Rate: 0.8941373132308837\r\nVoice name: en-AU-Wavenet-B\r\n\r\n\r\n\"I cannot teach anybody anything, I can only make them think.\"\r\n- Socrates\r\n\r\n\r\nSUMMARY\r\n=======\r\nThe human genome is the complete set of nucleic acid sequences for humans, encoded as DNA within the 23 chromosome pairs in cell nuclei and in a small DNA molecule found within individual mitochondria. These are usually treated separately as the nuclear genome, and the mitochondrial genome. Human genomes include both protein-coding DNA genes and noncoding DNA. Haploid human genomes, which are contained in germ cells (the egg and sperm gamete cells created in the meiosis phase of sexual reproduction before fertilization creates a zygote) consist of three billion DNA base pairs, while diploid genomes (found in somatic cells) have twice the DNA content.  While there are significant differences among the genomes of human individuals (on the order of 0.1% due to single-nucleotide variants and 0.6% when considering indels), these are considerably smaller than the differences between humans and their closest living relatives, the  bonobos and chimpanzees (~1.1% fixed single-nucleotide variants  and 4% when including indels).The first human genome sequences were published in nearly complete draft form in February 2001 by the Human Genome Project and Celera Corporation. Completion of the Human Genome Project's sequencing effort was announced in 2004 with the publication of a draft genome sequence, leaving just 341 gaps in the sequence, representing highly-repetitive and other DNA that could not be sequenced with the technology available at the time. The human genome was the first of all vertebrates to be sequenced to such near-completion, and as of 2018, the diploid genomes of over a million individual humans had been determined using next-generation sequencing. These data are used worldwide in biomedical science, anthropology, forensics and other branches of science.  Such genomic studies have lead to advances in the diagnosis and treatment of diseases, and to new insights in many fields of biology, including human evolution.\nAlthough the sequence of the human genome has been (almost) completely determined by DNA sequencing, it is not yet fully understood. Most (though probably not all) genes have been identified by a combination of high throughput experimental and bioinformatics approaches, yet much work still needs to be done to further elucidate the biological functions of their protein and RNA products. Recent results suggest that most of the vast quantities of noncoding DNA within the genome have associated biochemical activities, including regulation of gene expression, organization of chromosome architecture, and signals controlling epigenetic inheritance.\nPrior to the acquisition of the full genome sequence, estimates of the number of human genes ranged from 50,000 to 140,000 (with occasional ...", "defaultLanguage": "en", "channelTitle": "wikipedia tts", "publishedAt": "2019-10-03T12:10:03Z", "title": "Human genome | Wikipedia audio article", "tags": ["human genome", "chromosomes (human)", "genetic mapping", "genomics", "human evolution", "human genetics", "wikipedia audio article", "learning by listening", "improves your listening skills", "learn while on the move", "reduce eye strain", "text to speech"], "defaultAudioLanguage": "en"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "WtWxOhhZWX0", "statistics": {"dislikeCount": "2", "commentCount": "2", "likeCount": "50", "viewCount": "1883", "favoriteCount": "0"}, "contentDetails": {"duration": "PT35M45S"}, "snippet": {"channelId": "UCsvqVGtbbyHaMoevxPAq9Fg", "description": "Ensemble Learning is a popular machine learning technique for building models. This video on Ensemble Learning covers the basics of Ensemble Learning Methods. You will learn about the Adaboost and Gradient boosting algorithm. You will get an understanding of the Xgboost technique. The video will make you learn how to create a model using Ensemble Learning. Finally, you will look into the model selection and cross-validation skills.\n\n1. Ensemble Learning\n2. Overview\n3. Ensemble Learning Methods: Part a\n4. Ensemble Learning Methods: Part B\n5. Working with Adaboost\n6. Adaboost Algorithm and Flowchart\n7. Gradient Boosting\n8. Xgboost\n9. Xgboost Parameters: Part a\n10. Xgboost Parameters: Partb\n11. Demo: Pima Indians Diabetes\n12. Model Selection\n13. Common Splitting Strategies\n14. Demo: Cross Validation\n15. Key Takeaways\n\n\u2705Subscribe to our Channel to learn more about the top Technologies: https://bit.ly/2VT4WtH\n\n\u23e9 Check out the Artificial Intelligence training videos: https://bit.ly/2Li4Rur\n\n#EnsembleLearning #EnsembleLearningInMachineLearning #WhatIsEnsembleLearning #EnsembleLearningMethod #MachineLearningTutorial #Simplilearn\n\nSimplilearn\u2019s Artificial Intelligence course provides training in the skills required for a career in AI. You will master TensorFlow, Machine Learning and other AI concepts, plus the programming languages needed to design intelligent agents, deep learning algorithms & advanced artificial neural networks that use predictive analytics to solve real-time decision-making problems without explicit programming.\n\nWhy learn Artificial Intelligence? \nThe current and future demand for AI engineers is staggering. The New York Times reports a candidate shortage for certified AI Engineers, with fewer than 10,000 qualified people in the world to fill these jobs, which according to Paysa earn an average salary of $172,000 per year in the U.S. (or Rs.17 lakhs to Rs. 25 lakhs in India) for engineers with the required skills.\n\nYou can gain in-depth knowledge of Artificial Intelligence by taking our Artificial Intelligence certification training course. Those who complete the course will be able to: \n1. Master the concepts of supervised and unsupervised learning\n2. Gain practical mastery over principles, algorithms, and applications of machine learning through a hands-on approach which includes working on 28 projects and one capstone project.\n3. Acquire thorough knowledge of the mathematical and heuristic aspects of machine learning.\n4. Understand the concepts and operation of support vector machines, kernel SVM, naive bayes, decision tree classifier, random forest classifier, logistic regression, K-nearest neighbors, K-means clustering and more.\n5. Comprehend the theoretical concepts and how they relate to the practical aspects of machine learning.\n6. Be able to model a wide variety of robust machine learning algorithms including deep learning, clustering, and recommendation systems\n\n\ud83d\udc49Learn more at: https://bit.ly/2AlrLiB\n\nFor more updates on courses and tips follow us on:\n- Facebook: https://www.facebook.com/Simplilearn \n- Twitter: https://twitter.com/simplilearn \n- LinkedIn: https://www.linkedin.com/company/simplilearn/\n- Website: https://www.simplilearn.com", "defaultAudioLanguage": "en", "channelTitle": "Simplilearn", "publishedAt": "2020-05-25T05:00:07Z", "title": "Ensemble Learning | Ensemble Learning In Machine Learning | Machine Learning Tutorial | Simplilearn", "tags": ["ensemble learning", "ensemble learning in machine learning", "ensemble learning in artificial intelligence", "what is ensemble learning", "what is ensemble technique in machine learning", "what is ensemble algorithm", "what is ensemble method in machine learning", "what is ensemble learning method", "what is ensemble method", "what is ensemble model", "ensemble learning example", "ensemble learning and random forests", "machine learning tutorial", "simpililearn machine learning", "simplilearn"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "7_YzyMYC2zM", "statistics": {"dislikeCount": "0", "commentCount": "24", "likeCount": "55", "viewCount": "2732", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H12M"}, "liveStreamingDetails": {"actualStartTime": "2020-02-03T23:48:49Z"}, "snippet": {"channelId": "UCMEXgDffQy6nS2a74Gby8ZA", "description": "Class materials: https://www.cs.columbia.edu/~amueller/comsw4995s20/", "defaultAudioLanguage": "en", "channelTitle": "Andreas Mueller", "publishedAt": "2020-02-03T23:48:49Z", "title": "Applied ML 2020 - 03 Supervised learning and model validation", "tags": ["machine learning", "scikit-learn", "artificial intelligence", "data science"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "32tNAhQ8x7M", "statistics": {"dislikeCount": "0", "commentCount": "4", "likeCount": "19", "viewCount": "472", "favoriteCount": "0"}, "contentDetails": {"duration": "PT5M40S"}, "snippet": {"channelId": "UCfOZIP26Jx-9ywDdmDUc5mw", "description": "Parameters vs Hyperparameters ( Parameter vs Hyperparameter ) Machine Learning\n\nParameters in a Machine Learning model are the parameters whose values are updated during training using some optimization procedure.\n\nHyperparameters in a Machine Learning model are the parameters whose values are decided before training of the model begins. Hyperparameters are not affected (do not change) by training, rather, these affect the quality and speed of the training. So in a sense, these parameters are over, above or beyond ( hyper- ) the process of training and hence called Hyperparameters. \n\nHyperparameters can be related to model selection, e.g.,  model type, model architecture, or learning algorithm, e.g., learning rate, batch size, number of epochs.\n\nThere are no efficient algorithms to select optimal (best) values of hyperparameters. So optimal values of hyperparameters are determined using a trial and error process by adjusting or adapting their values for a particular machine learning task. This process of determining values of hyperparameters is called Hyperparameter Tuning. For example select the hyperparameters that give best results on validation set.\n\nWhat is the difference between parameters and hyperparameters in machine learning ?\nWhat is the difference between parameter and hyperparameter in machine learning ?\nWhat is hyperparameter in machine learning ?\nWhat is hyperparameter ?\nWhy hyperparameters are called hyperparameters ?\nWhy hyperparameters are called so ?\nWhat is hyperparameter tuning ?\nHow do we find values of hyperparameters ?", "defaultAudioLanguage": "en", "channelTitle": "Pankaj Porwal", "publishedAt": "2020-02-16T06:41:03Z", "title": "Parameters vs Hyperparameters ( Parameter vs Hyperparameter ) in Machine Learning Detailed", "tags": ["Artificial Intelligence", "Machine Learning", "Deep Learning", "Learning", "Training", "Parameters", "Hyperparameters", "Hyperparameter", "Tuning", "Optimal", "Best", "Optimum", "Trial", "Error", "Trial and error", "Model selection", "Submodel type", "Model architecture", "Neural network", "Network architecture", "Number of layers", "Number of nodes", "Learning rate", "Batch size", "Number of epochs", "Epochs", "Epoch", "Validation", "Validation set", "Testing set", "Training set"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "Cwy2Qf8fT2A", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "23", "viewCount": "1196", "favoriteCount": "0"}, "contentDetails": {"duration": "PT24M31S"}, "snippet": {"channelId": "UCA3NwUB6Yn-EhddHNH5A5fg", "description": "Peter Prettenhofer: \"Dataset shift in machine learning\" in North Star conference powered by Proekspert.", "channelTitle": "ProekspertEst", "publishedAt": "2018-05-22T06:49:17Z", "title": "Peter Prettenhofer: \"Dataset shift in machine learning\"", "tags": ["Peter Prettenhofer", "North Star", "Proekspert", "Machine Learning", "ML", "AI", "Artificial Intelligence", "Data Science"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "dQgzVigCVto", "statistics": {"dislikeCount": "0", "commentCount": "2", "likeCount": "4", "viewCount": "175", "favoriteCount": "0"}, "contentDetails": {"duration": "PT16M12S"}, "snippet": {"channelId": "UC3tExxdDT9plEIwKWfoCNNw", "description": "In this video I talked about Ensemble learning. Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Ensemble learning is primarily used to improve the (classification, prediction, function approximation, etc.) performance of a model, or reduce the likelihood of an unfortunate selection of a poor one.", "defaultAudioLanguage": "en", "channelTitle": "Splunk & Machine Learning", "publishedAt": "2020-01-09T10:10:58Z", "title": "Machine Learning : Introduction to Ensemble Learning", "tags": ["machine learning", "ml", "ensemble", "learning"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "TsqTuwTKFSs", "statistics": {"dislikeCount": "57", "commentCount": "44", "likeCount": "397", "viewCount": "43631", "favoriteCount": "0"}, "contentDetails": {"duration": "PT56M25S"}, "snippet": {"channelId": "UC2XO4HDxzfMOZIV1l795g1Q", "description": "Feature selection is an important step in machine learning model building process. The performance of models depends in the following : Choice of algorithm\nFeature Selection\nFeature Creation\nModel Selection\n\nSo feature selection is one important reason for good performance. They are primarily of three types: \n\nFilter Methods\nWrapper Methods\nEmbedded Methods\n\nYou will learn a number of techniques such as variable selection through Correlation matrix, subset selection, stepwise forward, stepwise backward, hybrid method etc. You will also learn regularization (shrinkage) methods such as lasso and Ridge regression that can well be used for variable selection.\n\nFinally you will learn difference between variable selection and dimension reduction \n\nANalytics Study Pack : http://analyticuniversity.com/\n\nComplete Data Science Course : http://bit.ly/34Sucmb\n\nData Science Books on Amazon : \n\nPython Data Science : https://amzn.to/2Qg6g8m\nBusiness ANalytics : https://amzn.to/2F7RhGT\nSTatistics : https://amzn.to/2ZGcSjb\nStatistical Leanring : https://amzn.to/2ZHV6fn\nPython : https://amzn.to/2u0uKJR\nAudio books : https://amzn.to/2SSynMD\n\nCoursera : \n\nData Science : http://bit.ly/37nABr6\nData Science Python : http://bit.ly/2ZK5oMm                                      \n\nDiscounted courses on Udemy (for $11): http://bit.ly/2LYU6hp\n\nUdacity Nanodegree:\n\nData Science : http://bit.ly/39IzAfc\nMachine Learning : http://bit.ly/2sOinRb\n\nFree access to Skillshare: http://bit.ly/2thklJu\n\n20$ discounts on below LIVE courses : use coupon YOUTUBE20 \n\nData Science Live Training :\nAI and Tensorflow: http://bit.ly/2tOnOzA\nPython : http://bit.ly/2QkH1QQ\nData Analytics : http://bit.ly/2PR4eez\nData Science : http://bit.ly/2QhxmdR\nSAS : http://bit.ly/2Mpx83m\n\nBig Data  Training:\nHadoop : http://bit.ly/2sgHWdb\nSplunk : http://bit.ly/2Ms0A8\nAnalytics University on Twitter : https://twitter.com/AnalyticsUniver\n\nAnalytics University on Facebook : https://www.facebook.com/AnalyticsUniversity\n\nLogistic Regression in R: https://goo.gl/S7DkRy\n\nLogistic Regression in SAS: https://goo.gl/S7DkRy\n\nLogistic Regression Theory: https://goo.gl/PbGv1h\n\nTime Series Theory : https://goo.gl/54vaDk\n\nTime ARIMA Model in R : https://goo.gl/UcPNWx\n\nSurvival Model : https://goo.gl/nz5kgu\n\nData Science Career : https://goo.gl/Ca9z6r\n\nMachine Learning : https://goo.gl/giqqmx\n\n\nData Science Case Study : https://goo.gl/KzY5Iu\n\nBig Data & Hadoop & Spark: https://goo.gl/ZTmHOA", "defaultAudioLanguage": "en", "channelTitle": "Analytics University", "publishedAt": "2017-09-20T20:53:45Z", "title": "Feature Selection in Machine learning| Variable selection| Dimension Reduction", "tags": ["feature selection", "Machine Learning", "dimension reduction", "regularization", "lasso", "ridge regression", "stepwise regression", "subset selection", "feature creation", "data science", "kaggle", "statistical models", "deep learning", "Logistic Regression", "Random forest", "Data science career", "filter methods", "wrapper methods", "shrinkage", "forward selection", "support vector machine", "neural network", "backword selection"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "9Sx2qWKGzlc", "statistics": {"dislikeCount": "0", "commentCount": "2", "likeCount": "21", "viewCount": "2491", "favoriteCount": "0"}, "contentDetails": {"duration": "PT2H6M48S"}, "snippet": {"channelId": "UC0n76gicaarsN_Y9YShWwhw", "description": "Link to indexed video of session: https://conftube.com/video/9Sx2qWKGzlc\n\n1. Exploring Randomly Wired Neural Networks for Image Recognition \nSaining Xie, Alexander Kirillov, Ross Girshick, Kaiming He\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=2\n2. Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation \nXin Chen, Lingxi Xie, Jun Wu, Qi Tian\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=12\n3. Multinomial Distribution Learning for Effective Neural Architecture Search \nXiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang, Jianzhuang Liu, Qi Tian\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=23\n4. Searching for MobileNetV3 \nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=32\n5. Data-Free Quantization Through Weight Equalization and Bias Correction \nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=40\n6. A Camera That CNNs: Towards Embedded Neural Networks on Pixel Processor Arrays \nLaurie Bose, Jianing Chen, Stephen J. Carey, Piotr Dudek, Walterio Mayol-Cuevas\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=53\n7. Knowledge Distillation via Route Constrained Optimization \nXiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=69\n8. Distillation-Based Training for Multi-Exit Architectures \nMary Phuong, Christoph H. Lampert\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=77\n9. Similarity-Preserving Knowledge Distillation \nFrederick Tung, Greg Mori\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=88\n10. Many Task Learning With Task Routing \nGjorgji Strezoski, Nanne van Noord, Marcel Worring\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=94\n11. Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels \nFelix J.S. Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C. Alexander, Jorge Cardoso\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=100\n12. Transferability and Hardness of Supervised Classification Tasks \nAnh T. Tran, Cuong V. Nguyen, Tal Hassner\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=109\n13. Moment Matching for Multi-Source Domain Adaptation \nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=116\n14. Unsupervised Domain Adaptation via Regularized Conditional Alignment \nSafa Cicek, Stefano Soatto\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=124\n15. Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation \nRuijia Xu, Guanbin Li, Jihan Yang, Liang Lin\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=136\n16. UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation \nJogendra Nath Kundu, Nishank Lakkakula, R. Venkatesh Babu\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=148\n17. Episodic Training for Domain Generalization \nDa Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, Timothy M. Hospedales\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=155\n18. Domain Adaptation for Structured Output via Discriminative Patch Representations \nYi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, Manmohan Chandraker\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=162\n19. Semi-Supervised Learning by Augmented Distribution Alignment \nQin Wang, Wen Li, Luc Van Gool\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=172\n20. S4L: Self-Supervised Semi-Supervised Learning \nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer\nhttps://conftube.com/video/9Sx2qWKGzlc?tocitem=180", "defaultAudioLanguage": "en", "channelTitle": "ComputerVisionFoundation Videos", "publishedAt": "2019-11-20T06:00:12Z", "title": "ICCV19: Oral Session 1.2A - Architectures, Multi-Task Learning, Domain Adaptation", "tags": ["ICCV", "ICCV19", "ICCV2019"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "6KIJB4A6VbI", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "27", "viewCount": "660", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H4M43S"}, "snippet": {"channelId": "UC2zHjOF9Womes-SEjdzTleA", "description": "In this session, you'll learn all about classifiers (both supervised and unsupervised), training data, test data, the dreaded \"computed value too large\" message, and the awesome linear regression reducer. Presented by  Arian Karbasi of Google.", "defaultAudioLanguage": "en-US", "channelTitle": "Google Earth", "publishedAt": "2020-05-18T19:34:11Z", "title": "Geo for Good 2019: Earth Engine Classification & Regression", "tags": ["Google Earth", "Google Earth Engine", "Geo for Good Summit"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "D0efHEJsfHo", "statistics": {"dislikeCount": "7", "commentCount": "177", "likeCount": "775", "viewCount": "26747", "favoriteCount": "0"}, "contentDetails": {"duration": "PT16M15S"}, "snippet": {"channelId": "UCtYLUTtgS3k1Fg4y5tAhLbw", "description": "Pruning Regression Trees is one the most important ways we can prevent them from overfitting the Training Data. This video walks you through Cost Complexity Pruning, aka Weakest Link Pruning, step-by-step so that you can learn how it works and see it in action.\nNOTE: This StatQuest assumes you already know about...\nRegression Trees: https://youtu.be/g9c66TUylZ4\n\nALSO NOTE: This StatQuest is based on the Cost Complexity Pruning algorithm found on pages 307 to 309 of the Introduction to Statistical Learning in R: http://faculty.marshall.usc.edu/gareth-james/ISL/\n\nFor a complete index of all the StatQuest videos, check out:\nhttps://statquest.org/video-index/\n\nIf you'd like to support StatQuest, please consider...\nPatreon: https://www.patreon.com/statquest\n...or...\nYouTube Membership: https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw/join\n\n...a cool StatQuest t-shirt or sweatshirt (USA/Europe): https://teespring.com/stores/statquest\n(everywhere):\nhttps://www.redbubble.com/people/starmer/works/40421224-statquest-double-bam?asc=u&p=t-shirt\n\n...buying one or two of my songs (or go large and get a whole album!)\nhttps://joshuastarmer.bandcamp.com/\n\n...or just donating to StatQuest!\nhttps://www.paypal.me/statquest\n\nLastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter:\nhttps://twitter.com/joshuastarmer\n\n0:00 Awesome song and introduction\n0:59 Motivation for pruning a tree\n3:58 Calculating the sum of squared residuals for pruned trees\n7:50 Comparing pruned trees with alpha.\n11:17 Step 1: Use all of the data to build trees with different alphas\n13:05 Step 2: Use cross validation to compare alphas\n15:02 Step 3: Select the alpha that, on average, gives the best results\n15:27 Step 4: Select the original tree that corresponds to that alpha\n\n\n\n\n#statquest #regression #tree", "defaultLanguage": "en", "channelTitle": "StatQuest with Josh Starmer", "publishedAt": "2019-11-25T10:59:44Z", "title": "How to Prune Regression Trees, Clearly Explained!!!", "tags": ["Josh Starmer", "StatQuest", "Regression Trees", "Pruning", "Cost Complexity Pruning", "Weakest Link Pruning", "Data Science", "Machine Learning", "Statistics", "CART"], "defaultAudioLanguage": "en"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "PQV5qtS1E1U", "statistics": {"dislikeCount": "2", "commentCount": "2", "likeCount": "37", "viewCount": "1379", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H28S"}, "snippet": {"channelId": "UCCb9_Kn8F_Opb3UCGm-lILQ", "description": "Modern analytics depend on high-effort tasks like data preparation and data cleaning to produce accurate results. This talk describes recent work on making routine data preparation tasks such as data cleaning dramatically easier. I will first introduce a formal probabilistic framework to describe the quality of structured data and demonstrate how this framework allows us to cast data cleaning as a statistical learning and inference problem. I will then show how this connection allows us to obtain formal guarantees on automated data cleaning and describe how it forms the basis of the HoloClean framework, a state-of-the-art ML-based solution for managing noisy structured data. I will close with additional examples of how a statistical learning view on managing noisy data can lead to new solutions to classical database problems such as the discovery of functional dependencies in structured data.\n\nSee more at https://www.microsoft.com/en-us/research/video/a-machine-learning-perspective-on-managing-noisy-structured-data/", "defaultAudioLanguage": "en-US", "channelTitle": "Microsoft Research", "publishedAt": "2019-11-11T22:45:00Z", "title": "A Machine Learning Perspective on Managing Noisy Structured Data", "tags": ["AI", "data platforms and analytics", "data cleaning", "probabilistic framework", "structured data", "HoloClean framework", "machine learning", "microsoft research", "Theodoros Rekatsinas"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "6QJjZoWknEI", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "12", "viewCount": "490", "favoriteCount": "0"}, "contentDetails": {"duration": "PT30M23S"}, "snippet": {"channelId": "UCLqEr-xV-ceHdXXXrTId5ig", "description": "Lecture on feature transformations. Perhaps not exciting, but commonly required for data analytics, geostatistics and machine learning workflows.", "channelTitle": "GeostatsGuy Lectures", "publishedAt": "2019-08-20T19:33:01Z", "title": "05d Machine Learning: Feature Transformations", "tags": ["statistics", "data analytics", "machine learning"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "CYkAhje2o-4", "statistics": {"dislikeCount": "11", "commentCount": "5", "likeCount": "137", "viewCount": "13263", "favoriteCount": "0"}, "contentDetails": {"duration": "PT32M11S"}, "snippet": {"channelId": "UCu-El65PtQm46aSbXkzykYQ", "description": "Philip Sterne\nhttps://2016.za.pycon.org/talks/39/\nAny time you have noisy data where you would like to see the underlying trend then you should think about using Gaussian processes. They will smooth out any noise and give you a great visualisation of the error bars as well. Rather than fitting a specific model to the data, Gaussian processes can model _any_ smooth function.\n\nI will show you how to use Python to:\n\n  * fit Gaussian Processes to data\n  * display the results intuitively\n  * handle large datasets\n\nThis talk will gloss over mathematical detail and instead focus on the options available to the python programmer. There will be code posted to github beforehand so you can follow along in the talk, or just scoop up the useful bits afterwards.", "channelTitle": "PyCon South Africa", "publishedAt": "2016-10-09T15:52:37Z", "title": "Machine Learning in Python - Gaussian Processes", "tags": ["pyconza", "pyconza2016", "python", "PhilipSterne"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "7RdKnACscjA", "statistics": {"dislikeCount": "4", "commentCount": "4", "likeCount": "274", "viewCount": "7912", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H2M28S"}, "liveStreamingDetails": {"actualStartTime": "2019-12-03T16:59:53Z"}, "snippet": {"channelId": "UCSNeZleDn9c74yQc-EKnVTA", "description": "For the first day of the practical model evaluation workshop, we'll be talking about what's important to consider when deciding what model to put into production.\n\nLink to notebook: https://www.kaggle.com/rtatman/practical-model-evaluation-day-1\n\nAbout Kaggle:\nKaggle is the world's largest community of data scientists. Join us to compete, collaborate, learn, and do your data science work. Kaggle's platform is the fastest way to get started on a new data science project. Spin up a Jupyter notebook with a single click. Build with our huge repository of free code and data. Stumped? Ask the friendly Kaggle community for help.\n\nFollow Kaggle online:\nVisit the WEBSITE: http://www.kaggle.com/?utm_medium=you...\nLike Kaggle on FACEBOOK: http://www.facebook.com/kaggle?utm_me...\nFollow Kaggle on TWITTER: http://twitter.com/kaggle?utm_medium=...\nCheck out our BLOG: http://blog.kaggle.com/?utm_medium=yo...\nConnect with us on LINKEDIN: http://www.linkedin.com/company/kaggle\n\nAdvance your data science skills:\nTake our free online courses: http://www.kaggle.com/learn/overview?...\nGet started with Kaggle Kernels: http://www.kaggle.com/docs/kernels?ut...\nDownload clean datasets from Kaggle: http://www.kaggle.com/docs/datasets?u...\nSign up for a Kaggle Competition: http://www.kaggle.com/docs/competitio...\nExplore the Kaggle Public API: http://www.kaggle.com/docs/api?utm_me...\n\nKaggle\nhttps://www.youtube.com/c/kaggle", "channelTitle": "Kaggle", "publishedAt": "2019-12-03T18:40:26Z", "title": "Practical Model Evaluation: What matters for your model? | Kaggle", "tags": ["data science", "deep learning", "nlp", "neural networks", "nlu", "natural language", "python", "programming", "coding", "machine learning", "ai", "artificial intelligence", "kaggle", "research", "technology", "reading group"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "g0tHha20Djs", "statistics": {"dislikeCount": "1", "commentCount": "1", "likeCount": "38", "viewCount": "1531", "favoriteCount": "0"}, "contentDetails": {"duration": "PT2H16M33S"}, "snippet": {"channelId": "UCujajlLGBNPDbjUZim24NSQ", "description": "SYDE 522 \u2013 Machine Intelligence (Winter 2019, University of Waterloo)\n\nTarget Audience: Senior Undergraduate Engineering Students\n  \nInstructor: Professor H.R.Tizhoosh (http://kimia.uwaterloo.ca/)\n\nCourse Outline - The objective of this course is to introduce the students to the main concepts of machine intelligence as parts of a broader framework of \u201cartificial intelligence\u201d. An overview of different learning, inference and optimization schemes will be provided, including Principal Component Analysis, Support Vector Machines, Self-Organizing Maps, Decision Trees, Backpropagation Networks, Autoencoders, Convolutional Networks, Fuzzy Inferencing, Bayesian Inferencing, Evolutionary algorithms, and Ant Colonies. \n\nLecture 11 - Backpropagation, Topology, Overfitting, Autoencoders", "defaultLanguage": "en", "channelTitle": "Kimia Lab", "publishedAt": "2019-03-12T13:36:30Z", "title": "Machine Intelligence - Lecture 11 (Backpropagation, Topology, Overfitting, Autoencoders)", "tags": ["Course", "Lecture", "AI", "Artifcial Intelligence", "Machine LEarning", "Waterloo", "backpropagation", "autoencoders", "topology", "neural networks", "overfitting", "delta rule", "Hebb", "Hebbian rule"], "defaultAudioLanguage": "en"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "9kAQ8Em7SdM", "statistics": {"dislikeCount": "1", "commentCount": "6", "likeCount": "133", "viewCount": "6560", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H18M43S"}, "snippet": {"channelTitle": "Carnegie Mellon University Deep Learning", "channelId": "UC8hYZGEkI2dDO8scT8C5UQA", "publishedAt": "2019-09-04T16:56:34Z", "title": "Lecture 3 | Learning, Empirical Risk Minimization, and Optimization", "description": "Carnegie Mellon University\nCourse: 11-785, Intro to Deep Learning\nOffering: Fall 2019\n\nFor more information, please visit: http://deeplearning.cs.cmu.edu/\n\nContents:\n\u2022 Training a neural network\n\u2022 Perceptron learning rule\n\u2022 Empirical Risk Minimization\n\u2022 Optimization by gradient descent"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "sniHkkrAsOc", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "57", "viewCount": "1883", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H53S"}, "liveStreamingDetails": {"actualStartTime": "2020-05-07T18:03:11.200000Z"}, "snippet": {"channelTitle": "Why R? Foundation", "channelId": "UCwLy_PYrnCEhCU-Ay2F5Drw", "publishedAt": "2020-05-08T07:08:01Z", "title": "Why R? Webinar 006 - N.Zumel + J.Mount - Advanced Data Preparation for Supervised Machine Learning", "description": "40% discount code for Practical Data Science with R book: mtpwhyr20 https://www.manning.com/books/practical-data-science-with-r-second-edition\nBios + Abstract of the talk: Below  \n\n- donate: http://whyr.pl/donate/\n- date: every Thursday 8:00 pm GMT+2\n- format: one 45 minutes long stream + 10 minutes for Q&A\n- comments: ask questions on YouTube live chat\n- join Why R? Slack whyr.pl/slack/\n\nTitle: Advanced Data Preparation for Supervised Machine Learning\n\n\nBrief abstract: Dr. Nina Zumel and Dr. John Mount will present methods for advanced data preparation for supervised machine learning. In particular we will show how to safely pre-process high cardinality categorical variables for later use.  We will spend time on the important points of cross or out of sample methods to reduce over-fit.  We will work theory and examples, and show how the vtreat package can be used in projects.  We wil also preview chapter 8 of Practical Data Science with R, 2nd Edition: Advanced Data Preparation. \n\nBios:\n\nNina Zumel is a Principal Consultant with Win-Vector, LLC, a data science consultancy in San Francisco. She has a Ph.D. in robotics from Carnegie Mellon and is one of the authors of Practical Data Science with R, a popular text on data science.\n\nJohn Mount is a Principal Consultant with Win-Vector LLC, and co-author of \"Practical Data Science with R, 2nd Edition\", Manning 2019. He has a Ph.D. in computer science from Carnegie Mellon\n\nBoth John and Nina maintain a number of open source R and Python packages for data science"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "gv20VsKqgpc", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "36", "viewCount": "1029", "favoriteCount": "0"}, "contentDetails": {"duration": "PT53M31S"}, "snippet": {"channelId": "UCGzuiiLdQZu9wxDNJHO_JnA", "description": "Machine Learning for Physics and the Physics of Learning 2019\nWorkshop III: Validation and Guarantees in Learning Physical Models: from Patterns to Governing Equations to Laws of Nature\n\n\"Machine Learning for Fluid Mechanics\"\nPetros Koumoutsakos - ETH Z\u00fcrich\n\nAbstract: The field of fluid mechanics experiences today a shift from first principles to data driven approaches. While fluid mechanics has always involved massive volumes of data from experiments, field measurements, and large-scale simulations and despite early connections dating back to Kolmogorov, the link between Fluid Mechanics and Machine Learning (ML) has been weak. The situation is rapidly changing with ML algorithms entering in numerous efforts for modeling, optimizing, and controlling fluid flows. In this talk I will present works from our group on the interface of Fluid Mechanics and ML ranging from low order models for turbulent flows to deep reinforcement learning algorithms and Bayesian experimental design for collective swimming. I hope to demonstrate that ML has the potential to augment, and possibly even transform, current lines of fluid mechanics research. I will also discuss how fluid mechanics problems and approaches may be of value to the ML community.\n\nInstitute for Pure and Applied Mathematics, UCLA\nOctober 28, 2019\n\nFor more information: http://www.ipam.ucla.edu/mlpws3", "defaultLanguage": "en", "channelTitle": "Institute for Pure & Applied Mathematics (IPAM)", "publishedAt": "2019-11-12T21:53:18Z", "title": "Petros Koumoutsakos: \"Machine Learning for Fluid Mechanics\"", "tags": ["ipam", "ucla", "math", "petros koumoutsakos", "machine learning", "fluid mechanics", "data science", "big data", "deep learning", "reinforcement learning"], "defaultAudioLanguage": "en"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "p6z-6DNvMDM", "statistics": {"dislikeCount": "0", "commentCount": "3", "likeCount": "19", "viewCount": "1547", "favoriteCount": "0"}, "contentDetails": {"duration": "PT51M9S"}, "snippet": {"channelTitle": "Allen Institute for AI", "channelId": "UCEqgmyWChwvt6MFGGlmUQCQ", "publishedAt": "2018-04-01T18:39:44Z", "title": "Keisuke Sakaguchi: Robust Text Correction for Grammar and Fluency", "description": "Keisuke Sakaguchi\n\nTitle: \"Robust Text Correction for Grammar and Fluency\"\n\nAbstract: \nRobustness has always been a desirable property for natural language processing. In many cases, NLP models (e.g., parsing) and downstream applications (e.g., MT) perform poorly when the input contains noise such as spelling errors, grammatical errors, and disfluency. In this talk, I will present three recent results on error correction models: character, word, and sentence level respectively. For character level, I propose semi-character recurrent neural network, which is motivated by a finding in Psycholinguistics, called Cmabrigde Uinervtisy (Cambridge University) effect. For word-level robustness, I propose an error-repair dependency parsing algorithm for ungrammatical texts. The algorithm can parse sentences and correct grammatical errors simultaneously. Finally, I propose a neural encoder-decoder model with reinforcement learning for sentence-level error correction. To avoid exposure bias in standard encoder-decoders, the model directly optimizes towards a metric for grammatical error correction performance."}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "zo4oRqfMrgo", "statistics": {"dislikeCount": "2", "commentCount": "0", "likeCount": "52", "viewCount": "2644", "favoriteCount": "0"}, "contentDetails": {"duration": "PT50M19S"}, "snippet": {"channelId": "UCW1C2xOfXsIzPgjXyuhkw9g", "description": "Zack Lipton (Carnegie Mellon University)\nhttps://simons.berkeley.edu/talks/tbd-53\nFrontiers of Deep Learning", "defaultAudioLanguage": "en", "channelTitle": "Simons Institute", "publishedAt": "2019-07-18T18:12:22Z", "title": "Robust Deep Learning Under Distribution Shift", "tags": ["Simons Institute", "Theory of Computing", "Theory of Computation", "Theoretical Computer Science", "Computer Science", "UC Berkeley", "Frontiers of Deep Learning", "Zack Lipton"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "THcH0tMdZ6o", "statistics": {"dislikeCount": "1", "commentCount": "4", "likeCount": "17", "viewCount": "2004", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H1M2S"}, "snippet": {"channelId": "UCd6MoB9NC6uYN2grvUNT-Zg", "description": "Watch Kris Skrinak, AWS Partner Solution Architect, demonstrate why XGBoost built into Amazon SageMaker is the ultimate weapon in Machine Learning. Learn more at - https://amzn.to/2TrFx8o.\n\nLearn more - https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html", "defaultAudioLanguage": "en", "channelTitle": "Amazon Web Services", "publishedAt": "2018-11-19T21:31:29Z", "title": "Amazon SageMaker\u2019s Built-in Algorithm Webinar Series: XGBoost", "tags": ["AWS", "Amazon Web Services", "Cloud", "cloud computing", "AWS Cloud"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "HyYZWP8S2bA", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "57", "viewCount": "2063", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H18M22S"}, "snippet": {"channelId": "UCujajlLGBNPDbjUZim24NSQ", "description": "SYDE 522 \u2013 Machine Intelligence (Winter 2019, University of Waterloo)\n\nTarget Audience: Senior Undergraduate Engineering Students\n  \nInstructor: Professor H.R.Tizhoosh (http://kimia.uwaterloo.ca/)\n\nCourse Outline - The objective of this course is to introduce the students to the main concepts of machine intelligence as parts of a broader framework of \u201cartificial intelligence\u201d. An overview of different learning, inference and optimization schemes will be provided, including Principal Component Analysis, Support Vector Machines, Self-Organizing Maps, Decision Trees, Backpropagation Networks, Autoencoders, Convolutional Networks, Fuzzy Inferencing, Bayesian Inferencing, Evolutionary algorithms, and Ant Colonies. \n\nLecture 4 - Dimensionality Reduction and Visualization (LDA, t-SNE)", "channelTitle": "Kimia Lab", "publishedAt": "2019-03-07T02:16:04Z", "title": "Machine Intelligence - Lecture 4 (LDA, t-SNE)", "tags": ["artificial intelligence", "ai", "course", "lecture", "waterloo", "machine learning", "LDA", "t-SNE"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "L_0efNkdGMc", "statistics": {"dislikeCount": "23", "commentCount": "56", "likeCount": "972", "viewCount": "190724", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H18M22S"}, "snippet": {"channelId": "UClGTZDyz3CSl92TgDqIr0nw", "description": "Error and Noise - The principled choice of error measures. What happens when the target we want to learn is noisy. Lecture 4 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. View course materials in iTunes U Course App - https://itunes.apple.com/us/course/machine-learning/id515364596 and on the course website - http://work.caltech.edu/telecourse.html\n\nProduced in association with Caltech Academic Media Technologies under the Attribution-NonCommercial-NoDerivs Creative Commons License (CC BY-NC-ND). To learn more about this license, http://creativecommons.org/licenses/by-nc-nd/3.0/\n\nThis lecture was recorded on April 12, 2012, in Hameetman Auditorium at Caltech, Pasadena, CA, USA.", "defaultAudioLanguage": "en", "channelTitle": "caltech", "publishedAt": "2012-04-15T19:25:16Z", "title": "Lecture 04 - Error and Noise", "tags": ["Machine Learning (Field Of Study)", "Caltech", "MOOC", "data", "computer", "science", "course", "Data Mining (Technology Class)", "Big Data", "Data Science", "learning from data", "error measure", "error function", "noise", "noisy data", "Technology (Professional Field)", "Computer Science (Industry)", "Learning (Quotation Subject)", "Lecture (Type Of Public Presentation)", "California Institute Of Technology (Organization)", "Abu-Mostafa", "Yaser"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "BajPM1X9KfQ", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "12", "viewCount": "392", "favoriteCount": "0"}, "contentDetails": {"duration": "PT51M28S"}, "snippet": {"channelId": "UCUTmZqIY7C0ckCB86leyScw", "description": "Over the past year, discourse about the ethical risks of machine learning has largely shifted from speculative fear about rogue super intelligent systems to critical examination of machine learning's propensity to exacerbate patterns of discrimination in society. This talk explains how and why bias creeps into supervised machine learning systems and proposes a framework businesses can apply to hold algorithmic systems accountable in a way that is meaningful to people impacted by systems. You'll learn why it's important to consider bias throughout the entire machine learning product lifecycle (not just algorithms), how to assess tradeoffs between accuracy and explainability, and what technical solutions are available to reduce bias and promote fairness.", "defaultAudioLanguage": "en", "channelTitle": "NYAI (New York Artificial Intelligence)", "publishedAt": "2018-06-18T20:22:34Z", "title": "NYAI #20: Ethical Algorithms | Bias and Explainability in Machine Learning Systems", "tags": ["nyai", "artificial intelligence", "machine learning", "kathryn hume", "bias", "ethical algorithms", "ethics", "ai", "ml"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "wPxDibqdg_w", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "9", "viewCount": "272", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H10M50S"}, "snippet": {"channelId": "UCDS20hpBFiv_Kdp5Ibh0vew", "description": "Dynamic price optimisation represents an increasingly profitable yet challenging process, especially for large and established businesses with long-standing practices and legacy data retention systems. Machine learning models built on often large amounts of sales data provide opportunities to grow revenue both by increasing price and reducing lost sales. David, Alexey and their team have developed approaches to solving such problems, from initial data exploration to cloud-based deployment. Key insights from this experience will be covered. \n \nDavid Adey, PhD & Alexey Drozdetskiy, PhD were speaking at ODSC Europe 2019\n\n\u2192 To watch more videos like this, visit https://aiplus.odsc.com \u2190\n\nInitial exploration often covers statistical analysis of available sales data, stakeholder engagement and reverse engineering of legacy systems, with the primary aim of understanding the key feature sets to be used in the models. Given the frequent dominance of large categorical features, methods of encoding have been developed to best fit with the optimal base algorithm for each solution. Choices of models based on such a feature set have been explored including the option of chaining clustering, regression and time series algorithms, with particular focus on choices between regression trees and neural networks. With a view to rapid re-training during production, efficiency modifications have been made to the use of standard python library approaches with C++ extensions, and the merits of this versus distributed computing will be discussed.\n \nFinally how the solutions have been deployed in a cloud environment will be summarised with a focus on reliability, scalability and user experience.\n\nDo You Like This Video? Share Your Thoughts in Comments Below\nAlso, You can visit our website and choose the nearest ODSC Event to attend and experience all our Trainings and Workshops:\nhttps://odsc.com/europe/ \nhttps://odsc.com/california/ \n\nSign up for the newsletter to stay up to date with the latest trends in data science: https://opendatascience.com/newsletter/\n\nFollow Us Online!\n \u2022 Facebook: https://www.facebook.com/OPENDATASCI/ \n \u2022 Instagram: https://www.instagram.com/odsc/ \n \u2022 Blog: https://opendatascience.com/ \n \u2022 Linkedin: https://www.linkedin.com/company/open-data-science/ \n \u2022 Learning Videos: https://learnai.odsc.com\n\n#AIBusiness #ODSC #AI", "defaultAudioLanguage": "en", "channelTitle": "Open Data Science", "publishedAt": "2020-07-07T15:36:11Z", "title": "Price Optimisation: From Exploration to Productionising - David Adey, PhD & Alexey Drozdetskiy, PhD", "tags": ["AI for Business", "ODSC", "AI"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "HWBlhCK4xTU", "statistics": {"dislikeCount": "0", "commentCount": "1", "likeCount": "14", "viewCount": "1538", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H33S"}, "snippet": {"channelId": "UCCb9_Kn8F_Opb3UCGm-lILQ", "description": "User interaction data is at the heart of interactive machine learning systems (IMLSs), such as voice-activated digital assistants, e-commerce destinations, news content hubs, and movie streaming portals. In my talk, I will show how we can improve machine learning in such systems through a principled treatment of biases in interaction data via causal inference and counterfactual learning, and through interface interventions that increase the quality and quantity of interaction data from users. All these efforts are part of my larger vision that improving machine learning accuracy in IMLSs is not only a question of improving machine learning algorithms, but that there are also numerous other crucial questions, such as how interfaces affect interaction data quality and quantity.\n\nSee more at https://www.microsoft.com/en-us/research/video/improving-machine-learning-beyond-the-algorithm/", "defaultAudioLanguage": "en", "channelTitle": "Microsoft Research", "publishedAt": "2018-03-06T04:00:59Z", "title": "Improving Machine Learning Beyond the Algorithm", "tags": ["microsoft research", "machine learning", "ai", "artificial intelligence"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "TK671HxrufE", "statistics": {"dislikeCount": "1", "commentCount": "5", "likeCount": "105", "viewCount": "12569", "favoriteCount": "0"}, "contentDetails": {"duration": "PT53M43S"}, "snippet": {"channelId": "UCCb9_Kn8F_Opb3UCGm-lILQ", "description": "Speaker: Frank Seide \n\nThis talk will introduce CNTK, Microsoft\u2019s cutting-edge open-source deep-learning toolkit for Windows and Linux. CNTK is a computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. CNTK supports feed-forward, convolutional, and recurrent networks for speech, image, and text workloads, also in combination. Popular network types are supported either natively (convolution) or can be described as a CNTK configuration (LSTM, sequence-to-sequence). CNTK scales to multiple GPU servers and is designed around efficiency. We will give an overview of CNTK's general architecture and describe the specific methods and algorithms used for automatic differentiation, recurrent-loop inference and execution, memory sharing, on-the-fly randomization of large corpora, and multi-server parallelization. We will then discuss how typical uses looks like for relevant tasks like image recognition, sequence-to-sequence modeling, and speech recognition.\n\nhttp://research.microsoft.com/latamfacsum2016", "defaultAudioLanguage": "en", "channelTitle": "Microsoft Research", "publishedAt": "2016-05-27T18:48:13Z", "title": "CNTK: Microsoft's Open-Source Deep-Learning Toolkit", "tags": ["Frank Seide", "Microsoft Research", "Latin America Faculty Summit", "CNTK", "Deep learning toolkit", "Cortana", "Natural language processing and speech", "Computer vision", "Image recognition"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "7E0bsgG6TgU", "statistics": {"dislikeCount": "1", "commentCount": "0", "likeCount": "19", "viewCount": "948", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H36M2S"}, "snippet": {"channelTitle": "Wolfram", "channelId": "UCJekgf6k62CQHdENWf2NgAQ", "publishedAt": "2018-12-19T22:02:42Z", "title": "Deep Learning for Audio and Natural Language Processing", "description": "In the third webinar in the Machine Learning webinar series, learn to use machine learning for audio analysis with some real-world applications of neural net models. Also featured is a demonstration of using neural net models for natural language processing."}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "-0gmXn0M7bg", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "4", "viewCount": "456", "favoriteCount": "0"}, "contentDetails": {"duration": "PT57M41S"}, "snippet": {"channelId": "UCk40c_9_HL3IVj_2lCIBu0g", "description": "Invited Research Seminar. Daniel Rueckert. Imperial College London, UK. September, 21st 2018. Room 55.309  Campus del Poblenou.", "channelTitle": "Universitat Pompeu Fabra - Barcelona", "publishedAt": "2018-11-26T16:11:02Z", "title": "Deep learning for medical image reconstruction, super-resolution, classification and segmentation", "tags": ["upf", "universitat pompeu fabra", "pompeu fabra university", "universidad pompeu fabra"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "U1aPRX_SIZM", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "66", "viewCount": "2778", "favoriteCount": "0"}, "contentDetails": {"duration": "PT25M52S"}, "snippet": {"channelId": "UCQua2ZAkbr_Shsgfk1LCy6A", "description": "Setting the learning rate for stochastic gradient descent (SGD) is crucially important when training neural network because it controls both the speed of convergence and the ultimate performance of the network. Set the learning too low and you could be twiddling your thumbs for quite some time as the parameters update very slowly. Set it too high and the updates will skip over optimal solutions, or worse the optimizer might not converge at all!\n\nLeslie Smith from the U.S. Naval Research Laboratory presented a method for finding a good learning rate in a paper called \"Cyclical Learning Rates for Training Neural Networks\" (https://arxiv.org/abs/1506.01186). In this video we implement a 'Learning Rate Finder' in MXNet with the Gluon API that you can use while training your own networks.\n\nCode and more information can be found at https://mxnet.incubator.apache.org/tutorials/gluon/learning_rate_finder.html\n\nIntro Music: Candy-Coloured Sky by Catmosphere", "defaultAudioLanguage": "en", "channelTitle": "Apache MXNet", "publishedAt": "2018-07-05T17:17:04Z", "title": "Choosing the Learning Rate with LR Finder", "tags": ["mxnet", "deep learning", "ai", "python", "learning rate"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "i6-ZHgmKgY8", "statistics": {"dislikeCount": "0", "commentCount": "3", "likeCount": "11", "viewCount": "348", "favoriteCount": "0"}, "contentDetails": {"duration": "PT29M29S"}, "snippet": {"channelId": "UCSa6RXTmvXOUGnQic_BOwIg", "description": "Wanjala Fridah offers a solution walkthrough in R on the Sendy Logistics challenge currently hosted on Zindi ( http://bit.ly/sendychallenge). The resources being used in this tutorial can be found on http://bit.ly/datathon26 under the R folder. \n\nThe following resources, frameworks have been highlighted in this video: \nCRISP- DM : https://www.ibm.com/support/knowledgecenter/en/SS3RA7_15.0.0/com.ibm.spss.crispdm.help/crisp_overview.htm\n Tidyverse : https://www.tidyverse.org/\nLubridate : https://lubridate.tidyverse.org/\nCaret : http://topepo.github.io/caret/index.html\nPurr : https://purrr.tidyverse.org/\nBias variance tradeoff : https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n\nSubscribe to our channel  on http://bit.ly/aiksubscribe for more content on data science, , machine learning and artificial intelligence with a focus on Kenya, \n\nVisit the Ai Kenya website(https://kenya.ai/)  for learning resources and opportunities in machine learning, data science and robotics in Kenya. Join the Ai Kenya meetup group on https://www.meetup.com/AI-KENYA/ for updates on all our upcoming events. \n\n#Cape2Nairo #DataHack4FI #Datathon #AiKenya #ProblemSolving #BusinessChallenge #Logistics #DataScience #MachineLearning #Rstats #CrispDM #TidyVerse #Lubridate #Caret #Purr #BiasVarianceTradeoff", "channelTitle": "Ai Kenya", "publishedAt": "2019-11-14T11:16:43Z", "title": "Sendy Logistics challenge solution walkthrough in R, Wanjala Frida", "tags": ["Cape2Nairo", "DataHack4FI", "Datathon", "AiKenya", "ProblemSolving", "BusinessChallenge", "Logistics", "Datascience", "Machine Learning", "Rstats", "CrispDM", "TidyVerse Lubridate", "Caret", "Purr", "BiasVarianceTradeoff"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "OQQ-W_63UgQ", "statistics": {"dislikeCount": "84", "likeCount": "5437", "viewCount": "655439", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H11M41S"}, "snippet": {"channelId": "UCdKG2JnvPu6mY1NDXYFfN0g", "description": "Lecture 1 introduces the concept of Natural Language Processing (NLP) and the problems NLP faces today. The concept of representing words as numeric vectors is then introduced, and popular approaches to designing word vectors are discussed. \n\nKey phrases: Natural Language Processing. Word Vectors. Singular Value Decomposition. Skip-gram. Continuous Bag of Words (CBOW). Negative Sampling. Hierarchical Softmax. Word2Vec.\n\n-------------------------------------------------------------------------------\n\nNatural Language Processing with Deep Learning\n\nInstructors:\n- Chris Manning\n- Richard Socher\n\nNatural language processing (NLP) deals with the key artificial intelligence technology of understanding complex human language communication. This lecture series provides a thorough introduction to the cutting-edge research in deep learning applied to NLP, an approach that has recently obtained very high performance across many different NLP tasks including question answering and machine translation. It emphasizes how to implement, train, debug, visualize, and design neural network models, covering the main technologies of word vectors, feed-forward models, recurrent neural networks, recursive neural networks, convolutional neural networks, and recent models involving a memory component.\n\nFor additional learning opportunities please visit:\nhttp://stanfordonline.stanford.edu/", "defaultAudioLanguage": "en", "channelTitle": "Stanford University School of Engineering", "publishedAt": "2017-04-03T19:49:55Z", "title": "Lecture 1 | Natural Language Processing with Deep Learning", "tags": ["Word2Vec", "Natural Language Processing", "Word Vectors", "Singular Value Decomposition", "Skip-gram", "Continuous Bag of Words", "CBOW", "Negative Sampling", "Hierarchical Softmax"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "8jazNUpO3lQ", "statistics": {"dislikeCount": "63", "commentCount": "541", "likeCount": "2224", "viewCount": "165000", "favoriteCount": "0"}, "contentDetails": {"duration": "PT15M14S"}, "snippet": {"channelId": "UCh9nVJoWXmFb7sLApWGcLPQ", "description": "In this tutorial we will predict home prices using linear regression. We use training data that has home areas in square feet and corresponding prices and train a linear regression model using sklearn linear regression class. Later on predict method is used on linear regression object to make actual forecast. \n\n#MachineLearning #PythonMachineLearning #MachineLearningTutorial #Python #PythonTutorial #PythonTraining #MachineLearningCource #LinearRegression\n\nTopics that are covered in this Machine Learning Video:\n0:00 Simple linear regression\n1:59 Linear equation\n2:43 Import sklearn library\n2:22 Import data in dataframe \n3:52 Plot scatter plot \n5:26 Create Linear Regression object \n13:35 Exercise at the end to predict canada's per capita income\n\nTopic Highlights:\n1) What is linear regression\n2) Mean squared error\n3) Predict home prices by minimizing mean squared error (or MSE)\n4) Exercise at the end to predict canada's per capita income \n\nNext Video: \nMachine Learning Tutorial Python - 3: Linear Regression Multiple Variables: https://www.youtube.com/watch?v=J_LnPL3Qg70&list=PLeo1K3hjS3uvCeTYTeyfe0-rN5r8zn9rw&index=3\n\nVery Simple Explanation Of Neural Network: https://www.youtube.com/watch?v=ER2It2mIagI\n\nCode: https://github.com/codebasics/py/tree/master/ML/1_linear_reg\nCorrection: at 6:53, use reg.predict([[3300]]) instead of reg.predict(3300) as api specification has changed.\nExercise solution: https://github.com/codebasics/py/blob/master/ML/1_linear_reg/1_linear_regression.ipynb\n\nPopulor Playlist:\nData Science Full Course: https://www.youtube.com/playlist?list=PLeo1K3hjS3us_ELKYSj_Fth2tIEkdKXvV\n\nData Science Project: https://www.youtube.com/watch?v=rdfbcdP75KI&list=PLeo1K3hjS3uu7clOTtwsp94PcHbzqpAdg\n\nMachine learning tutorials: https://www.youtube.com/watch?v=gmvvaobm7eQ&list=PLeo1K3hjS3uvCeTYTeyfe0-rN5r8zn9rw\n\nPandas: https://www.youtube.com/watch?v=CmorAWRsCAw&list=PLeo1K3hjS3uuASpe-1LjfG5f14Bnozjwy\n\nmatplotlib: https://www.youtube.com/watch?v=qqwf4Vuj8oM&list=PLeo1K3hjS3uu4Lr8_kro2AqaO6CFYgKOl\n\nPython: https://www.youtube.com/watch?v=eykoKxsYtow&list=PLeo1K3hjS3usILfyvQlvUBokXkHPSve6S\n\nJupyter Notebook: https://www.youtube.com/watch?v=q_BzsPxwLOE&list=PLeo1K3hjS3uuZPwzACannnFSn9qHn8to8\n\nTo download csv and code for all tutorials: go to https://github.com/codebasics/py, click on a green button to clone or download the entire repository and then go to relevant folder to get access to that specific file.\n\nWebsite: http://codebasicshub.com/\nFacebook: https://www.facebook.com/codebasicshub\nTwitter: https://twitter.com/codebasicshub", "defaultAudioLanguage": "en", "channelTitle": "codebasics", "publishedAt": "2018-07-01T19:19:47Z", "title": "Machine Learning Tutorial Python - 2: Linear Regression Single Variable", "tags": ["linear regression in python", "python linear regression", "python linear regression tutorial", "sklearn linear regression", "sklearn tutorial python", "python machine learning regression", "sklearn linear_model", "linear regression using sklearn python machine learning for beginners", "machine learning with python", "machine learning tutorial", "python machine learning for beginners", "machine learning tutorial for beginners", "machine learning tutorial for beginners python"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "URERdVb-lpg", "statistics": {"commentCount": "7", "viewCount": "14156", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H23M46S"}, "snippet": {"channelTitle": "H2O.ai", "channelId": "UCk6ONJlPzjw3DohAeMSgsng", "publishedAt": "2018-03-19T23:44:24Z", "title": "Introduction to Deep Learning, Keras, and TensorFlow", "description": "This meetup was held in Mountain View on March 13, 2018. \n\nThis fast-paced session starts with a simple yet complete neural network (no frameworks), followed by an overview of activation functions, cost functions, backpropagation, and then a quick dive into CNNs. Next, we'll create a neural network using Keras, followed by an introduction to TensorFlow and TensorBoard. For best results, familiarity with basic vectors and matrices, inner (aka \"dot\") products of vectors, and rudimentary Python is definitely helpful. If time permits, we'll look at the UAT, CLT, and the Fixed Point Theorem. (Bonus points if you know Zorn's Lemma, the Well-Ordering Theorem, and the Axiom of Choice.)\n\nOswald's Bio:\n\nOswald is an education junkie: a former Ph.D. Candidate in Mathematics (ABD), with multiple Master's and 2 Bachelor's degrees. In a previous career, he worked in South America, Italy, and the French Riviera, which enabled him to travel to 70 countries throughout the world.\n\nHe has worked in American and Japanese corporations and start-ups, as C/C++ and Java developer to CTO. He works in the web and mobile space, conducts training sessions in Android, Java, Angular 2, and ReactJS, and he writes graphics code for fun. He's comfortable in four languages and aspires to become proficient in Japanese, ideally sometime in the next two decades. He enjoys collaborating with people who share his passion for learning the latest cool stuff, and he's currently working on his 15th book, which is about Angular 2."}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "bs2dgoeNrkQ", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "32", "viewCount": "1577", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H2M22S"}, "snippet": {"channelId": "UCbaxUExGKrH2zxY4AkY9wCg", "description": "Catch Lecture 2 from the 3-week, 9-part SIGGRAPH Now webinar series, \u201cHands-on Workshop: Machine Learning and Neural Networks.\u201d This lecture offers a discussion of Regression, Feed-forward Neural Networks, and Classification, from instructor Rajesh Sharma, of Walt Disney Animation Studios. Held weekly during the month of June 2020, you can view each Course lecture here: https://www.youtube.com/watch?v=gfY2LfRfE1E&list=PLUPhVMQuDB_b2kcOooEduedthcBH53mvC \n\nBelow are some helpful links and resources.\n\nPrerequisites \u2013 https://blog.siggraph.org/2020/06/participate-in-the-next-siggraph-webinar-machine-learning-and-neural-networks.html/ \nCourse files \u2013 https://drive.google.com/drive/folders/1TM5oz4Rr2ji8mVzmMzrHMJlPg9k1pdud?usp=sharing \n\nWant to attend future live, SIGGRAPH Now webinars? Visit the SIGGRAPH 2020 website for a list of upcoming sessions: https://s2020.siggraph.org/conference/siggraph-now-supplemental-learning/ \n\n#SIGGRAPHNow #SIGGRAPH2020 #MachineLearning", "defaultAudioLanguage": "en", "channelTitle": "ACMSIGGRAPH", "publishedAt": "2020-06-12T00:37:53Z", "title": "SIGGRAPH Now | Hands-on Workshop: Machine Learning and Neural Networks \u2013 Lecture 2", "tags": ["ACM", "ACM SIGGRAPH", "#SIGGRAPH2020", "SIGGRAPH 2020", "Neural Networks", "Machine Learning", "Walt Disney Animation Studios"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "5e0TbyCkbCY", "statistics": {"dislikeCount": "3", "commentCount": "10", "likeCount": "162", "viewCount": "10679", "favoriteCount": "0"}, "contentDetails": {"duration": "PT41M15S"}, "snippet": {"channelTitle": "PyCon 2017", "channelId": "UCrJhliKNQ8g0qoE_zvL8eVg", "publishedAt": "2017-05-20T17:57:15Z", "title": "Michelle Fullwood   A gentle introduction to deep learning with TensorFlow   PyCon 2017", "description": "\"Speaker: Michelle Fullwood\n\nDeep learning's explosion of spectacular results over the past few years may make it appear esoteric and daunting, but in reality, if you are familiar with traditional machine learning, you're more than ready to start exploring deep learning. This talk aims to gently bridge the divide by demonstrating how deep learning operates on core machine learning concepts and getting attendees started coding deep neural networks using Google's TensorFlow library.\n\nSlides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides\""}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "NA7Fn8KVdSA", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "12", "viewCount": "1100", "favoriteCount": "0"}, "contentDetails": {"duration": "PT57M43S"}, "snippet": {"channelId": "UCkzRf1fcN88dP73DLNYIwLg", "description": "Wondering how you can use machine learning, and more specifically deep learning technologies, to get a jump on the competition? This webinar will provide a brief, high-level overview of machine learning and its applications before delving into Harris\u2019 five year head start developing deep learning technologies that are being deployed today. Harris has applied deep learning algorithms to solve remote sensing problems like target detection, feature extraction, and classification challenges.", "defaultAudioLanguage": "en-US", "channelTitle": "L3Harris Geospatial Solutions", "publishedAt": "2019-02-08T20:10:32Z", "title": "Machine Learning: Automate Remote Sensing Analytics to Gain a Competitive Advantage | Webinar", "tags": ["MEGA", "deep learning", "machine learning", "remote sensing", "webinar"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "IDNXZitcQng", "statistics": {"commentCount": "0", "viewCount": "218", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H3M14S"}, "snippet": {"channelId": "UCjnWysJh9-r9wo82zlbMT3A", "description": "An ICERM Public Lecture - Bias in bios: fairness in a high-stakes machine-learning setting\n\nMachine learning algorithms form biases, like humans, based on the data they observe. However, unlike humans, the algorithms can readily admit their biases when probed appropriately. Using publicly available lists of names, we enumerate biases in an unsupervised fashion from word embeddings trained on public data. Gender, racial, and religious biases emerge, among others. We then analyze the effects of these biases on a problem motivated by recommending jobs to candidates. To collect data for this task, we extract hundreds of thousands of third-person bios from the web. The straightforward application of machine learning is found to amplify some biases. However, unlike humans, it is easy to put in place algorithmic corrections to mitigate this bias amplification.\n\nJoint work with: Maria De Arteaga (CMU); Alexey Romanov (UMass Lowell); Nat Swinger (Lexington HS); Tom Heffernan (Shrewsbury HS); Christian Borgs, Jennifer Chayes, and Hanna Wallach (MSR); Alex Chouldechova (CMU; Mark Leiserson (UMD); Sahin Geyik and Krishnaram Kenthapadi (LinkedIn)\n\n\nAbout the speaker: \n\nAdam Tauman Kalai received his BA from Harvard, and MA and PhD under the supervision of Avrim Blum from CMU. After an NSF postdoctoral fellowship at M.I.T. with Santosh Vempala, he served as an assistant professor at the Toyota Technological Institute at Chicago and then at Georgia Tech. He is now a Principal Researcher at Microsoft Research New England. His honors include an NSF CAREER award and an Alfred P. Sloan fellowship. His research focuses on Artificial Intelligence and Machine Learning algorithms.\n\nThursday, March 21, 2019\nBrown University", "defaultAudioLanguage": "en", "channelTitle": "Brown University", "publishedAt": "2019-04-18T20:17:46Z", "title": "An ICERM Public Lecture - Bias in bios: fairness in a high-stakes machine-learning setting", "tags": ["brown", "brown u", "brown university", "brown providence", "providence", "rhode island", "ivy league", "brown university youtube", "brown u youtube", "Adam Tauman Kalai", "ICERM", "mathematics", "Brown University", "math", "math research", "lecture", "education", "computer science", "machine learning", "data visualization", "algorithms", "data analysis"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "PdjV8seCKH4", "statistics": {"dislikeCount": "0", "likeCount": "15", "viewCount": "2125", "favoriteCount": "0"}, "contentDetails": {"duration": "PT48M8S"}, "snippet": {"channelId": "UCIvcN8QNgRGNW9osYLGsjQQ", "description": "Speaker\nStevan E Plote, Nokia\nSteve Plote, is Optics Consulting Engineer at Nokia. He is currently responsible for the support of all Nokia sales teams in the Americas as well as Channel Partners. Focusing on Network designs for the delivery of real time, next generation services for Video transport, Carrier Ethernet, Consumer Content Distribution, and Cloud Computing. He has personal responsibility for the network engineering and support for the WEB2.0 and Content Service Providers in North America. Mr. Plote has more than 30 years of experience in Data Center Interconnect, Telecommunications and LAN switching and transmission solutions. Prior to joining Nokia, he was Solutions Business Development and CSP Systems Engineering at BTI Systems and prior to that was Solutions Sales Director at Tellabs. He has many professional memberships and committee involvements including NANOG PC, OFC, MEF, IEEE, OSA, IEC and IETF.\nJesse Simsarian, Nokia Bell Labs\nJesse Simsarian is a Member of Technical Staff at Nokia Bell Labs in New Jersey. Since joining Bell Labs in 2000, he has been researching optical networks and switching, including software-defined transport networks, fast-switching coherent optical receivers, and scalable optical packet routers. From 1998 to 2000 he had a National Research Council Fellowship at the National Institute of Standards and Technology in Gaithersburg, MD. He received the Masters and Ph.D. degrees in physics from SUNY Stony Brook in 1995 and 1998, respectively, and is a Senior Member of the OSA and IEEE. He has authored or co-authored 80 publications and holds several patents.\n\nAbstract\n Optical transport networks are evolving to have unprecedented flexibility with advances such as finely tunable bitrate transponders, allowing dynamically-adaptive operation near the fiber Shannon capacity limit at near zero system margin. How Network Operators and Content Providers can take advantage of this without having a staff of optical experts to drive network performance optimization is by using machine learning. Furthermore, the development of network operating systems (OS) enables network programmability and support for multi-vendor network elements. Network operating systems lay the foundations for advanced machine learning algorithms that operate on an abstracted network representation presented by the network OS. Refinement of the network model parameters improves the machine learning results and its representation of the actual network. In this talk we will discuss how network sensing, machine learning, and actions taken by the network OS can lead to a more optimized network that can efficiently support traditional and cloud network services. We will also point to challenges that will have to be overcome to make such networks commercially viable.", "channelTitle": "TeamNANOG", "publishedAt": "2017-10-05T17:13:03Z", "title": "How to use Machine Learning for Testing and Implementing Optical Networks", "tags": ["NANOG 71"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "f2S4hVs-ESw", "statistics": {"dislikeCount": "2", "likeCount": "50", "viewCount": "5824", "favoriteCount": "0"}, "contentDetails": {"duration": "PT33M48S"}, "snippet": {"channelId": "UCfkof0bu-9c3JDYo46Z8NIA", "description": "We motivate bagging as follows: Consider the regression case, and suppose we could create a bunch (say B) prediction functions based on independent training samples of size n. If we average together these prediction functions, the expected value of the average is the same as any one of the functions, but the variance would have decreased by a factor of 1/B -- a clear win! Of course, this would require an overall sample of size nB. The idea of bagging is to replace independent samples with bootstrap samples from a single data set of size n. Of course, the bootstrap samples are not independent, so much of our discussion is about when bagging does and does not lead to improved performance. Random forests were invented as a way to create conditions in which bagging works better.\n\nMore...Although it's hard to find crisp theoretical results describing when bagging helps, conventional wisdom says that it helps most for models that are \"high variance\", which in this context means the prediction function may change a lot when you train with a new random sample from the same distribution, and \"low bias\", which basically means fitting the training data well. Large decision trees have these characteristics and are usually the model of choice for bagging. Random forests are just bagged trees with one additional twist: only a random subset of features are considered when splitting a node of a tree. The hope, very roughly speaking, is that by injecting this randomness, the resulting prediction functions are less dependent, and thus we'll get a larger reduction in variance. In practice, random forests are one of the most effective machine learning models in many domains.\n\nAccess the full course at  https://bloom.bg/2ui2T4q", "channelTitle": "Inside Bloomberg", "publishedAt": "2018-07-11T13:08:06Z", "title": "22. Bagging and Random Forests", "tags": ["machine learning", "machine learning jobs", "applications of machine learning", "David Rosenberg", "machine learning engineering jobs", "bloomberg careers", "mathematics", "python", "Bagging and Random Forests"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "keb1TSg-bAs", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "28", "viewCount": "1755", "favoriteCount": "0"}, "contentDetails": {"duration": "PT33M32S"}, "snippet": {"channelId": "UCIhgggPw5xfIpFf-Fxqpmxw", "description": "Informations et inscription sur http://www.usievents.com\n\nDr. Colin Williams is Vice President of Strategy & Corporate Development. He holds a Ph.D. in artificial intelligence from the University of Edinburgh, a M.Sc. and D.I.C. in atmospheric physics and dynamics from Imperial College, University of London, and a B.Sc. (with Hons.) in mathematical physics from the University of Nottingham. He was formerly a research assistant in general relativity & quantum cosmology to Prof. Stephen W. Hawking, at the University of Cambridge, a research scientist at Xerox PARC, and an acting Associate Professor of Computer Science at Stanford University. \n\nDr. Williams wrote the first book on quantum computing, \"Explorations in Quantum Computing\", and followed it up with two others, started the Quantum Computing Group at the NASA Jet Propulsion Laboratory, Caltech, and quickly broadened its scope to include research on quantum communications, quantum key distribution, quantum sensors, and quantum metrology.\n\nSuivez USI sur Twitter : https://twitter.com/USIEvents\nRetrouvez USI sur LinkedIn : http://linkd.in/13Ls21Y\nAbonnez-vous \u00e0 notre chaine : http://bit.ly/19sPpSp", "defaultAudioLanguage": "fr", "channelTitle": "USI Events", "publishedAt": "2018-07-05T08:13:31Z", "title": "Quantum Computing : From Theory to Real-World Applications - Colin Williams, at USI", "tags": ["USIEvents", "USI", "OCTO", "Technology", "talk", "innovation", "quantum", "computing"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "Ow25mjFjSmg", "statistics": {"dislikeCount": "3", "commentCount": "44", "likeCount": "1064", "viewCount": "34562", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H19M21S"}, "snippet": {"channelId": "UCSHZKyawb77ixDdsGog4iWA", "description": "Lecture by Vladimir Vapnik in January 2020, part of the MIT Deep Learning Lecture Series.\nSlides: http://bit.ly/2ORVofC\nAssociated podcast conversation: https://www.youtube.com/watch?v=bQa7hpUpMzM\nSeries website: https://deeplearning.mit.edu\nPlaylist: http://bit.ly/deep-learning-playlist\n\nOUTLINE:\n0:00 - Introduction\n0:46 - Overview: Complete Statistical Theory of Learning\n3:47 - Part 1: VC Theory of Generalization\n11:04 - Part 2: Target Functional for Minimization\n27:13 - Part 3: Selection of Admissible Set of Functions\n37:26 - Part 4: Complete Solution in Reproducing Kernel Hilbert Space (RKHS)\n53:16 - Part 5: LUSI Approach in Neural Networks\n59:28 - Part 6: Examples of Predicates\n1:10:39 - Conclusion\n1:16:10 - Q&A: Overfitting\n1:17:18 - Q&A: Language\n\nCONNECT:\n- If you enjoyed this video, please subscribe to this channel.\n- Twitter: https://twitter.com/lexfridman\n- LinkedIn: https://www.linkedin.com/in/lexfridman\n- Facebook: https://www.facebook.com/lexfridman\n- Instagram: https://www.instagram.com/lexfridman", "defaultAudioLanguage": "en-US", "channelTitle": "Lex Fridman", "publishedAt": "2020-02-15T15:11:09Z", "title": "Complete Statistical Theory of Learning (Vladimir Vapnik) | MIT Deep Learning Series", "tags": ["statistical learning theory", "vladimir vapnik", "deep learning", "artificial intelligence", "mit deep learning"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "lslW1xNR0Co", "statistics": {"dislikeCount": "0", "likeCount": "10", "viewCount": "435", "favoriteCount": "0"}, "contentDetails": {"duration": "PT48M10S"}, "snippet": {"channelId": "UCciKHCG06rnq31toLTfAiyw", "description": "Jack Moffitt\n\nhttps://2019.linux.conf.au/schedule/presentation/116/\n\nMassive amounts of data have accelerated advances in machine learning, and companies are collecting as much as they can to power their algorithms. Deep learning has many amazing and practical use cases, but the privacy and social implications of current machine learning systems leave much to be desired.\r\n\r\nAt Mozilla, we're trying to make machine learning systems that empower people and ensure their privacy. I'll give an overview of deep learning, its applications, and how it all works. Then I'll demo and talk about several machine learning projects at Mozilla, including Deep Speech, speech synthesis, grammar checking, and more. At the end, you'll know what the hype is about, and how you can use these technologies for good.\n\nlinux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/\n\n#linux.conf.au #linux #foss #opensource", "channelTitle": "linux.conf.au", "publishedAt": "2019-01-24T21:24:07Z", "title": "Deep Learning, Not Deep Creepy", "tags": ["lca", "lca2019", "#linux.conf.au#linux#foss#opensource", "JackMoffitt"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "rgQxVdOr9b8", "statistics": {"dislikeCount": "1", "commentCount": "2", "likeCount": "62", "viewCount": "4559", "favoriteCount": "0"}, "contentDetails": {"duration": "PT9M42S"}, "snippet": {"channelId": "UCDia_lkNYKLJVLRLQl_-pFw", "description": "Keras in Motion is your key to learning how to use the Keras Deep Learning Python library. Here's a free clip! Get the entire course for 40% off with code ytvanboxel at: http://bit.ly/2iHsZMw.\n\nIf you want all the latest news coming out of Manning Publications, including exclusive deals on all our products, sign up for our mailing list here: \nhttps://www.manning.com/mail-preferences?utm_medium=social&utm_source=youtube&utm_campaign=mailing_list&utm_content=kerasinmotion_promo_04\n\nJust getting started with data science. Learn more about it here, with this FREE ebook! https://www.manning.com/books/exploring-data-science?utm_medium=social&utm_source=youtube&utm_campaign=free_ebook_exploringdatascience&utm_content=kerasinmotion_promo_04", "channelTitle": "Manning Publications", "publishedAt": "2017-07-18T15:00:06Z", "title": "Linear Regression in Keras", "tags": ["Keras", "Machine Learning", "Deep Learning", "Python", "Data Science"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "Ntl_WNW8yGc", "statistics": {"dislikeCount": "2", "commentCount": "1", "likeCount": "39", "viewCount": "5708", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H16M57S"}, "liveStreamingDetails": {"actualStartTime": "2019-05-28T16:28:45Z"}, "snippet": {"channelId": "UCW1C2xOfXsIzPgjXyuhkw9g", "description": "Peter Bartlett (UC Berkeley) and Sasha Rakhlin (Massachusetts Institute of Technology)\nhttps://simons.berkeley.edu/talks/generalization-i\nDeep Learning Boot Camp", "channelTitle": "Simons Institute", "publishedAt": "2019-05-28T18:26:29Z", "title": "Generalization I", "tags": ["Simons Institute", "Theory of Computing", "Theory of Computation", "Theoretical Computer Science", "Computer Science", "UC Berkeley", "Deep Learning Boot Camp", "Peter Bartlett", "Sasha Rakhlin"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "fChBkJ_UjRw", "statistics": {"dislikeCount": "0", "commentCount": "3", "likeCount": "19", "viewCount": "2572", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H19M6S"}, "snippet": {"channelTitle": "Carnegie Mellon University Deep Learning", "channelId": "UC8hYZGEkI2dDO8scT8C5UQA", "publishedAt": "2019-09-23T23:21:41Z", "title": "Lecture 7 | Acceleration, Regularization, and Normalization", "description": "Carnegie Mellon University\nCourse: 11-785, Intro to Deep Learning\nOffering: Fall 2019\n\nFor more information, please visit: http://deeplearning.cs.cmu.edu/\n\nContents:\n\u2022 Stochastic gradient descent\n\u2022 Acceleration\n\u2022 Overfitting and regularization\n\u2022 Tricks of the trade:\n  - Choosing a divergence (loss) function\n  - Batch normalization\n  - Dropout"}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "5R5tAlKd2SY", "statistics": {"dislikeCount": "3", "commentCount": "11", "likeCount": "45", "viewCount": "2174", "favoriteCount": "0"}, "contentDetails": {"duration": "PT9M9S"}, "snippet": {"channelId": "UCuWECsa_za4gm7B3TLgeV_A", "description": "Provides an overview of top 10 machine learning algorithms for beginners and discussion about data quality. \nBecoming Data Scientist: https://goo.gl/JWyyQc\nIntroductory R Videos:  https://goo.gl/NZ55SJ\nMachine Learning videos: https://goo.gl/WHHqWP\nDeep Learning with TensorFlow: https://goo.gl/5VtSuC\nImage Analysis & Classification:  https://goo.gl/Md3fMi\nText mining: https://goo.gl/7FJGmd\nData Visualization: https://goo.gl/Q7Q2A8\nPlaylist: https://goo.gl/iwbhnE", "defaultAudioLanguage": "en", "channelTitle": "Dr. Bharatendra Rai", "publishedAt": "2018-07-22T09:24:48Z", "title": "Top Ten Machine Learning Algorithms | The Bad, The good, The Better data", "tags": ["data science", "big data", "analytics", "machine learning", "ai"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "mWVvFFyfU3w", "statistics": {"dislikeCount": "0", "commentCount": "2", "likeCount": "2", "viewCount": "833", "favoriteCount": "0"}, "contentDetails": {"duration": "PT12M41S"}, "snippet": {"channelId": "UCtOiaxdcY_6RsRUpBg_2LoQ", "description": "Presenter: G. Gordon Brown\nGordon Brown discusses the new procedures, features, and functions that are available in SAS/STAT 14.3, which was released in the Fall of 2017. \n\nSUBSCRIBE TO THE SAS SOFTWARE YOUTUBE CHANNEL\nhttp://www.youtube.com/subscription_center?add_user=sassoftware\n\nABOUT SAS\nSAS is the leader in analytics. Through innovative analytics, business intelligence and data management software and services, SAS helps customers at more than 75,000 sites make better decisions faster. Since 1976, SAS has been giving customers around the world THE POWER TO KNOW\u00ae.\n\nVISIT SAS\nhttp://www.sas.com\n\nCONNECT WITH SAS\nSAS \u25ba http://www.sas.com\nSAS Customer Support \u25ba http://support.sas.com\nSAS Communities \u25ba http://communities.sas.com\nFacebook \u25ba https://www.facebook.com/SASsoftware\nTwitter \u25ba https://www.twitter.com/SASsoftware\nLinkedIn \u25ba http://www.linkedin.com/company/sas\nGoogle+  \u25ba https://plus.google.com/+sassoftware\nBlogs \u25ba http://blogs.sas.com\nRSS  \u25bahttp://www.sas.com/rss", "defaultAudioLanguage": "en", "channelTitle": "SAS Software", "publishedAt": "2018-11-28T20:19:22Z", "title": "SAS/STAT 14.3: Modern Methods for the Modern Statistician", "tags": ["SAS/STAT", "Advanced Analytics"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "YJP7sF70g5U", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "5", "viewCount": "340", "favoriteCount": "0"}, "contentDetails": {"duration": "PT1H5M22S"}, "snippet": {"channelId": "UCXiLz7XTVCJ02CuFmwtcvQw", "description": "Full Talk Title: \"Bottlenecks, Representations, and Fairness: Information-Theoretic Tools for Machine Learning\"\n\nPresented by Flavio P. Calmon, Assistant Professor of Electrical Engineering at Harvard's School of Engineering\n\nTalk Description:  Information theory can shed light on the algorithm-independent limits of learning from data and serve as a design driver for new machine learning algorithms. In this talk, Dr. Calmon will discuss a set of flexible information-theoretic tools that can be used to (i) understand fairness and discrimination by machine learning models and (ii) characterize data representations learned by complex learning models. He will illustrate these techniques in both synthetic and real-world datasets, and discuss future research directions. \n\nSpeaker Bio:  Flavio P. Calmon is an Assistant Professor of Electrical Engineering at Harvard's John A. Paulson School of Engineering and Applied Sciences. Before joining Harvard, he was the inaugural data science for social good post-doctoral fellow at IBM Research in Yorktown Heights, New York. He received his Ph.D. in Electrical Engineering and Computer Science at MIT. His main research interests are information theory, inference, and statistics, with applications to privacy, fairness, machine learning, and content distribution. \n\nPlease visit our website for additional information about IACS and our  Data Science/Computational Science seminar series: iacs.seas.harvard.edu", "defaultAudioLanguage": "en", "channelTitle": "Harvard Institute for Applied Computational Science", "publishedAt": "2018-11-13T20:55:00Z", "title": "IACS Seminar: Information-Theoretic Tools for Machine Learning 11/9", "tags": ["Harvard University", "Harvard School of Engineering and Applied Sciences", "IACS", "Harvard SEAS", "Institute for Applied Computational Science", "Data Science", "Computational Science", "Flavio Calmon HSEAS", "Information Theory", "Machine Learning", "Information-Theoretic Tools"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "dRg7tHetvQQ", "statistics": {"dislikeCount": "0", "commentCount": "0", "likeCount": "2", "viewCount": "141", "favoriteCount": "0"}, "contentDetails": {"duration": "PT25M25S"}, "snippet": {"channelId": "UCy0iTe4GdNbyq2S7_Chligg", "description": "Dr. Daniel Soudry, Technion\n\n\"Hardware for AI Track\"\nAI Week\nYuval Ne'eman Workshop for Science, Technology and Security\nTel Aviv University\n18.11.19", "defaultAudioLanguage": "iw", "channelTitle": "TAUVOD", "publishedAt": "2019-11-28T13:29:01Z", "title": "Resource-Efficient Quantized Deep Learning", "tags": ["\u05d0\u05d5\u05e0\u05d9\u05d1\u05e8\u05e1\u05d9\u05d8\u05ea \u05ea\u05dc \u05d0\u05d1\u05d9\u05d1", "Hardware for AI Track", "AI Week", "Yuval Ne'eman Workshop for Science", "Technology and Security", "Tel Aviv University"]}}]}, {"q": "bias correction for selecting the minimal-error classifier from many machine learning models", "idx_paper": "10.1093/bioinformatics/btu520", "items": [{"id": "ehGMpLeVgPs", "statistics": {"dislikeCount": "1", "commentCount": "0", "likeCount": "13", "viewCount": "998", "favoriteCount": "0"}, "contentDetails": {"duration": "PT55M47S"}, "snippet": {"channelTitle": "Machine Learning at Berkeley", "channelId": "UCXweTmAk9K-Uo9R6SmfGtjg", "publishedAt": "2017-11-02T04:45:01Z", "title": "Data Science Decal Fall 2017 Lecture 4: Logistic Regression and Regularization", "description": "The fourth lecture of the data science decal on Logistic Regression and Regularization. Check the website for updates: https://ml.berkeley.edu/decals/DLD\nand the repository for slides: https://github.com/mlberkeley/Deep-Learning-Decal-Fall-2017"}}]}]