{"citation_id": "63258664", "queriedAt": "2020-05-09 12:36:48", "completed": "0", "twitter": {"1240479469162414081": {"followers": "1,747", "content_summary": "@pfau There's the capsule autoencoder for MNIST, but for ImageNet this is still state of the art when it comes to feature extraction with latent variable generative models, I think: https://t.co/RJrtgTKBER", "author": "@peligrietzer", "datetime": "2020-03-19 03:25:27"}, "1199896318887518211": {"followers": "47", "content_summary": "RT @DeepMindAI: We\u2019ve released pre-trained BigBiGAN representation learning models https://t.co/Rhm94rOuX5 on TF Hub: https://t.co/E18skH2i\u2026", "author": "@shivanishah33", "datetime": "2019-11-28 03:42:30"}, "1199922881712386048": {"followers": "48", "content_summary": "RT @DeepMindAI: We\u2019ve released pre-trained BigBiGAN representation learning models https://t.co/Rhm94rOuX5 on TF Hub: https://t.co/E18skH2i\u2026", "author": "@congnghia0609", "datetime": "2019-11-28 05:28:03"}, "1200075819273592832": {"followers": "179,040", "content_summary": "RT @Montreal_AI: \"Large Scale Adversarial Representation Learning\" Jeff Donahue and Karen Simonyan: https://t.co/Xy5VLuUoXl #GenerativeMo\u2026", "author": "@Montreal_AI", "datetime": "2019-11-28 15:35:46"}}, "tab": "twitter"}