{"citation_id": "74424065", "completed": "1", "queriedAt": "2020-05-14 12:32:00", "tab": "twitter", "twitter": {"1250654260745900032": {"content_summary": "RT @Thinker_no_pain: \u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647 \u06cc\u0647 \u062a\u0639\u062f\u0627\u062f \u0641\u0631\u0645\u0648\u0644 \u0627\u0631\u0627\u0626\u0647 \u0645\u06cc\u062f\u0647 \u06a9\u0647 \u0646\u0634\u0648\u0646 \u0645\u06cc\u062f\u0647 \u062f\u0631\u0646\u0647\u0627\u06cc\u062a \u0627\u0646\u062f\u0627\u0632\u0647 batch\u060c \u062a\u0639\u062f\u0627\u062f \u0646\u0645\u0648\u0646\u0647\u200c\u0647\u0627\u060c \u062a\u0627\u0628\u0639 \u0644\u0627\u0633 \u0648 \u062a\u0639\u062f\u0627\u062f epoch \u0645\u0648\u062b\u0631 \u0686\u0647\u2026", "followers": "34", "datetime": "2020-04-16 05:16:26", "author": "@SSepahyar"}, "1231264153957879808": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "15", "datetime": "2020-02-22 17:07:04", "author": "@uthamkamath"}, "1220661573653532672": {"content_summary": "RT @arankomatsuzaki: Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size,\u2026", "followers": "1,217", "datetime": "2020-01-24 10:56:12", "author": "@pragmaticml"}, "1230235576210804737": {"content_summary": "RT @LukaszJDebowski: I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team! https://t.co/s\u2026", "followers": "3,577", "datetime": "2020-02-19 20:59:52", "author": "@hubofml"}, "1221640414236676098": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "3,577", "datetime": "2020-01-27 03:45:46", "author": "@hubofml"}, "1220908145079463937": {"content_summary": "RT @theshawwn: This is a highly interesting paper. I haven't dug into it deeply yet, but they examine the scaling effects of model size for\u2026", "followers": "485", "datetime": "2020-01-25 03:15:59", "author": "@bitking69"}, "1253398495408594944": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "5", "datetime": "2020-04-23 19:01:02", "author": "@AR_Kazemipour"}, "1253556684657737728": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "30", "datetime": "2020-04-24 05:29:38", "author": "@NdyNyah"}, "1255030714896015360": {"content_summary": "Much needed...", "followers": "2", "datetime": "2020-04-28 07:06:54", "author": "@raahulanand"}, "1221555611751669760": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "1,593", "datetime": "2020-01-26 22:08:47", "author": "@YohkoHatada"}, "1224304885283336192": {"content_summary": "RT @milesboard: Scaling Laws for Neural Language Models #deeplearning https://t.co/2e42Tb4K2K https://t.co/nGbrXzyTi4", "followers": "267", "datetime": "2020-02-03 12:13:25", "author": "@LazyBot6"}, "1250508714446065665": {"content_summary": "RT @Thinker_no_pain: \u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647 \u06cc\u0647 \u062a\u0639\u062f\u0627\u062f \u0641\u0631\u0645\u0648\u0644 \u0627\u0631\u0627\u0626\u0647 \u0645\u06cc\u062f\u0647 \u06a9\u0647 \u0646\u0634\u0648\u0646 \u0645\u06cc\u062f\u0647 \u062f\u0631\u0646\u0647\u0627\u06cc\u062a \u0627\u0646\u062f\u0627\u0632\u0647 batch\u060c \u062a\u0639\u062f\u0627\u062f \u0646\u0645\u0648\u0646\u0647\u200c\u0647\u0627\u060c \u062a\u0627\u0628\u0639 \u0644\u0627\u0633 \u0648 \u062a\u0639\u062f\u0627\u062f epoch \u0645\u0648\u062b\u0631 \u0686\u0647\u2026", "followers": "147", "datetime": "2020-04-15 19:38:05", "author": "@bigmpx"}, "1230938025225383936": {"content_summary": "Nice paper \"Scaling Laws for Neural Language Models\" shows the trade-offs between model complexity, data size, and compute time: https://t.co/T8wCKijNj2 For optimal performance all three factors must be scaled up in tandem. https://t.co/wZdH0BiaLb", "followers": "7,309", "datetime": "2020-02-21 19:31:09", "author": "@sweis"}, "1248718645607280642": {"content_summary": "https://t.co/m1fqe58Qid", "followers": "361", "datetime": "2020-04-10 21:04:59", "author": "@saswat_sahu"}, "1254141299533504513": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "166", "datetime": "2020-04-25 20:12:41", "author": "@dr_levan"}, "1231264176376438785": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "3,577", "datetime": "2020-02-22 17:07:10", "author": "@hubofml"}, "1221014928036896768": {"content_summary": "RT @theshawwn: This is a highly interesting paper. I haven't dug into it deeply yet, but they examine the scaling effects of model size for\u2026", "followers": "39", "datetime": "2020-01-25 10:20:18", "author": "@WilliamAriasZ"}, "1229568133129572352": {"content_summary": "Scaling Laws for Neural Language Models from @OpenAI https://t.co/kdOGvQCGsn", "followers": "6,127", "datetime": "2020-02-18 00:47:41", "author": "@StephenPiment"}, "1220622568534749186": {"content_summary": "Scaling Laws for Neural Language Models https://t.co/syxVwhgetH", "followers": "1,086", "datetime": "2020-01-24 08:21:13", "author": "@ak1010"}, "1233242750956589056": {"content_summary": "Roundup: Scaling laws for language models: https://t.co/ggxMlZDqMr Training a trillion parameter model: https://t.co/CaHbvhhSIh Improving GANs with compressed sensing: https://t.co/GuuhJkfkmS", "followers": "0", "datetime": "2020-02-28 04:09:19", "author": "@academic_spam"}, "1229729446204182533": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "303", "datetime": "2020-02-18 11:28:41", "author": "@cepera_ang"}, "1229886046160277504": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "111", "datetime": "2020-02-18 21:50:58", "author": "@alsombra7"}, "1220854309069312000": {"content_summary": "[2001.08361] Scaling Laws for Neural Language Models https://t.co/72Rv8kl9Fl", "followers": "511", "datetime": "2020-01-24 23:42:04", "author": "@tmasada"}, "1235632990753034241": {"content_summary": "@stormtroper1721 @Eric_Wallace_ @jeremyphoward Didn't OA already demonstrate exactly this back in January about compute/sample-efficiency increasing with model size and the implication being that it's cheapest to train as big as possible? https://t.co/nkoj", "followers": "20,465", "datetime": "2020-03-05 18:27:16", "author": "@gwern"}, "1220975736200278017": {"content_summary": "RT @theshawwn: This is a highly interesting paper. I haven't dug into it deeply yet, but they examine the scaling effects of model size for\u2026", "followers": "2,187", "datetime": "2020-01-25 07:44:34", "author": "@calabi_and_yau"}, "1220916697286291456": {"content_summary": "RT @theshawwn: This is a highly interesting paper. I haven't dug into it deeply yet, but they examine the scaling effects of model size for\u2026", "followers": "371", "datetime": "2020-01-25 03:49:58", "author": "@aviopene"}, "1221640392086622215": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "11,914", "datetime": "2020-01-27 03:45:41", "author": "@Rosenchild"}, "1253453294128828416": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "29", "datetime": "2020-04-23 22:38:48", "author": "@PercyElbis"}, "1222320324982657024": {"content_summary": "2020/01/22 \u6295\u7a3f 5\u4f4d LG(Machine Learning) Scaling Laws for Neural Language Models https://t.co/omqjJHpUYw 7 Tweets 5 Retweets 47 Favorites", "followers": "691", "datetime": "2020-01-29 00:47:29", "author": "@arxiv_pop"}, "1229582561032073216": {"content_summary": "RT @arankomatsuzaki: Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size,\u2026", "followers": "361", "datetime": "2020-02-18 01:45:01", "author": "@iugoaoj"}, "1235968016543813635": {"content_summary": "@NexWebSites @OpenAI https://t.co/G4doYPYlsZ https://t.co/nkoj0EdGzk https://t.co/HJJtf4Judu https://t.co/SJhrDKW6ub https://t.co/nkoj0EdGzk https://t.co/mxuJnqZxoC https://t.co/VTB4WadpHx https://t.co/6yP1UnvVHl", "followers": "20,465", "datetime": "2020-03-06 16:38:32", "author": "@gwern"}, "1251103404870512646": {"content_summary": "Understanding scale in natural language #DeepLearning models. #Transformers outperform LSTMs. When limited by data/compute, it\u2019s more efficient to train a large model in fewer training steps than a smaller one to convergence. Batch size is tunable https://", "followers": "70", "datetime": "2020-04-17 11:01:10", "author": "@georgejoseph88"}, "1229744957923172356": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "523", "datetime": "2020-02-18 12:30:20", "author": "@langdon"}, "1220623853954748416": {"content_summary": "Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/dQLYSL6Fld", "followers": "9,664", "datetime": "2020-01-24 08:26:19", "author": "@StatMLPapers"}, "1229719093625925633": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "2,187", "datetime": "2020-02-18 10:47:33", "author": "@calabi_and_yau"}, "1254763314338508800": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "72", "datetime": "2020-04-27 13:24:21", "author": "@AymenBenBrik"}, "1220874058415460352": {"content_summary": "[2001.08361] Scaling Laws for Neural Language Models https://t.co/FLcKDBTBei", "followers": "4,070", "datetime": "2020-01-25 01:00:32", "author": "@bgoncalves"}, "1221315647671373824": {"content_summary": "RT @arankomatsuzaki: Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size,\u2026", "followers": "415", "datetime": "2020-01-26 06:15:16", "author": "@NonLocalityGuy"}, "1220740986223112192": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "5,309", "datetime": "2020-01-24 16:11:46", "author": "@HubBucket"}, "1230938483423948800": {"content_summary": "RT @sweis: Nice paper \"Scaling Laws for Neural Language Models\" shows the trade-offs between model complexity, data size, and compute time:\u2026", "followers": "2,860", "datetime": "2020-02-21 19:32:58", "author": "@durumcrustulum"}, "1229716892992114688": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "2,071", "datetime": "2020-02-18 10:38:48", "author": "@ducha_aiki"}, "1220723021373231106": {"content_summary": "RT @arankomatsuzaki: Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size,\u2026", "followers": "43", "datetime": "2020-01-24 15:00:22", "author": "@MishakinSergey"}, "1220922797414895616": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "172", "datetime": "2020-01-25 04:14:13", "author": "@Vega_2221"}, "1229737824343265281": {"content_summary": "Interesting empirical research on how language models scale, revealing clear power laws. Great that OpenAI conducted and published this; the computing resources must have cost a fortune. https://t.co/ZuwC7x67GP https://t.co/rOcA81KCl9", "followers": "106", "datetime": "2020-02-18 12:01:59", "author": "@perttu_h"}, "1253390814765514755": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "158", "datetime": "2020-04-23 18:30:31", "author": "@ronitneve"}, "1224304023324459008": {"content_summary": "Scaling Laws for Neural Language Models #deeplearning https://t.co/2e42Tb4K2K https://t.co/nGbrXzyTi4", "followers": "1,378", "datetime": "2020-02-03 12:10:00", "author": "@milesboard"}, "1229729100316606464": {"content_summary": "nice power laws ;)", "followers": "90", "datetime": "2020-02-18 11:27:19", "author": "@algorithm87"}, "1233175078239531008": {"content_summary": "More (data + compute) have usually shown to perform better. However, scaling relationships with other factors at a scale were never studied (or published). Great fundamental work and \ud83d\udc4f to publishing @OpenAI https://t.co/EdmVBY27sm https://t.co/32ch8GqvoM", "followers": "112", "datetime": "2020-02-27 23:40:24", "author": "@JovanSardinha"}, "1220900763490115589": {"content_summary": "This is a highly interesting paper. I haven't dug into it deeply yet, but they examine the scaling effects of model size for transformers in terms of total tokens processed. I like measuring our performance in terms of tokens/sec. It's nicer than examples", "followers": "1,396", "datetime": "2020-01-25 02:46:39", "author": "@theshawwn"}, "1220983066576867328": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "66", "datetime": "2020-01-25 08:13:42", "author": "@shubh_300595"}, "1238964426650181632": {"content_summary": "@pragmaticml Though https://t.co/EUA0CDXsKj isn't about long-range context per se, it's a very valuable summary of how to improve transformer lm for a given budget in general. Esp., it implies that sufficient parameter budget is crucial for the model to be", "followers": "338", "datetime": "2020-03-14 23:05:12", "author": "@arankomatsuzaki"}, "1220828710108114945": {"content_summary": "RT @LukaszJDebowski: I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team! https://t.co/s\u2026", "followers": "2,677", "datetime": "2020-01-24 22:00:21", "author": "@jhuclsp"}, "1253412517537177606": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "263", "datetime": "2020-04-23 19:56:46", "author": "@AhamVishwanath"}, "1253518838445953026": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "33", "datetime": "2020-04-24 02:59:14", "author": "@ConfettiAi"}, "1229657567741255680": {"content_summary": "@stormtroper1721 @kearonis Yup. Found this interesting paper which shows computable values for final loss given parameters/dataset/compute etc. https://t.co/tToTap73ms", "followers": "269", "datetime": "2020-02-18 06:43:04", "author": "@NaxAlpha"}, "1229764134704746496": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "6", "datetime": "2020-02-18 13:46:32", "author": "@Lucy_Look"}, "1220550122372661248": {"content_summary": "Scaling Laws for Neural Language Models. https://t.co/Xg36ezIps7", "followers": "5,454", "datetime": "2020-01-24 03:33:20", "author": "@StatsPapers"}, "1229717093576314881": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "1,061", "datetime": "2020-02-18 10:39:36", "author": "@mvaldenegro"}, "1230235591830364167": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "3,577", "datetime": "2020-02-19 20:59:56", "author": "@hubofml"}, "1253702570847199235": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "0", "datetime": "2020-04-24 15:09:20", "author": "@BrunoVD6"}, "1221128690169470977": {"content_summary": "OpenAI paper without a press release? Something seems odd. \ud83d\ude43", "followers": "607", "datetime": "2020-01-25 17:52:21", "author": "@sanjaykamath"}, "1220834476873351171": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "19", "datetime": "2020-01-24 22:23:15", "author": "@MassBassLol"}, "1230941859834236928": {"content_summary": "RT @sweis: Nice paper \"Scaling Laws for Neural Language Models\" shows the trade-offs between model complexity, data size, and compute time:\u2026", "followers": "207", "datetime": "2020-02-21 19:46:23", "author": "@metavalent"}, "1220610186236264448": {"content_summary": "\"Scaling Laws for Neural Language Models\", Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\uff0e Brown, Benjamin Chess\u2026 https://t.co/yVjNHZoY51", "followers": "767", "datetime": "2020-01-24 07:32:00", "author": "@arxivml"}, "1253393138250657792": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "26", "datetime": "2020-04-23 18:39:45", "author": "@zakariarguibi05"}, "1253383154079019010": {"content_summary": "Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter count, training corpus size, batch size, and training time on language model performance: https://t.co/4G5DcmknZE https://t.co/h6ykFpvtQA", "followers": "31,411", "datetime": "2020-04-23 18:00:05", "author": "@deeplearningai_"}, "1220926995413946369": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "25", "datetime": "2020-01-25 04:30:54", "author": "@sn64567334"}, "1221856390794358785": {"content_summary": "The most compute-efficient way to train a language model is to take a large model and train it with small compute on small data? There is so much we don\u2019t understand. \u201cScaling Laws for Neural Language Models\u201d from @OpenAI: https://t.co/1he9AYlbCa #NLProc", "followers": "1,188", "datetime": "2020-01-27 18:03:59", "author": "@joeddav"}, "1249662566596411392": {"content_summary": "@ytay017 According to many recent papers (e.g. https://t.co/94tFRPDO4j), the compute-optimal training of LM is to train for just one epoch, which results in less overfitting. So, if we use PG-19 instead of Wikitext-103, we can just train for a few epochs w", "followers": "338", "datetime": "2020-04-13 11:35:48", "author": "@arankomatsuzaki"}, "1227706807382089728": {"content_summary": "@alienelf this might be relevant: Scaling Laws for Neural Language Models https://t.co/eySQc5PXzU", "followers": "319", "datetime": "2020-02-12 21:31:27", "author": "@ihsgnef"}, "1220897234058764288": {"content_summary": "Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "12,714", "datetime": "2020-01-25 02:32:38", "author": "@arxiv_org"}, "1229665921221816320": {"content_summary": "@rbhar90 100B model according to OpenAI wouldn\u2019t make sense (diminishing returns) https://t.co/yAdMHjxhf5", "followers": "387", "datetime": "2020-02-18 07:16:16", "author": "@suryadsuryad"}, "1220922669685710848": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "7,296", "datetime": "2020-01-25 04:13:42", "author": "@KriegeskorteLab"}, "1220718637188833280": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "267", "datetime": "2020-01-24 14:42:57", "author": "@LazyBot6"}, "1253600024686202881": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "25", "datetime": "2020-04-24 08:21:51", "author": "@_Duft_"}, "1220966503408918529": {"content_summary": "RT @LukaszJDebowski: I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team!", "followers": "183", "datetime": "2020-01-25 07:07:53", "author": "@OlegBaskov"}, "1220537148366970882": {"content_summary": "Scaling Laws for Neural Language Models. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei https://t.co/MGIeNRSV1n", "followers": "308", "datetime": "2020-01-24 02:41:47", "author": "@arxiv_cs_LG"}, "1221007298769948673": {"content_summary": "RT @LukaszJDebowski: I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team!", "followers": "122", "datetime": "2020-01-25 09:49:59", "author": "@DanielAdamec5"}, "1227942489241333760": {"content_summary": "@alienelf OpenAI published pretty much exactly that recently - scaling of reformers with respect to more data, compute and model size. Very good paper - https://t.co/K063l8AI8H", "followers": "69", "datetime": "2020-02-13 13:07:58", "author": "@Tenoke_"}, "1220874051314384896": {"content_summary": "[2001.08361] Scaling Laws for Neural Language Models https://t.co/RJQRA1X6Qk", "followers": "670", "datetime": "2020-01-25 01:00:31", "author": "@data4sci"}, "1253383785799757829": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "181", "datetime": "2020-04-23 18:02:35", "author": "@PradipVSoundar"}, "1220703055269621760": {"content_summary": "\u81ea\u7136\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306b\u5bfe\u3059\u308b\u7d4c\u9a13\u7684\u306a\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u5247\u306e\u7814\u7a76\u3002\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30b5\u30a4\u30ba\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5e45\u3084\u6df1\u3055\u7b49\u304c\u30c6\u30b9\u30c8\u30ed\u30b9\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u304c\u307e\u3068\u3081\u3089\u308c\u3066\u3044\u308b\u3002 https://t.co/DlW1nBEMPe", "followers": "10", "datetime": "2020-01-24 13:41:02", "author": "@38_76"}, "1220581884238811136": {"content_summary": "[R] Scaling Laws for Neural Language Models: submitted by /u/Aran_Komatsuzaki [visit reddit] [comments] https://t.co/ckGAwWHDJi", "followers": "1,186", "datetime": "2020-01-24 05:39:33", "author": "@CarlRioux"}, "1253442656052506624": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "2,532", "datetime": "2020-04-23 21:56:31", "author": "@bizintsolutions"}, "1220810145258786817": {"content_summary": "RT @LukaszJDebowski: I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team! https://t.co/s\u2026", "followers": "400", "datetime": "2020-01-24 20:46:34", "author": "@therfer"}, "1220718499728936967": {"content_summary": "#arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8cL We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with mod", "followers": "1,700", "datetime": "2020-01-24 14:42:24", "author": "@arXiv__ml"}, "1220624139448225792": {"content_summary": "RT @ak1010: Scaling Laws for Neural Language Models https://t.co/syxVwhgetH", "followers": "2,123", "datetime": "2020-01-24 08:27:27", "author": "@rseymour"}, "1253424622411771910": {"content_summary": "Scaling Laws for Neural Language Model https://t.co/fFAyq4SHV4 https://t.co/XWHlCvK8ur", "followers": "713", "datetime": "2020-04-23 20:44:52", "author": "@Seanku"}, "1229802044086575110": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "21", "datetime": "2020-02-18 16:17:10", "author": "@ntd_moewmoew"}, "1250469258309652480": {"content_summary": "\u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647 \u06cc\u0647 \u062a\u0639\u062f\u0627\u062f \u0641\u0631\u0645\u0648\u0644 \u0627\u0631\u0627\u0626\u0647 \u0645\u06cc\u062f\u0647 \u06a9\u0647 \u0646\u0634\u0648\u0646 \u0645\u06cc\u062f\u0647 \u062f\u0631\u0646\u0647\u0627\u06cc\u062a \u0627\u0646\u062f\u0627\u0632\u0647 batch\u060c \u062a\u0639\u062f\u0627\u062f \u0646\u0645\u0648\u0646\u0647\u200c\u0647\u0627\u060c \u062a\u0627\u0628\u0639 \u0644\u0627\u0633 \u0648 \u062a\u0639\u062f\u0627\u062f epoch \u0645\u0648\u062b\u0631 \u0686\u0647 \u0627\u0631\u062a\u0628\u0627\u0637\u06cc \u0628\u0627 \u0647\u0645 \u062f\u0627\u0631\u0646\u062f. \u062f\u0648\u0633\u062a \u062f\u0627\u0631\u0645 \u0627\u06af\u0631 \u0641\u0631\u0635\u062a \u06a9\u0646\u0645 \u0628\u0639\u062f\u0627 \u06cc\u0647 \u062c\u0645\u0639\u200c\u0628\u0646\u062f\u06cc \u0627\u0632\u0634 \u0627\u0631\u0627\u0626\u0647 \u0628\u062f\u0645. https://t.co/IIjzboQIxa", "followers": "443", "datetime": "2020-04-15 17:01:18", "author": "@Thinker_no_pain"}, "1220934641613324288": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "43", "datetime": "2020-01-25 05:01:17", "author": "@santhoshkolloju"}, "1229667873427247105": {"content_summary": "Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of data, compute, model size. Eg. larger models are more sample efficient; for a given budget, train larger models, stop before convergence. h", "followers": "1,304", "datetime": "2020-02-18 07:24:01", "author": "@chriswolfvision"}, "1233007396743716864": {"content_summary": "RT @arxiv_org: Scaling Laws for Neural Language Models. https://t.co/LGRXWw1cpA https://t.co/XktaNuPfw6", "followers": "76", "datetime": "2020-02-27 12:34:06", "author": "@sorenmind"}, "1232720531914293248": {"content_summary": "@LeonDerczynski Intuitively, there are fewer solutions if you have fewer parameters. Overparameterized models have more solutions, so it is easier for gradient descent to stumble towards one of them. Also see the section on sample efficiency of large model", "followers": "222", "datetime": "2020-02-26 17:34:12", "author": "@benbenhh"}, "1250654053085954048": {"content_summary": "RT @Thinker_no_pain: \u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647 \u06cc\u0647 \u062a\u0639\u062f\u0627\u062f \u0641\u0631\u0645\u0648\u0644 \u0627\u0631\u0627\u0626\u0647 \u0645\u06cc\u062f\u0647 \u06a9\u0647 \u0646\u0634\u0648\u0646 \u0645\u06cc\u062f\u0647 \u062f\u0631\u0646\u0647\u0627\u06cc\u062a \u0627\u0646\u062f\u0627\u0632\u0647 batch\u060c \u062a\u0639\u062f\u0627\u062f \u0646\u0645\u0648\u0646\u0647\u200c\u0647\u0627\u060c \u062a\u0627\u0628\u0639 \u0644\u0627\u0633 \u0648 \u062a\u0639\u062f\u0627\u062f epoch \u0645\u0648\u062b\u0631 \u0686\u0647\u2026", "followers": "338", "datetime": "2020-04-16 05:15:36", "author": "@sed_amin"}, "1220754769343434752": {"content_summary": "RT @arankomatsuzaki: Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size,\u2026", "followers": "484", "datetime": "2020-01-24 17:06:32", "author": "@timothy_lkh_"}, "1253383302335148033": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "357", "datetime": "2020-04-23 18:00:40", "author": "@AhmadMustafaAn1"}, "1220919014261739521": {"content_summary": "RT @theshawwn: This is a highly interesting paper. I haven't dug into it deeply yet, but they examine the scaling effects of model size for\u2026", "followers": "2,071", "datetime": "2020-01-25 03:59:11", "author": "@EricSchles"}, "1230235554064715776": {"content_summary": "RT @LukaszJDebowski: I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team!", "followers": "2,673", "datetime": "2020-02-19 20:59:47", "author": "@ayirpelle"}, "1220539550012973056": {"content_summary": "Scaling Laws for Neural Language Models. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei https://t.co/Byg1397mHE", "followers": "3,858", "datetime": "2020-01-24 02:51:19", "author": "@BrundageBot"}, "1220574711899275265": {"content_summary": "Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size, dataset size, and the amount of compute used for training up to seven order of magnitudes. https://t.co/EUA0CDXsKj", "followers": "338", "datetime": "2020-01-24 05:11:03", "author": "@arankomatsuzaki"}, "1230235569738747904": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "2,673", "datetime": "2020-02-19 20:59:51", "author": "@ayirpelle"}, "1220756282480766976": {"content_summary": "I have been waiting for that: Scaling Laws for Neural Language Models, by the @OpenAI / @jhuclsp team!", "followers": "271", "datetime": "2020-01-24 17:12:32", "author": "@LukaszJDebowski"}, "1238646655752134657": {"content_summary": "RT @JovanSardinha: More (data + compute) have usually shown to perform better. However, scaling relationships with other factors at a scale\u2026", "followers": "0", "datetime": "2020-03-14 02:02:30", "author": "@JiChen36146394"}, "1229544453758545920": {"content_summary": "RT @arankomatsuzaki: Scaling Laws for Neural Language Models. OpenAI team found that the loss of LM scales as a power-law with model size,\u2026", "followers": "4,327", "datetime": "2020-02-17 23:13:36", "author": "@jekbradbury"}, "1221007797103366144": {"content_summary": "RT @arXiv__ml: #arXiv #machinelearning [cs.LG] Scaling Laws for Neural Language Models. (arXiv:2001.08361v1 [cs.LG]) https://t.co/5NxI37D8c\u2026", "followers": "169", "datetime": "2020-01-25 09:51:58", "author": "@data4gud"}, "1254141013980876801": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "3,981", "datetime": "2020-04-25 20:11:33", "author": "@asifrazzaq1988"}, "1231009813745881090": {"content_summary": "RT @sweis: Nice paper \"Scaling Laws for Neural Language Models\" shows the trade-offs between model complexity, data size, and compute time:\u2026", "followers": "489", "datetime": "2020-02-22 00:16:25", "author": "@Natanael_L"}, "1220656434670518273": {"content_summary": "Really interesting & detailed work! from Kaplan, McCandish, @AlecRad , Jeff Wu, Tom Brown, Ben Chess, @scottgray76, R. Child, T. Henighan, D. Amodei, https://t.co/SR08SD1OCw", "followers": "674", "datetime": "2020-01-24 10:35:47", "author": "@jonathanrraiman"}, "1229763622659854337": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "287", "datetime": "2020-02-18 13:44:30", "author": "@dannyehb"}, "1253386210942136320": {"content_summary": "RT @deeplearningai_: Researchers at Johns Hopkins and @OpenAI derived new equations that optimize training parameters, including parameter\u2026", "followers": "191", "datetime": "2020-04-23 18:12:14", "author": "@F4izy"}, "1229756432020594694": {"content_summary": "RT @chriswolfvision: Work by Kaplan et al. (open-AI) with incredible findings on how machine learning performance scales as a function of d\u2026", "followers": "466", "datetime": "2020-02-18 13:15:55", "author": "@souzatharsis"}}}