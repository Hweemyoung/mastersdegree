{"twitter": {"1100601864221933568": {"author": "@skr1125", "datetime": "2019-02-27 03:41:47", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "117"}, "1090128567143264256": {"author": "@ElectronNest", "datetime": "2019-01-29 06:04:38", "content_summary": "RT @yu4u: \u3069\u3046\u3044\u3046\u3053\u3068\u3060\u3063\u3066\u3070\u3088 https://t.co/cinXo0W59S https://t.co/gUqN1HMKeb", "followers": "228"}, "1100610893082025984": {"author": "@tomaxent", "datetime": "2019-02-27 04:17:39", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "4"}, "1092179404078366721": {"author": "@battle8500", "datetime": "2019-02-03 21:53:56", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "66"}, "1092209477858799618": {"author": "@Nijumich", "datetime": "2019-02-03 23:53:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "133"}, "1091941212792078336": {"author": "@tengyuma", "datetime": "2019-02-03 06:07:27", "content_summary": "If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our paper! Arxiv link https://t.co/TEQ8psuk6x . Thanks to @ajmooch for the tweet and re-implementation!", "followers": "4,457"}, "1100666070631776257": {"author": "@Mr_PO96", "datetime": "2019-02-27 07:56:55", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "58"}, "1092096288135335936": {"author": "@philipmlong", "datetime": "2019-02-03 16:23:39", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "657"}, "1092224799403737088": {"author": "@SythonUK", "datetime": "2019-02-04 00:54:19", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "690"}, "1092334059764752384": {"author": "@vonum123", "datetime": "2019-02-04 08:08:29", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "19"}, "1100600772750966785": {"author": "@CSProfKGD", "datetime": "2019-02-27 03:37:27", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "2,851"}, "1100660198233042944": {"author": "@kaalam_ai", "datetime": "2019-02-27 07:33:35", "content_summary": "An alternative to batch-normalization. #DeepLearning", "followers": "10,655"}, "1113762549458243584": {"author": "@drsxr", "datetime": "2019-04-04 11:17:39", "content_summary": "@rasbt @jeremyphoward I know you have seen this, but as a reminder how do you feel about FIXUP initialization? https://t.co/w4jKvEg8pX", "followers": "3,864"}, "1100603259725758465": {"author": "@313V", "datetime": "2019-02-27 03:47:19", "content_summary": "Pytorch version, thanks @ajmooch https://t.co/VDNyhybIBY", "followers": "518"}, "1090273500550488065": {"author": "@arxiv_cscv", "datetime": "2019-01-29 15:40:33", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMo9A2o", "followers": "4,105"}, "1100654676196945920": {"author": "@kadarakos", "datetime": "2019-02-27 07:11:38", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "276"}, "1113497965111336965": {"author": "@TheGregYang", "datetime": "2019-04-03 17:46:17", "content_summary": "@jeremyphoward Though for deep resnet kaiming init is not optimal either, can cause gradient problems https://t.co/06TXCLa8e2 https://t.co/9y8zAYItxp", "followers": "3,651"}, "1092993210413023232": {"author": "@pasoconhoshii", "datetime": "2019-02-06 03:47:42", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "689"}, "1092208972205981697": {"author": "@AssistedEvolve", "datetime": "2019-02-03 23:51:25", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "217"}, "1100618138113138689": {"author": "@bdean_", "datetime": "2019-02-27 04:46:27", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "362"}, "1092104993191149570": {"author": "@IgorCarron", "datetime": "2019-02-03 16:58:15", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "4,897"}, "1100663951237341184": {"author": "@surajkothawade", "datetime": "2019-02-27 07:48:29", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "5"}, "1092189012201074688": {"author": "@arunprakashml", "datetime": "2019-02-03 22:32:07", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,135"}, "1092306105626091521": {"author": "@KeeganKosasih", "datetime": "2019-02-04 06:17:24", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "12"}, "1092178131199410183": {"author": "@aureliengeron", "datetime": "2019-02-03 21:48:52", "content_summary": "How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e", "followers": "13,638"}, "1101104311709315073": {"author": "@AssistedEvolve", "datetime": "2019-02-28 12:58:20", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "217"}, "1100860125344985088": {"author": "@surmenok", "datetime": "2019-02-27 20:48:01", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "232"}, "1100668682299154432": {"author": "@aye_pete", "datetime": "2019-02-27 08:07:17", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "51"}, "1105324965807587329": {"author": "@prototechno", "datetime": "2019-03-12 04:29:42", "content_summary": "RT @icoxfog417: \u3053\u3061\u3089\u306e\u4e3b\u5f35(\u521d\u671f\u5316\u306e\u307f\u3067\u306f\u52fe\u914d\u306e\u7206\u767a\u306b\u306f\u5bfe\u5fdc\u3067\u304d\u306a\u3044)\u3001\u4ee5\u4e0b\u3068\u30d0\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u306e\u3060\u304c\u3069\u3061\u3089\u304c\u6b63\u3057\u3044\u304b\u30fb\u30fb\u30fb(\u5f15\u7528\u304c\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u2193\u306b\u95a2\u3059\u308b\u30b3\u30e1\u30f3\u30c8\u304c\u53d6\u308c\u306a\u304b\u3063\u305f) Fixup Initialization: Residual Learning\u2026", "followers": "4,825"}, "1092255629392121857": {"author": "@MingSeow", "datetime": "2019-02-04 02:56:49", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "108"}, "1092290174325731328": {"author": "@sohom_vision", "datetime": "2019-02-04 05:14:05", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "72"}, "1100980328066211840": {"author": "@Pomecloud", "datetime": "2019-02-28 04:45:40", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "97"}, "1092227404351717380": {"author": "@jaialkdanel", "datetime": "2019-02-04 01:04:40", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "1,770"}, "1116805673692610563": {"author": "@adamoprogresso", "datetime": "2019-04-12 20:49:56", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,865"}, "1092745599315660800": {"author": "@rohwid2603", "datetime": "2019-02-05 11:23:47", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "104"}, "1092390614207852545": {"author": "@NORA__0013", "datetime": "2019-02-04 11:53:12", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "24"}, "1114116093784743938": {"author": "@PerthMLGroup", "datetime": "2019-04-05 10:42:30", "content_summary": "RT @TheGregYang: @jeremyphoward Though for deep resnet kaiming init is not optimal either, can cause gradient problems https://t.co/06TXCLa\u2026", "followers": "456"}, "1090099531088199680": {"author": "@HITStales", "datetime": "2019-01-29 04:09:15", "content_summary": "RT @yu4u: \u3069\u3046\u3044\u3046\u3053\u3068\u3060\u3063\u3066\u3070\u3088 https://t.co/cinXo0W59S https://t.co/gUqN1HMKeb", "followers": "4,615"}, "1092001466452197377": {"author": "@ekenel", "datetime": "2019-02-03 10:06:52", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "469"}, "1100602969630756864": {"author": "@ID9112", "datetime": "2019-02-27 03:46:10", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "109"}, "1092254087855333376": {"author": "@ballforest", "datetime": "2019-02-04 02:50:42", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "4,048"}, "1092093698555277312": {"author": "@akashjainx", "datetime": "2019-02-03 16:13:22", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "265"}, "1100937155180822528": {"author": "@tranlaman", "datetime": "2019-02-28 01:54:06", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "113"}, "1100661369215557632": {"author": "@esvhd", "datetime": "2019-02-27 07:38:14", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "145"}, "1105614525011648512": {"author": "@BioDecoded", "datetime": "2019-03-12 23:40:18", "content_summary": "Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT9lH6n #DeepLearning https://t.co/gqHyKK2uta", "followers": "1,140"}, "1092283616950996992": {"author": "@ceobillionaire", "datetime": "2019-02-04 04:48:02", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "163,744"}, "1115142439621283840": {"author": "@Rosenchild", "datetime": "2019-04-08 06:40:50", "content_summary": "RT @BioDecoded: Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT9lH6n #DeepLearning https://t.co/g\u2026", "followers": "11,928"}, "1092628894920798208": {"author": "@zhang163220", "datetime": "2019-02-05 03:40:03", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "6"}, "1100616794392403969": {"author": "@bhargavbardipur", "datetime": "2019-02-27 04:41:06", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "432"}, "1105856251014062080": {"author": "@arxiv_cscv", "datetime": "2019-03-13 15:40:50", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMorbqY", "followers": "4,105"}, "1103849965326032896": {"author": "@James9201063522", "datetime": "2019-03-08 02:48:34", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "0"}, "1157419241215332353": {"author": "@Rosenchild", "datetime": "2019-08-02 22:33:45", "content_summary": "RT @BioDecoded: Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT946eP #DeepLearning https://t.co/w\u2026", "followers": "11,928"}, "1100662007597068288": {"author": "@alexhock", "datetime": "2019-02-27 07:40:46", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "192"}, "1096939977252818944": {"author": "@akihiro_akichan", "datetime": "2019-02-17 01:10:45", "content_summary": "https://t.co/wraC20Pg8H \u521d\u671f\u5316\u3092\u8abf\u6574\u3059\u308b\u3068\u3001BatchNormalization\u3092\u4f7f\u3063\u305f\u6642\u3068\u540c\u69d8\u306b\u52fe\u914d\u6d88\u5931\u30fb\u767a\u6563\u3092\u6291\u3048\u3089\u308c\u305f\u3068\u3044\u3046\u7814\u7a76\u3002 \u306a\u304a\u3001MIXUP\u304c\u306a\u3044\u3068\u52dd\u3066\u306a\u3044\u6a21\u69d8", "followers": "1,309"}, "1092298126533189633": {"author": "@abonthu", "datetime": "2019-02-04 05:45:41", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "16"}, "1092316476269182982": {"author": "@LevSchweitzer", "datetime": "2019-02-04 06:58:36", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "388"}, "1092234160452726784": {"author": "@92HsChoi", "datetime": "2019-02-04 01:31:31", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "82"}, "1100796826129698816": {"author": "@Olly_Olivier", "datetime": "2019-02-27 16:36:29", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "721"}, "1092247051188940800": {"author": "@geethasaikrish", "datetime": "2019-02-04 02:22:44", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "37"}, "1092175292255387649": {"author": "@ceobillionaire", "datetime": "2019-02-03 21:37:35", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "163,744"}, "1100609802453975040": {"author": "@SalehCU", "datetime": "2019-02-27 04:13:19", "content_summary": "Looks like interesting write-up on fixup initialization... @zacharylipton @roydanroy", "followers": "200"}, "1090227469150343168": {"author": "@asam9891", "datetime": "2019-01-29 12:37:38", "content_summary": "[1901.09321] Fixup Initialization: Residual Learning Without Normalization \u306a\u3093\u304b\u898b\u305f\u3053\u3068\u3042\u308b\u306a\u30fc\u3068\u601d\u3063\u305f\u3089ZeroInit\u3060\u3063\u305f\u3002(\u540d\u524d\u5909\u308f\u3063\u3066\u305f\u3002)ICLR2019\u3002 https://t.co/ovFCQuslJR", "followers": "564"}, "1091943302671196160": {"author": "@jigarkdoshi", "datetime": "2019-02-03 06:15:45", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,336"}, "1100944575747756039": {"author": "@renato_umeton", "datetime": "2019-02-28 02:23:36", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "3,372"}, "1092224741543276544": {"author": "@mosko_mule", "datetime": "2019-02-04 00:54:05", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "2,679"}, "1097475297098764288": {"author": "@Huang15J", "datetime": "2019-02-18 12:37:55", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "31"}, "1106221820288471042": {"author": "@MinhTungLuu", "datetime": "2019-03-14 15:53:29", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "4"}, "1100625216646967296": {"author": "@idgmatrix", "datetime": "2019-02-27 05:14:34", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "4,473"}, "1100663708030779392": {"author": "@DataForager", "datetime": "2019-02-27 07:47:31", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "171"}, "1113589076517511168": {"author": "@AssistedEvolve", "datetime": "2019-04-03 23:48:19", "content_summary": "RT @TheGregYang: @jeremyphoward Though for deep resnet kaiming init is not optimal either, can cause gradient problems https://t.co/06TXCLa\u2026", "followers": "217"}, "1092075440494252032": {"author": "@machinelearnflx", "datetime": "2019-02-03 15:00:49", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "79,518"}, "1100674724634480640": {"author": "@uranc__", "datetime": "2019-02-27 08:31:18", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "222"}, "1092324002511519745": {"author": "@gatheluck", "datetime": "2019-02-04 07:28:31", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "196"}, "1092118869458735105": {"author": "@permutans", "datetime": "2019-02-03 17:53:23", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,112"}, "1100718241071222784": {"author": "@m_khurram_amin", "datetime": "2019-02-27 11:24:13", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "99"}, "1092135803055071233": {"author": "@stats385", "datetime": "2019-02-03 19:00:41", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "970"}, "1100661704931848192": {"author": "@EZavarygin", "datetime": "2019-02-27 07:39:34", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "12"}, "1100904345019670528": {"author": "@MarziehParandeh", "datetime": "2019-02-27 23:43:44", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "38"}, "1092431187728044033": {"author": "@bhagirathl", "datetime": "2019-02-04 14:34:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "508"}, "1092250682911019010": {"author": "@JaeDukSeo", "datetime": "2019-02-04 02:37:10", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "464"}, "1092282329865904128": {"author": "@TDLS_TO", "datetime": "2019-02-04 04:42:55", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "909"}, "1100600210474954757": {"author": "@adssidhu86", "datetime": "2019-02-27 03:35:12", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "99"}, "1101121209377406976": {"author": "@gv_sagar", "datetime": "2019-02-28 14:05:28", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "56"}, "1092144522245689344": {"author": "@Tzeny25", "datetime": "2019-02-03 19:35:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "16"}, "1092241272843071489": {"author": "@aavella77", "datetime": "2019-02-04 01:59:46", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "196"}, "1097366427042893824": {"author": "@kuanchen22", "datetime": "2019-02-18 05:25:18", "content_summary": "This work looks crazy \ud83d\ude2f", "followers": "63"}, "1092980475633651713": {"author": "@takuya_araki", "datetime": "2019-02-06 02:57:06", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "50"}, "1092188297537904640": {"author": "@hosjiu", "datetime": "2019-02-03 22:29:16", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "10"}, "1092107605311475712": {"author": "@TerencePlizga", "datetime": "2019-02-03 17:08:38", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "256"}, "1100911141285326848": {"author": "@iamknighton", "datetime": "2019-02-28 00:10:44", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "425"}, "1092234013404626944": {"author": "@shaunak38", "datetime": "2019-02-04 01:30:56", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "638"}, "1092228847649472512": {"author": "@Dmiag2008", "datetime": "2019-02-04 01:10:24", "content_summary": "RT @hillbig: Fixup initialization achieves similar stable training and generalization performance (w/ regularization) without batch normali\u2026", "followers": "3,339"}, "1092266854742089728": {"author": "@rormandi", "datetime": "2019-02-04 03:41:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "41"}, "1092962871430938624": {"author": "@LevSchweitzer", "datetime": "2019-02-06 01:47:09", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "388"}, "1100718826973548544": {"author": "@MatRazor", "datetime": "2019-02-27 11:26:33", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "35"}, "1103315060426924032": {"author": "@vaibhaw_vipul", "datetime": "2019-03-06 15:23:03", "content_summary": "An interesting paper to read.. #machinelearning #deeplearning #neuralnetworks Fixup Initialization: Residual Learning Without Normalization https://t.co/5Mx7hv0Zgb", "followers": "199"}, "1092069065609351170": {"author": "@thomasjelonek", "datetime": "2019-02-03 14:35:29", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "381"}, "1092616406003109888": {"author": "@kushashwa", "datetime": "2019-02-05 02:50:25", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "110"}, "1107967278740467713": {"author": "@AssistedEvolve", "datetime": "2019-03-19 11:29:18", "content_summary": "RT @PiotrCzapla: \"Everything makes more sense in excel.\" - it brings a smile to my face each time I hear @jeremyphoward saying this, especi\u2026", "followers": "217"}, "1092225789439832065": {"author": "@hardmaru", "datetime": "2019-02-04 00:58:15", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "81,997"}, "1093088573710954496": {"author": "@ElectronNest", "datetime": "2019-02-06 10:06:39", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "228"}, "1100635258213580800": {"author": "@bessimaestro", "datetime": "2019-02-27 05:54:29", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "783"}, "1092380758675808258": {"author": "@captivedata1", "datetime": "2019-02-04 11:14:02", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "8"}, "1100608510805434368": {"author": "@evolvingstuff", "datetime": "2019-02-27 04:08:11", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "2,930"}, "1092075933253672960": {"author": "@Communicate_AI", "datetime": "2019-02-03 15:02:46", "content_summary": "stanfordnlp: RT tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our paper! Arxiv link https://t.co/xUXOAGdzkd . Thanks to ajmooch for the tweet and re-implementation! https://t", "followers": "56"}, "1092252791911792641": {"author": "@toto_toilet", "datetime": "2019-02-04 02:45:33", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "119"}, "1092232661802803200": {"author": "@a_maumau_", "datetime": "2019-02-04 01:25:33", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "433"}, "1092222277297393669": {"author": "@mpsampat", "datetime": "2019-02-04 00:44:18", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "48"}, "1100743854817345536": {"author": "@KumarSamalkha", "datetime": "2019-02-27 13:06:00", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "133"}, "1093158313586556928": {"author": "@Nurzat19959669", "datetime": "2019-02-06 14:43:46", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "10"}, "1100668378048385024": {"author": "@leetcat", "datetime": "2019-02-27 08:06:05", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "25"}, "1100667620121022464": {"author": "@Jhaz09118449", "datetime": "2019-02-27 08:03:04", "content_summary": "RT @kaalam_ai: An alternative to batch-normalization. #DeepLearning https://t.co/L3BwICFA4C", "followers": "5"}, "1100786364017229824": {"author": "@pacocp9", "datetime": "2019-02-27 15:54:55", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "354"}, "1092233133351755776": {"author": "@I99666", "datetime": "2019-02-04 01:27:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "236"}, "1092236116009185280": {"author": "@emorsstuu", "datetime": "2019-02-04 01:39:17", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "104"}, "1092241910977171456": {"author": "@AiSkellig", "datetime": "2019-02-04 02:02:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "275"}, "1092388620869271552": {"author": "@alxndrkalinin", "datetime": "2019-02-04 11:45:17", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "3,414"}, "1092398286319669248": {"author": "@summer4an", "datetime": "2019-02-04 12:23:41", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "95"}, "1128270617524858880": {"author": "@crcrpar", "datetime": "2019-05-14 12:07:32", "content_summary": "> We believe our work serves as a compelling motivation for Fixup and other techniques that aim to reduce usage of batch normalization. https://t.co/nftiFCTfoi", "followers": "1,326"}, "1092256667922771968": {"author": "@drsxr", "datetime": "2019-02-04 03:00:57", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "3,864"}, "1092100900443906048": {"author": "@watchmanralph", "datetime": "2019-02-03 16:41:59", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,446"}, "1092070223698644992": {"author": "@fabtar", "datetime": "2019-02-03 14:40:05", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "3,136"}, "1103475361734709250": {"author": "@BenSingletonNYC", "datetime": "2019-03-07 02:00:02", "content_summary": "Fixup Initialization: Residual Learning Without Normalization #MachineLearning #DataScientist https://t.co/w5qBWL7g8p", "followers": "689"}, "1159249919028449280": {"author": "@HubBucket", "datetime": "2019-08-07 23:48:13", "content_summary": "RT @BioDecoded: Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT946eP #DeepLearning https://t.co/w\u2026", "followers": "5,320"}, "1092397626643959808": {"author": "@cmndln", "datetime": "2019-02-04 12:21:04", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "21"}, "1092165184695689217": {"author": "@daisuzu", "datetime": "2019-02-03 20:57:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,244"}, "1092884680913887240": {"author": "@CherguiSafouane", "datetime": "2019-02-05 20:36:27", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "31"}, "1092990720338579458": {"author": "@shunsuke_sasaki", "datetime": "2019-02-06 03:37:49", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "287"}, "1092213255328665601": {"author": "@Cochabamba_AI", "datetime": "2019-02-04 00:08:27", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "67"}, "1100814632732770304": {"author": "@alsombra7", "datetime": "2019-02-27 17:47:15", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "111"}, "1092371676170313730": {"author": "@AssistedEvolve", "datetime": "2019-02-04 10:37:57", "content_summary": "RT @hillbig: Fixup initialization achieves similar stable training and generalization performance (w/ regularization) without batch normali\u2026", "followers": "217"}, "1092275870373371906": {"author": "@muktabh", "datetime": "2019-02-04 04:17:15", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "783"}, "1101908900780142592": {"author": "@drsxr", "datetime": "2019-03-02 18:15:29", "content_summary": "CC: @bbriniotis", "followers": "3,864"}, "1100798162455805952": {"author": "@chenlailin", "datetime": "2019-02-27 16:41:48", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "31"}, "1100606699180830720": {"author": "@mahesh21aug", "datetime": "2019-02-27 04:01:00", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "27"}, "1092097793253093378": {"author": "@jastner109", "datetime": "2019-02-03 16:29:38", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "194"}, "1090090225265590274": {"author": "@omitawo", "datetime": "2019-01-29 03:32:17", "content_summary": "RT @yu4u: \u3069\u3046\u3044\u3046\u3053\u3068\u3060\u3063\u3066\u3070\u3088 https://t.co/cinXo0W59S https://t.co/gUqN1HMKeb", "followers": "129"}, "1092410039585914880": {"author": "@diegovogeid", "datetime": "2019-02-04 13:10:24", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "69"}, "1100645985678696448": {"author": "@pavelkordik", "datetime": "2019-02-27 06:37:06", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "936"}, "1092285023967862785": {"author": "@PuneetG21271872", "datetime": "2019-02-04 04:53:38", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "44"}, "1132810399299067906": {"author": "@hasdid", "datetime": "2019-05-27 00:47:00", "content_summary": "#DeepLearning #AI #Automated | Fixup Initialization: Residual Learning Without Normalization https://t.co/BggJy0rlZ7", "followers": "3,645"}, "1092942367756185600": {"author": "@n_kats_", "datetime": "2019-02-06 00:25:41", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "143"}, "1092228244990976000": {"author": "@63556poiuytrewq", "datetime": "2019-02-04 01:08:00", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "417"}, "1093392583961784320": {"author": "@giovenko", "datetime": "2019-02-07 06:14:40", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "165"}, "1092066002446225408": {"author": "@ogrisel", "datetime": "2019-02-03 14:23:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "28,466"}, "1092264188662038528": {"author": "@hereticreader", "datetime": "2019-02-04 03:30:50", "content_summary": "Fixup Initialization: Residual Learning Without Normalization - https://t.co/9D1COPFWPY https://t.co/qV8eOUncyE", "followers": "195"}, "1093150863231602689": {"author": "@morioka", "datetime": "2019-02-06 14:14:10", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "828"}, "1092322890307891202": {"author": "@jaguring1", "datetime": "2019-02-04 07:24:06", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "12,771"}, "1100602482676379649": {"author": "@wwwAIblog", "datetime": "2019-02-27 03:44:14", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/dHccTcgFLi looks great if it works. Batch norm works well but is a huge design headache for both software & hardware, and creates the most subtle and unintuitive bugs and issues", "followers": "562"}, "1092375999751151616": {"author": "@HuzairUje", "datetime": "2019-02-04 10:55:08", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "350"}, "1100789659121127424": {"author": "@hussainmuzahid7", "datetime": "2019-02-27 16:08:01", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "7"}, "1101107320677900289": {"author": "@norbst", "datetime": "2019-02-28 13:10:17", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "38"}, "1093952916065529860": {"author": "@andrscordova", "datetime": "2019-02-08 19:21:14", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "239"}, "1092077755628507136": {"author": "@pathakraul", "datetime": "2019-02-03 15:10:01", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "142"}, "1153763053693034496": {"author": "@HubBucket", "datetime": "2019-07-23 20:25:22", "content_summary": "RT @BioDecoded: Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT946eP #DeepLearning https://t.co/w\u2026", "followers": "5,320"}, "1124007506437652487": {"author": "@adam____adam", "datetime": "2019-05-02 17:47:27", "content_summary": "Pusing sama orang pinter, teori yang keliatan keren bisa dibantah juga", "followers": "102"}, "1101272670652358656": {"author": "@PerthMLGroup", "datetime": "2019-03-01 00:07:19", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "456"}, "1100655581361303555": {"author": "@julienmichot3", "datetime": "2019-02-27 07:15:14", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "49"}, "1092291602071068673": {"author": "@sqcai", "datetime": "2019-02-04 05:19:46", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2,412"}, "1100944341974110208": {"author": "@DataSciNews", "datetime": "2019-02-28 02:22:40", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "21,339"}, "1092274099630436352": {"author": "@gshashank84", "datetime": "2019-02-04 04:10:13", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "58"}, "1092070836633186304": {"author": "@c_gunesh", "datetime": "2019-02-03 14:42:31", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "698"}, "1092088627914510336": {"author": "@jeremyphoward", "datetime": "2019-02-03 15:53:13", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "100,162"}, "1090077302334124033": {"author": "@arxiv_cscv", "datetime": "2019-01-29 02:40:56", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMorbqY", "followers": "4,105"}, "1100655976703774721": {"author": "@artuskg", "datetime": "2019-02-27 07:16:48", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "45"}, "1092315562397462528": {"author": "@harujoh", "datetime": "2019-02-04 06:54:58", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "380"}, "1100832707758776320": {"author": "@CShorten30", "datetime": "2019-02-27 18:59:04", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "4,578"}, "1092125385037398017": {"author": "@iugoaoj", "datetime": "2019-02-03 18:19:17", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "363"}, "1105644973796536320": {"author": "@arxiv_cscv", "datetime": "2019-03-13 01:41:18", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMorbqY", "followers": "4,105"}, "1090084650297450497": {"author": "@yu4u", "datetime": "2019-01-29 03:10:08", "content_summary": "\u3069\u3046\u3044\u3046\u3053\u3068\u3060\u3063\u3066\u3070\u3088 https://t.co/cinXo0W59S https://t.co/gUqN1HMKeb", "followers": "5,423"}, "1092088801072099329": {"author": "@kishoreugal", "datetime": "2019-02-03 15:53:54", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "66"}, "1092378269612163073": {"author": "@KindiBALDE", "datetime": "2019-02-04 11:04:09", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "751"}, "1110857347163811840": {"author": "@ElectronNest", "datetime": "2019-03-27 10:53:24", "content_summary": "\"Fixup Initialization: Residual Learning Without Normalization\" https://t.co/F4tmRvqbse", "followers": "228"}, "1100610304222867461": {"author": "@ceobillionaire", "datetime": "2019-02-27 04:15:19", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "163,744"}, "1092203325301256192": {"author": "@d4n1elchen", "datetime": "2019-02-03 23:28:59", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "7"}, "1094971084431151105": {"author": "@bitcurian", "datetime": "2019-02-11 14:47:04", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2"}, "1092742785986322433": {"author": "@JunehaoChing", "datetime": "2019-02-05 11:12:37", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "161"}, "1100721380717481984": {"author": "@diegovogeid", "datetime": "2019-02-27 11:36:42", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "69"}, "1133893715129356288": {"author": "@mpsampat", "datetime": "2019-05-30 00:31:43", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "48"}, "1100795508509929473": {"author": "@seaandsailor", "datetime": "2019-02-27 16:31:15", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "1,593"}, "1092176295683915776": {"author": "@RyanRod96136776", "datetime": "2019-02-03 21:41:35", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "49"}, "1092127850277285888": {"author": "@Tsingggg", "datetime": "2019-02-03 18:29:04", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "22"}, "1092327507871916032": {"author": "@Rghv_Bali", "datetime": "2019-02-04 07:42:26", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "291"}, "1092309614127972352": {"author": "@wannabe_OG", "datetime": "2019-02-04 06:31:20", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "34"}, "1092277667657703424": {"author": "@Maxwell_110", "datetime": "2019-02-04 04:24:24", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "3,190"}, "1092580602488877058": {"author": "@toshihiro_yama", "datetime": "2019-02-05 00:28:09", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "146"}, "1100649141661782016": {"author": "@Ar_Douillard", "datetime": "2019-02-27 06:49:39", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "282"}, "1092268996001333248": {"author": "@morioka", "datetime": "2019-02-04 03:49:56", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "828"}, "1092241152370126848": {"author": "@kotone_nyt", "datetime": "2019-02-04 01:59:18", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "1,380"}, "1092035857182351361": {"author": "@sara_thiru", "datetime": "2019-02-03 12:23:32", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "43"}, "1100731720243003394": {"author": "@matasramon", "datetime": "2019-02-27 12:17:47", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "159"}, "1092331172535902208": {"author": "@Mr_PO96", "datetime": "2019-02-04 07:57:00", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "58"}, "1092259192004460544": {"author": "@AkimiyaF", "datetime": "2019-02-04 03:10:59", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "222"}, "1092373492568223745": {"author": "@PerthMLGroup", "datetime": "2019-02-04 10:45:10", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "456"}, "1100807527221080064": {"author": "@future_of_AI", "datetime": "2019-02-27 17:19:01", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/KoRRgDAocU #AI #Research via @karpathy", "followers": "2,302"}, "1092226860853776384": {"author": "@shunk031", "datetime": "2019-02-04 01:02:30", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "2,050"}, "1100662751578320897": {"author": "@anshulkundaje", "datetime": "2019-02-27 07:43:43", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "7,165"}, "1092074995101052928": {"author": "@stanfordnlp", "datetime": "2019-02-03 14:59:03", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "90,847"}, "1092569536014909440": {"author": "@sappy_and_sappy", "datetime": "2019-02-04 23:44:11", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "175"}, "1092380656217116672": {"author": "@shunsuke_sasaki", "datetime": "2019-02-04 11:13:38", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "287"}, "1092255745842733056": {"author": "@gih50123", "datetime": "2019-02-04 02:57:17", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "18"}, "1092224697486340096": {"author": "@Dmiag2008", "datetime": "2019-02-04 00:53:55", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "3,339"}, "1100665406912552960": {"author": "@ElectronNest", "datetime": "2019-02-27 07:54:17", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "228"}, "1092226770659508224": {"author": "@evolvingstuff", "datetime": "2019-02-04 01:02:09", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2,930"}, "1100728885757513728": {"author": "@sinatv52", "datetime": "2019-02-27 12:06:31", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "54"}, "1092316438763790336": {"author": "@marsee101", "datetime": "2019-02-04 06:58:27", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "2,435"}, "1128907602002231296": {"author": "@Liloer2", "datetime": "2019-05-16 06:18:41", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "48"}, "1132810428378230784": {"author": "@imabit_inc", "datetime": "2019-05-27 00:47:07", "content_summary": "RT @hasdid: #DeepLearning #AI #Automated | Fixup Initialization: Residual Learning Without Normalization https://t.co/BggJy0rlZ7", "followers": "88"}, "1092294600226758661": {"author": "@EldarSilver", "datetime": "2019-02-04 05:31:41", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "1,942"}, "1105376467976781825": {"author": "@ElectronNest", "datetime": "2019-03-12 07:54:21", "content_summary": "RT @icoxfog417: \u3053\u3061\u3089\u306e\u4e3b\u5f35(\u521d\u671f\u5316\u306e\u307f\u3067\u306f\u52fe\u914d\u306e\u7206\u767a\u306b\u306f\u5bfe\u5fdc\u3067\u304d\u306a\u3044)\u3001\u4ee5\u4e0b\u3068\u30d0\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u306e\u3060\u304c\u3069\u3061\u3089\u304c\u6b63\u3057\u3044\u304b\u30fb\u30fb\u30fb(\u5f15\u7528\u304c\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u2193\u306b\u95a2\u3059\u308b\u30b3\u30e1\u30f3\u30c8\u304c\u53d6\u308c\u306a\u304b\u3063\u305f) Fixup Initialization: Residual Learning\u2026", "followers": "228"}, "1107882433637244928": {"author": "@PiotrCzapla", "datetime": "2019-03-19 05:52:10", "content_summary": "\"Everything makes more sense in excel.\" - it brings a smile to my face each time I hear @jeremyphoward saying this, especially when he is right :). @fastdotai part 2 is superb this year, it has the knowledge does not depreciate like this paper: https://t.", "followers": "844"}, "1119263338985185280": {"author": "@shunk031", "datetime": "2019-04-19 15:35:49", "content_summary": "\u3053\u3046\u3044\u3046\u9ed2\u9b54\u8853\u306f\u3042\u308a\u307e\u3059\u3088\u306d https://t.co/KSd5FoST8F", "followers": "2,050"}, "1092121410380750849": {"author": "@alexhock", "datetime": "2019-02-03 18:03:29", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "192"}, "1092053037105987587": {"author": "@vishnu_lsvsr", "datetime": "2019-02-03 13:31:48", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "67"}, "1090079210134224896": {"author": "@BrundageBot", "datetime": "2019-01-29 02:48:31", "content_summary": "Fixup Initialization: Residual Learning Without Normalization. Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma https://t.co/1KnUeTVSER", "followers": "3,890"}, "1092226483890937857": {"author": "@CSProfKGD", "datetime": "2019-02-04 01:01:01", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2,851"}, "1092228964481781761": {"author": "@linkoffate", "datetime": "2019-02-04 01:10:52", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "394"}, "1092349795576905728": {"author": "@royam0820", "datetime": "2019-02-04 09:11:00", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "232"}, "1131182932859576321": {"author": "@therealjpittman", "datetime": "2019-05-22 13:00:02", "content_summary": "[R] Fixup Initialization: Residual Learning Without Normalization (up to 10K layer networks w/o batch norm) https://t.co/sEnLU62h5O #MachineLearning", "followers": "729"}, "1092080063212793858": {"author": "@ayirpelle", "datetime": "2019-02-03 15:19:11", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2,675"}, "1092703823192240128": {"author": "@t_teruya", "datetime": "2019-02-05 08:37:47", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "578"}, "1092107076426440705": {"author": "@kastnerkyle", "datetime": "2019-02-03 17:06:32", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "3,103"}, "1092304881036263425": {"author": "@ialuronico", "datetime": "2019-02-04 06:12:32", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "996"}, "1092300702825689088": {"author": "@trinity_site", "datetime": "2019-02-04 05:55:56", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "1,823"}, "1092229319202488320": {"author": "@jinbeizame007", "datetime": "2019-02-04 01:12:16", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "2,456"}, "1092445913962897408": {"author": "@adamrocker", "datetime": "2019-02-04 15:32:57", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "6,182"}, "1092537239895187459": {"author": "@conormacd", "datetime": "2019-02-04 21:35:51", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "415"}, "1100748066028580864": {"author": "@gopalkalpande", "datetime": "2019-02-27 13:22:44", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "13"}, "1092475560775831552": {"author": "@JoaoVictor_AC", "datetime": "2019-02-04 17:30:45", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "742"}, "1100661862948061185": {"author": "@souitsch", "datetime": "2019-02-27 07:40:12", "content_summary": "RT @kaalam_ai: An alternative to batch-normalization. #DeepLearning https://t.co/L3BwICFA4C", "followers": "467"}, "1092391866463649792": {"author": "@ManbinderSingh", "datetime": "2019-02-04 11:58:11", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "721"}, "1092170441106640898": {"author": "@KlimZaporojets", "datetime": "2019-02-03 21:18:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "26"}, "1093368977080823808": {"author": "@ku21fan", "datetime": "2019-02-07 04:40:52", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "69"}, "1092189858775220224": {"author": "@udoooom", "datetime": "2019-02-03 22:35:28", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2,176"}, "1090178002866954240": {"author": "@AssistedEvolve", "datetime": "2019-01-29 09:21:05", "content_summary": "RT @BrundageBot: Fixup Initialization: Residual Learning Without Normalization. Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma https://t.co/1\u2026", "followers": "217"}, "1092235455230205954": {"author": "@imenurok", "datetime": "2019-02-04 01:36:39", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "2,043"}, "1093217475972423680": {"author": "@ElectronNest", "datetime": "2019-02-06 18:38:51", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "228"}, "1092096328908398592": {"author": "@EricSchles", "datetime": "2019-02-03 16:23:49", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "2,074"}, "1100952510913744896": {"author": "@aneomatrix", "datetime": "2019-02-28 02:55:07", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "227"}, "1092238387359039488": {"author": "@HouhouinK", "datetime": "2019-02-04 01:48:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "163"}, "1092205184736997376": {"author": "@MassBassLol", "datetime": "2019-02-03 23:36:22", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "20"}, "1092535679979061251": {"author": "@renato_umeton", "datetime": "2019-02-04 21:29:39", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "3,372"}, "1095430442969456640": {"author": "@fjfbupt", "datetime": "2019-02-12 21:12:24", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "59"}, "1092226494804324352": {"author": "@nskm_m", "datetime": "2019-02-04 01:01:03", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "190"}, "1090075673425793024": {"author": "@deep_rl", "datetime": "2019-01-29 02:34:27", "content_summary": "Fixup Initialization: Residual Learning Without Normalization - Hongyi Zhang https://t.co/QVUo6C0FEy", "followers": "858"}, "1092533507451170816": {"author": "@DataSciNews", "datetime": "2019-02-04 21:21:01", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "21,339"}, "1101144810688253952": {"author": "@johnnyprothero", "datetime": "2019-02-28 15:39:15", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "184"}, "1092128905740210176": {"author": "@TheGradient", "datetime": "2019-02-03 18:33:16", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,715"}, "1100778441907916801": {"author": "@WhiteLotusPatal", "datetime": "2019-02-27 15:23:26", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "18"}, "1092142283171024897": {"author": "@fallou__tall", "datetime": "2019-02-03 19:26:25", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "98"}, "1100607677309153280": {"author": "@octonion", "datetime": "2019-02-27 04:04:53", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "16,821"}, "1090288617212297216": {"author": "@arxiv_cscv", "datetime": "2019-01-29 16:40:37", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMorbqY", "followers": "4,105"}, "1091952255262064642": {"author": "@hiropon_matsu", "datetime": "2019-02-03 06:51:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "87"}, "1093291740017164288": {"author": "@dannyehb", "datetime": "2019-02-06 23:33:57", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "294"}, "1092079346494275584": {"author": "@ashwinids89", "datetime": "2019-02-03 15:16:20", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "166"}, "1092098422218346496": {"author": "@lievAnastazia", "datetime": "2019-02-03 16:32:08", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "635"}, "1092261363253161984": {"author": "@sathya04", "datetime": "2019-02-04 03:19:36", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "208"}, "1092092546614841345": {"author": "@cghosh_", "datetime": "2019-02-03 16:08:47", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "279"}, "1100859525329010688": {"author": "@pragmaticml", "datetime": "2019-02-27 20:45:38", "content_summary": "Hongyi Zhang, @ynd, and @tengyuma's new paper on \"FixUp\" initialization helps explain one of the architecture decisions made in the GPT-2 paper (per layer init scaling) and why layernorm works. Better resnet initializations mean no layernorm required. htt", "followers": "1,232"}, "1092236678205366272": {"author": "@hrs1985", "datetime": "2019-02-04 01:41:31", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,194"}, "1100654946771496960": {"author": "@JeanMarcJAzzi", "datetime": "2019-02-27 07:12:43", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "365"}, "1092383128876912641": {"author": "@lebigot", "datetime": "2019-02-04 11:23:28", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "506"}, "1100406058986528768": {"author": "@alisterta", "datetime": "2019-02-26 14:43:43", "content_summary": "@mohammad_d1993 This is also a good read https://t.co/h0laUDYP2l", "followers": "159"}, "1100653152481697794": {"author": "@joyqiao2016", "datetime": "2019-02-27 07:05:35", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "38"}, "1090076002678726658": {"author": "@MLandDL_papers", "datetime": "2019-01-29 02:35:46", "content_summary": "Fixup Initialization: Residual Learning Without Normalization. (arXiv:1901.09321v1 [cs.LG]) https://t.co/naqJ1XXbXI", "followers": "380"}, "1092495170199801857": {"author": "@strokelive", "datetime": "2019-02-04 18:48:40", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "6"}, "1093278911692263425": {"author": "@phorizon20", "datetime": "2019-02-06 22:42:59", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "39"}, "1101032690655014912": {"author": "@aiton5", "datetime": "2019-02-28 08:13:44", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "41"}, "1092263707898302464": {"author": "@amitabhgadosey", "datetime": "2019-02-04 03:28:55", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "25"}, "1092309892197543936": {"author": "@flute_ud", "datetime": "2019-02-04 06:32:27", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "34"}, "1092096583687114752": {"author": "@koenboeckx", "datetime": "2019-02-03 16:24:50", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "31"}, "1100621629430865920": {"author": "@A_K_Nain", "datetime": "2019-02-27 05:00:19", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "3,312"}, "1101054006627991554": {"author": "@edersantana", "datetime": "2019-02-28 09:38:26", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "5,014"}, "1132822762031124483": {"author": "@WIOMAX_PA", "datetime": "2019-05-27 01:36:07", "content_summary": "RT @hasdid: #DeepLearning #AI #Automated | Fixup Initialization: Residual Learning Without Normalization https://t.co/BggJy0rlZ7", "followers": "4,128"}, "1092316282026811393": {"author": "@erik_nijkamp", "datetime": "2019-02-04 06:57:50", "content_summary": "Fixup Initialization: Training ResNets with 10k layers without BN: https://t.co/0n6b902Rtz Finally. Stateful BN is not desirable.", "followers": "219"}, "1100602241608691712": {"author": "@pyykkis81", "datetime": "2019-02-27 03:43:17", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "260"}, "1092142254192439296": {"author": "@skavulya", "datetime": "2019-02-03 19:26:19", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "52"}, "1092076329946669057": {"author": "@SigP226", "datetime": "2019-02-03 15:04:21", "content_summary": "#stanfordnlp RT tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our paper! Arxiv link https://t.co/2npALhq6hD . Thanks to ajmooch for the tweet and re-implementation! https://t", "followers": "257"}, "1100639280395964416": {"author": "@fujikanaeda", "datetime": "2019-02-27 06:10:27", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "343"}, "1092325256922947585": {"author": "@davidmatheson", "datetime": "2019-02-04 07:33:30", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "209"}, "1092075720795480064": {"author": "@RichmanRonald", "datetime": "2019-02-03 15:01:56", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "892"}, "1092225532035444736": {"author": "@hillbig", "datetime": "2019-02-04 00:57:14", "content_summary": "Fixup initialization achieves similar stable training and generalization performance (w/ regularization) without batch normalization, by scaling the weights of a residual branch with L^{-1/(2m-2)} where L is # of blocks and m is # of layers in each block h", "followers": "18,274"}, "1092255644206264320": {"author": "@yasuokajihei", "datetime": "2019-02-04 02:56:53", "content_summary": "RT @hillbig: Fixup initialization achieves similar stable training and generalization performance (w/ regularization) without batch normali\u2026", "followers": "480"}, "1090213427858034688": {"author": "@arxivml", "datetime": "2019-01-29 11:41:51", "content_summary": "\"Fixup Initialization: Residual Learning Without Normalization\", Hongyi Zhang, Yann N\uff0e Dauphin, Tengyu Ma https://t.co/czOMRTA6Oc", "followers": "779"}, "1092183304747581442": {"author": "@dvgodoy", "datetime": "2019-02-03 22:09:26", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "201"}, "1105318747454111749": {"author": "@icoxfog417", "datetime": "2019-03-12 04:04:59", "content_summary": "\u3053\u3061\u3089\u306e\u4e3b\u5f35(\u521d\u671f\u5316\u306e\u307f\u3067\u306f\u52fe\u914d\u306e\u7206\u767a\u306b\u306f\u5bfe\u5fdc\u3067\u304d\u306a\u3044)\u3001\u4ee5\u4e0b\u3068\u30d0\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u306e\u3060\u304c\u3069\u3061\u3089\u304c\u6b63\u3057\u3044\u304b\u30fb\u30fb\u30fb(\u5f15\u7528\u304c\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u2193\u306b\u95a2\u3059\u308b\u30b3\u30e1\u30f3\u30c8\u304c\u53d6\u308c\u306a\u304b\u3063\u305f) Fixup Initialization: Residual Learning Without Normalization https://t.co/Nkfq1cXvJ3", "followers": "11,502"}, "1092102031853084674": {"author": "@SeanWan49774662", "datetime": "2019-02-03 16:46:29", "content_summary": "[1901.09321] Fixup Initialization: Residual Learning Without Normalization https://t.co/E07Whh878x", "followers": "12"}, "1101037158268309505": {"author": "@ImanisMind", "datetime": "2019-02-28 08:31:29", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "612"}, "1100795302305501185": {"author": "@iskander", "datetime": "2019-02-27 16:30:26", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "2,176"}, "1092096148414832646": {"author": "@adauphin4", "datetime": "2019-02-03 16:23:06", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "279"}, "1092080613379706880": {"author": "@PolSciDataNerd", "datetime": "2019-02-03 15:21:22", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "326"}, "1092681427471089666": {"author": "@daisuzu", "datetime": "2019-02-05 07:08:48", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "1,244"}, "1100608113508417537": {"author": "@ayirpelle", "datetime": "2019-02-27 04:06:37", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "2,675"}, "1115763877587836928": {"author": "@R5GcRBsgDszVCou", "datetime": "2019-04-09 23:50:12", "content_summary": "great", "followers": "10"}, "1092391565379751936": {"author": "@shubh_300595", "datetime": "2019-02-04 11:56:59", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "66"}, "1092363667709067264": {"author": "@HengjianJia", "datetime": "2019-02-04 10:06:08", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "54"}, "1092448014826901506": {"author": "@udmrzn", "datetime": "2019-02-04 15:41:18", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,348"}, "1092261121472454656": {"author": "@farhanhubble", "datetime": "2019-02-04 03:18:39", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "160"}, "1092242042887925762": {"author": "@Rohitpatil5", "datetime": "2019-02-04 02:02:50", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "331"}, "1094143984702763008": {"author": "@AndySugs", "datetime": "2019-02-09 08:00:28", "content_summary": "RT: DataSciNews: RT tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our paper! Arxiv link https://t.co/87XqE5jnAS . Thanks to ajmooch for the tweet and re-implementation! https", "followers": "3,314"}, "1092241321547493382": {"author": "@HaoTan5", "datetime": "2019-02-04 01:59:58", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "198"}, "1092176579298496512": {"author": "@tmarzagao", "datetime": "2019-02-03 21:42:42", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,110"}, "1092395694680244224": {"author": "@treasured_write", "datetime": "2019-02-04 12:13:23", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "101"}, "1150097390386450433": {"author": "@BioDecoded", "datetime": "2019-07-13 17:39:20", "content_summary": "Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT946eP #DeepLearning https://t.co/wbPdTzJOml", "followers": "1,140"}, "1092123364569829378": {"author": "@udayadampage", "datetime": "2019-02-03 18:11:15", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "472"}, "1092240790129123328": {"author": "@Prajakta191", "datetime": "2019-02-04 01:57:51", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "38"}, "1100975287251161088": {"author": "@udmrzn", "datetime": "2019-02-28 04:25:38", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "1,348"}, "1100909940292628480": {"author": "@MassBassLol", "datetime": "2019-02-28 00:05:58", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "20"}, "1141264656817938433": {"author": "@jeandut14000", "datetime": "2019-06-19 08:41:12", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "43"}, "1100605760461893633": {"author": "@tomsonoda", "datetime": "2019-02-27 03:57:16", "content_summary": "Fixup Initialization. Tested with 10,000 layers dnn. https://t.co/HQoSOF6qXO", "followers": "96"}, "1092255779548102661": {"author": "@cruseakshay", "datetime": "2019-02-04 02:57:25", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "227"}, "1092095850770247680": {"author": "@AymenDirac", "datetime": "2019-02-03 16:21:55", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "13"}, "1092127771633958918": {"author": "@tqchenml", "datetime": "2019-02-03 18:28:46", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "7,907"}, "1092098843519401984": {"author": "@ErmiaBivatan", "datetime": "2019-02-03 16:33:49", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "819"}, "1092077736267440128": {"author": "@Collonville", "datetime": "2019-02-03 15:09:56", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "4,599"}, "1092314200926547969": {"author": "@RomainSabathe", "datetime": "2019-02-04 06:49:34", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "89"}, "1092307651239784449": {"author": "@mducoffe", "datetime": "2019-02-04 06:23:32", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "108"}, "1092181171100966913": {"author": "@BADRINATHJAYAK1", "datetime": "2019-02-03 22:00:57", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "15"}, "1092224625952485377": {"author": "@hillbig", "datetime": "2019-02-04 00:53:38", "content_summary": "\u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBlock\u306e\u6700\u5f8c\u306e\u91cd\u307f\u306f0\u306b\u3059\u308b https://t.co/ZWSR6QeUGv", "followers": "18,274"}, "1092227799241261057": {"author": "@bamboo4031", "datetime": "2019-02-04 01:06:14", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "233"}, "1092106718018973697": {"author": "@sermakarevich", "datetime": "2019-02-03 17:05:06", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "287"}, "1092416636005449728": {"author": "@dougabug69", "datetime": "2019-02-04 13:36:36", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "30"}, "1104403498307743746": {"author": "@letranger14", "datetime": "2019-03-09 15:28:07", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "324"}, "1108069860783861765": {"author": "@RichmanRonald", "datetime": "2019-03-19 18:16:56", "content_summary": "RT @PiotrCzapla: \"Everything makes more sense in excel.\" - it brings a smile to my face each time I hear @jeremyphoward saying this, especi\u2026", "followers": "892"}, "1092241428393033729": {"author": "@Pranjal_Yadav", "datetime": "2019-02-04 02:00:24", "content_summary": "RT @aureliengeron: How to train (very) deep nets without batchnorm and still get state-of-the-art performance! \ud83d\ude0e https://t.co/NZDx9gWgpE", "followers": "298"}, "1106959806055755776": {"author": "@Pythonista1", "datetime": "2019-03-16 16:45:58", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "249"}, "1092182298122809344": {"author": "@Doradorazio0512", "datetime": "2019-02-03 22:05:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "13"}, "1090258498422824960": {"author": "@arxiv_cscv", "datetime": "2019-01-29 14:40:56", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMorbqY", "followers": "4,105"}, "1092325930167689216": {"author": "@n0mad_0", "datetime": "2019-02-04 07:36:10", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "936"}, "1092226672919760896": {"author": "@PavitSankar", "datetime": "2019-02-04 01:01:46", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "76"}, "1100849193554448386": {"author": "@bartoldson", "datetime": "2019-02-27 20:04:35", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "12"}, "1100599801228386304": {"author": "@karpathy", "datetime": "2019-02-27 03:33:35", "content_summary": "Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm works well but is a huge design headache for both software & hardware, and creates the most subtle and unintuitive bugs and issues", "followers": "271,307"}, "1100691494137200640": {"author": "@hktxt", "datetime": "2019-02-27 09:37:56", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "19"}, "1100939744358293505": {"author": "@Delfox29", "datetime": "2019-02-28 02:04:24", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "97"}, "1092273072084865024": {"author": "@saghiali051", "datetime": "2019-02-04 04:06:08", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "797"}, "1100813826373664773": {"author": "@Geeks_Sid", "datetime": "2019-02-27 17:44:02", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "84"}, "1100716845437308928": {"author": "@ZachBessinger", "datetime": "2019-02-27 11:18:40", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "162"}, "1092565078564470785": {"author": "@vikasnitr", "datetime": "2019-02-04 23:26:28", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "304"}, "1092227914073079808": {"author": "@syinari0123", "datetime": "2019-02-04 01:06:41", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,609"}, "1126284828909826049": {"author": "@rharang", "datetime": "2019-05-09 00:36:43", "content_summary": "@decodyng There's a google brain paper on initialization (https://t.co/GfdLSCsksF) -- of the do-a-forward-pass-and-rescale variety -- that cites it, and seems to suggest BN mitigates unstable gradients. Not quite a 1-1 map for 'smooths loss surface' but d", "followers": "606"}, "1100642639274139649": {"author": "@hengcherkeng", "datetime": "2019-02-27 06:23:48", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "228"}, "1105965615053922304": {"author": "@kuz44ma69", "datetime": "2019-03-13 22:55:25", "content_summary": "RT @arxiv_cscv: Fixup Initialization: Residual Learning Without Normalization https://t.co/JyZWMorbqY", "followers": "36"}, "1092226540044201985": {"author": "@_sbr1", "datetime": "2019-02-04 01:01:14", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "1,283"}, "1092227222054699008": {"author": "@y_yammt", "datetime": "2019-02-04 01:03:56", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "150"}, "1100744054105608198": {"author": "@juarelerrr", "datetime": "2019-02-27 13:06:47", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "306"}, "1092373390143381505": {"author": "@PerthMLGroup", "datetime": "2019-02-04 10:44:46", "content_summary": "RT @hillbig: Fixup initialization achieves similar stable training and generalization performance (w/ regularization) without batch normali\u2026", "followers": "456"}, "1092389631306285057": {"author": "@KSKSKSKS2", "datetime": "2019-02-04 11:49:18", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "216"}, "1092359311018229761": {"author": "@hkawaguc", "datetime": "2019-02-04 09:48:49", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "961"}, "1100609665107279872": {"author": "@NilayShri", "datetime": "2019-02-27 04:12:47", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "182"}, "1104491041405788160": {"author": "@BodyachavlQuest", "datetime": "2019-03-09 21:15:59", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "1"}, "1092107578375618561": {"author": "@ThingsReallyR", "datetime": "2019-02-03 17:08:31", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "48"}, "1104664178097250304": {"author": "@bnaik2611", "datetime": "2019-03-10 08:43:58", "content_summary": "will of Batch Normalization layers fade into obscurity?", "followers": "4"}, "1092407957730586626": {"author": "@ZilongZhong", "datetime": "2019-02-04 13:02:07", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "4"}, "1100773197727784960": {"author": "@Koundinya33", "datetime": "2019-02-27 15:02:36", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "5"}, "1100808563163844612": {"author": "@EricSchles", "datetime": "2019-02-27 17:23:08", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "2,074"}, "1092286702234660864": {"author": "@KouroshMeshgi", "datetime": "2019-02-04 05:00:18", "content_summary": "RT @Communicate_AI: stanfordnlp: RT tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help tr\u2026", "followers": "619"}, "1108001542102093825": {"author": "@Jiburiru002", "datetime": "2019-03-19 13:45:27", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "12"}, "1092403764772782080": {"author": "@ElectronNest", "datetime": "2019-02-04 12:45:28", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "228"}, "1092238103157211136": {"author": "@_arohan_", "datetime": "2019-02-04 01:47:11", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "366"}, "1092240989232623618": {"author": "@tackman", "datetime": "2019-02-04 01:58:39", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "1,554"}, "1092414910196301824": {"author": "@Roger_M_Taylor", "datetime": "2019-02-04 13:29:45", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "6,118"}, "1153725777218867200": {"author": "@Rosenchild", "datetime": "2019-07-23 17:57:15", "content_summary": "RT @BioDecoded: Fixup Initialization: Residual Learning Without Normalization | arXiv https://t.co/K9YpT946eP #DeepLearning https://t.co/w\u2026", "followers": "11,928"}, "1100622863059447808": {"author": "@russell_lliu", "datetime": "2019-02-27 05:05:13", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "65"}, "1092266202670514176": {"author": "@brandondamos", "datetime": "2019-02-04 03:38:50", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "7,572"}, "1212086607446081536": {"author": "@kumar_ucsy_mm", "datetime": "2019-12-31 19:02:21", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "86"}, "1092185152858607618": {"author": "@jonasrbati", "datetime": "2019-02-03 22:16:46", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "72"}, "1092255312344625153": {"author": "@yasuokajihei", "datetime": "2019-02-04 02:55:34", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "480"}, "1092256860399267840": {"author": "@lemeprabhu", "datetime": "2019-02-04 03:01:43", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "5"}, "1101191000637100032": {"author": "@DatasciBot", "datetime": "2019-02-28 18:42:48", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "55"}, "1092240876946972672": {"author": "@takuya_van", "datetime": "2019-02-04 01:58:12", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "54"}, "1100752608149393409": {"author": "@philtor", "datetime": "2019-02-27 13:40:47", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "622"}, "1100652394269020160": {"author": "@octadero", "datetime": "2019-02-27 07:02:34", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "12"}, "1092142209665810433": {"author": "@wvi32", "datetime": "2019-02-03 19:26:08", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "10"}, "1092304309365104640": {"author": "@Supriyuadi", "datetime": "2019-02-04 06:10:16", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "423"}, "1092915031774318593": {"author": "@Antonio_M_85", "datetime": "2019-02-05 22:37:03", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "174"}, "1092227908972761088": {"author": "@William33712308", "datetime": "2019-02-04 01:06:40", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "33"}, "1092092339592548353": {"author": "@johnnyprothero", "datetime": "2019-02-03 16:07:58", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "184"}, "1093106784900771845": {"author": "@morioka", "datetime": "2019-02-06 11:19:01", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "828"}, "1092461763868667907": {"author": "@nagachika", "datetime": "2019-02-04 16:35:56", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "1,552"}, "1092943028765876229": {"author": "@marsee101", "datetime": "2019-02-06 00:28:18", "content_summary": "RT @icoxfog417: \u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u2026", "followers": "2,435"}, "1092664594940624897": {"author": "@WisdomHistory", "datetime": "2019-02-05 06:01:54", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "8"}, "1100599894140628992": {"author": "@keurplkar", "datetime": "2019-02-27 03:33:57", "content_summary": "RT @karpathy: Fixup Initialization: Residual Learning Without Normalization https://t.co/hupA0avEfi looks great if it works. Batch norm wor\u2026", "followers": "63"}, "1092248239292571648": {"author": "@ougai_quantum", "datetime": "2019-02-04 02:27:27", "content_summary": "RT @hillbig: \u91cd\u307f\u306e\u521d\u671f\u5316\u3092\u6b21\u306e\u3088\u3046\u306b\u5de5\u592b\u3059\u308b\u3068\u3001\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u540c\u7a0b\u5ea6\u306e\u5b66\u7fd2\u5b89\u5b9a\u5316\u3001\uff08\u6b63\u5247\u5316\u3092\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\uff09\u540c\u3058\u6c4e\u5316\u6027\u80fd\u3092\u5f97\u3089\u308c\u308b\u30021) ResBlock\u304cL\u500b\u3001\u5404\u30d6\u30ed\u30c3\u30af\u304cm\u500b\u3042\u308b\u306a\u3089\u91cd\u307f\u3092L^{-1/(2m-2)}\u3067\u30b9\u30b1\u30fc\u30eb\u3059\u308b 2)ResBloc\u2026", "followers": "914"}, "1090318713327370240": {"author": "@MannyDePresso", "datetime": "2019-01-29 18:40:13", "content_summary": "[1901.09321] Fixup Initialization: Residual Learning Without Normalization https://t.co/WmsRucMAVX", "followers": "273"}, "1092941771019972608": {"author": "@icoxfog417", "datetime": "2019-02-06 00:23:18", "content_summary": "\u30d0\u30c3\u30c1\u6b63\u898f\u5316(BN)\u3092\u4f7f\u308f\u305a\u3068\u3082\u91cd\u307f\u306e\u521d\u671f\u5316\u3060\u3051\u3067\u540c\u7a0b\u5ea6\u306e\u6027\u80fd\u304c\u51fa\u305b\u308b\u3068\u3057\u305f\u7814\u7a76(\u753b\u50cf\u5206\u985e/\u7ffb\u8a33\u306e\u53cc\u65b9\u3067\u691c\u8a3c)\u3002ResNet Block\u306f2\u30eb\u30fc\u30c8\u304c\u5408\u6d41\u3059\u308b\u5f62\u3092\u3068\u308b\u305f\u3081\u57fa\u672c\u5206\u6563\u304c2\u500d\u3068\u306a\u308a\u52fe\u914d\u7206\u767a\u304c\u8d77\u3053\u308b\u3002BN\u306f\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b\u304c\u3001\u91cd\u307f\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3067\u3053\u308c\u3092\u963b\u6b62\u3059\u308b https://t.co/Nkfq1cXvJ3", "followers": "11,502"}, "1092299264053567488": {"author": "@SinghNaruto", "datetime": "2019-02-04 05:50:13", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "90"}, "1092126177387528192": {"author": "@willem_meints", "datetime": "2019-02-03 18:22:26", "content_summary": "RT @tengyuma: If you are interested in training deep models without batchnorm, or why batchnorm can help training, please check out our pap\u2026", "followers": "813"}}, "queriedAt": "2020-05-21 20:18:25", "completed": "1", "citation_id": "54634271", "tab": "twitter"}