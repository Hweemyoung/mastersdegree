{"twitter": {"1094786672657645568": {"content_summary": "Combining learning rate decay and weight decay with complexity gradient descent - Part I - Pierre H. Richemond https://t.co/5a5UucvHEB", "datetime": "2019-02-11 02:34:17", "followers": "849", "author": "@deep_rl"}, "1185286621194870786": {"content_summary": "@lnsmith613 @prfsanjeevarora Very interesting, the 3/2 exponent also comes in derivations inspired by statistical physics (shameless plug: https://t.co/rg9pAVj0M0 ) - would you have a write-up ?", "datetime": "2019-10-18 20:08:46", "followers": "912", "author": "@KloudStrife"}, "1094869056413417472": {"content_summary": "\"Combining learning rate decay and weight decay with complexity gradient descent - Part I\", Pierre H\uff0e Richemond, Yi\u2026 https://t.co/yfbMu1fm9H", "datetime": "2019-02-11 08:01:39", "followers": "758", "author": "@arxivml"}, "1094777439874678785": {"content_summary": "Combining learning rate decay and weight decay with complexity gradient descent - Part I. (arXiv:1902.02881v1 [cs.LG]) https://t.co/sV6JzAw1MV", "datetime": "2019-02-11 01:57:36", "followers": "9,595", "author": "@StatMLPapers"}, "1095106093624176641": {"content_summary": "RT @BrundageBot: Combining learning rate decay and weight decay with complexity gradient descent - Part I. Pierre H. Richemond and Yike Guo\u2026", "datetime": "2019-02-11 23:43:33", "followers": "212", "author": "@AssistedEvolve"}, "1095292353353400320": {"content_summary": "RT @BrundageBot: Combining learning rate decay and weight decay with complexity gradient descent - Part I. Pierre H. Richemond and Yike Guo\u2026", "datetime": "2019-02-12 12:03:41", "followers": "456", "author": "@PerthMLGroup"}, "1094786486967377920": {"content_summary": "Combining learning rate decay and weight decay with complexity gradient descent - Part I. Pierre H. Richemond and Yike Guo https://t.co/UaFufZTdpk", "datetime": "2019-02-11 02:33:33", "followers": "3,819", "author": "@BrundageBot"}, "1100810321747423233": {"content_summary": "Turns out under that framework, you can compute the number of local minima for (wide) Gaussian NNs and hope to perform gradient descent on that objective for generalization. My latest on ArXiv : \u2018Complexity Gradient Descent, pt.1\u2019 https://t.co/rg9pAVj0M0", "datetime": "2019-02-27 17:30:07", "followers": "912", "author": "@KloudStrife"}, "1094931661647695872": {"content_summary": "RT @StatMLPapers: Combining learning rate decay and weight decay with complexity gradient descent - Part I. (arXiv:1902.02881v1 [cs.LG]) ht\u2026", "datetime": "2019-02-11 12:10:25", "followers": "342", "author": "@jefkine"}, "1094951443306434562": {"content_summary": "Combining learning rate decay and weight decay with complexity gradient descent - Part I. (arXiv:1902.02881v1 [cs.LG]) https://t.co/WUqfxkeMdV #papers- ai #ml #feedly", "datetime": "2019-02-11 13:29:01", "followers": "42", "author": "@owltrainlab"}, "1094783283592941568": {"content_summary": "Combining learning rate decay and weight decay with complexity gradient descent - Part I. (arXiv:1902.02881v1 [cs.LG]) https://t.co/vkb7AA4u50 The role of $L^2$ regularization, in the specific case of deep neural networks rather than more traditional mach", "datetime": "2019-02-11 02:20:49", "followers": "46", "author": "@yapp1e"}}, "queriedAt": "2020-06-03 22:55:40", "tab": "twitter", "completed": "1", "citation_id": "55271462"}