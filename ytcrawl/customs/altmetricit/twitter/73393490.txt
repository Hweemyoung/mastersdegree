{"citation_id": "73393490", "queriedAt": "2020-05-09 12:42:22", "completed": "0", "twitter": {"1212082269730426880": {"followers": "301", "content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "author": "@subhobrata1", "datetime": "2019-12-31 18:45:07"}, "1211794423081779201": {"followers": "765", "content_summary": "\"Axial Attention in Multidimensional Transformers\", Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans https://t.co/BDtnAIrQ1e", "author": "@arxivml", "datetime": "2019-12-30 23:41:19"}, "1228631036310904832": {"followers": "319", "content_summary": "@theshawwn You and Gwern should try Axial Attention https://t.co/sIFcctWqft", "author": "@lucidrains", "datetime": "2020-02-15 10:44:00"}, "1211754583116132352": {"followers": "2,072", "content_summary": "RT @ak92501: Axial Attention in Multidimensional Transformers pdf: https://t.co/JjBf8yCmrP abs: https://t.co/qoGYATBec9 https://t.co/wvd3NA\u2026", "author": "@EricSchles", "datetime": "2019-12-30 21:03:00"}}, "tab": "twitter"}