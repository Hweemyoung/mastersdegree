{"citation_id": "8138665", "queriedAt": "2020-05-09 12:42:01", "completed": "0", "twitter": {"1019172023975665664": {"followers": "18", "content_summary": "Wide Residual Networks https://t.co/gwYNpDg07K", "author": "@ShinobuKinjo1", "datetime": "2018-07-17 10:48:40"}, "1169908829292597248": {"followers": "63", "content_summary": "ABST100\u30e1\u30e2\uff1a7 WideResNet\uff082016\uff09\u2192https://t.co/W3fymjXTfK\u3000\uff0cwide residual networks\uff08WRN\uff09\u306f\u6d45\u304f\u5e83\u3044ResNet", "author": "@Ronmemo1", "datetime": "2019-09-06 09:42:55"}, "1036601489584574464": {"followers": "1,157", "content_summary": "\u201c[1605.07146] Wide Residual Networks\u201d https://t.co/krXoCfsKew", "author": "@colspan", "datetime": "2018-09-03 13:07:08"}, "1060655340251951110": {"followers": "51", "content_summary": "Wide Resnet: https://t.co/wQAtSqq2nz Dropout (between convolutions vs on shortcut link in ResNet), no bottleneck block, wider means more channels (factor k up to 10), shallower (less convolutions) #OneMLPaperADay #MachineLearning", "author": "@antbrl", "datetime": "2018-11-08 22:08:33"}}, "tab": "twitter"}